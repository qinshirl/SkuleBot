## Page 1

![Page 1](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_001.png)

382EC7CB-316D-423A-BC64-619A49FD214E
Last name
Student ID number
yio
# 1: 
Do NOT turn this page until you have received the signal to start.
/lO
# 2: 
Do NOT write anything over the QR Codes on top of the pages.
./lO
#3: 
/lO
#4: 
/lO
#5: 
./lO
#6: 
./lO
/lO
#8: 
./lO
#9: 
TOTAL:
Good Luck!
Page 1 of 18
University of Toronto 
Faculty Of Applied Science & Engineering 
Applied Fundamentals of Deep Learning
Date :
Duration:
Aids Allowed:
April 29th, 2023
150 minutes
Open Book &: Non-Programmable Calculators
final-ffaSf
#268 Page 1 of 18
__/90
OVER. . .
18 pages (including this one), 
When you receive the signal to 
start, please make sure that your copy of the test is complete, fill in the 
identification section above. Answer each question directly on the test 
paper, in the space provided. Make sure to answer the questions in their 
designated place.
Write up your solutions carefully! Marks cannot be awarded for solutions 
that are not understandable by the grader, and may be deducted if you make 
false assertions. If you are giving only one part of an answer, indicate clearly 
what you are doing. Part marks might be given for incomplete solutions.
This test consists of 9 questions on 
printed on both sides of the paper.
First name (please write as legibly as possible within the boxes)
0S£[


## Page 2

![Page 2](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_002.png)

6A360C07-9E4C-43B5-A0D4-F6E3C6CF8985
Page 2 of 18
c o n t ’d . ..
Question 1. [lO ma r k s ]
Circle the best answer for each of the questions below. Do not circle more than one answer per question.
(A) 13
(B) 15
(C) 17
(D) 19
(A) Reduce the overall number of parameters
(B) Encode the position information of input tokens
(C) Attend to different aspects of the input simultaneously
(D) Reduce the computational complexity
Part (b) [1 ma r k ]
What is the primary purpose of the atten­
tion mechanism in a transformer network?
Part (c) [1 ma r k ]
In the context of RNNs, what is the pri­
mary difference between a gated recurrent 
unit (GRU) and an LSTM cell?
Part (e) [1 ma r k ]
In the context of transformers, the multi­
head self-attention mechanism helps to:
final-ffa6f
#268 Page 2 of 18
Part (a) [1 ma r k ]
How many parameters are in a fully con­
nected neural network with 2 inputs, 3 hid­
den units, and 2 outputs?
(A) LSTM uses a cell state, while GRU does not
(B) GRU has a higher computational complexity
(C) LSTM is used for sequence-to-sequence tasks, while 
GRU is used for sequence classification
(D) GRU has a higher risk of overfitting
(A) To generate new data samples that mimic the distribu­
tion of the training data
(B) To classify data samples into different categories
(G) To cluster data samples based on their similarity
(D) To learn a compressed representation of the input data
(A) To enable parallel processing of sequences
(B) To mitigate the vanishing gradient problem
(G) To reduce the number of parameters in the network
(D) To selectively focus on specific parts of the input
Part (d) [1 ma r k ]
Which of the following best describes the 
primary goal of a generative adversarial 
network (GAN)?
□
□


## Page 3

![Page 3](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_003.png)

07852A4B-8FD5-4433-8BAD-706B22FBDF6B
Page 3 of 18
OVER. ..
□
□
□ t-
(A) Mean Squared Error (MSE)
(B) Root Mean Squared Error (RMSE)
(C) Cross-Entropy Loss
(D) Cosine Similarity
(A) Node features only
(B) Edge features only
(C) Both node and edge features
(D) Neither node nor edge features
Part (f) [1 ma r k ]
In an autoencoder network, what is the pri­
mary purpose of the bottleneck layer?
Part (h) [1 ma r k ]
Which of the following types of informa­
tion can be included as features in Graph 
Neural Networks to enhance their learning 
capabilities?
Part (i) [1 ma r k ]
In the context of Convolutional Neural 
Networks (CNNs), what does the term 
’’stride” refer to?
final-ffaSf
#268 Page 3 of 18
(A) To improve training speed
(B) To obtain a compressed representation of the input data
(C) To reduce the risk of overfitting
(D) To facilitate unsupervised learning
(A) (batch_jsize, height - filter .height -I- 1, width - filter .width 
■+ 1, numJilters)
(B) (batch.size, height - filterJieight, width - filter.width, 
numTilters)
(C) (batchjsize, height -I- filterJieight - 1, width -I- fil­
ter .width - 1, numJilters)
(D) (batchjsize, height, width, numJilters)
(A) The number of filters in a convolutional layer
(B) The amount by which the filter moves during each con­
volution operation
(C) The size of the filter used in the convolutional layer
(D) The dimensionality reduction factor applied after a pool­
ing layer
Part (g) [1 ma r k ]
Which of the following functions is com­
monly used as a loss function for training 
classification models in deep learning?
Part (j) [1 ma r k ]
For a given input tensor of shape 
(batch.size, height, width, channels) and 
a filter of shape (filterJieight, filter .width, 
channels, numJilters), which of the follow­
ing correctly represents the output tensor 
shape after applying a ’’VALID” convolu­
tion in a CNN, assuming a stride of 1 and 
no padding?


## Page 4

![Page 4](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_004.png)

I
B5FEE4FE-ED76-4159-8ADA—ClEFEl431DC3
Actual
Page 4 of 18
c o n t ’d . ..
□
□
□
Part (b) [2 ma r k s ]
For which category did the model achieve the best prediction result? What percent were correctly identi­
fied?
final-ffaSf
#268 Page 4 of 18
cats 
dogs 
birds
dogs 
17 
“0“ 
22
birds 
"^0“ 
12
Predicted 
cats 
“30" 
56 
21
Part (a) [4 ma r k s ]
How are the samples distributed? Remark on each category. Is this a balanced dataset?
Question 2. [lO ma r k s ]
Provided below is a confusion matrix applied to a test dataset for a three-class model used to identify cats, 
dogs or birds.


## Page 5

![Page 5](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_005.png)

1EFC9A48-066E-4CEC-A3E7-E0607A72DBB9
■J;
Z
Page 5 of 18
OVER. ..
□
□
□
Part (c) [2 ma r k s ]
Calculate the total accuracy of the model.
Part (d) [2 ma r k s ]
Explain why a confusion matrix can be more informative than just reporting overall accuracy for a classi­
fication problem.
final-ffa6f
#268 Page 5 of 18


## Page 6

![Page 6](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_006.png)

68DF0AA8-A52C-4051-85C3-377E9FE4823B
(1)
Page 6 of 18
c o n t ’d . ..
Part (a) [7 ma r k s ]
Find the closed form expression for the gradient
final-ffa6f
#268 Page 6 of 18
Question 3. [lo ma r k s ]
Suppose we want to update the weights of a single neuron. The neuron models the relationship between 
a binary dependent variable such as a yes/no outcome. Suppose we decide that we want to update the 
weights of the model using the update rule
t 5z:
where Wj is the zth weight in the model, C. = (y — 
+ A 
is the loss function, y G IR is the model
output, t G IR is the ground truth, and 7 is the learning rate.
We can define our model as the mapping y : X —> ]R where the input x = [xi,. .., x/v]^ G -T G IR*^. More
specifically, y(x) = 
where f = w^Xj + b, the bias term b G IR, and the activation function
= 14^.


## Page 7

![Page 7](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_007.png)

D5BCABA0-48EC-4356-8DC3-89608C88F0BC
final-ffaSf
#268 Page 7 of 18
Page 7 of 18
OVER. . .
Part (b) [3 ma r k s ]
Find the closed form expression for the gradient


## Page 8

![Page 8](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_008.png)

60E1F9D0-EE8B-4786-9635-860C4AB3D65F
nn
c o n t ’d . . .
Page 8 of 18
final-ffa6f
#268 Page 8 of 18
1 lin =
2 
3
Question 4. [lO ma r k s ]
What is the expected output of the commands below? If you expect that an error would occur, write 
“ERROR” without further explanation.
• import torch.
2
3 Xl
4
Part (a) [2 ma r k s ]
Fully connected linear layer:
nn . Linear ( in_f eatures = 16> out:_f eatures = 8) 
print (lin (xl) .shape) : 
print(lin(x2).shape)
nn as 
import torch ■ ■
=; torch . randn (30 , 16) 
x2 = tor ch . randn (30 , 16 , 32 , 32)
The shapes xl.shape and x2.shape are torch.Size([30, 16]) and torch.Size([30, 16, 32, 32]), respectively.
Part (b) [2 ma r k s ]
Convolutional layer:
■ convl = nn.Conv2d(in^channels = 16, out_channels=40, kernel_si2e=8, :padding=2)
2 print ( cbnvl (x2 ) . shhpie )


## Page 9

![Page 9](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_009.png)

0E8CB5EC—B6AC-42DD-97FB-6C60F5CE0B47
final-ffa6f
#268 Page 9 of 18
2
2
Page 9 of 18
OVER. ..
□
□
□
! convt
= 2 ,
Part (d) [2 ma r k s ]
Pooling layer:
1 pool = nn.MaxPool2d(kernel_size=16, 
:: print (pool (x2) . shape)
stride=3, padding=4)
Part (c) [2 ma r k s ]
Convolutional layer:
3 conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=16,padding=O, 
stride=4)
print(conv2(x2).shape)
Part (e) [2 ma r k s ]
Convolutional transpose layer
nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride 
padding = l, output.padding = 1)
print(convt(x2).shape)


## Page 10

![Page 10](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_010.png)

1D051221-5543-4C3B-8DA2-E69B14B822F8
Page 10 of 18
c o n t ’d . ..
□FSflD
MP
□
I
final-ffa6f
#268 Page 10 of 18
hidden) 
]')■
11 i d d e n i size = hi dd en l s i z e
Part (b) [8 ma r k s ]
Describe the training process for a GRU-based text generation model, including how the data is prepared, 
how the model is trained, and the role of the loss function. Additionally, explain how the trained model 
can be used for generating new text.
s ::
3
10
:i:i.
12
13
:i4
def forward (self 
output 
output 
output
/ returii output
i - hidden_size 
nn.GRU(input_size=j_ 
bat ch_f irst =Titie)
= nn.Linear(hiddGn_size, Output_size) 
.LogSoftmax(dim=l)
hidden_sizeeutput_size); 
self);__initii()
ixi hidden):J 
hidden = j self .__ _ _ ___fk 
= ' seif . f C ( output [ 1 , -1 id; 
5 "self ./sof tmax ( output): oi 
;, hidden
Question 5. [10 ma r k s ]
Suppose you are tasked with building a deep learning model using a Gated Recurrent Unit (GRU) for the 
purpose of text generation. The model will be trained on a dataset of text to generate new sentences that 
follow the style and structure of the input text. Answer the following questions related to the GRU;
Part (a) [2 ma r k s ]
Fill in the blanks in the following Python code snippet to build a simple RNN using the PyTorch library. 
You may assume that all necessary packages have been imported and that the code has no syntax errors.
■ class TextGenerationGRU;(nn . Module) :
2 
def < __init _^ ( self .dvocab.size
3 
: super(TextGenerationGRU ,
4 :
5 ■ 
dself .h i dden_ s i ze
6 
"d-Self i.gru'-==? 1
num_layers=i
; self,, f C;
/Self.softmax - nn


## Page 11

![Page 11](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_011.png)

609C69SF-6600-4BCC-8702-993CC71BBFAD
Page 11 of 18
OVER. ..
Question 6. [10 ma r k s ]
In the context of Transformer networks, the concept of neural dictionaries plays a crucial role in un­
derstanding the attention mechanism. Answer the following questions related to neural dictionaries and 
attention in transformers:
Part (a) [6 ma r k s ]
Explain the role of Query (Q), Key (K), and Value (V) matrices in the attention mechanism of a Trans­
former network.
final-ffa6f
#268 Page 11 of 18


## Page 12

![Page 12](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_012.png)

07A9D6CA-001C-47F6-83CB-DE8D9A7F2862
Part (b)
(2)
attention((5, Tf, V) = softmax
V
Page 12 of 18
c o n t ’d . . .
□
□
□
Part (c) [2 ma r k s ]
In the Transformer architecture, the self-attention mechanism is responsible for computing the relationships 
between different input elements. Explain how this mechanism contributes to the neural dictionary aspect 
of the Transformer.
final-ffaSf
#268 Page 12 of 18
QK^
[2 ma r k s ]
Consider the computation of the scaled dot-product attention
where Q G K G and V G matrices. What is the purpose of scaling the dot-product 
using \/^ , and what is the final output shape of this operation?


## Page 13

![Page 13](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_013.png)

8864759C-C568-419A-98A3-DFBDCBE2FD26
Page 13 of 18
OVER. ..
□
Part (a) [6 ma r k s ]
Describe the general architecture of a GNN model for this task, mentioning the key components and their 
roles in the process. Include an explanation of how the GNN handles the input chemical compound graph 
data and how it generates the final prediction for the binary classification.
final-ffaSf
#268 Page 13 of 18
Question 7. [10 ma r k s ]
Gonsider a binary classification task where the goal is to predict whether a given chemical compound is 
toxic or not using Graph Neural Networks (GNNs). Ghemical compounds can be represented as graphs, 
where atoms are nodes and bonds are edges. Due to the irregular structure of chemical compounds, GNNs 
are particularly well-suited for this task.
I


## Page 14

![Page 14](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_014.png)

122BF828-16F0-45B4-82CC-270FB092CDC1
Part (b)
Page 14 of 18
c o n t ’d . ..
final-ffa6f
#258 Page 14 of 18
[4 ma r k s ]
In the context of the chemical compound classification task, it is important to consider not only local 
features around each atom but also global features that capture the overall structure of the compound. 
Propose a modification to the GNN architecture described in Part (a) that enables the model to better 
capture global information about the input compound. Discuss the benefits and potential challenges of 
incorporating this modification into the model.
S:
!]


## Page 15

![Page 15](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_015.png)

SES30
651DB436-9BC8-47C1-9572-69EEC1F5FC1D
Page 15 of 18
OVER. ..
□
Part (a) [4 ma r k s ]
Describe the architecture and main components of a GAN, including the roles of the generator and dis­
criminator networks. Explain how these two networks interact during the training process.
final-ffa6f
#268 Page 15 of 18
Question 8. [10 ma r k s ]
Consider a deep learning model that utilizes a Generative Adversarial Network (GAN) for generating 
realistic synthetic data. Answer the following questions related to the GAN model:


## Page 16

![Page 16](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_016.png)

4379B988-FC72-4D51-922E-5AC0403053F8
c o n t ’d . . .
Page 16 of 18
Part (b) [3 ma r k s ]
Explain the concept of mode collapse in the context of GANs. Discuss its implications on the quality of 
generated samples and suggest one method to mitigate the problem of mode collapse.
Part (c) [3 ma r k s ]
How do we define the loss function for the generator? How do we define the loss function for the discrimi­
nator?
final-ffa6f
#268 Page 16 of 18


## Page 17

![Page 17](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_017.png)

DB94C6D8-F3FA-41AE-97FC-DF96C523E4C8
Page 17 of 18
OVER. ..
Part (a) [3 ma r k s ]
Identify and briefly explain three common features or architectural elements present in the majority of 
the deep learning models mentioned above. Discuss their roles and how they contribute to the learning 
capabilities of these models.
final-ffaSf
#268 Page 17 of 18
Question 9. [lO ma r k s ]
Deep learning models, such as GNNs, Transformers, GANs, RNNs, LSTMs, GRUs, GNNs, and ANNs, 
share several common features and challenges during training. This question explores these commonalities 
in the context of training deep neural networks. Please answer the following questions below:


## Page 18

![Page 18](https://raw.githubusercontent.com/qinshirl/SkuleBot/shifang/images/Electrical%20and%20Computer%20Engineering/page_018.png)

522294F0-ED66-4FE2-A45B-BFCB7C037DC2
Jt'
Total Marks = 90
Page 18 of 18
En d o f Fin a l  Ex a m
□
□
□
Part (c) [4 ma r k s ]
Compare and contrast the training process of a Convolutional Neural Network (CNN) and a Transformer 
network. Discuss the main differences in terms of architecture, model type, and dataset requirements, as 
well as any similarities in the steps involved in the training process. Provide at least two differences and 
one similarity between these two types of networks.
final-ffaSf
#268 Page 18 of 18
Part (b) [3 ma r k s ]
During the training process, deep learning models often face various challenges or issues. Identify and 
briefly explain three common challenges encountered when training deep neural networks, regardless of the 
specific model architecture.

