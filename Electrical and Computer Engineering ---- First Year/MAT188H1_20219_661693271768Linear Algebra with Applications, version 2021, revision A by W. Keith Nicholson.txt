with Open Texts

Linear Algebra 
with 
Applications

W. Keith Nicholson

Linear Algebra 

with 

Applications

with Open Texts

Open Texts and Editorial

Support

Digital  access  to  our  high-quality 
texts is entirely FREE! All content is 
reviewed and updated regularly by 
subject  matter  experts,  and  the 
texts  are  adaptable;  custom  edi-
tions can be produced by the Lyryx 
editorial  team  for  those  adopting 
Lyryx  assessment.  Access  to  the 
original source ﬁles is also open to 
anyone!

Access to our in-house support team 
is  available  7  days/week  to  provide 
prompt resolution to both student and 
instructor  inquiries.  In  addition,  we 
work  one-on-one  with  instructors  to 
provide a comprehensive system, cus-
tomized  for  their  course.  This  can 
include  managing  multiple  sections, 
assistance  with  preparing  online 
examinations, help with LMS integra-
tion, and much more!

Online Assessment

Instructor Supplements

Lyryx has been developing online for-
mative  and  summative  assessment 
(homework and exams) for more than 
20  years,  and  as  with  the  textbook 
content,  questions  and  problems  are 
regularly  reviewed  and  updated.  Stu-
dents receive immediate personalized 
feedback to guide their learning. Stu-
dent  grade  reports  and  performance 
statistics  are  also  provided,  and  full 
LMS integration, including gradebook 
synchronization, is available.

Additional 
instructor  resources  are 
also freely accessible. Product depen-
dent, these include: full sets of adapt-
able  slides,  solutions  manuals,  and 
multiple  choice  question  banks  with 
an exam building tool.

Contact Lyryx Today!
info@lyryx.com

Linear Algebra with Applications
Open Edition
Version 2021 — Revision A

Be a champion of OER!
Contribute suggestions for improvements, new content, or errata:

• A new topic

• A new example

• An interesting new question

• Any other suggestions to improve the material

Contact Lyryx at info@lyryx.com with your ideas.

Creative Commons License (CC BY-NC-SA): This text, including the art and illustrations, are available under
the Creative Commons license (CC BY-NC-SA), allowing anyone to reuse, revise, remix and redistribute the
text.

To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/4.0/

Linear Algebra with Applications
Open Edition
Version 2021 — Revision A

Attribution

To redistribute all of this book in its original form, please follow the guide below:

The front matter of the text should include a “License” page that includes the following statement.

This text is Linear Algebra with Applications by W. Keith Nicholson and Lyryx Learning Inc. View the text for
free at https://lyryx.com/linear-algebra-applications/

To redistribute part of this book in its original form, please follow the guide below. Clearly indicate which content has been
redistributed

The front matter of the text should include a “License” page that includes the following statement.

This text includes the following content from Linear Algebra with Applications by W. Keith Nicholson and Lyryx
Learning Inc. View the entire text for free at https://lyryx.com/linear-algebra-applications/.

<List of content from the original text>

The following must also be included at the beginning of the applicable content. Please clearly indicate which content has
been redistributed from the Lyryx text.

This chapter is redistributed from the original Linear Algebra with Applications by W. Keith Nicholson and Lyryx
Learning Inc. View the original text for free at https://lyryx.com/linear-algebra-applications/.

To adapt and redistribute all or part of this book in its original form, please follow the guide below. Clearly indicate which
content has been adapted/redistributed and summarize the changes made.

The front matter of the text should include a “License” page that includes the following statement.

This text contains content adapted from the original Linear Algebra with Applications by W. Keith Nicholson and
Lyryx Learning Inc. View the original text for free at https://lyryx.com/linear-algebra-applications/.

<List of content and summary of changes>

The following must also be included at the beginning of the applicable content. Please clearly indicate which content has
been adapted from the Lyryx text.

This chapter was adapted from the original Linear Algebra with Applications by W. Keith Nicholson and Lyryx
Learning Inc. View the original text for free at https://lyryx.com/linear-algebra-applications/.

Citation

Use the information below to create a citation:

Author: W. Keith Nicholson

Publisher: Lyryx Learning Inc.

Book title: Linear Algebra with Applications

Book version: 2021A

Publication date: December 15, 2020

Location: Calgary, Alberta, Canada

Book URL: https://lyryx.com/linear-algebra-applications

For questions or comments please contact editorial@lyryx.com

Linear Algebra with Applications
Open Edition

Base Text Revision History
Current Revision: Version 2021 — Revision A

2021 A

• Front matter has been updated including cover, Lyryx with Open Texts, copyright, and revision pages.

Attribution page has been added.

• Typo and other minor ﬁxes have been implemented throughout.

2019 A

• New Section on Singular Value Decomposition (8.6) is included.

• New Example 2.3.2 and Theorem 2.2.4. Please note that this will impact the numbering of subsequent

examples and theorems in the relevant sections.

• Section 2.2 is renamed as Matrix-Vector Multiplication.

• Minor revisions made throughout, including ﬁxing typos, adding exercises, expanding explanations,

and other small edits.

2018 B

• Images have been converted to LaTeX throughout.

• Text has been converted to LaTeX with minor ﬁxes throughout. Page numbers will differ from 2018A

revision. Full index has been implemented.

2018 A

• Text has been released with a Creative Commons license.

Table of Contents

iii

vii

ix

1
1
.
.
9
. 20
. 27
. 29
. 31
. 32

35
. 35
. 47
. 64
. 79
. 94
. 103
. 117
. 127
. 133
. 141

143
. 143
. 156
. 169
. 190
. 196
. 202
. 206

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

Table of Contents

Foreward

Preface

1 Systems of Linear Equations

.

. .
1.1 Solutions and Elementary Operations
. .
.
. .
1.2 Gaussian Elimination .
. .
. .
.
1.3 Homogeneous Equations . .
. .
. .
.
1.4 An Application to Network Flow .
1.5 An Application to Electrical Networks . .
1.6 An Application to Chemical Reactions . .
. .
Supplementary Exercises for Chapter 1 .

.
.
.

.
.

.

.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

2 Matrix Algebra

2.1 Matrix Addition, Scalar Multiplication, and Transposition .
.
.
2.2 Matrix-Vector Multiplication .
.
.
.
.
2.3 Matrix Multiplication .
.
.
.
.
.
. .
2.4 Matrix Inverses .
.
.
.
2.5 Elementary Matrices
.
.
.
.
.
2.6 Linear Transformations .
.
.
.
.
.
2.7 LU-Factorization . .
.
2.8 An Application to Input-Output Economic Models
.
.
2.9 An Application to Markov Chains
.
.
Supplementary Exercises for Chapter 2 .

. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

.
.
.
.
.
.
.
.
.

. .
. .

. .
. .

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.

.
.

.
.

.
.

3 Determinants and Diagonalization
.
. .
. .
.
3.1 The Cofactor Expansion . .
.
. .
3.2 Determinants and Matrix Inverses
.
3.3 Diagonalization and Eigenvalues .
. .
.
3.4 An Application to Linear Recurrences . .
3.5 An Application to Systems of Differential Equations
.
3.6 Proof of the Cofactor Expansion Theorem .
.
.
Supplementary Exercises for Chapter 3 .

. .
. .
. .
. .

. .
. .

.
.
.
.

.
.
.
.

. .

.
.
.

.
.
.

.
.

.

.

. .
. .
. .
. .
.
. .
. .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

iii

iv

Table of Contents

4 Vector Geometry

.

.
.

. .
4.1 Vectors and Lines . .
. .
4.2 Projections and Planes
. .
4.3 More on the Cross Product
4.4 Linear Operators on R3 .
. .
4.5 An Application to Computer Graphics . .
. .
Supplementary Exercises for Chapter 4 .

. .
. .
. .
. .

. .
. .
.
. .

.
.
.
.

.
.
.
.

.
.
.
.

.

.

.
.
.
.
.
.

. .
. .
. .
. .
. .
. .

5 Vector Space Rn

.

.
Independence and Dimension .
.
.

. .
. .
. .
5.1 Subspaces and Spanning . .
. .
. .
. .
5.2
. .
. .
. .
5.3 Orthogonality .
. .
. .
. .
. .
5.4 Rank of a Matrix . .
. .
5.5 Similarity and Diagonalization . .
. .
5.6 Best Approximation and Least Squares . .
. .
5.7 An Application to Correlation and Variance . .
. .
Supplementary Exercises for Chapter 5 .

. .
. .

.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .

.
.

.
.

.

.

.

6 Vector Spaces

.
.

. .
.
6.1 Examples and Basic Properties . .
. .
6.2 Subspaces and Spanning Sets .
.
. .
. .
6.3 Linear Independence and Dimension .
. .
.
6.4 Finite Dimensional Spaces .
. .
6.5 An Application to Polynomials . .
. .
.
6.6 An Application to Differential Equations .
. .
Supplementary Exercises for Chapter 6 .

.
.

.

.

.

.
.
.
.
.
.
.

. .
. .
. .
. .
. .
. .
. .

7 Linear Transformations

7.1 Examples and Elementary Properties .
. .
7.2 Kernel and Image of a Linear Transformation .
. .
. .
Isomorphisms and Composition .
7.3
. .
7.4 A Theorem about Differential Equations .
. .
. .
7.5 More on Linear Recurrences .

. .

. .

.
.
.

.

.

.

.

.

8 Orthogonality

8.1 Orthogonal Complements and Projections .
.
8.2 Orthogonal Diagonalization .
.
8.3 Positive Deﬁnite Matrices . .
.
. .
8.4 QR-Factorization . .
.
.
. .
8.5 Computing Eigenvalues .

. .
. .
. .
. .

. .
. .
. .
. .

.
.
.
.

.
.
.
.

.
.
.
.

.

. .
. .
. .
. .
. .

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

. .
. .
. .
. .
. .

207
. 207
. 224
. 242
. 248
. 255
. 258

261
. 261
. 269
. 280
. 288
. 296
. 308
. 320
. 325

327
. 327
. 336
. 343
. 352
. 360
. 365
. 370

371
. 371
. 378
. 388
. 398
. 401

409
. 409
. 418
. 427
. 431
. 435

.

.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

. .
.
. .

. .
. .
. .

. .
. .
. .

8.6 The Singular Value Decomposition .

.
Singular Value Decompositions
.
Fundamental Subspaces

. .
. .
8.6.1
8.6.2
. .
8.6.3 The Polar Decomposition of a Real Square Matrix . .
. .
8.6.4 The Pseudoinverse of a Matrix . .
.
. .
. .
8.7 Complex Matrices . .
.
.
. .
8.8 An Application to Linear Codes over Finite Fields .
. .
.
8.9 An Application to Quadratic Forms
. .
8.10 An Application to Constrained Optimization . .
. .
.
8.11 An Application to Statistical Principal Component Analysis . .

. .
. .
. .
. .
. .

. .
. .

.
.
.
.
.

.
.
.
.
.

. .

. .

. .

.
.

.
.

.
.

.

.

.

.

.

.

9 Change of Basis

.
.
.

.
.
.
.
.

.
.

.
.
.

.
.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .
. .

. .
. .

.
.
.

.
.
.
.
.

.
.

.
.
.

.
.
.
.
.

.
.

. .
. .
. .

. .
. .
. .
. .
. .

. .
. .

9.1 The Matrix of a Linear Transformation . .
. .
9.2 Operators and Similarity . .
. .
9.3

.
Invariant Subspaces and Direct Sums .

. .

.

.

.
.
.

. .
. .
. .

10 Inner Product Spaces

.
10.1 Inner Products and Norms
.
.
10.2 Orthogonal Sets of Vectors .
.
10.3 Orthogonal Diagonalization .
10.4 Isometries .
.
. .
10.5 An Application to Fourier Approximation .

. .
. .
. .
. .

. .
. .
. .
. .

.
.
.
.

.
.
.
.

.
.
.
.

. .

. .

.

.

.

. .
. .
. .
. .
. .

11 Canonical Forms

. .
11.1 Block Triangular Form .
11.2 The Jordan Canonical Form .

.
.

. .
. .

.
.

.
.

. .
. .

.
.

. .
. .

A Complex Numbers

B Proofs

C Mathematical Induction

D Polynomials

Selected Exercise Answers

Index

.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .

. .
. .
. .
. .
. .

. .
. .

.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.

.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .

. .
. .
. .
. .
. .

. .
. .

.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .

. .
. .
. .
. .
. .

. .
. .

.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.

.
.
.
.
.
.
.
.
.
.

.
.
.

.
.
.
.
.

.
.

. .
. .
. .
. .
. .
. .
. .
. .
. .
. .

. .
. .
. .

. .
. .
. .
. .
. .

. .
. .

v

. 439
. 439
. 445
. 448
. 450
. 454
. 465
. 479
. 489
. 492

495
. 495
. 504
. 514

529
. 529
. 538
. 548
. 555
. 568

573
. 573
. 581

587

601

607

613

617

653

Foreward

Mathematics education at the beginning university level is closely tied to the traditional publishers. In my
opinion, it gives them too much control of both cost and content. The main goal of most publishers is
proﬁt, and the result has been a sales-driven business model as opposed to a pedagogical one. This results
in frequent new “editions” of textbooks motivated largely to reduce the sale of used books rather than to
update content quality. It also introduces copyright restrictions which stiﬂe the creation and use of new
pedagogical methods and materials. The overall result is high cost textbooks which may not meet the
evolving educational needs of instructors and students.

To be fair, publishers do try to produce material that reﬂects new trends. But their goal is to sell books
and not necessarily to create tools for student success in mathematics education. Sadly, this has led to
a model where the primary choice for adapting to (or initiating) curriculum change is to ﬁnd a different
commercial textbook. My editor once said that the text that is adopted is often everyone’s third choice.

Of course instructors can produce their own lecture notes, and have done so for years, but this remains
an onerous task. The publishing industry arose from the need to provide authors with copy-editing, edi-
torial, and marketing services, as well as extensive reviews of prospective customers to ascertain market
trends and content updates. These are necessary skills and services that the industry continues to offer.

Authors of open educational resources (OER) including (but not limited to) textbooks and lecture
notes, cannot afford this on their own. But they do have two great advantages: The cost to students is
signiﬁcantly lower, and open licenses return content control to instructors. Through editable ﬁle formats
and open licenses, OER can be developed, maintained, reviewed, edited, and improved by a variety of
contributors. Instructors can now respond to curriculum change by revising and reordering material to
create content that meets the needs of their students. While editorial and quality control remain daunting
tasks, great strides have been made in addressing the issues of accessibility, affordability and adaptability
of the material.

For the above reasons I have decided to release my text under an open license, even though it was

published for many years through a traditional publisher.

Supporting students and instructors in a typical classroom requires much more than a textbook. Thus,
while anyone is welcome to use and adapt my text at no cost, I also decided to work closely with Lyryx
Learning. With colleagues at the University of Calgary, I helped create Lyryx almost 20 years ago. The
original idea was to develop quality online assessment (with feedback) well beyond the multiple-choice
style then available. Now Lyryx also works to provide and sustain open textbooks; working with authors,
contributors, and reviewers to ensure instructors need not sacriﬁce quality and rigour when switching to
an open text.

I believe this is the right direction for mathematical publishing going forward, and look forward to

being a part of how this new approach develops.

W. Keith Nicholson, Author
University of Calgary

vii

Preface

This textbook is an introduction to the ideas and techniques of linear algebra for ﬁrst- or second-year
students with a working knowledge of high school algebra. The contents have enough ﬂexibility to present
a traditional introduction to the subject, or to allow for a more applied course. Chapters 1–4 contain a one-
semester course for beginners whereas Chapters 5–9 contain a second semester course (see the Suggested
Course Outlines below). The text is primarily about real linear algebra with complex numbers being
mentioned when appropriate (reviewed in Appendix A). Overall, the aim of the text is to achieve a balance
among computational skills, theory, and applications of linear algebra. Calculus is not a prerequisite;
places where it is mentioned may be omitted.

As a rule, students of linear algebra learn by studying examples and solving problems. Accordingly,
the book contains a variety of exercises (over 1200, many with multiple parts), ordered as to their difﬁculty.
In addition, more than 375 solved examples are included in the text, many of which are computational in
nature. The examples are also used to motivate (and illustrate) concepts and theorems, carrying the student
from concrete to abstract. While the treatment is rigorous, proofs are presented at a level appropriate to
the student and may be omitted with no loss of continuity. As a result, the book can be used to give a
course that emphasizes computation and examples, or to give a more theoretical treatment (some longer
proofs are deferred to the end of the Section).

Linear Algebra has application to the natural sciences, engineering, management, and the social sci-
ences as well as mathematics. Consequently, 18 optional “applications” sections are included in the text
introducing topics as diverse as electrical networks, economic models, Markov chains, linear recurrences,
systems of differential equations, and linear codes over ﬁnite ﬁelds. Additionally some applications (for
example linear dynamical systems, and directed graphs) are introduced in context. The applications sec-
tions appear at the end of the relevant chapters to encourage students to browse.

SUGGESTED COURSE OUTLINES

This text includes the basis for a two-semester course in linear algebra.

• Chapters 1–4 provide a standard one-semester course of 35 lectures, including linear equations, ma-
trix algebra, determinants, diagonalization, and geometric vectors, with applications as time permits.
At Calgary, we cover Sections 1.1–1.3, 2.1–2.6, 3.1–3.3, and 4.1–4.4 and the course is taken by all
science and engineering students in their ﬁrst semester. Prerequisites include a working knowledge
of high school algebra (algebraic manipulations and some familiarity with polynomials); calculus is
not required.

• Chapters 5–9 contain a second semester course including Rn, abstract vector spaces, linear trans-
formations (and their matrices), orthogonality, complex matrices (up to the spectral theorem) and
applications. There is more material here than can be covered in one semester, and at Calgary we
cover Sections 5.1–5.5, 6.1–6.4, 7.1–7.3, 8.1–8.7, and 9.1–9.3 with a couple of applications as time
permits.

• Chapter 5 is a “bridging” chapter that introduces concepts like spanning, independence, and basis
in the concrete setting of Rn, before venturing into the abstract in Chapter 6. The duplication is

ix

x

Preface

balanced by the value of reviewing these notions, and it enables the student to focus in Chapter 6
on the new idea of an abstract system. Moreover, Chapter 5 completes the discussion of rank and
diagonalization from earlier chapters, and includes a brief introduction to orthogonality in Rn, which
creates the possibility of a one-semester, matrix-oriented course covering Chapter 1–5 for students
not wanting to study the abstract theory.

CHAPTER DEPENDENCIES

The following chart suggests how the material introduced in each chapter draws on concepts covered in
certain earlier chapters. A solid arrow means that ready assimilation of ideas and techniques presented
in the later chapter depends on familiarity with the earlier chapter. A broken arrow indicates that some
reference to the earlier chapter is made but the chapter need not be covered.

Chapter 1: Systems of Linear Equations

Chapter 2: Matrix Algebra

Chapter 3: Determinants and Diagonalization

Chapter 4: Vector Geometry

Chapter 5: The Vector Space Rn

Chapter 6: Vector Spaces

Chapter 7: Linear Transformations

Chapter 8: Orthogonality

Chapter 9: Change of Basis

Chapter 10: Inner Product Spaces

Chapter 11: Canonical Forms

HIGHLIGHTS OF THE TEXT

• Two-stage deﬁnition of matrix multiplication. First, in Section 2.2 matrix-vector products are
introduced naturally by viewing the left side of a system of linear equations as a product. Second,
matrix-matrix products are deﬁned in Section 2.3 by taking the columns of a product AB to be A
times the corresponding columns of B. This is motivated by viewing the matrix product as compo-
sition of maps (see next item). This works well pedagogically and the usual dot-product deﬁnition
follows easily. As a bonus, the proof of associativity of matrix multiplication now takes four lines.

• Matrices as transformations. Matrix-column multiplications are viewed (in Section 2.2) as trans-
Rm. These maps are then used to describe simple geometric reﬂections and rota-

formations Rn
tions in R2 as well as systems of linear equations.

→

• Early linear transformations. It has been said that vector spaces exist so that linear transformations
can act on them—consequently these maps are a recurring theme in the text. Motivated by the matrix
transformations introduced earlier, linear transformations Rn
Rm are deﬁned in Section 2.6, their
standard matrices are derived, and they are then used to describe rotations, reﬂections, projections,
and other operators on R2.

→

xi

• Early diagonalization. As requested by engineers and scientists, this important technique is pre-
sented in the ﬁrst term using only determinants and matrix inverses (before deﬁning independence
and dimension). Applications to population growth and linear recurrences are given.

• Early dynamical systems. These are introduced in Chapter 3, and lead (via diagonalization) to
applications like the possible extinction of species. Beginning students in science and engineering
can relate to this because they can see (often for the ﬁrst time) the relevance of the subject to the real
world.

• Bridging chapter. Chapter 5 lets students deal with tough concepts (like independence, spanning,
and basis) in the concrete setting of Rn before having to cope with abstract vector spaces in Chap-
ter 6.

• Examples. The text contains over 375 worked examples, which present the main techniques of the

subject, illustrate the central ideas, and are keyed to the exercises in each section.

• Exercises. The text contains a variety of exercises (nearly 1175, many with multiple parts), starting
with computational problems and gradually progressing to more theoretical exercises. Select solu-
tions are available at the end of the book or in the Student Solution Manual. There is a complete
Solution Manual is available for instructors.

• Applications. There are optional applications at the end of most chapters (see the list below).
While some are presented in the course of the text, most appear at the end of the relevant chapter to
encourage students to browse.

• Appendices. Because complex numbers are needed in the text, they are described in Appendix A,
which includes the polar form and roots of unity. Methods of proofs are discussed in Appendix B,
followed by mathematical induction in Appendix C. A brief discussion of polynomials is included
in Appendix D. All these topics are presented at the high-school level.

• Self-Study. This text is self-contained and therefore is suitable for self-study.

• Rigour. Proofs are presented as clearly as possible (some at the end of the section), but they are
optional and the instructor can choose how much he or she wants to prove. However the proofs are
there, so this text is more rigorous than most. Linear algebra provides one of the better venues where
students begin to think logically and argue concisely. To this end, there are exercises that ask the
student to “show” some simple implication, and others that ask her or him to either prove a given
statement or give a counterexample. I personally present a few proofs in the ﬁrst semester course
and more in the second (see the Suggested Course Outlines).

• Major Theorems. Several major results are presented in the book. Examples: Uniqueness of the
reduced row-echelon form; the cofactor expansion for determinants; the Cayley-Hamilton theorem;
the Jordan canonical form; Schur’s theorem on block triangular form; the principal axes and spectral
theorems; and others. Proofs are included because the stronger students should at least be aware of
what is involved.

xii

Preface

CHAPTER SUMMARIES

Chapter 1: Systems of Linear Equations.

A standard treatment of gaussian elimination is given. The rank of a matrix is introduced via the row-
echelon form, and solutions to a homogeneous system are presented as linear combinations of basic solu-
tions. Applications to network ﬂows, electrical networks, and chemical reactions are provided.

Chapter 2: Matrix Algebra.

After a traditional look at matrix addition, scalar multiplication, and transposition in Section 2.1, matrix-
vector multiplication is introduced in Section 2.2 by viewing the left side of a system of linear equations
as the product Ax of the coefﬁcient matrix A with the column x of variables. The usual dot-product
deﬁnition of a matrix-vector multiplication follows. Section 2.2 ends by viewing an m
n matrix A as a
transformation Rn
R2 by describing reﬂection in the x axis, rotation of
R2 through π

Rm. This is illustrated for R2

→

→

×

2 , shears, and so on.

In Section 2.3, the product of matrices A and B is deﬁned by AB =

, where
the bi are the columns of B. A routine computation shows that this is the matrix of the transformation B
followed by A. This observation is used frequently throughout the book, and leads to simple, conceptual
proofs of the basic axioms of matrix algebra. Note that linearity is not required—all that is needed is some
basic properties of matrix-vector multiplication developed in Section 2.2. Thus the usual arcane deﬁnition
of matrix multiplication is split into two well motivated parts, each an important aspect of matrix algebra.
Of course, this has the pedagogical advantage that the conceptual power of geometry can be invoked to
illuminate and clarify algebraic techniques and deﬁnitions.

Ab1 Ab2

Abn

· · ·

(cid:2)

(cid:3)

In Section 2.4 and 2.5 matrix inverses are characterized, their geometrical meaning is explored, and
block multiplication is introduced, emphasizing those cases needed later in the book. Elementary ma-
trices are discussed, and the Smith normal form is derived. Then in Section 2.6, linear transformations
Rn
Rm are deﬁned and shown to be matrix transformations. The matrices of reﬂections, rotations, and
projections in the plane are determined. Finally, matrix multiplication is related to directed graphs, matrix
LU-factorization is introduced, and applications to economic models and Markov chains are presented.

→

Chapter 3: Determinants and Diagonalization.

The cofactor expansion is stated (proved by induction later) and used to deﬁne determinants inductively
and to deduce the basic rules. The product and adjugate theorems are proved. Then the diagonalization
algorithm is presented (motivated by an example about the possible extinction of a species of birds). As
requested by our Engineering Faculty, this is done earlier than in most texts because it requires only deter-
minants and matrix inverses, avoiding any need for subspaces, independence and dimension. Eigenvectors
2 matrix A are described geometrically (using the A-invariance of lines through the origin). Di-
of a 2
agonalization is then used to study discrete linear dynamical systems and to discuss applications to linear
recurrences and systems of differential equations. A brief discussion of Google PageRank is included.

×

xiii

Chapter 4: Vector Geometry.

Vectors are presented intrinsically in terms of length and direction, and are related to matrices via coordi-
nates. Then vector operations are deﬁned using matrices and shown to be the same as the corresponding
intrinsic deﬁnitions. Next, dot products and projections are introduced to solve problems about lines and
planes. This leads to the cross product. Then matrix transformations are introduced in R3, matrices of pro-
jections and reﬂections are derived, and areas and volumes are computed using determinants. The chapter
closes with an application to computer graphics.

Chapter 5: The Vector Space Rn.
Subspaces, spanning, independence, and dimensions are introduced in the context of Rn in the ﬁrst two
sections. Orthogonal bases are introduced and used to derive the expansion theorem. The basic properties
of rank are presented and used to justify the deﬁnition given in Section 1.2. Then, after a rigorous study of
diagonalization, best approximation and least squares are discussed. The chapter closes with an application
to correlation and variance.

This is a “bridging” chapter, easing the transition to abstract spaces. Concern about duplication with
Chapter 6 is mitigated by the fact that this is the most difﬁcult part of the course and many students
welcome a repeat discussion of concepts like independence and spanning, albeit in the abstract setting.
In a different direction, Chapter 1–5 could serve as a solid introduction to linear algebra for students not
requiring abstract theory.

Chapter 6: Vector Spaces.

Building on the work on Rn in Chapter 5, the basic theory of abstract ﬁnite dimensional vector spaces is
developed emphasizing new examples like matrices, polynomials and functions. This is the ﬁrst acquain-
tance most students have had with an abstract system, so not having to deal with spanning, independence
and dimension in the general context eases the transition to abstract thinking. Applications to polynomials
and to differential equations are included.

Chapter 7: Linear Transformations.

General linear transformations are introduced, motivated by many examples from geometry, matrix theory,
and calculus. Then kernels and images are deﬁned, the dimension theorem is proved, and isomorphisms
are discussed. The chapter ends with an application to linear recurrences. A proof is included that the
order of a differential equation (with constant coefﬁcients) equals the dimension of the space of solutions.

Chapter 8: Orthogonality.

The study of orthogonality in Rn, begun in Chapter 5, is continued. Orthogonal complements and pro-
jections are deﬁned and used to study orthogonal diagonalization. This leads to the principal axes theo-
rem, the Cholesky factorization of a positive deﬁnite matrix, QR-factorization, and to a discussion of the
singular value decomposition, the polar form, and the pseudoinverse. The theory is extended to Cn in
Section 8.7 where hermitian and unitary matrices are discussed, culminating in Schur’s theorem and the
spectral theorem. A short proof of the Cayley-Hamilton theorem is also presented. In Section 8.8 the ﬁeld
Zp of integers modulo p is constructed informally for any prime p, and codes are discussed over any ﬁnite
ﬁeld. The chapter concludes with applications to quadratic forms, constrained optimization, and statistical
principal component analysis.

xiv

Preface

Chapter 9: Change of Basis.

The matrix of general linear transformation is deﬁned and studied. In the case of an operator, the rela-
tionship between basis changes and similarity is revealed. This is illustrated by computing the matrix of a
rotation about a line through the origin in R3. Finally, invariant subspaces and direct sums are introduced,
related to similarity, and (as an example) used to show that every involution is similar to a diagonal matrix
with diagonal entries

1.

±

Chapter 10: Inner Product Spaces.

General inner products are introduced and distance, norms, and the Cauchy-Schwarz inequality are dis-
cussed. The Gram-Schmidt algorithm is presented, projections are deﬁned and the approximation theorem
is proved (with an application to Fourier approximation). Finally, isometries are characterized, and dis-
tance preserving operators are shown to be composites of a translations and isometries.
Chapter 11: Canonical Forms.

The work in Chapter 9 is continued. Invariant subspaces and direct sums are used to derive the block
triangular form. That, in turn, is used to give a compact proof of the Jordan canonical form. Of course the
level is higher.

Appendices

In Appendix A, complex arithmetic is developed far enough to ﬁnd nth roots. In Appendix B, methods of
proof are discussed, while Appendix C presents mathematical induction. Finally, Appendix D describes
the properties of polynomials in elementary terms.

LIST OF APPLICATIONS

• Network Flow (Section 1.4)

• Electrical Networks (Section 1.5)

• Chemical Reactions (Section 1.6)

• Directed Graphs (in Section 2.3)

• Input-Output Economic Models (Section 2.8)

• Markov Chains (Section 2.9)

• Polynomial Interpolation (in Section 3.2)

• Population Growth (Examples 3.3.1 and 3.3.12, Section 3.3)

• Google PageRank (in Section 3.3)

• Linear Recurrences (Section 3.4; see also Section 7.5)

• Systems of Differential Equations (Section 3.5)

• Computer Graphics (Section 4.5)

xv

• Least Squares Approximation (in Section 5.6)

• Correlation and Variance (Section 5.7)

• Polynomials (Section 6.5)

• Differential Equations (Section 6.6)

• Linear Recurrences (Section 7.5)

• Error Correcting Codes (Section 8.8)

• Quadratic Forms (Section 8.9)

• Constrained Optimization (Section 8.10)

• Statistical Principal Component Analysis (Section 8.11)

• Fourier Approximation (Section 10.5)

ACKNOWLEDGMENTS

Many colleagues have contributed to the development of this text over many years of publication, and I
specially thank the following instructors for their reviews of the 7th edition:

Robert Andre

University of Waterloo

Dietrich Burbulla

University of Toronto

Dzung M. Ha

Ryerson University

Mark Solomonovich
Grant MacEwan

Fred Szabo

Concordia University

Edward Wang

Wilfred Laurier

Petr Zizler

Mount Royal University

It is also a pleasure to recognize the contributions of several people. Discussions with Thi Dinh and
Jean Springer have been invaluable and many of their suggestions have been incorporated. Thanks are
also due to Kristine Bauer and Clifton Cunningham for several conversations about the new way to look
at matrix multiplication. I also wish to extend my thanks to Joanne Canape for being there when I had
technical questions. Thanks also go to Jason Nicholson for his help in various aspects of the book, partic-
ularly the Solutions Manual. Finally, I want to thank my wife Kathleen, without whose understanding and
cooperation, this book would not exist.

As we undertake this new publishing model with the text as an open educational resource, I would also

like to thank my previous publisher. The team who supported my text greatly contributed to its success.

xvi

Preface

Now that the text has an open license, we have a much more ﬂuid and powerful mechanism to incorpo-
rate comments and suggestions. The editorial group at Lyryx invites instructors and students to contribute
to the text, and also offers to provide adaptations of the material for speciﬁc courses. Moreover the LaTeX
source ﬁles are available to anyone wishing to do the adaptation and editorial work themselves!

W. Keith Nicholson

University of Calgary

Chapter 1

Systems of Linear Equations

1.1 Solutions and Elementary Operations

Practical problems in many ﬁelds of study—such as biology, business, chemistry, computer science, eco-
nomics, electronics, engineering, physics and the social sciences—can often be reduced to solving a sys-
tem of linear equations. Linear algebra arose from attempts to ﬁnd systematic methods for solving these
systems, so it is natural to begin this book by studying linear equations.
If a, b, and c are real numbers, the graph of an equation of the form

ax + by = c

is a straight line (if a and b are not both zero), so such an equation is called a linear equation in the
variables x and y. However, it is often convenient to write the variables as x1, x2, . . . , xn, particularly
when more than two variables are involved. An equation of the form

a1x1 + a2x2 +

+ anxn = b

· · ·

is called a linear equation in the n variables x1, x2, . . . , xn. Here a1, a2, . . . , an denote real numbers
(called the coefﬁcients of x1, x2, . . . , xn, respectively) and b is also a number (called the constant term
of the equation). A ﬁnite collection of linear equations in the variables x1, x2, . . . , xn is called a system of
linear equations in these variables. Hence,

2x1

−

3x2 + 5x3 = 7

is a linear equation; the coefﬁcients of x1, x2, and x3 are 2,
each variable in a linear equation occurs to the ﬁrst power only.

−

3, and 5, and the constant term is 7. Note that

Given a linear equation a1x1 + a2x2 +

a solution to the equation if

· · ·

+ anxn = b, a sequence s1, s2, . . . , sn of n numbers is called

· · ·
that is, if the equation is satisﬁed when the substitutions x1 = s1, x2 = s2, . . . , xn = sn are made. A
sequence of numbers is called a solution to a system of equations if it is a solution to every equation in
the system.

a1s1 + a2s2 +

+ ansn = b

For example, x =

2, y = 5, z = 0 and x = 0, y = 4, z =

−

1 are both solutions to the system

−

x + y + z = 3
2x + y + 3z = 1

A system may have no solution at all, or it may have a unique solution, or it may have an inﬁnite family of
solutions. For instance, the system x + y = 2, x + y = 3 has no solution because the sum of two numbers
cannot be 2 and 3 simultaneously. A system that has no solution is called inconsistent; a system with at
least one solution is called consistent. The system in the following example has inﬁnitely many solutions.

1

2

Systems of Linear Equations

Example 1.1.1

Show that, for arbitrary values of s and t,

is a solution to the system

x1
2x1

−
−

s + 1
x1 = t
−
x2 = t + s + 2
x3 = s
x4 = t

2x2 +3x3 +x4 =
x2 +3x3

3
−
x4 = 0

−

Solution. Simply substitute these values of x1, x2, x3, and x4 in each equation.

x1
−
2x1
−

2x2 + 3x3 + x4 = (t
−
x4 = 2(t
x2 + 3x3

s + 1)
−
s + 1)

2(t + s + 2) + 3s + t =
(t + s + 2) + 3s

3
−
t = 0

−

−

−

−

Because both equations are satisﬁed, it is a solution for all choices of s and t.

The quantities s and t in Example 1.1.1 are called parameters, and the set of solutions, described in
this way, is said to be given in parametric form and is called the general solution to the system. It turns
out that the solutions to every system of equations (if there are solutions) can be given in parametric form
(that is, the variables x1, x2, . . . are given in terms of new independent variables s, t, etc.). The following
example shows how this happens in the simplest systems where only one equation is present.

Example 1.1.2

Describe all solutions to 3x

−

y + 2z = 6 in parametric form.

Solution. Solving the equation for y in terms of x and z, we get y = 3x + 2z
arbitrary then, setting x = s, z = t, we get solutions

−

6. If s and t are

x = s
y = 3s + 2t
z = t

−

6

s and t arbitrary

Of course we could have solved for x: x = 1
solutions are represented as follows:

3(y

−

2z + 6). Then, if we take y = p, z = q, the

2q + 6)

3 (p

x = 1
y = p
z = q

−

p and q arbitrary

The same family of solutions can “look” quite different!

1.1. Solutions and Elementary Operations

3

y

y = 1

x

−

x + y = 3

P(2, 1)

x

(a) Unique Solution
(x = 2, y = 1)

When only two variables are involved, the solutions to systems of lin-
ear equations can be described geometrically because the graph of a lin-
ear equation ax + by = c is a straight line if a and b are not both zero.
Moreover, a point P(s, t) with coordinates s and t lies on the line if and
only if as + bt = c—that is when x = s, y = t is a solution to the equa-
tion. Hence the solutions to a system of linear equations correspond to the
points P(s, t) that lie on all the lines in question.

In particular, if the system consists of just one equation, there must
be inﬁnitely many solutions because there are inﬁnitely many points on a
line. If the system has two equations, there are three possibilities for the
corresponding straight lines:

y

y

x + y = 4

x + y = 2

(b) No Solution

6x + 2y =

8

−

−

y = 4

3x

−

x

x

(c) Inﬁnitely many solutions

4)

(x = t, y = 3t

−
Figure 1.1.1

1. The lines intersect at a single point. Then the system has a unique

solution corresponding to that point.

2. The lines are parallel (and distinct) and so do not intersect. Then

the system has no solution.

3. The lines are identical.

Then the system has inﬁnitely many

solutions—one for each point on the (common) line.

These three situations are illustrated in Figure 1.1.1. In each case the
graphs of two speciﬁc lines are plotted and the corresponding equations are
indicated. In the last case, the equations are 3x
8,
which have identical graphs.

6x+2y =

y = 4 and

−

−

−

With three variables, the graph of an equation ax + by + cz = d can be
shown to be a plane (see Section 4.2) and so again provides a “picture”
of the set of solutions. However, this graphical method has its limitations:
When more than three variables are involved, no physical image of the
graphs (called hyperplanes) is possible. It is necessary to turn to a more
“algebraic” method of solution.

Before describing the method, we introduce a concept that simpliﬁes

the computations involved. Consider the following system

3x1 + 2x2
x3 + x4 =
1
−
x3 + 2x4 = 0
2x1
3x1 + x2 + 2x3 + 5x4 = 2

−
−

of three equations in four variables. The array of numbers1

3 2
2 0
3 1

1 1
1 2
2 5

−
−





1
−
0
2 


occurring in the system is called the augmented matrix of the system. Each row of the matrix consists
of the coefﬁcients of the variables (in order) from the corresponding equation, together with the constant

1A rectangular array of numbers is called a matrix. Matrices will be discussed in more detail in Chapter 2.

4

Systems of Linear Equations

term. For clarity, the constants are separated by a vertical line. The augmented matrix is just a different
way of describing the system of equations. The array of coefﬁcients of the variables

3 2
2 0
3 1





is called the coefﬁcient matrix of the system and

Elementary Operations





−
−

1 1
1 2
2 5 


1
−
0
2 


is called the constant matrix of the system.

The algebraic method for solving systems of linear equations is described as follows. Two such systems
are said to be equivalent if they have the same set of solutions. A system is solved by writing a series of
systems, one after the other, each equivalent to the previous system. Each of these systems has the same
set of solutions as the original one; the aim is to end up with a system that is easy to solve. Each system
in the series is obtained from the preceding system by a simple manipulation chosen so that it does not
change the set of solutions.

As an illustration, we solve the system x + 2y =

2, 2x + y = 7 in this manner. At each stage, the

corresponding augmented matrix is displayed. The original system is

−

First, subtract twice the ﬁrst equation from the second. The resulting system is

x + 2y =
2
−
2x + y = 7

1 2
2 1

2
−
7

(cid:21)

(cid:20)

x + 2y =

2
−
3y = 11

−

1
0

(cid:20)

2
3
−

2
−
11

(cid:21)

which is equivalent to the original (see Theorem 1.1.1). At this stage we obtain y =
the second equation by

1
3. The result is the equivalent system

−

11
3 by multiplying

−

2
−
11
3
−
Finally, we subtract twice the second equation from the ﬁrst to get another equivalent system.

2
−
11
3 (cid:21)
−

x + 2y =
y =

1 2
0 1

(cid:20)

x = 16
3
11
3

y =

−

1 0



0 1

16
3
11
3



−


Now this system is easy to solve! And because it is equivalent to the original system, it provides the
solution to that system.



Observe that, at each stage, a certain operation is performed on the system (and thus on the augmented

matrix) to produce an equivalent system.

1.1. Solutions and Elementary Operations

5

Deﬁnition 1.1 Elementary Operations

Thefollowingoperations,calledelementaryoperations,canroutinelybeperformedonsystems
oflinearequationstoproduceequivalentsystems.

I. Interchangetwoequations.

II. Multiplyoneequationbya nonzero number.

III. Addamultipleofoneequationtoa different equation.

Theorem 1.1.1

Supposethatasequenceofelementaryoperationsisperformedonasystemoflinearequations.
Thentheresultingsystemhasthesamesetofsolutionsastheoriginal,sothetwosystemsare
equivalent.

The proof is given at the end of this section.

Elementary operations performed on a system of equations produce corresponding manipulations of
the rows of the augmented matrix. Thus, multiplying a row of a matrix by a number k means multiplying
every entry of the row by k. Adding one row to another row means adding each entry of that row to the
corresponding entry of the other row. Subtracting two rows is done similarly. Note that we regard two
rows as equal when corresponding entries are the same.

In hand calculations (and in computer programs) we manipulate the rows of the augmented matrix

rather than the equations. For this reason we restate these elementary operations for matrices.

Deﬁnition 1.2 Elementary Row Operations

Thefollowingarecalledelementaryrowoperationsonamatrix.

I. Interchangetworows.

II. Multiplyonerowbya nonzero number.

III. Addamultipleofonerowtoa different row.

In the illustration above, a series of such operations led to a matrix of the form

where the asterisks represent arbitrary numbers. In the case of three equations in three variables, the goal
is to produce a matrix of the form

1 0
0 1

(cid:20)

∗
∗ (cid:21)

This does not always happen, as we will see in the next section. Here is an example in which it does
happen.



1 0 0
0 1 0
0 0 1



∗
∗
∗





6

Systems of Linear Equations

Example 1.1.3

Find all solutions to the following system of equations.

3x + 4y + z = 1
2x + 3y
= 0
4x + 3y
2
−

z =

−

Solution. The augmented matrix of the original system is

3 4
2 3
4 3





1
0
1
−

1
0
2 


−

To create a 1 in the upper left corner we could multiply row 1 through by 1
obtained without introducing fractions by subtracting row 2 from row 1. The result is

3 . However, the 1 can be

1 1
2 3
4 3





1 1
0 1
4 3





1
0
1
−

1
2
−
1
−

1
0
2 


−

1
2
−
2 
−


The upper left 1 is now used to “clean up” the ﬁrst column, that is create zeros in the other
positions in that column. First subtract 2 times row 1 from row 2 to obtain

Next subtract 4 times row 1 from row 3. The result is

1
0
0





1
1
1
−

1
2
−
5
−

1
2
−
6 
−


This completes the work on column 1. We now use the 1 in the second position of the second row
to clean up the second column by subtracting row 2 from row 1 and then adding row 2 to row 3.
For convenience, both row operations are done in one step. The result is

1 0
0 1
0 0





3
2
−
7
−

3
2
8 
−
−


Note that the last two manipulations did not affect the ﬁrst column (the second row has a zero
there), so our previous effort there has not been undermined. Finally we clean up the third column.
Begin by multiplying row 3 by

1
7 to obtain

−

1 0
0 1
0 0

3
2
−
1

3
2
−
8
7









Now subtract 3 times row 3 from row 1, and then add 2 times row 3 to row 2 to get

1.1. Solutions and Elementary Operations

7

1 0 0



0 1 0

−



3
7
2
7
8
7

0 0 1





7 , y = 2
3





7, and z = 8
7, which give the (unique) solution.

The corresponding equations are x =

−

Every elementary row operation can be reversed by another elementary row operation of the same

type (called its inverse). To see how, we look at types I, II, and III separately:

Type I

Interchanging two rows is reversed by interchanging them again.

Type II Multiplying a row by a nonzero number k is reversed by multiplying by 1/k.

Type III Adding k times row p to a different row q is reversed by adding

(in the new matrix). Note that p

= q is essential here.

k times row p to row q

−

To illustrate the Type III situation, suppose there are four rows in the original matrix, denoted R1, R2,
k times R2, to R3. The

R3, and R4, and that k times R2 is added to R3. Then the reverse operation adds
following diagram illustrates the effect of doing the operation ﬁrst and then the reverse:

−

R1
R2
R3
R4

R1
R2
R3 + kR2
R4

R1
R2
(R3 + kR2)
R4

−







→





→



= 

kR2



R1
R2
R3
R4




























The existence of inverses for elementary row operations and hence for elementary operations on a system
of equations, gives:
Proof of Theorem 1.1.1. Suppose that a system of linear equations is transformed into a new system
by a sequence of elementary operations. Then every solution of the original system is automatically a
solution of the new system because adding equations, or multiplying an equation by a nonzero number,
always results in a valid equation. In the same way, each solution of the new system must be a solution
to the original system because the original system can be obtained from the new one by another series of
elementary operations (the inverses of the originals). It follows that the original and new systems have the
same solutions. This proves Theorem 1.1.1.





6
8

Systems of Linear Equations

Exercises for 1.1

Exercise 1.1.1 In each case verify that the following are
solutions for all values of s and t.

Exercise 1.1.7 Write the augmented matrix for each of
the following systems of linear equations.

a. x = 19t
y = 25
z = t

−
−

35
13t

is a solution of

2x + 3y + z = 5
4z = 0
5x + 7y

−

b. x1 = 2s + 12t + 13

x2 = s
x3 =
−
x4 = t

s

3t

3

−

−

is a solution of

2x1 + 5x2 + 9x3 + 3x4 =
x1 + 2x2 + 4x3

1
−
= 1

Exercise 1.1.2 Find all solutions to the following in
parametric form in two ways.

a.

3x + y = 2

b.

2x + 3y = 1

c.

3x

−

y + 2z = 5

d.

x

−

2y + 5z = 1

Exercise 1.1.3 Regarding 2x = 5 as the equation
2x + 0y = 5 in two variables, ﬁnd all solutions in para-
metric form.

2y = 3 as the equation
−
2y + 0z = 3 in three variables, ﬁnd all solutions in

Exercise 1.1.4 Regarding 4x
4x
parametric form.

−

Exercise 1.1.5 Find all solutions to the general system
ax = b of one equation in one variable (a) when a = 0
and (b) when a

= 0.

Exercise 1.1.6 Show that a system consisting of exactly
one linear equation can have no solution, one solution, or
inﬁnitely many solutions. Give examples.

a.

x
3y = 5
2x + y = 1

−

c.

x

−

y + z = 2
x
z = 1
y + 2x = 0

−

b.

d.

x + 2y = 0
y = 1

x + y = 1
y + z = 0
x = 2
z

−

Exercise 1.1.8 Write a system of linear equations that
has each of the following augmented matrices.

a.

1
0
2





1 6 0
1 0 3
1 0 1

−

−





2
3
0

−

1 0
2 1
1 1

−

1
0
3





b.



−



Exercise 1.1.9 Find the solution of each of the following
systems of linear equations using augmented matrices.

a.

c.

x
2x

3y = 1
7y = 3

−
−
2x + 3y =
1
−
3x + 4y = 2

b.

d.

x + 2y = 1
3x + 4y =
1

−
3x + 4y = 1
4x + 5y =
3

−

Exercise 1.1.10 Find the solution of each of the follow-
ing systems of linear equations using augmented matri-
ces.

a.

x + y + 2z =
1
−
2x + y + 3z = 0
2y + z = 2

−

b.

2x + y + z =
1
−
x + 2y + z = 0
2z = 5
3x

−

Exercise 1.1.11 Find all solutions (if any) of the follow-
ing systems of linear equations.

a.

−

3x
−
12x + 8y =

2y = 5
20

−

b.

−

2y = 5
3x
12x + 8y = 16

−

Exercise 1.1.12 Show that the system

x + 2y
z = a
−
2x + y + 3z = b
4y + 9z = c
x

−




is inconsistent unless c = 2b



3a.

−

Exercise 1.1.13 By examining the possible positions of
lines in the plane, show that two equations in two vari-
ables can have zero, one, or inﬁnitely many solutions.

6
Exercise 1.1.14 In each case either show that the state-
ment is true, or give an example2 showing it is false.

a. If a linear system has n variables and m equations,

then the augmented matrix has n rows.

b. A consistent linear system must have inﬁnitely

many solutions.

c. If a row operation is done to a consistent linear
system, the resulting system must be consistent.

d. If a series of row operations on a linear system re-
sults in an inconsistent system, the original system
is inconsistent.

Exercise 1.1.15 Find a quadratic a + bx + cx2 such that
the graph of y = a + bx + cx2 contains each of the points
(
−

1, 6), (2, 0), and (3, 2).

Exercise 1.1.16

Solve the system

changing variables

sulting equations for x′ and y′.

x = 5x′ −
y =
−

2y′
7x′ + 3y′

(cid:26)

3x + 2y = 5
7x + 5y = 1

by

(cid:26)
and solving the re-

1.2 Gaussian Elimination

1.2. Gaussian Elimination

9

Exercise 1.1.17 Find a, b, and c such that

x2
x+3
−
(x2+2)(2x

1) = ax+b

x2+2 + c

2x

1
−

−

[Hint: Multiply through by (x2 + 2)(2x
coefﬁcients of powers of x.]

−

1) and equate

Exercise 1.1.18 A zookeeper wants to give an animal 42
mg of vitamin A and 65 mg of vitamin D per day. He has
two supplements: the ﬁrst contains 10% vitamin A and
25% vitamin D; the second contains 20% vitamin A and
25% vitamin D. How much of each supplement should
he give the animal each day?

Exercise 1.1.19 Workmen John and Joe earn a total of
$24.60 when John works 2 hours and Joe works 3 hours.
If John works 3 hours and Joe works 2 hours, they get
$23.90. Find their hourly rates.

Exercise 1.1.20 A biologist wants to create a diet from
ﬁsh and meal containing 183 grams of protein and 93
grams of carbohydrate per day. If ﬁsh contains 70% pro-
tein and 10% carbohydrate, and meal contains 30% pro-
tein and 60% carbohydrate, how much of each food is
required each day?

The algebraic method introduced in the preceding section can be summarized as follows: Given a system
of linear equations, use a sequence of elementary row operations to carry the augmented matrix to a “nice”
matrix (meaning that the corresponding equations are easy to solve). In Example 1.1.3, this nice matrix
took the form

The following deﬁnitions identify the nice matrices that arise in this process.

1 0 0
0 1 0
0 0 1





∗
∗
∗





2Such an example is called a counterexample. For example, if the statement is that “all philosophers have beards”, the
existence of a non-bearded philosopher would be a counterexample proving that the statement is false. This is discussed again
in Appendix B.

10

Systems of Linear Equations

Deﬁnition 1.3 Row-Echelon Form (Reduced)

Amatrixissaidtobeinrow-echelon form(andwillbecalledarow-echelonmatrix)ifit
satisﬁesthefollowingthreeconditions:

1. Allzerorows(consistingentirelyofzeros)areatthebottom.

2. Theﬁrstnonzeroentryfromtheleftineachnonzerorowisa 1,calledtheleading1forthat

row.

3. Eachleading 1 istotherightofallleading 1sintherowsaboveit.

Arow-echelonmatrixissaidtobeinreduced row-echelonform(andwillbecalledareduced
row-echelonmatrix)if,inaddition,itsatisﬁesthefollowingcondition:

4. Eachleading 1 istheonlynonzeroentryinitscolumn.

The row-echelon matrices have a “staircase” form, as indicated by the following example (the asterisks
indicate arbitrary numbers).

0 1
∗ ∗ ∗ ∗ ∗
0 0 0 1
∗ ∗ ∗
0 0 0 0 1
∗ ∗
0 0 0 0 0 0 1
0 0 0 0 0 0 0

















The leading 1s proceed “down and to the right” through the matrix. Entries above and to the right of the
leading 1s are arbitrary, but all entries below and to the left of them are zero. Hence, a matrix in row-
echelon form is in reduced form if, in addition, the entries directly above each leading 1 are all zero. Note
that a matrix in row-echelon form can, with a few more row operations, be carried to reduced form (use
row operations to create zeros above each leading one in succession, beginning from the right).

Example 1.2.1

The following matrices are in row-echelon form (for any choice of numbers in

-positions).

∗

1
∗ ∗
0 0 1

(cid:20)



(cid:21)

∗ ∗
∗

0 1
0 0 1
0 0 0 0 




1
∗ ∗ ∗
0 1
0 0 0 1 
∗ ∗




1
∗ ∗
0 1
0 0 1 
∗




The following, on the other hand, are in reduced row-echelon form.



1
0
∗
0 0 1

0 1 0
0 0 1
0 0 0 0 

The choice of the positions for the leading 1s determines the (reduced) row-echelon form (apart
from the numbers in

1 0
∗
0 1
0 0 0 1 
∗


1 0 0
0 1 0
0 0 1 


-positions).

∗
∗

0
0













(cid:20)

(cid:21)

∗

The importance of row-echelon matrices comes from the following theorem.

1.2. Gaussian Elimination

11

Theorem 1.2.1

Everymatrixcanbebroughtto(reduced)row-echelonformbyasequenceofelementaryrow
operations.

In fact we can give a step-by-step procedure for actually ﬁnding a row-echelon matrix. Observe that
while there are many sequences of row operations that will bring a matrix to row-echelon form, the one
we use is systematic and is easy to program on a computer. Note that the algorithm deals with matrices in
general, possibly with columns of zeros.

Gaussian3Algorithm4

Step1. Ifthematrixconsistsentirelyofzeros,stop—itisalreadyinrow-echelonform.

Step2. Otherwise,ﬁndtheﬁrstcolumnfromtheleftcontaininganonzeroentry(callit a),
andmovetherowcontainingthatentrytothetopposition.

Step3. Nowmultiplythenewtoprowby 1/a tocreatealeading 1.

Step4. Bysubtractingmultiplesofthatrowfromrowsbelowit,makeeachentrybelowthe
leading 1 zero.

Thiscompletestheﬁrstrow,andallfurtherrowoperationsarecarriedoutontheremainingrows.

Step5. Repeatsteps1–4onthematrixconsistingoftheremainingrows.

Theprocessstopswheneithernorowsremainatstep5ortheremainingrowsconsistentirelyof
zeros.

Observe that the gaussian algorithm is recursive: When the ﬁrst leading 1 has been obtained, the
procedure is repeated on the remaining rows of the matrix. This makes the algorithm easy to use on a
computer. Note that the solution to Example 1.1.3 did not use the gaussian algorithm as written because
the ﬁrst leading 1 was not created by dividing row 1 by 3. The reason for this is that it avoids fractions.
However, the general pattern is clear: Create the leading 1s from left to right, using each of them in turn
to create zeros below it. Here are two more examples.

Example 1.2.2

Solve the following system of equations.

4z =
3x + y
1
−
−
+ 10z = 5
x
4x + y + 6z = 1

3Carl Friedrich Gauss (1777–1855) ranks with Archimedes and Newton as one of the three greatest mathematicians of all
time. He was a child prodigy and, at the age of 21, he gave the ﬁrst proof that every polynomial has a complex root. In
1801 he published a timeless masterpiece, Disquisitiones Arithmeticae, in which he founded modern number theory. He went
on to make ground-breaking contributions to nearly every branch of mathematics, often well before others rediscovered and
published the results.

4The algorithm was known to the ancient Chinese.

12

Systems of Linear Equations

Solution. The corresponding augmented matrix is

3 1
1 0
4 1

4
−
10
6



Create the ﬁrst leading one by interchanging rows 1 and 2

1
−
5
1 


5
1
−
1 






1 0
3 1
4 1

10
4
−
6



Now subtract 3 times row 1 from row 2, and subtract 4 times row 1 from row 3. The result is

1 0
0 1
0 1



Now subtract row 2 from row 3 to obtain



10
34
34

−
−

5
16
19 


−
−

1 0
0 1
0 0

10
34
0

−





−

5
16
3 


−

This means that the following reduced system of equations

y

x + 10z = 5
34z =
16
3
0 =
−

−

−

is equivalent to the original system. In other words, the two have the same solutions. But this last
system clearly has no solution (the last equation requires that x, y and z satisfy 0x + 0y + 0z =
3,
and no such numbers exist). Hence the original system has no solution.

−

Example 1.2.3

Solve the following system of equations.

x1
2x1
x1

−
−
−

Solution. The augmented matrix is

2x2
−
4x2 + x3
2x2 + 2x3

x3 + 3x4 = 1
= 5
3x4 = 4

−

1
2
1

2
−
4
−
2
−

1
−
1
2





3 1
0 5
3 4 


−

Subtracting twice row 1 from row 2 and subtracting row 1 from row 3 gives

1.2. Gaussian Elimination

13

−
−
Now subtract row 2 from row 3 and multiply row 2 by 1





1
0
0

2
−
0
0

1
−
3
3

3 1
6 3
6 3 

3 to get

1
0
0

2
−
0
0

1
−
1
0





−

3 1
2 1
0 0 


This is in row-echelon form, and we take it to reduced form by adding row 2 to row 1:

1
0
0

−

2 0
0 1
0 0



The corresponding reduced system of equations is



−

1 2
2 1
0 0 


x1

−

2x2 + x4 = 2
2x4 = 1
0 = 0

x3

−

The leading ones are in columns 1 and 3 here, so the corresponding variables x1 and x3 are called
leading variables. Because the matrix is in reduced row-echelon form, these equations can be used
to solve for the leading variables in terms of the nonleading variables x2 and x4. More precisely, in
the present example we set x2 = s and x4 = t where s and t are arbitrary, so these equations become

Finally the solutions are given by

2s + t = 2 and

x1

−

2t = 1

x3

−

t

−

x1 = 2 + 2s
x2 = s
x3 = 1 + 2t
x4 = t

where s and t are arbitrary.

The solution of Example 1.2.3 is typical of the general case. To solve a linear system, the augmented
matrix is carried to reduced row-echelon form, and the variables corresponding to the leading ones are
called leading variables. Because the matrix is in reduced form, each leading variable occurs in exactly
one equation, so that equation can be solved to give a formula for the leading variable in terms of the
nonleading variables. It is customary to call the nonleading variables “free” variables, and to label them
by new variables s, t, . . . , called parameters. Hence, as in Example 1.2.3, every variable xi is given by a
formula in terms of the parameters s and t. Moreover, every choice of these parameters leads to a solution

14

Systems of Linear Equations

to the system, and every solution arises in this way. This procedure works in general, and has come to be
called

Gaussian Elimination

Tosolveasystemoflinearequationsproceedasfollows:

1. Carrytheaugmentedmatrixtoareducedrow-echelonmatrixusingelementaryrow

operations.

2. Ifarow

0 0 0

· · ·

0 1

occurs,thesystemisinconsistent.

3. Otherwise,assignthenonleadingvariables(ifany)asparameters,andusetheequations

(cid:3)

(cid:2)

correspondingtothereducedrow-echelonmatrixtosolvefortheleadingvariablesinterms
oftheparameters.

There is a variant of this procedure, wherein the augmented matrix is carried only to row-echelon form.
The nonleading variables are assigned as parameters as before. Then the last equation (corresponding to
the row-echelon form) is used to solve for the last leading variable in terms of the parameters. This last
leading variable is then substituted into all the preceding equations. Then, the second last equation yields
the second last leading variable, which is also substituted back. The process continues to give the general
solution. This procedure is called back-substitution. This procedure can be shown to be numerically
more efﬁcient and so is important when solving very large systems.5

Example 1.2.4

Find a condition on the numbers a, b, and c such that the following system of equations is
consistent. When that condition is satisﬁed, ﬁnd all solutions (in terms of a, b, and c).

x1 + 3x2 + x3 = a
2x2 + x3 = b
x1
−
−
x3 = c
3x1 + 7x2

−

Solution. We use gaussian elimination except that now the augmented matrix

has entries a, b, and c as well as known numbers. The ﬁrst leading one is in place, so we create
zeros below it in column 1:

1
1
−
3

3
2
−
7



1 a
1 b
1 c 


−



1
0
0





3
1
2
−

a
a + b

1
2
4 c

3a 


−

−

5With n equations where n is large, gaussian elimination requires roughly n3/2 multiplications and divisions, whereas this

number is roughly n3/3 if back substitution is used.

The second leading 1 has appeared, so use it to create zeros in the rest of column 2:

1.2. Gaussian Elimination

15

1 0
0 1
0 0



−

5
−
2
0 c



3b

2a
−
a + b
a + 2b 
−

a + 2b = c

Now the whole solution depends on the number c
2b). If c
corresponds to an equation 0 = c
1.2.2). Hence:

(a

−

−

−
= a

−

(a

2b). The last row

2b, there is no solution (just as in Example

−

−

The system is consistent if and only if c = a

2b.

−

In this case the last matrix becomes

1 0
0 1
0 0

5
−
2
0



3b

−

2a
−
a + b
0





2b, taking x3 = t where t is a parameter gives the solutions



x1 = 5t

−

(2a + 3b)

x2 = (a + b)

2t

x3 = t.

−

Thus, if c = a

−

Rank

It can be proven that the reduced row-echelon form of a matrix A is uniquely determined by A. That is,
no matter which series of row operations is used to carry A to a reduced row-echelon matrix, the result
will always be the same matrix. (A proof is given at the end of Section 2.5.) By contrast, this is not
true for row-echelon matrices: Different series of row operations can carry the same matrix A to different

row-echelon matrices. Indeed, the matrix A =

1
2

(cid:20)

1 4
1 2

−
−

(cid:21)

can be carried (by one row operation) to

the row-echelon matrix

, and then by another row operation to the (reduced) row-echelon

1
0

1
−
1

(cid:20)

4
6
−

(cid:21)

. However, it is true that the number r of leading 1s must be the same in each of

these row-echelon matrices (this will be proved in Chapter 5). Hence, the number r depends only on A
and not on the way in which A is carried to row-echelon form.

matrix

1 0
0 1

(cid:20)

2
−
6
−

(cid:21)

Deﬁnition 1.4 Rank of a Matrix

Therankofmatrix A isthenumberofleading 1sinanyrow-echelonmatrixtowhich A canbe
carriedbyrowoperations.

6
16

Systems of Linear Equations

Example 1.2.5

Compute the rank of A =

1 1
2 1
0 1



−

−

.

1 4
3 0
5 8 



Solution. The reduction of A to row-echelon form is

A =



1 1
2 1
0 1



−

−

1 4
3 0
5 8 


→ 



1
0
0

1
1
−
1

1
−
5
5
−

4
8
−
8 


1 1
0 1
0 0

→ 



−
−

1 4
5 8
0 0 


Because this row-echelon matrix has two leading 1s, rank A = 2.

Suppose that rank A = r, where A is a matrix with m rows and n columns. Then r

m because the
leading 1s lie in different rows, and r
n because the leading 1s lie in different columns. Moreover, the
rank has a useful application to equations. Recall that a system of linear equations is called consistent if it
has at least one solution.

≤

≤

Theorem 1.2.2

Supposeasystemof m equationsin n variablesisconsistent,andthattherankoftheaugmented
matrixis r.

1. Thesetofsolutionsinvolvesexactly n

r parameters.

−

2. If r < n,thesystemhasinﬁnitelymanysolutions.

3. If r = n,thesystemhasauniquesolution.

Proof. The fact that the rank of the augmented matrix is r means there are exactly r leading variables, and
r nonleading variables. These nonleading variables are all assigned as parameters in the
hence exactly n
r parameters. Hence if r < n, there is at
gaussian algorithm, so the set of solutions involves exactly n
least one parameter, and so inﬁnitely many solutions. If r = n, there are no parameters and so a unique
solution.

−

−

Theorem 1.2.2 shows that, for any system of linear equations, exactly three possibilities exist:

1. No solution. This occurs when a row

0 0

the case where the system is inconsistent.

(cid:2)

· · ·

0 1

occurs in the row-echelon form. This is

(cid:3)

2. Unique solution. This occurs when every variable is a leading variable.

3. Inﬁnitely many solutions. This occurs when the system is consistent and there is at least one

nonleading variable, so at least one parameter is involved.

Example 1.2.6

1.2. Gaussian Elimination

17

Suppose the matrix A in Example 1.2.5 is the augmented matrix of a system of m = 3 linear
equations in n = 3 variables. As rank A = r = 2, the set of solutions will have n
The reader can verify this fact directly.

−

r = 1 parameter.

Many important problems involve linear inequalities rather than linear equations. For example, a
4 rather than an equality
5y = 4. There is a technique (called the simplex algorithm) for ﬁnding solutions to a system of such

condition on the variables x and y might take the form of an inequality 2x
2x
inequalities that maximizes a function of the form p = ax + by where a and b are ﬁxed constants.

5y

−

≤

−

Exercises for 1.2

Exercise 1.2.1 Which of the following matrices are in
reduced row-echelon form? Which are in row-echelon
form?

1
0
0

1
0

−

1 2
0 0
0 1




2 3 5
0 0 1

−

(cid:21)

1 1
0 1

(cid:21)

a.

c.

e.





(cid:20)

(cid:20)

b.

d.

f.

(cid:20)









2 1
0 0

−

1 3
0 0

(cid:21)

1 0 0 3 1
0 0 0 1 1
0 0 0 0 1

0 0 1
0 0 1
0 0 1















1
0
0
0

−

2 0 2 0
0 1 5 0
0 0 0 1
0 0 0 0

−

1
3
6
0

−

1
1
1
0

1 2
0 1
0 0
0 0

−

1 3
1 0
0 1
0 0

1
0
0
0

−

1 2 4
1 2 1
0 0 1
0 0 0

1 1
1 1
1 0
0 0

6
1
0
0

−

−

−






2
1
1
0







b. 





c. 





d. 





Exercise 1.2.2 Carry each of the following matrices to
reduced row-echelon form.

Exercise 1.2.4 Find all solutions (if any) to each of the
following systems of linear equations.

−

−

−
−

1
1
2
3

1
2
3
1

0
0
0
0

0
0
0
0

a. 





b. 





−

−
3
6
9
3

−
−

2 1 2 1
2 2 7 2
4 3 7 1
6 1 6 4

−

1
4
0
1

−

3 2
5 0
4 1
3 0

1
1
2
1

−






1
1
1
1

−
−







Exercise 1.2.3 The augmented matrix of a system of
linear equations has been carried to the following by row
operations. In each case solve the system.

1 2 0
0 0 1
0 0 0
0 0 0

−

3 1 0
1 1 0
0 0 1
0 0 0

1
−
2
3
0







a. 





a.

c.

e.

2y = 1
x =
2

x
4y

−
−

−
2x + y = 5
3x + 2y = 6

y = 4
6x = 1

3x
2y

−
−

b.

d.

f.

y = 0
3y = 1

y = 2
6x =
4

−
3y = 5
2x = 2

3x
2x

3x
2y

2x
3y

−
−

−
−

−
−

Exercise 1.2.5 Find all solutions (if any) to each of the
following systems of linear equations.

a.

c.

−

x + y + 2z = 8
3x
y + z = 0
x + 3y + 4z =
4

−

−
z = 10
x + y
−
x + 4y + 5z =
5
−
x + 6y + 3z = 15

−

b.

−

2x + 3y + 3z =
3x
5x + 7y + 2z =

9
−
4y + z = 5
14

−

−

d.

−
x + 2y
2x + 5y
x + 4y

z = 2
3z = 1
3z = 3

−
−
−

18

Systems of Linear Equations

e.

g.

5x + y
3x
−
x + y

= 2
y + 2z = 1
z = 5

−
x + y + z = 2
x
+ z = 1
2x + 5y + 2z = 7

f.

h.

3x
x
x + y + z =

2y + z =
2
−
y + 3z = 5
1

−
−

−
x + 2y
2x
−
x + y

−
4z = 10
y + 2z = 5
2z = 7

−

−

Exercise 1.2.6 Express the last equation of each system
as a sum of multiples of the ﬁrst two equations. [Hint:
Label the equations, use the gaussian algorithm.]

a.

c.

e.

f.

a.

x1 + x2 + x3 = 1
x2 + 3x3 = 3
2x1
2x2 + 2x3 = 2
x1

−
−

b.

x1 + 2x2
x1 + 3x2
x1

3x3 =
3
−
5x3 = 5
35

−
−
2x2 + 5x3 =

−

−

Exercise 1.2.7 Find all solutions to the following sys-
tems.

b.

c.

−

−
−

3x3
x3

a. 3x1 + 8x2
2x1 + 3x2
x1
−
x1 + 5x2

14x4 = 2
−
2x4 = 1
−
2x2 + x3 + 10x4 = 0
12x4 = 1

−
−

2x3

−

−
x2 + x3

−
x1
x4 = 0
x1 + x2 + x3 + x4 = 0
x1 + x2
x3 + x4 = 0
x1 + x2 + x3 + x4 = 0

−

−

−

−

x1
x2 + x3
x1 + x2 + x3 + x4 =
x1 + 2x2 + 3x3
x1

2x4 = 1
1
−
x4 = 2
−
x2 + 2x3 + x4 = 1

−
d. x1 + x2 + 2x3
3x2
x1 + 2x2
x1 + x2

−

x4 = 4
x3 + 4x4 = 2
3x3 + 5x4 = 0
5x3 + 6x4 =
3

−
−
−

−

Exercise 1.2.8 In each of the following, ﬁnd (if possi-
ble) conditions on a and b such that the system has no
solution, one solution, and inﬁnitely many solutions.

a.

c.

2y = 1
x
ax + by = 5

−

x
by =
1
−
x + ay = 3

−

b.

d.

x + by =
1
−
ax + 2y = 5

ax + y = 1
2x + y = b

Exercise 1.2.9 In each of the following, ﬁnd (if possi-
ble) conditions on a, b, and c such that the system has no
solution, one solution, or inﬁnitely many solutions.

3x + y
x
−
5x + 3y

z = a
y + 2z = b
4z = c

−

−

−
x + 3y + 2z =
8
−
x
+ z = 2
3x + 3y + az = b

b.

d.

2x + y

−

z = a
2y + 3z = b
z = c

x

−
x + ay = 0
y + bz = 0
z + cx = 0

3x
−
x + y
2x

y + 2z = 3
z = 2
2y + 3z = b

−

−
x +
x + (a
−
2x +

ay
−
2)y +
2y + (a

−

z = 1
z =
1
−
2)z = 1

−

Exercise 1.2.10 Find the rank of each of the matrices in
Exercise 1.2.1.

Exercise 1.2.11 Find the rank of each of the following
matrices.

1 2
1 1
3 4

−



b.

−

1
5
3



3
2
−
4

d.



3 3
4 1
7 2

−

−
−

2 1
1 3
1 1

2
3
5

3
1
1

−

−

−












2
−
5
1

−





1
3
1

−

1 1
1 4
1 6

−

1
0
1 2

1
1 1
2 2

2
a

−
1

−
−

a.

c.

e.

f.

















a

0


1
−
a a2 + 1
1
−
1
−
2
2

2a2 


−
a2
0
4





a
a 6

a

−

Exercise 1.2.12 Consider a system of linear equations
with augmented matrix A and coefﬁcient matrix C.
In
each case either prove the statement or give an example
showing that it is false.

a. If there is more than one solution, A has a row of

zeros.

b. If A has a row of zeros, there is more than one

solution.

c. If there is no solution, the reduced row-echelon

form of C has a row of zeros.

d. If the row-echelon form of C has a row of zeros,

there is no solution.

e. There is no system that is inconsistent for every

choice of constants.

f. If the system is consistent for some choice of con-
stants, it is consistent for every choice of con-
stants.

Now assume that the augmented matrix A has 3 rows and
5 columns.

g. If the system is consistent, there is more than one

solution.

h. The rank of A is at most 3.

i. If rank A = 3, the system is consistent.

j. If rank C = 3, the system is consistent.

Exercise 1.2.13 Find a sequence of row operations car-
rying





b3 + c3
b2 + c2
b1 + c1
c1 + a1
c3 + a3
c2 + a2
a1 + b1 a2 + b2 a3 + b3

to





a1 a2 a3
b1 b2 b3
c3
c2
c1



Exercise 1.2.14
row-echelon form is as given.


In each case, show that the reduced





with abc

1 0 0
0 1 0
0 0 1





= 0;





where c

= a or b

= a;





p 0 a
b 0 0
q c

r 

1 a b + c
1 b c + a
1 c a + b
1 0
∗
0 1
∗
0 0 0





a.

b.













Exercise 1.2.15

Show that

az + by + cz = 0
a1x + b1y + c1z = 0

al-

(cid:26)

ways has a solution other than x = 0, y = 0, z = 0.
Exercise 1.2.16 Find the circle x2 + y2 + ax + by + c = 0
passing through the following points.

1.2. Gaussian Elimination

19

a. (

−

2, 1), (5, 0), and (4, 1)

b. (1, 1), (5,

3), and (

−

3,

3)

−

−

Exercise 1.2.17 Three Nissans, two Fords, and four
Chevrolets can be rented for $106 per day. At the same
rates two Nissans, four Fords, and three Chevrolets cost
$107 per day, whereas four Nissans, three Fords, and two
Chevrolets cost $102 per day. Find the rental rates for all
three kinds of cars.

Exercise 1.2.18 A school has three clubs and each stu-
dent is required to belong to exactly one club. One year
the students switched club membership as follows:
Club A. 4
Club B. 7
Club C. 6

10 switch to C.
10 switch to C.
10 switch to B.
If the fraction of the student population in each club

10 remain in A, 1
10 remain in B, 2
10 remain in C, 2

10 switch to B, 5
10 switch to A, 1
10 switch to A, 2

is unchanged, ﬁnd each of these fractions.

Exercise 1.2.19 Given points (p1, q1), (p2, q2), and
(p3, q3) in the plane with p1, p2, and p3 distinct, show
that they lie on some curve with equation y = a + bx +
cx2. [Hint: Solve for a, b, and c.]

Exercise 1.2.20 The scores of three players in a tour-
nament have been lost. The only information available
is the total of the scores for players 1 and 2, the total for
players 2 and 3, and the total for players 3 and 1.

a. Show that the individual scores can be rediscov-

ered.

b. Is this possible with four players (knowing the to-
tals for players 1 and 2, 2 and 3, 3 and 4, and 4 and
1)?

Exercise 1.2.21 A boy ﬁnds $1.05 in dimes, nickels,
and pennies. If there are 17 coins in all, how many coins
of each type can he have?

Exercise 1.2.22 If a consistent system has more vari-
ables than equations, show that it has inﬁnitely many so-
lutions. [Hint: Use Theorem 1.2.2.]

6
6
6
20

Systems of Linear Equations

1.3 Homogeneous Equations

A system of equations in the variables x1, x2, . . . , xn is called homogeneous if all the constant terms are
zero—that is, if each equation of the system has the form

a1x1 + a2x2 +

+ anxn = 0

· · ·
Clearly x1 = 0, x2 = 0, . . . , xn = 0 is a solution to such a system; it is called the trivial solution. Any
solution in which at least one variable has a nonzero value is called a nontrivial solution. Our chief goal
in this section is to give a useful condition for a homogeneous system to have nontrivial solutions. The
following example is instructive.

Example 1.3.1

Show that the following homogeneous system has nontrivial solutions.

x2 + 2x3

x1
−
2x1 + 2x2
3x1 + x2 + 2x3

x4 = 0
−
+ x4 = 0
x4 = 0

−

Solution. The reduction of the augmented matrix to reduced row-echelon form is outlined below.

1
2
3

−

1 2
2 0
1 2





−

−

1 0
1 0
1 0 


→ 



1
0
0

1
−
4
4

2
4
−
4
−

−

1 0
3 0
2 0 


1 0
0 1
0 0

→ 



−

1 0 0
1 0 0
0 1 0 


The leading variables are x1, x2, and x4, so x3 is assigned as a parameter—say x3 = t. Then the
general solution is x1 =
t, x2 = t, x3 = t, x4 = 0. Hence, taking t = 1 (say), we get a nontrivial
solution: x1 =

1, x2 = 1, x3 = 1, x4 = 0.

−

−

The existence of a nontrivial solution in Example 1.3.1 is ensured by the presence of a parameter in the
solution. This is due to the fact that there is a nonleading variable (x3 in this case). But there must be
a nonleading variable here because there are four variables and only three equations (and hence at most
three leading variables). This discussion generalizes to a proof of the following fundamental theorem.

Theorem 1.3.1

Ifahomogeneoussystemoflinearequationshasmorevariablesthanequations,thenithasa
nontrivialsolution(infact,inﬁnitelymany).

Proof. Suppose there are m equations in n variables where n > m, and let R denote the reduced row-echelon
form of the augmented matrix. If there are r leading variables, there are n
r nonleading variables, and so
n
m because R has r leading 1s and m rows,
and m < n by hypothesis. So r

r parameters. Hence, it sufﬁces to show that r < n. But r
m < n, which gives r < n.

−

≤

−

≤

Note that the converse of Theorem 1.3.1 is not true: if a homogeneous system has nontrivial solutions,
it need not have more variables than equations (the system x1 + x2 = 0, 2x1 + 2x2 = 0 has nontrivial
solutions but m = 2 = n.)

1.3. Homogeneous Equations

21

Theorem 1.3.1 is very useful in applications. The next example provides an illustration from geometry.

Example 1.3.2

We call the graph of an equation ax2 + bxy + cy2 + dx + ey + f = 0 a conic if the numbers a, b, and
c are not all zero. Show that there is at least one conic through any ﬁve points in the plane that are
not all on a line.

Solution. Let the coordinates of the ﬁve points be (p1, q1), (p2, q2), (p3, q3), (p4, q4), and
(p5, q5). The graph of ax2 + bxy + cy2 + dx + ey + f = 0 passes through (pi, qi) if

ap2

i + bpiqi + cq2

i + d pi + eqi + f = 0

This gives ﬁve equations, one for each i, linear in the six variables a, b, c, d, e, and f . Hence, there
is a nontrivial solution by Theorem 1.3.1. If a = b = c = 0, the ﬁve points all lie on the line with
equation dx + ey + f = 0, contrary to assumption. Hence, one of a, b, c is nonzero.

Linear Combinations and Basic Solutions

As for rows, two columns are regarded as equal if they have the same number of entries and corresponding
entries are the same. Let x and y be columns with the same number of entries. As for elementary row
operations, their sum x + y is obtained by adding corresponding entries and, if k is a number, the scalar
product kx is deﬁned by multiplying each entry of x by k. More precisely:

x1
x2
...
xn

y1
y2
...
yn

If x = 












and y = 












then x + y = 





x1 + y1
x2 + y2
...
xn + yn








and kx = 





kx1
kx2
...
kxn



.






A sum of scalar multiples of several columns is called a linear combination of these columns. For
example, sx + ty is a linear combination of x and y for any choice of numbers s and t.

Example 1.3.3

If x =

3
2
−

(cid:20)

(cid:21)

Example 1.3.4

and y =

1
−
1

(cid:21)

(cid:20)

then 2x + 5y =

6
4
−

(cid:20)

+

(cid:21)

(cid:20)

5
−
5

=

(cid:21)

(cid:20)

1
1

.

(cid:21)

Let x =

3
1
1 
and w are linear combinations of x, y and z.



2
1
0 


1
0
1 


and z =

, y =











. If v =

0
1
2 
−






and w =





, determine whether v

1
1
1 


22

Systems of Linear Equations

Solution. For v, we must determine whether numbers r, s, and t exist such that v = rx + sy + tz,
that is, whether



0
1
2 
−


= r



+ s



+ t



1
0
1 


2
1
0 


r + 2s + 3t
s + t
r + t



=



3
1
1 







Equating corresponding entries gives a system of linear equations r + 2s + 3t = 0, s + t =
r + t = 2 for r, s, and t. By gaussian elimination, the solution is r = 2
where k is a parameter. Taking k = 0, we see that v = 2x
Turning to w, we again look for r, s, and t such that w = rx + sy + tz; that is,

1
−
y is a linear combination of x, y, and z.

−
k, and t = k

1, and

k, s =

−

−

−







2
1
0 

leading to equations r + 2s + 3t = 1, s + t = 1, and r + t = 1 for real numbers r, s, and t. But this
time there is no solution as the reader can verify, so w is not a linear combination of x, y, and z.

r + 2s + 3t
s + t
r + t

3
1
1 


1
0
1 


1
1
1 


= r

+ s

+ t

=

























Our interest in linear combinations comes from the fact that they provide one of the best ways to
describe the general solution of a homogeneous system of linear equations. When solving such a system

with n variables x1, x2, . . . , xn, write the variables as a column6 matrix: x = 







. As an illustration, the general solution in Example 1.3.1 is x1 =

. The trivial solution

x1
x2
...
xn








t, x2 = t, x3 = t,

−

and x4 = 0, where t is a parameter, and we would now express this by saying that the general solution is

is denoted 0 = 





0
0
...
0






x = 

, where t is arbitrary.





Now let x and y be two solutions to a homogeneous system with n variables. Then any linear combi-






nation sx + ty of these solutions turns out to be again a solution to the system. More generally:

Any linear combination of solutions to a homogeneous system is again a solution.

(1.1)

In fact, suppose that a typical equation in the system is a1x1 + a2x2 +

+ anxn = 0, and suppose that

· · ·

y1
y2
...
yn



are solutions. Then a1x1 + a2x2 +

x = 







, y = 









6The reason for using columns will be apparent later.






+ anxn = 0 and a1y1 + a2y2 +

+ anyn = 0.

· · ·

· · ·

t
−
t
t
0

x1
x2
...
xn

1.3. Homogeneous Equations

23



is also a solution because

Hence sx + ty = 





sx1 + ty1
sx2 + ty2
...
sxn + tyn






a1(sx1 + ty1) + a2(sx2 + ty2) +

+ an(sxn + tyn)

· · ·

= [a1(sx1) + a2(sx2) +
= s(a1x1 + a2x2 +
= s(0) + t(0)
= 0

· · ·

+ an(sxn)] + [a1(ty1) + a2(ty2) +
+ anyn)

+ anxn) + t(a1y1 + a2y2 +

· · ·

+ an(tyn)]

· · ·

· · ·

A similar argument shows that Statement 1.1 is true for linear combinations of more than two solutions.

The remarkable thing is that every solution to a homogeneous system is a linear combination of certain
particular solutions and, in fact, these solutions are easily computed using the gaussian algorithm. Here is
an example.

Example 1.3.5

Solve the homogeneous system with coefﬁcient matrix

A =

1
3
−
2
−





−

2 3
6 1
4 4

2
−
0
2 


−

Solution. The reduction of the augmented matrix to reduced form is

1
3
−
2
−





−

−

2 3
6 1
4 4

2 0
0 0
2 0 

5t, x2 = s, x3 = 3

−

→

1

0

0

2 0

−

0 1

0 0

−

−

1
5 0
3
5 0
0 0















so the solutions are x1 = 2s + 1
can write the general solution x in the matrix form

5t, and x4 = t by gaussian elimination. Hence we

x1
x2
x3
x4

x = 







= 









2s + 1
5t
s
3
5t
t

1
5
0
3
5
1

2
1
0
0



+ t 











= s 











= sx1 + tx2.





2
1
0
0



and x2 = 









Here x1 = 





1
5
0
3
5
1

are particular solutions determined by the gaussian algorithm.







The solutions x1 and x2 in Example 1.3.5 are denoted as follows:

24

Systems of Linear Equations

Deﬁnition 1.5 Basic Solutions

Thegaussianalgorithmsystematicallyproducessolutionstoanyhomogeneouslinearsystem,
calledbasicsolutions,oneforeveryparameter.

Moreover, the algorithm gives a routine way to express every solution as a linear combination of basic
solutions as in Example 1.3.5, where the general solution x becomes

x = s 



+ t 



= s 

+ 1





2
1
0
0

1
5
0
3
5
1

2
1
0
0

1
0
3
5

5t 







Hence by introducing a new parameter r = t/5 we can multiply the original basic solution x2 by 5 and so
eliminate fractions. For this reason:

























Convention:
Any nonzero scalar multiple of a basic solution will still be called a basic solution.

In the same way, the gaussian algorithm produces basic solutions to every homogeneous system, one
for each parameter (there are no basic solutions if the system has only the trivial solution). Moreover every
solution is given by the algorithm as a linear combination of these basic solutions (as in Example 1.3.5).
If A has rank r, Theorem 1.2.2 shows that there are exactly n
r basic solutions.
This proves:

r parameters, and so n

−

−

Theorem 1.3.2

Let A bean m
coefﬁcientmatrix. Then:

×

n matrixofrank r,andconsiderthehomogeneoussystemin n variableswith A as

1. Thesystemhasexactly n

−

r basicsolutions,oneforeachparameter.

2. Everysolutionisalinearcombinationofthesebasicsolutions.

Example 1.3.6

Find basic solutions of the homogeneous system with coefﬁcient matrix A, and express every
solution as a linear combination of the basic solutions, where

1
2
−
3
3
−

A = 





3
−
6
9
−
9

0 2
1 2
1 0
2 6

−

2
5
−
7
8
−







Solution. The reduction of the augmented matrix to reduced row-echelon form is

1.3. Homogeneous Equations

25

3
−
6
9
−
9

0 2
1 2
1 0
2 6

−

1
2
−
3
3
−

2 0
5 0
7 0
8 0

−











→

1
0
0
0

−

3 0 2
0 1 6
0 0 0
0 0 0

2 0
1 0
0 0
0 0

−







−


2t, x2 = r, x3 =




6s + t, x4 = s, and x5 = t where r, s, and

−

2s
so the general solution is x1 = 3r
t are parameters. In matrix form this is

−

−



x =

x1
x2
x3
x4
x5







Hence basic solutions are

=

















x1 =

3r

−

−

−

2s
r
6s + t
s
t

2t

3
1
0
0
0

2
−
0
6
−
1
0

2
−
0
1
0
1

















+ t 














+ s 














= r 






, x2 =

3
1
0
0
0

















, x3 =

2
−
0
6
−
1
0

















2
−
0
1
0
1

















Exercises for 1.3

Exercise 1.3.1 Consider the following statements about
a system of linear equations with augmented matrix A. In
each case either prove the statement or give an example
for which it is false.

f. If there exists a solution, there are inﬁnitely many

solutions.

g. If there exist nontrivial solutions, the row-echelon

form of A has a row of zeros.

a. If the system is homogeneous, every solution is

h. If the row-echelon form of A has a row of zeros,

trivial.

there exist nontrivial solutions.

b. If the system has a nontrivial solution, it cannot be

i. If a row operation is applied to the system, the new

homogeneous.

system is also homogeneous.

c. If there exists a trivial solution, the system is ho-

mogeneous.

d. If the system is consistent, it must be homoge-

neous.

Now assume that the system is homogeneous.

e. If there exists a nontrivial solution, there is no triv-

ial solution.

Exercise 1.3.2 In each of the following, ﬁnd all values
of a for which the system has nontrivial solutions, and
determine all solutions in each case.

a.

c.

x
−
x + ay
x + 6y

2y + z = 0
3z = 0
5z = 0

−
−

−
x + y
ay

z = 0
−
z = 0
−
x + y + az = 0

b.

d.

x + 2y + z = 0
x + 3y + 6z = 0
2x + 3y + az = 0

ax + y + z = 0
z = 0
x + y
−
x + y + az = 0

26

Systems of Linear Equations

Exercise 1.3.3 Let x =

, y =

2
1
1

−









1
0
1





, and





z =



. In each case, either write v as a linear com-



(cid:26)

1
1
2

−



bination of x, y, and z, or show that it is not such a linear

combination.

a.

v =

c.

v =









0
1
3

−
3
1
0









b.

v =

d.

v =





4
3
4





−
3
0
3









Exercise 1.3.4 In each case, either express y as a linear
combination of a1, a2, and a3, or show that it is not such
a linear combination. Here:

−

1
3
0
1

a1 = 







, a2 = 









3
1
2
0

1
2
4
0







a.

y = 





1
1
1
1









, and a3 = 





b.

y = 















−

1
9
2
6

Exercise 1.3.5 For each of the following homogeneous
systems, ﬁnd a set of basic solutions and express the gen-
eral solution as a linear combination of these basic solu-
tions.

a.

b.

c.

d.

x1 + 2x2
x1 + 2x2 + 2x3
2x1 + 4x2

x3 + 2x4 + x5 = 0
+ x5 = 0
2x3 + 3x4 + x5 = 0

−

−

x1 + 2x2
x1
x1

x3 + x4 + x5 = 0
−
2x2 + 2x3
+ x5 = 0
2x2 + 3x3 + x4 + 3x5 = 0

−
−
−
−
x1 + x2
x1 + 2x2
2x1 + 3x2
4x1 + 5x2

−
−
−
−
x1 + x2
2x1 + 2x2
x1
2x1

−
−

−

x3 + 2x4 + x5 = 0
x3 + x4 + x5 = 0
x3 + 2x4 + x5 = 0
2x3 + 5x4 + 2x5 = 0

−
−

2x3
4x3

2x4 + 2x5 = 0
4x4 + x5 = 0
x2 + 2x3 + 4x4 + x5 = 0
4x2 + 8x3 + 10x4 + x5 = 0

−
−

Exercise 1.3.6

a. Does Theorem 1.3.1 imply that

the system

z + 3y = 0
6y = 0

−
2x

−

has nontrivial solutions? Explain.

b. Show that the converse to Theorem 1.3.1 is not
true. That is, show that the existence of nontrivial
solutions does not imply that there are more vari-
ables than equations.

Exercise 1.3.7 In each case determine how many solu-
tions (and how many parameters) are possible for a ho-
mogeneous system of four linear equations in six vari-
ables with augmented matrix A. Assume that A has
nonzero entries. Give all possibilities.

a.

c.

Rank A = 2.

b.

Rank A = 1.

A has a row of zeros.

d.

The row-echelon form of A has a row of zeros.

Exercise 1.3.8 The graph of an equation ax+by+cz = 0
is a plane through the origin (provided that not all of a,
b, and c are zero). Use Theorem 1.3.1 to show that two
planes through the origin have a point in common other
than the origin (0, 0, 0).

Exercise 1.3.9

a. Show that there is a line through any pair of points
[Hint: Every line has equation
in the plane.
ax + by + c = 0, where a, b, and c are not all zero.]

b. Generalize and show that there is a plane ax+by+
cz + d = 0 through any three points in space.

Exercise 1.3.10 The graph of

a(x2 + y2) + bx + cy + d = 0

is a circle if a
three points in the plane that are not all on a line.

= 0. Show that there is a circle through any

Exercise 1.3.11 Consider a homogeneous system of lin-
ear equations in n variables, and suppose that the aug-
mented matrix has rank r. Show that the system has non-
trivial solutions if and only if n > r.

Exercise 1.3.12 If a consistent (possibly nonhomoge-
neous) system of linear equations has more variables than
equations, prove that it has more than one solution.

6
1.4. An Application to Network Flow

27

1.4 An Application to Network Flow

There are many types of problems that concern a network of conductors along which some sort of ﬂow
is observed. Examples of these include an irrigation network and a network of streets or freeways. There
are often points in the system at which a net ﬂow either enters or leaves the system. The basic principle
behind the analysis of such systems is that the total ﬂow into the system must equal the total ﬂow out. In
fact, we apply this principle at every junction in the system.

Junction Rule

Ateachofthejunctionsinthenetwork,thetotalﬂowintothatjunctionmustequalthetotalﬂow
out.

This requirement gives a linear equation relating the ﬂows in conductors emanating from the junction.

Example 1.4.1

A network of one-way streets is shown in the accompanying diagram. The rate of ﬂow of cars into
intersection A is 500 cars per hour, and 400 and 100 cars per hour emerge from B and C,
respectively. Find the possible ﬂows along each street.

500

A

f2

f3

f1

D

f4

f6

f5

C

100

B

400

Solution. Suppose the ﬂows along the streets are f1, f2, f3, f4,
f5, and f6 cars per hour in the directions shown.
Then, equating the ﬂow in with the ﬂow out at each intersection,
we get

Intersection A
Intersection B
Intersection C
Intersection D

500 = f1 + f2 + f3

f1 + f4 + f6 = 400

f3 + f5 = f6 + 100

f2 = f4 + f5

These give four equations in the six variables f1, f2, . . . , f6.

f1 + f2 + f3
f1

f3

f2

+ f4

f4

−

+ f5
f5

−

= 500
+ f6 = 400
f6 = 100
= 0

−

The reduction of the augmented matrix is

1 1 1
1 0 0
0 0 1
0 1 0

0
1
0
1
−

0
0
1
1
−

0 500
1 400
1 100
0
0

−









→











1 0 0
0 1 0
0 0 1
0 0 0

1
1
−
0
0

0
1
−
1
0

1 400
0
0
1 100
−
0
0

Hence, when we use f4, f5, and f6 as parameters, the general solution is







f1 = 400

f4

−

f6

−

f2 = f4 + f5

f3 = 100

f5 + f6

−

This gives all solutions to the system of equations and hence all the possible ﬂows.

28

Systems of Linear Equations

Of course, not all these solutions may be acceptable in the real situation. For example, the ﬂows
f1, f2, . . . , f6 are all positive in the present context (if one came out negative, it would mean trafﬁc
ﬂowed in the opposite direction). This imposes constraints on the ﬂows: f1
0 become

0 and f3

≥

≥

Further constraints might be imposed by insisting on maximum values on the ﬂow in each street.

f4 + f6

400

≤

f5

−

f6

≤

100

Exercises for 1.4

Exercise 1.4.1 Find the possible ﬂows in each of the fol-
lowing networks of pipes.

a.

b.

f1

f4

f1

f5

50

f3

50

25

75

f3

f4

f6

40

f2

f5

f2

f7

60

60

40

50

55

A

20
f1

B

f4

f3

f2

15
C

f5

D
20

a. Find the possible ﬂows.

b. If canal BC is closed, what range of ﬂow on AD
must be maintained so that no canal carries a ﬂow
of more than 30?

Exercise 1.4.3 A trafﬁc circle has ﬁve one-way streets,
and vehicles enter and leave as shown in the accompany-
ing diagram.

f4

35
E

D

f5

50

A

f1

B

30

25

f3

C
f2

40

Exercise 1.4.2 A proposed network of irrigation canals
is described in the accompanying diagram. At peak de-
mand, the ﬂows at interchanges A, B, C, and D are as
shown.

a. Compute the possible ﬂows.

b. Which road has the heaviest ﬂow?

1.5. An Application to Electrical Networks

29

1.5 An Application to Electrical Networks7

In an electrical network it is often necessary to ﬁnd the current in amperes (A) ﬂowing in various parts of
the network. These networks usually contain resistors that retard the current. The resistors are indicated
by a symbol (
), and the resistance is measured in ohms (Ω). Also, the current is increased at various
points by voltage sources (for example, a battery). The voltage of these sources is measured in volts (V),
and they are represented by the symbol (
). We assume these voltage sources have no resistance. The
ﬂow of current is governed by the following principles.

Ohm’s Law

Thecurrent I andthevoltagedropV acrossaresistance R arerelatedbytheequationV = RI.

Kirchhoff’s Laws

1. (JunctionRule)Thecurrentﬂowintoajunctionequalsthecurrentﬂowoutofthatjunction.

2. (CircuitRule)Thealgebraicsumofthevoltagedrops(duetoresistances)aroundanyclosed

circuitofthenetworkmustequalthesumofthevoltageincreasesaroundthecircuit.

When applying rule 2, select a direction (clockwise or counterclockwise) around the closed circuit and
then consider all voltages and currents positive when in this direction and negative when in the opposite
direction. This is why the term algebraic sum is used in rule 2. Here is an example.

Example 1.5.1

Find the various currents in the circuit shown.

Solution.

10Ω

A

I3

10 V

20Ω

5 V

20 V

I2

5Ω

B

D

First apply the junction rule at junctions A, B, C, and D to obtain

Junction A
Junction B
Junction C I2 + I4 = I6
Junction D I3 + I5 = I4

I1 = I2 + I3
I6 = I1 + I5

I1

I4

I6

C

5Ω

10 V

Note that these equations are not independent
(in fact, the third is an easy consequence of the other three).
Next, the circuit rule insists that the sum of the voltage increases
(due to the sources) around a closed circuit must equal the sum of
the voltage drops (due to resistances). By Ohm’s law, the voltage
loss across a resistance R (in the direction of the current I) is RI. Going counterclockwise around
three closed circuits yields

I5

7This section is independent of Section 1.4

30

Systems of Linear Equations

Upper left
Upper right
Lower

10 + 5 = 20I1
5 + 20 = 10I3 + 5I4
5I4

10 =

5I5

−

−

−

−

Hence, disregarding the redundant equation obtained at junction C, we have six equations in the
six unknowns I1, . . . , I6. The solution is

I1 = 15
20
1
I2 = −
20
I3 = 16
20
The fact that I2 is negative means, of course, that this current is in the opposite direction, with a
magnitude of 1

I4 = 28
20
I5 = 12
20
I6 = 27
20

20 amperes.

Exercises for 1.5

In Exercises 1 to 4, ﬁnd the currents in the circuits.

Exercise 1.5.1

20 V

6Ω

4Ω

2Ω

I1

I2

I3

10 V

Exercise 1.5.3

5 V

I1

I3

20 V

20Ω

5 V

5 V

I6

I2

I4

10Ω

I5

20Ω

10Ω

10 V

Exercise 1.5.2

Exercise 1.5.4 All resistances are 10Ω.

5 V

5Ω

I2
I1

5Ω

10Ω

I3

10 V

Exercise 1.5.5
Find the voltage x such that the current I1 = 0.

I6

I4

I2
10 V

I3

I5

I1

20 V

2Ω

1Ω

2 V

I3

I1

1Ω

I2

x V

5 V

1.6. An Application to Chemical Reactions

31

1.6 An Application to Chemical Reactions

When a chemical reaction takes place a number of molecules combine to produce new molecules. Hence,
when hydrogen H2 and oxygen O2 molecules combine, the result is water H2O. We express this as

H2 + O2

H2O

→

Individual atoms are neither created nor destroyed, so the number of hydrogen and oxygen atoms going
into the reaction must equal the number coming out (in the form of water). In this case the reaction is
said to be balanced. Note that each hydrogen molecule H2 consists of two atoms as does each oxygen
molecule O2, while a water molecule H2O consists of two hydrogen atoms and one oxygen atom. In the
above reaction, this requires that twice as many hydrogen molecules enter the reaction; we express this as
follows:

This is now balanced because there are 4 hydrogen atoms and 2 oxygen atoms on each side of the reaction.

2H2 + O2

2H2O

→

Example 1.6.1

Balance the following reaction for burning octane C8H18 in oxygen O2:

where CO2 represents carbon dioxide. We must ﬁnd positive integers x, y, z, and w such that

C8H18 + O2

CO2 + H2O

→

→
Equating the number of carbon, hydrogen, and oxygen atoms on each side gives 8x = z, 18x = 2w
and 2y = 2z + w, respectively. These can be written as a homogeneous linear system

xC8H18 + yO2

zCO2 + wH2O

8x
18x

z

−

2y

2z

= 0
2w = 0
w = 0

−
−

−
which can be solved by gaussian elimination. In larger systems this is necessary but, in such a
simple situation, it is easier to solve directly. Set w = t, so that x = 1
9 t + t = 25
9 t.
But x, y, z, and w must be positive integers, so the smallest value of t that eliminates fractions is 18.
Hence, x = 2, y = 25, z = 16, and w = 18, and the balanced reaction is

9t, 2y = 16

9t, z = 8

→
The reader can verify that this is indeed balanced.

2C8H18 + 25O2

16CO2 + 18H2O

It is worth noting that this problem introduces a new element into the theory of linear equations: the

insistence that the solution must consist of positive integers.

32

Systems of Linear Equations

Exercises for 1.6

In each case balance the chemical reaction.

Exercise 1.6.1 CH4 + O2
burning of methane CH4.

→

CO2 + H2O. This is the

Exercise 1.6.2 NH3 + CuO
N2 + Cu + H2O. Here
NH3 is ammonia, CuO is copper oxide, Cu is copper,
and N2 is nitrogen.

→

Exercise 1.6.3 CO2 + H2O
C6H12O6 + O2. This
is called the photosynthesis reaction—C6H12O6 is glu-
cose.

→

Exercise 1.6.4
MnO2 + Pb3O4 + NO.

Pb(N3)2 + Cr(MnO4)2

Cr2O3 +

→

Supplementary Exercises for Chapter 1

Exercise 1.1 We show in Chapter 4 that the graph of an
equation ax + by + cz = d is a plane in space when not all
of a, b, and c are zero.

a. By examining the possible positions of planes in
space, show that three equations in three variables
can have zero, one, or inﬁnitely many solutions.

b. Can two equations in three variables have a unique

solution? Give reasons for your answer.

Exercise 1.2 Find all solutions to the following systems
of linear equations.

a.

b.

−

−

x1 + x2 + x3
3x1 + 5x2
3x1
−
x1 + 3x2

x4 = 3
2x3 + x4 = 1
5x4 = 7
5

−
4x3 + 3x4 =

−
7x2 + 7x3

−

−
x3 + x4 = 2
x1 + 4x2
3x1 + 2x2 + x3 + 2x4 = 5
x1
= 1
−
5x3 + 2x4 = 3
x1 + 14x2

6x2 + 3x3

−

−

Exercise 1.3 In each case ﬁnd (if possible) conditions
on a, b, and c such that the system has zero, one, or in-
ﬁnitely many solutions.

a.

4z =
x + 2y
4
3x
y + 13z =
2
4x + y + a2z = a + 3

−

−

b.

x + y + 3z = a
ax + y + 5z = 4
x + ay + 4z = a

Exercise 1.4 Show that any two rows of a matrix can be
interchanged by elementary row transformations of the
other two types.

Exercise 1.5 If ad

= bc, show that

duced row-echelon form

1 0
0 1

.

(cid:21)

(cid:20)

Exercise 1.6 Find a, b, and c so that the system

a b
c d

(cid:20)

(cid:21)

has re-

x + ay + cz = 0
bx + cy
3z = 1
−
ax + 2y + bz = 5

has the solution x = 3, y =

1, z = 2.

−

Exercise 1.7 Solve the system

x + 2y + 2z =
2x + y + z =
x

3
−
4
−
y + iz = i

−

1. [See Appendix A.]

where i2 =

−

Exercise 1.8 Show that the real system

x + y + z = 5
z = 1
2x
−
3x + 2y + 2z = 0

−

y




−

i where
1. Explain. What happens when such a real sys-

has a complex solution: x = 2, y = i, z = 3
i2 =
tem has a unique solution?



−

−

Exercise 1.9 A man is ordered by his doctor to take 5
units of vitamin A, 13 units of vitamin B, and 23 units
of vitamin C each day. Three brands of vitamin pills are
available, and the number of units of each vitamin per
pill are shown in the accompanying table.

Vitamin
Brand A B C
4
1
3
1
1
0

1
2
3

2
1
1

6
a. Find all combinations of pills that provide exactly
the required amount of vitamins (no partial pills
allowed).

b. If brands 1, 2, and 3 cost 3¢, 2¢, and 5¢ per pill,
respectively, ﬁnd the least expensive treatment.

Exercise 1.10 A restaurant owner plans to use x tables
seating 4, y tables seating 6, and z tables seating 8, for a
total of 20 tables. When fully occupied, the tables seat
108 customers. If only half of the x tables, half of the y
tables, and one-fourth of the z tables are used, each fully
occupied, then 46 customers will be seated. Find x, y,
and z.

Exercise 1.11

1.6. An Application to Chemical Reactions

33

[Hint: The leading 1 in the ﬁrst row must be in
column 1 or 2 or not exist.]

b. List the seven reduced row-echelon forms for ma-

trices with two rows and three columns.

c. List the four reduced row-echelon forms for ma-

trices with three rows and two columns.

Exercise 1.12 An amusement park charges $7 for
adults, $2 for youths, and $0.50 for children. If 150 peo-
ple enter and pay a total of $100, ﬁnd the numbers of
adults, youths, and children. [Hint: These numbers are
nonnegative integers.]

Exercise 1.13 Solve the following system of equations
for x and y.

a. Show that a matrix with two rows and two
columns that is in reduced row-echelon form must
have one of the following forms:

y2 = 1
x2 + xy
−
2x2
xy + 3y2 = 13
x2 + 3xy + 2y2 = 0

−

1 0
0 1

0 1
0 0

0 0
0 0

1
∗0 0

(cid:21)

(cid:21) (cid:20)

(cid:21) (cid:20)

(cid:21) (cid:20)

(cid:20)

[Hint: These equations are linear in the new variables
x1 = x2, x2 = xy, and x3 = y2.]

Chapter 2

Matrix Algebra

In the study of systems of linear equations in Chapter 1, we found it convenient to manipulate the aug-
mented matrix of the system. Our aim was to reduce it to row-echelon form (using elementary row oper-
ations) and hence to write down all solutions to the system. In the present chapter we consider matrices
for their own sake. While some of the motivation comes from linear equations, it turns out that matrices
can be multiplied and added and so form an algebraic system somewhat analogous to the real numbers.
This “matrix algebra” is useful in ways that are quite different from the study of linear equations. For
example, the geometrical transformations obtained by rotating the euclidean plane about the origin can be
viewed as multiplications by certain 2
2 matrices. These “matrix transformations” are an important tool
in geometry and, in turn, the geometry provides a “picture” of the matrices. Furthermore, matrix algebra
has many other applications, some of which will be explored in this chapter. This subject is quite old and
was ﬁrst studied systematically in 1858 by Arthur Cayley.1

×

2.1 Matrix Addition, Scalar Multiplication, and

Transposition

A rectangular array of numbers is called a matrix (the plural is matrices), and the numbers are called the
entries of the matrix. Matrices are usually denoted by uppercase letters: A, B, C, and so on. Hence,

A =

(cid:20)

1 2
0 5

1
−
6

B =

(cid:21)

(cid:20)

1
0

1
−
2

(cid:21)

C =



1
3
2 




are matrices. Clearly matrices come in various shapes depending on the number of rows and columns.
For example, the matrix A shown has 2 rows and 3 columns. In general, a matrix with m rows and n
nnn. Thus matrices A, B, and C above have
columns is referred to as an mmm
n is called a row matrix, whereas one of
sizes 2
size m

3, 2
1 is called a column matrix. Matrices of size n

nnn matrix or as having size mmm
1, respectively. A matrix of size 1

n for some n are called square matrices.

2, and 3

×
×

×

×

×

Each entry of a matrix is identiﬁed by the row and column in which it lies. The rows are numbered
from the top down, and the columns are numbered from left to right. Then the (((iii,,, jjj)))-entry of a matrix is
the number lying simultaneously in row i and column j. For example,

×
×

×

The (1, 2)-entry of

(cid:20)

The (2, 3)-entry of

is

1
0

1
−
1

1 2
0 5

(cid:21)
1
−
6

1.

−

is 6.

(cid:20)
1Arthur Cayley (1821-1895) showed his mathematical talent early and graduated from Cambridge in 1842 as senior wran-
gler. With no employment in mathematics in view, he took legal training and worked as a lawyer while continuing to do
mathematics, publishing nearly 300 papers in fourteen years. Finally, in 1863, he accepted the Sadlerian professorship in Cam-
bridge and remained there for the rest of his life, valued for his administrative and teaching skills as well as for his scholarship.
His mathematical achievements were of the ﬁrst rank. In addition to originating matrix theory and the theory of determinants,
he did fundamental work in group theory, in higher-dimensional geometry, and in the theory of invariants. He was one of the
most proliﬁc mathematicians of all time and produced 966 papers.

(cid:21)

35

36

Matrix Algebra

A special notation is commonly used for the entries of a matrix. If A is an m

(i, j)-entry of A is denoted as ai j, then A is displayed as follows:

n matrix, and if the

×

A = 




ai j

This is usually denoted simply as A =
a 3

4 matrix in this notation is written
(cid:2)

×

(cid:3)

a13
a12
a11
a23
a22
a21
...
...
...
am1 am2 am3

a1n
a2n
...
amn

· · ·
· · ·

· · ·








. Thus ai j is the entry in row i and column j of A. For example,

A =



a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34



It is worth pointing out a convention regarding rows and columns: Rows are mentioned before columns.
For example:





• If a matrix has size m

×

n, it has m rows and n columns.

• If we speak of the (i, j)-entry of a matrix, it lies in row i and column j.

• If an entry is denoted ai j, the ﬁrst subscript i refers to the row and the second subscript j to the

column in which ai j lies.

Two points (x1, y1) and (x2, y2) in the plane are equal if and only if2 they have the same coordinates,
that is x1 = x2 and y1 = y2. Similarly, two matrices A and B are called equal (written A = B) if and only if:

1. They have the same size.

2. Corresponding entries are equal.

If the entries of A and B are written in the form A =
condition takes the following form:

ai j

, B =

bi j

, described earlier, then the second

(cid:2)

(cid:3)

(cid:2)

(cid:3)

A =

ai j

=

bi j

means ai j = bi j for all i and j

(cid:2)

(cid:3)

(cid:2)

(cid:3)

Example 2.1.1

Given A =

B = C, A = C.

(cid:20)

a b
c d

, B =

(cid:21)

(cid:20)

1 2
3 0

1
−
1

(cid:21)

and C =

1 0
1 2

−

(cid:20)

(cid:21)

discuss the possibility that A = B,

Solution. A = B is impossible because A and B are of different sizes: A is 2
Similarly, B = C is impossible. But A = C is possible provided that corresponding entries are

2 whereas B is 2

×

3.

×

2If p and q are statements, we say that p implies q if q is true whenever p is true. Then “p if and only if q” means that both

p implies q and q implies p. See Appendix B for more on this.

2.1. Matrix Addition, Scalar Multiplication, and Transposition

37

equal:

a b
c d

=

(cid:21)

(cid:20)

1 0
1 2

−

(cid:21)

(cid:20)

means a = 1, b = 0, c =

1, and d = 2.

−

Matrix Addition

Deﬁnition 2.1 Matrix Addition

If A and B arematricesofthesamesize,theirsum A + B isthematrixformedbyadding
correspondingentries.

If A =

ai j

and B =

bi j

, this takes the form

(cid:2)

(cid:3)

(cid:2)

(cid:3)

A + B =

ai j + bi j

(cid:3)
Note that addition is not deﬁned for matrices of different sizes.

(cid:2)

Example 2.1.2

If A =

2 1 3
1 2 0

−

(cid:20)

and B =

(cid:21)

(cid:20)

1 1
2 0

1
−
6

(cid:21)

, compute A + B.

Solution.

A + B =

(cid:20)

2 + 1 1 + 1 3
1
1 + 2 2 + 0 0 + 6

−

−

=

(cid:21)

(cid:20)

3 2 2
1 2 6

(cid:21)

Example 2.1.3

Find a, b, and c if

a b c

+

c a b

=

3 2

Solution. Add the matrices on the left side to obtain

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:2)

1
−

.

(cid:3)

a + c b + a c + b

=

3 2

1
−

Because corresponding entries must be equal, this gives three equations: a + c = 3, b + a = 2, and
c + b =

1. Solving these yields a = 3, b =

1, c = 0.

(cid:3)

(cid:2)

(cid:3)

(cid:2)

−

−

If A, B, and C are any matrices of the same size, then

A + B = B + A
A + (B +C) = (A + B) +C

(commutative law)
(associative law)

In fact, if A =
bi j + ai j. Since these are equal for all i and j, we get

and B =

bi j

ai j

(cid:2)

(cid:3)

(cid:2)

(cid:3)

, then the (i, j)-entries of A + B and B + A are, respectively, ai j + bi j and

A + B =

ai j + bi j

=

bi j + ai j

= B + A

(cid:2)

(cid:3)

(cid:2)

(cid:3)

38

Matrix Algebra

The associative law is veriﬁed similarly.

The m

n matrix in which every entry is zero is called the m

0mn if it is important to emphasize the size). Hence,

×

n zero matrix and is denoted as 0 (or

×

0 + X = X

holds for all m
matrix obtained by multiplying each entry of A by

n matrices X . The negative of an m

×

n matrix A (written
ai j

×
1. If A =

, this becomes

−

A) is deﬁned to be the m

n

A =

−

ai j

−

×
. Hence,

−

A + (

A) = 0

−

(cid:2)

(cid:3)

(cid:2)

(cid:3)

holds for all matrices A where, of course, 0 is the zero matrix of the same size as A.

A closely related notion is that of subtracting matrices.

If A and B are two m

difference A

−

B is deﬁned by

Note that if A =

ai j

and B =

bi j

, then

A

−

B = A + (

B)

−

n matrices, their

×

(cid:2)

(cid:3)

(cid:2)

(cid:3)
A

B =

ai j

+

bi j

=

ai j

−
n matrix formed by subtracting corresponding entries.

−

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

bi j

−

(cid:3)

is the m

×

Example 2.1.4

3
1

1
−
2

0
4
−

(cid:21)

, B =

1
2
−

(cid:20)

1 1
0 6

−

, C =

(cid:21)

(cid:20)

1 0
3 1

2
−
1

. Compute

(cid:21)

A, A

−

−

B, and

Let A =

A + B

(cid:20)
C.

−

Solution.

A =

−

B =

A

−

A + B

C =

−

(cid:20)

(cid:20)

(cid:20)

1 0
2 4

3
−
1
−
3
1

1
−
(
−
3 + 1
2
1

−

−

−

2)
1
3

−
−

(

(cid:21)
1
−
−
2
−
1
1
−
−
2 + 0

1)

1
6

0
−
4
−
−
0 0 + 1
1

−
4 + 6

−

−
0

−
−

=

(cid:21)
(

(cid:20)
2)
−
1

−

2 0
3 2

=

(cid:21)

(cid:20)

1
−
10

−
3
4
−

(cid:21)
2 3
1 1

−

(cid:21)

Example 2.1.5

Solve

3 2
1 1

−

(cid:21)

(cid:20)

+ X =

1 0
1 2

−

(cid:21)

(cid:20)

where X is a matrix.

Solution. We solve a numerical equation a + x = b by subtracting the number a from both sides to

obtain x = b

−

a. This also works for matrices. To solve

3 2
1 1

−

(cid:21)

(cid:20)

+ X =

1 0
1 2

−

(cid:21)

(cid:20)

simply

2.1. Matrix Addition, Scalar Multiplication, and Transposition

39

subtract the matrix

3 2
1 1

−

(cid:20)

(cid:21)

from both sides to get

X =

1 0
1 2

−

3 2
1 1

=

3

1
1
−

−
(
−

0
1) 2

2
1

−
−

=

2
−
0

2
−
1

(cid:21)
(cid:21)
The reader should verify that this matrix X does indeed satisfy the original equation.

−

−

−

(cid:20)

(cid:20)

(cid:21)

(cid:20)

(cid:20)

(cid:21)

tion: X = B

The solution in Example 2.1.5 solves the single matrix equation A + X = B directly via matrix subtrac-
A. This ability to work with matrices as entities lies at the heart of matrix algebra.
It is important to note that the sizes of matrices involved in some calculations are often determined by

−

the context. For example, if

A +C =

1 3
2 0

1
−
1

(cid:20)

(cid:21)

then A and C must be the same size (so that A +C makes sense), and that size must be 2
sum is 2
context.

3 (so that the
3). For simplicity we shall often omit reference to such facts when they are clear from the

×

×

Scalar Multiplication

In gaussian elimination, multiplying a row of a matrix by a number k means multiplying every entry of
that row by k.

Deﬁnition 2.2 Matrix Scalar Multiplication

Moregenerally,if A isanymatrixand k isanynumber,thescalarmultiple kA isthematrix
obtainedfrom A bymultiplyingeachentryof A by k.

If A =

ai j

, this is

(cid:2)

(cid:3)

Thus 1A = A and (

1)A =

A for any matrix A.

−

−

kA =

kai j

(cid:2)

(cid:3)

The term scalar arises here because the set of numbers from which the entries are drawn is usually
referred to as the set of scalars. We have been using real numbers as scalars, but we could equally well
have been using complex numbers.

Example 2.1.6

3
2

1 4
0 6

−

If A =

(cid:20)

and B =

(cid:21)

(cid:20)

1 2
0 3

1
−
2

Solution.

compute 5A, 1

2 B, and 3A

2B.

−

5A =

3A

−

2B =

15
10

9
6

(cid:20)

(cid:20)

5 20
0 30

−

3 12
0 18

−

1
2B =

2 4
0 6

1
1
2
0 3
2
2
−
4

(cid:20)

=

(cid:21)

1
2
1

−

7
6

(cid:20)

(cid:21)

−
−

7 14
6 14

(cid:21)

(cid:21)

,

(cid:21)

−

(cid:21)

(cid:20)

40

Matrix Algebra

If A is any matrix, note that kA is the same size as A for all scalars k. We also have

0A = 0

and

k0 = 0

because the zero matrix has every entry zero. In other words, kA = 0 if either k = 0 or A = 0. The converse
of this statement is also true, as Example 2.1.7 shows.

Example 2.1.7

If kA = 0, show that either k = 0 or A = 0.

Solution. Write A =
do. If k

(cid:2)

(cid:3)

ai j

so that kA = 0 means kai j = 0 for all i and j. If k = 0, there is nothing to

= 0, then kai j = 0 implies that ai j = 0 for all i and j; that is, A = 0.

For future reference, the basic properties of matrix addition and scalar multiplication are listed in

Theorem 2.1.1.

Theorem 2.1.1

Let A, B,andC denotearbitrary m
arbitraryrealnumbers. Then

×

1. A + B = B + A.

2. A + (B +C) = (A + B) +C.

n matriceswhere m and n areﬁxed. Let k and p denote

3. Thereisan m

×

n matrix 0,suchthat 0 + A = A foreach A.

4. Foreach A thereisan m

n matrix,

×

A,suchthat A + (

−

A) = 0.

−

5. k(A + B) = kA + kB.

6. (k + p)A = kA + pA.

7. (kp)A = k(pA).

8. 1A = A.

Proof. Properties 1–4 were given previously. To check Property 5, let A =
matrices of the same size. Then A + B =

ai j + bi j

, as before, so the (i, j)-entry of k(A + B) is
(cid:2)

(cid:3)

(cid:2)

(cid:3)

ai j

and B =

bi j

denote

(cid:2)
k(ai j + bi j) = kai j + kbi j

(cid:3)

But this is just the (i, j)-entry of kA + kB, and it follows that k(A + B) = kA + kB. The other Properties
can be similarly veriﬁed; the details are left to the reader.

The Properties in Theorem 2.1.1 enable us to do calculations with matrices in much the same way that

numerical calculations are carried out. To begin, Property 2 implies that the sum

(A + B) +C = A + (B +C)

6
2.1. Matrix Addition, Scalar Multiplication, and Transposition

41

is the same no matter how it is formed and so is written as A + B +C. Similarly, the sum

A + B +C + D

is independent of how it is formed; for example, it equals both (A + B) + (C + D) and A + [B + (C + D)].
Furthermore, property 1 ensures that, for example,

B + D + A +C = A + B +C + D

In other words, the order in which the matrices are added does not matter. A similar remark applies to
sums of ﬁve (or more) matrices.

Properties 5 and 6 in Theorem 2.1.1 are called distributive laws for scalar multiplication, and they

extend to sums of more than two terms. For example,

k(A + B

(k + p

−

C) = kA + kB

−
m)A = kA + pA

kC

mA

−

−

Similar observations hold for more than three summands. These facts, together with properties 7 and
8, enable us to simplify expressions by collecting like terms, expanding, and taking common factors in
exactly the same way that algebraic expressions involving variables and real numbers are manipulated.
The following example illustrates these techniques.

Example 2.1.8

Simplify 2(A + 3C)
matrices of the same size.

−

3(2C

B)

−

−

3 [2(2A + B

4C)

4(A

−

−

−

2C)] where A, B, and C are all

Solution. The reduction proceeds as though A, B, and C were variables.

2(A + 3C)

3(2C

−

= 2A + 6C
= 2A + 3B
3B
= 2A

−
−

−

3 [2(2A + B

−
3 [4A + 2B

B)
−
−
6C + 3B
3 [2B]

−

4C)
8C

−

−
−

4(A
2C)]
−
4A + 8C]

Transpose of a Matrix

Many results about a matrix A involve the rows of A, and the corresponding result for columns is derived
in an analogous way, essentially by replacing the word row by the word column throughout. The following
deﬁnition is made with such applications in mind.

Deﬁnition 2.3 Transpose of a Matrix

If A isan m
columnsof A inthesameorder.

×

n matrix,thetransposeof A,written AT,isthe n

m matrixwhoserowsarejustthe

×

In other words, the ﬁrst row of AT is the ﬁrst column of A (that is it consists of the entries of column 1 in
order). Similarly the second row of AT is the second column of A, and so on.

42

Matrix Algebra

Example 2.1.9

Write down the transpose of each of the following matrices.

A =

1
3
2 






B =

5 2 6

C =

(cid:2)

(cid:3)

1 2
3 4
5 6 






D =

3 1
1 3
1 2

−





1
−
2
1 


Solution.

AT =

1 3 2

, BT =

(cid:2)

(cid:3)

5
2
6 






, CT =

1 3 5
2 4 6

(cid:21)

(cid:20)

, and DT = D.

If A =

. Then bi j is the jth element of the ith row of AT and so is the
jth element of the ith column of A. This means bi j = a ji, so the deﬁnition of AT can be stated as follows:

is a matrix, write AT =

ai j

bi j

(cid:2)

(cid:3)

(cid:2)
(cid:3)
If A =

ai j

, then AT =

a ji

.

(2.1)

(cid:3)
This is useful in verifying the following properties of transposition.

(cid:2)

(cid:3)

(cid:2)

Theorem 2.1.2

Let A and B denotematricesofthesamesize,andlet k denoteascalar.

n matrix,then AT isan n

m matrix.

×

1. If A isan m

×

2. (AT )T = A.

3. (kA)T = kAT.

4. (A + B)T = AT + BT.

Proof. Property 1 is part of the deﬁnition of AT , and Property 2 follows from (2.1). As to Property 3: If
A =

, so (2.1) gives

, then kA =

kai j

ai j

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(kA)T =

ka ji

= k

a ji

= kAT

Finally, if B =

bi j

, then A + B =

ci j

where ci j = ai j + bi j Then (2.1) gives Property 4:

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(A + B)T =

(cid:2)
ci j

T

(cid:3)
=

c ji

=

a ji + b ji

=

a ji

+

b ji

= AT + BT

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

2.1. Matrix Addition, Scalar Multiplication, and Transposition

43

There is another useful way to think of transposition. If A =

n matrix, the elements
a11, a22, a33, . . . are called the main diagonal of A. Hence the main diagonal extends down and to the
right from the upper left corner of the matrix A; it is outlined in the following examples:

is an m

ai j

×

(cid:3)

(cid:2)

a11 a12
a21 a22
a31 a32





(cid:20)

a11 a12 a13
a21 a22 a23

a11 a12 a13
a21 a22 a23
a31 a32 a33

a11
a21

(cid:21)



(cid:20)



(cid:21)









Thus forming the transpose of a matrix A can be viewed as “ﬂipping” A about its main diagonal, or
as “rotating” A through 180◦ about the line containing the main diagonal. This makes Property 2 in
Theorem 2.1.2 transparent.

Example 2.1.10

Solve for A if

2AT

(cid:18)

3

−

(cid:20)

1 2
1 1

−

T

=

(cid:21)(cid:19)

(cid:20)

2 3
1 2

−

.

(cid:21)

Solution. Using Theorem 2.1.2, the left side of the equation is

2AT

3

−

1 2
1 1

(cid:18)
Hence the equation becomes

−

(cid:20)

T

= 2

AT

T

(cid:21)(cid:19)

(cid:0)

(cid:1)

1 2
1 1

−

T

(cid:21)

3

−

(cid:20)

= 2A

3

−

(cid:20)

1
2

1
−
1

(cid:21)

2A

3

−

(cid:20)

1
2

1
−
1

=

(cid:21)

(cid:20)

2 3
1 2

−

(cid:21)

Thus 2A =

2 3
1 2

−

(cid:20)

+ 3

(cid:21)

(cid:20)

1
2

1
−
1

=

(cid:21)

(cid:20)

5 0
5 5

, so ﬁnally A = 1
2
(cid:21)

(cid:20)

5 0
5 5

= 5
2

(cid:21)

(cid:20)

1 0
1 1

.
(cid:21)

Note that Example 2.1.10 can also be solved by ﬁrst transposing both sides, then solving for AT , and so
obtaining A = (AT )T . The reader should do this.
1 2
2 5

in Example 2.1.9 has the property that D = DT . Such matrices are important;

The matrix D =

a matrix A is called symmetric if A = AT . A symmetric matrix A is necessarily square (if A is m
n, then
m, so A = AT forces n = m). The name comes from the fact that these matrices exhibit a symmetry
AT is n
about the main diagonal. That is, entries that are directly across the main diagonal from each other are
equal.

×

×

(cid:20)

(cid:21)

For example,

a
b′ d
e′
c′

b c
e
f 






is symmetric when b = b′, c = c′, and e = e′.

44

Matrix Algebra

Example 2.1.11

If A and B are symmetric n

×

n matrices, show that A + B is symmetric.

Solution. We have AT = A and BT = B, so, by Theorem 2.1.2, we have
(A + B)T = AT + BT = A + B. Hence A + B is symmetric.

Example 2.1.12

Suppose a square matrix A satisﬁes A = 2AT . Show that necessarily A = 0.

Solution. If we iterate the given equation, Theorem 2.1.2 gives

A = 2AT = 2

2AT

T

= 2

2(AT )T

= 4A

Subtracting A from both sides gives 3A = 0, so A = 1

(cid:2)

(cid:3)

(cid:2)

3(0) = 0.

(cid:3)

Exercises for 2.1

Exercise 2.1.1 Find a, b, c, and d if

Exercise 2.1.2 Compute the following:

a b
c d

a.

(cid:20)

=

(cid:21)

(cid:20)

3d

c
d
2a + d a + b

−

−

(cid:21)

b.

b b
d d

a
c

−
−

c
a

−
−

(cid:20)

= 2

(cid:21)

(cid:20)

−

1 1
3 1

(cid:21)

c. 3

a
b

(cid:20)

+ 2

(cid:21)

(cid:20)

b
a

=

(cid:21)

(cid:20)

1
2

(cid:21)

d.

(cid:20)

a b
c d

=

(cid:21)

(cid:20)

b c
d a

(cid:21)

a.

b.

c.

d.

e.

g.

h.

(cid:20)

3

(cid:20)

(cid:2)

(cid:20)

(cid:20)

3

3

1
2

3
2

(cid:20)

3 2 1
5 1 0

−

(cid:21)

5

(cid:20)

(cid:21)
−
2 1
3 2

−

3
1

5

−

(cid:20)

−

(cid:21)
1 2

−

4

(cid:20)

−

(cid:3)

−

5 4 0
1 0 6

3
1

0
1

−

+ 7

−

2
2

(cid:21)

1
1

−

(cid:21)

(cid:20)
6
2

(cid:20)
2
1

+ 3

−
−
9 3 4

(cid:21)

3
2

2
1

−
−
3 11

−

(cid:20)
+

(cid:21)

−
2
4
0

6

T
(cid:3)





f.

(cid:2)
0
1
2

−

1
0
4

(cid:3)




T

−

−

(cid:21)
1
0

2

(cid:2)
T

(cid:21)

1
1

1
−
1

(cid:21)
2 1
1 0

−

2

(cid:20)

−
T

2

−

(cid:20)

(cid:21)

2
−
1

(cid:21)
1
−
3

1
1
1
0

−
−

(cid:21)

,

(cid:21)

,

(cid:21)
1 0 1
0 1 0

1
2

2
0
3
2

(cid:20)

.

(cid:21)

Exercise 2.1.3 Let A =

(cid:20)
, C =

(cid:21)

(cid:20)

, and E =

3
0

−

1 2
1 4

B =

(cid:20)

D =



−



1 3
1 0
1 4





2.1. Matrix Addition, Scalar Multiplication, and Transposition

45

Compute the following (where possible).

Exercise 2.1.9 If A is any 2

2 matrix, show that:

×

a.

c.

e.

2B

3A

−
3E T

4AT

3C

g.

2B

i.

(B

−
3E

−

2E)T

−

5Cb.

d.

B + D

f.

(A +C)T

h.

A

D

−

Exercise 2.1.4 Find A if:

a. 5A

1 0
2 3

(cid:21)

−

(cid:20)

= 3A

5 2
6 1

(cid:21)

−

(cid:20)

b. 3A

2
1

(cid:21)

−

(cid:20)

= 5A

2

−

3
0

(cid:20)

(cid:21)

Exercise 2.1.5 Find A in terms of B if:

a.

A + B = 3A + 2B

b.

2A

−

B = 5(A + 2B)

Exercise 2.1.6 If X , Y , A, and B are matrices of the same
size, solve the following systems of equations to obtain
X and Y in terms of A and B.

a.

5X + 3Y = A
2X +Y = B

b.

4X + 3Y = A
5X + 4Y = B

Exercise 2.1.7 Find all matrices X and Y such that:

a.

3X

−

2Y =

3

1

−

b.

2X

−

5Y =

1 2

(cid:2)

(cid:3)

(cid:2)

(cid:3)

Exercise 2.1.8
where A, B, and C are matrices.

Simplify the following expressions

a. 2 [9(A

B) + 7(2B

−
2 [3(2B + A)

A)]
2(A + 3B)

−

−

−

5(A + B)]

−

b. 5 [3(A

−
+2 [3(3A

B + 2C)

2(3C
−
B +C) + 2(B

−

B)
−
2A)

A]

−

−
−

2C]

a. A = a

(cid:20)
0 0
0 1

d

(cid:20)

b. A = p

(cid:20)
0 1
1 0

s

(cid:20)

1 0
0 0

+ b

(cid:21)

(cid:20)

0 1
0 0

+ c

(cid:21)

(cid:20)

0 0
1 0

+

(cid:21)

for some numbers a, b, c, and d.

(cid:21)
1 0
0 1

+ q

(cid:21)

(cid:20)

1 1
0 0

+ r

(cid:21)

(cid:20)

1 0
1 0

+

(cid:21)

for some numbers p, q, r, and s.

(cid:21)

0 1 2

Exercise 2.1.10 Let A =
B =
, and C =
(cid:2)
rA + sB + tC = 0 for some scalars r, s, and t, show that
(cid:2)
necessarily r = s = t = 0.

1 1
1
−
3 0 1

,
. If
(cid:3)
(cid:3)

(cid:3)

(cid:2)

Exercise 2.1.11

a. If Q + A = A holds for every m

n matrix A, show

×

n matrix and A + A′ = 0mn, show that

that Q = 0mn.

b. If A is an m

A′ =

A.

−

×

Exercise 2.1.12 If A denotes an m
A if and only if A = 0.
A =

−

n matrix, show that

×

Exercise 2.1.13 A square matrix is called a diagonal
matrix if all the entries off the main diagonal are zero. If
A and B are diagonal matrices, show that the following
matrices are also diagonal.

a.

c.

A + B

kA for any number k

b.

A

B

−

Exercise 2.1.14 In each case determine all s and t such
that the given matrix is symmetric:

a.

c.

1 s
2 t

(cid:21)

2s
1
−
s2

−
s
t
t

(cid:20)





st
s
s 


b.

d.

(cid:20)





s
t
st 1

(cid:21)

t

s
2
2s 0 s + t
3
3

t 


Exercise 2.1.15 In each case ﬁnd the matrix A.

a.

A + 3

(cid:18)

(cid:20)

1
1

−

1 0
2 4

2 1
0 5
3 8

T

=

(cid:21)(cid:19)









46

Matrix Algebra

b.

3AT + 2

(cid:18)

(cid:20)

1 0
0 2

T

=

(cid:21)(cid:19)

(cid:20)

8 0
3 1

(cid:21)

c.

2A

−

3

1 2 0

T

= 3AT +

2 1

1

−

d.

2AT

(cid:2)
5

−

(cid:0)

(cid:18)

1 0
1 2

(cid:3)(cid:1)

T

(cid:21)(cid:19)

(cid:20)

−

= 4A

(cid:2)
9

−

1 1
1 0

(cid:21)

(cid:20)

−

T

(cid:3)

Exercise 2.1.16 Let A and B be symmetric (of the same
size). Show that each of the following is symmetric.

a.

(A

B)

−

b.

kA for any scalar k

Exercise 2.1.17 Show that A + AT is symmetric for any
square matrix A.
Exercise 2.1.18 If A is a square matrix and A = kAT
where k

1, show that A = 0.

=

Exercise 2.1.19 In each case either show that the state-
ment is true or give an example showing it is false.

±

a. If A + B = A +C, then B and C have the same size.

b. If A + B = 0, then B = 0.

c. If the (3, 1)-entry of A is 5, then the (1, 3)-entry

of AT is

5.

−

matrix A.

e. If B is symmetric and AT = 3B, then A = 3B.

f. If A and B are symmetric, then kA + mB is sym-

metric for any scalars k and m.

Exercise 2.1.20 A square matrix W is called skew-
symmetric if W T =

W . Let A be any square matrix.

−

a. Show that A

−

AT is skew-symmetric.

b. Find a symmetric matrix S and a skew-symmetric

matrix W such that A = S +W .

c. Show that S and W in part (b) are uniquely deter-

mined by A.

Exercise 2.1.21
If W is skew-symmetric (Exer-
cise 2.1.20), show that the entries on the main diagonal
are zero.

Exercise 2.1.22
rem 2.1.1.

Prove the following parts of Theo-

a.

(k + p)A = kA + pA

b.

(kp)A = k(pA)

Exercise 2.1.23 Let A, A1, A2, . . . , An denote matrices
of the same size. Use induction on n to verify the follow-
ing extensions of properties 5 and 6 of Theorem 2.1.1.

a. k(A1 + A2 +
any number k

· · ·

+ An) = kA1 + kA2 +

+ kAn for

+ knA for

· · ·

· · ·

Exercise 2.1.24 Let A be a square matrix. If A = pBT
and B = qAT for some matrix B and numbers p and q,
show that either A = 0 = B or pq = 1.
[Hint: Example 2.1.7.]

d. A and AT have the same main diagonal for every

b. (k1 + k2 +

+ kn)A = k1A + k2A +

any numbers k1, k2, . . . , kn

· · ·

6
2.2. Matrix-Vector Multiplication

47

2.2 Matrix-Vector Multiplication

Up to now we have used matrices to solve systems of linear equations by manipulating the rows of the
augmented matrix. In this section we introduce a different way of describing linear systems that makes
more use of the coefﬁcient matrix of the system and leads to a useful way of “multiplying” matrices.

Vectors

It is a well-known fact in analytic geometry that two points in the plane with coordinates (a1, a2) and
(b1, b2) are equal if and only if a1 = b1 and a2 = b2. Moreover, a similar condition applies to points
(a1, a2, a3) in space. We extend this idea as follows.

An ordered sequence (a1, a2, . . . , an) of real numbers is called an ordered nnn-tuple. The word “or-
dered” here reﬂects our insistence that two ordered n-tuples are equal if and only if corresponding entries
are the same. In other words,

(a1, a2, . . . , an) = (b1, b2, . . . , bn)

if and only if a1 = b1, a2 = b2, . . . , and an = bn.

Thus the ordered 2-tuples and 3-tuples are just the ordered pairs and triples familiar from geometry.

Deﬁnition 2.4 The set Rn of ordered n-tuples of real numbers

Let R denotethesetofallrealnumbers. Thesetof all ordered n-tuplesfrom R hasaspecial
notation:

Rn denotesthesetofallordered n-tuplesofrealnumbers.



There are two commonly used ways to denote the n-tuples in Rn: As rows (r1, r2, . . . , rn) or columns
r1
r2
...
rn

; the notation we use depends on the context. In any event they are called vectors or n-vectors and











will be denoted using bold type such as x or v. For example, an m

columns:

n matrix A will be written as a row of

×

A =

a1 a2

an

where a j denotes column j of A for each j.

· · ·

(cid:2)

If x and y are two n-vectors in Rn, it is clear that their matrix sum x + y is also in Rn as is the scalar
multiple kx for any real number k. We express this observation by saying that Rn is closed under addition
and scalar multiplication. In particular, all the basic properties in Theorem 2.1.1 are true of these n-vectors.
These properties are fundamental and will be used frequently below without comment. As for matrices in
x is
general, the n
called the negative x.

1 zero matrix is called the zero nnn-vector in Rn and, if x is an n-vector, the n-vector

−

×

(cid:3)

Of course, we have already encountered these n-vectors in Section 1.3 as the solutions to systems of
linear equations with n variables. In particular we deﬁned the notion of a linear combination of vectors
and showed that a linear combination of solutions to a homogeneous system is again a solution. Clearly, a
linear combination of n-vectors in Rn is again in Rn, a fact that we will be using.

48

Matrix Algebra

Matrix-Vector Multiplication

Given a system of linear equations, the left sides of the equations depend only on the coefﬁcient matrix A
and the column x of variables, and not on the constants. This observation leads to a fundamental idea in
linear algebra: We view the left sides of the equations as the “product” Ax of the matrix A and the vector
x. This simple change of perspective leads to a completely new way of viewing linear systems—one that
is very useful and will occupy our attention throughout this book.

To motivate the deﬁnition of the “product” Ax, consider ﬁrst the following system of two equations in

three variables:

ax1 + bx2 + cx3 = b1
a′x1 + b′x2 + c′x3 = b1

(2.2)

and let A =

c

a
b
a′ b′ c′ (cid:21)

(cid:20)

, x =



x1
x2
x3

, b =



b1
b2

(cid:20)

(cid:21)

denote the coefﬁcient matrix, the variable matrix, and

the constant matrix, respectively. The system (2.2) can be expressed as a single vector equation





(cid:20)
which in turn can be written as follows:

ax1 + bx2 + cx3
a′x1 + b′x2 + c′x3

=

(cid:21)

(cid:20)

b1
b2

(cid:21)

a
a′ (cid:21)
Now observe that the vectors appearing on the left side are just the columns

b
b′ (cid:21)

c
c′ (cid:21)

b1
b2

+ x3

+ x2

x1

=

(cid:20)

(cid:20)

(cid:21)

(cid:20)

(cid:20)

of the coefﬁcient matrix A. Hence the system (2.2) takes the form

a1 =

, a2 =

, and a3 =

a
a′ (cid:21)

(cid:20)

b
b′ (cid:21)

(cid:20)

c
c′ (cid:21)

(cid:20)

x1a1 + x2a2 + x3a3 = b
This shows that the system (2.2) has a solution if and only if the constant matrix b is a linear combination3
of the columns of A, and that in this case the entries of the solution are the coefﬁcients x1, x2, and x3 in
this linear combination.

(2.3)

Moreover, this holds in general. If A is any m

n matrix, it is often convenient to view A as a row of

columns. That is, if a1, a2, . . . , an are the columns of A, we write

×

A =

a1 a2

an

· · ·

an

is given in terms of its columns.

(cid:3)

(cid:2)

Now consider any system of linear equations with m

n coefﬁcient matrix A. If b is the constant

×



is the matrix of variables then, exactly as above, the system can

and say that A =

a1 a2

· · ·

(cid:2)

(cid:3)
x1
x2
...
xn
be written as a single vector equation

matrix of the system, and if x = 









x1a1 + x2a2 +

· · ·
3Linear combinations were introduced in Section 1.3 to describe the solutions of homogeneous systems of linear equations.

+ xnan = b

(2.4)

They will be used extensively in what follows.

2.2. Matrix-Vector Multiplication

49

in the form given in (2.4).

Example 2.2.1

Write the system

Solution.






3x1 + 2x2
x1

−

4x3 = 0
3x2 + x3 = 3
5x3 =
x2
1
−

−

−

3
1
0 


x1





+ x2

2
3
−
1 






+ x3

4
−
1
5 


−

=





0
3
1 


−





As mentioned above, we view the left side of (2.4) as the product of the matrix A and the vector x.

This basic idea is formalized in the following deﬁnition:

Deﬁnition 2.5 Matrix-Vector Multiplication

Let A =
x1
(cid:2)
x2
...
xn

x= 





a1 a2

· · ·

an

bean m

×

(cid:3)

n matrix,writtenintermsofitscolumnsa1, a2, . . . , an. If








isanyn-vector,theproduct Axisdeﬁnedtobethe m-vectorgivenby:

Ax= x1a1 + x2a2 +

+ xnan

· · ·

In other words, if A is m
of A where the coefﬁcients are the entries of x (in order).

×

n and x is an n-vector, the product Ax is the linear combination of the columns

Note that if A is an m

n matrix, the product Ax is only deﬁned if x is an n-vector and then the vector
Ax is an m-vector because this is true of each column a j of A. But in this case the system of linear equations
with coefﬁcient matrix A and constant vector b takes the form of a single matrix equation

×

The following theorem combines Deﬁnition 2.5 and equation (2.4) and summarizes the above discussion.
Recall that a system of linear equations is said to be consistent if it has at least one solution.

Ax = b

Theorem 2.2.1

1. Everysystemoflinearequationshastheform Ax= bwhere A isthecoefﬁcientmatrix,bis

theconstantmatrix,andxisthematrixofvariables.

2. Thesystem Ax= bisconsistentifandonlyifbisalinearcombinationofthecolumnsof A.

3. Ifa1, a2, . . . , an arethecolumnsof A andifx= 





x1
x2
...
xn



,thenxisasolutiontothelinear






50

Matrix Algebra

system Ax= bifandonlyif x1, x2, . . . , xn areasolutionofthevectorequation

x1a1 + x2a2 +

+ xnan = b

· · ·

A system of linear equations in the form Ax = b as in (1) of Theorem 2.2.1 is said to be written in matrix
form. This is a useful way to view linear systems as we shall see.

Theorem 2.2.1 transforms the problem of solving the linear system Ax = b into the problem of ex-
pressing the constant matrix B as a linear combination of the columns of the coefﬁcient matrix A. Such
a change in perspective is very useful because one approach or the other may be better in a particular
situation; the importance of the theorem is that there is a choice.

Example 2.2.2

If A =

2
0
3
−





1
−
2
4

−

3 5
3 1
1 2 


and x = 





Solution. By Deﬁnition 2.5: Ax = 2





Example 2.2.3

−

2
1
0
2
−

2
0
3 


, compute Ax.







+ 1

1
−
2
4 






+ 0

3
3
−
1 






2

−

5
1
2 


=





.

7
−
0
6 


−





Given columns a1, a2, a3, and a4 in R3, write 2a1
matrix and x is a vector.

−

3a2 + 5a3 + a4 in the form Ax where A is a

Solution. Here the column of coefﬁcients is x = 

. Hence Deﬁnition 2.5 gives



2
3
−
5
1







3a2 + 5a3 + a4

Ax = 2a1

−

where A =

a1 a2 a3 a4

is the matrix with a1, a2, a3, and a4 as its columns.

(cid:2)

(cid:3)

Example 2.2.4

Let A =

a1 a2 a3 a4

be the 3

×

a2 =

(cid:2)
1
1
1 






, a3 =





(cid:3)

3
1
−
3 
−


, and a4 =





3
1
0 


4 matrix given in terms of its columns a1 =

2
0
,
1 

. In each case below, either express b as a linear

−





2.2. Matrix-Vector Multiplication

51

combination of a1, a2, a3, and a4, or show that it is not such a linear combination. Explain what
your answer means for the corresponding system Ax = b of linear equations.

a. b =

1
2
3 






b. b =

4
2
1 






Solution. By Theorem 2.2.1, b is a linear combination of a1, a2, a3, and a4 if and only if the
system Ax = b is consistent (that is, it has a solution). So in each case we carry the augmented
matrix [A

b] of the system Ax = b to reduced form.

|

a. Here

1 0
0 1
0 0
solution in this case. Hence b is not a linear combination of a1, a2, a3, and a4.

2 1 0
1 1 0
0 0 1 


3 3 1
1 1 2
3 0 3 


2 1
0 1
1 1

→ 

−
−

−

−







, so the system Ax = b has no

b. Now



2 1
0 1
1 1

−

3 3 4
1 1 2
3 0 1 


−
−

1 0
0 1
0 0

→ 



−

2 1 1
1 1 2
0 0 0 




, so the system Ax = b is consistent.

Thus b is a linear combination of a1, a2, a3, and a4 in this case. In fact the general solution is
x1 = 1

t, x3 = s, and x4 = t where s and t are arbitrary parameters. Hence

t, x2 = 2 + s

2s

−

−

−

x1a1 + x2a2 + x3a3 + x4a4 = b =



4
2
1 


becomes a1 + 2a2 = b, whereas taking s = 1 = t gives



2a1 + 2a2 + a3 + a4 = b.

−

for any choice of s and t. If we take s = 0 and t = 0, this

Example 2.2.5

Taking A to be the zero matrix, we have 0x = 0 for all vectors x by Deﬁnition 2.5 because every
column of the zero matrix is zero. Similarly, A0 = 0 for all matrices A because every entry of the
zero vector is zero.

Example 2.2.6

, show that Ix = x for any vector x in R3.

then Deﬁnition 2.5 gives

1 0 0
0 1 0
0 0 1 


If I =





Solution. If x =



x1
x2
x3





+ x2

Ix = x1


1
0
0 






+ x3

0
1
0 






0
0
1 


=





x1
0
0 


+





0
x2
0 


+





0
0
x3





=









x1
x2
x3





= x

52

Matrix Algebra

The matrix I in Example 2.2.6 is called the 3

3 identity matrix, and we will encounter such matrices
again in Example 2.2.11 below. Before proceeding, we develop some algebraic properties of matrix-vector
multiplication that are used extensively throughout linear algebra.

×

Theorem 2.2.2

Let A and B be m

×

n matrices,andletxandybe n-vectorsin Rn. Then:

1. A(x+ y) = Ax+ Ay.

2. A(ax) = a(Ax) = (aA)xforallscalars a.

3. (A + B)x= Ax+ Bx.

Proof. We prove (3); the other veriﬁcations are similar and are left as exercises. Let A =
and B =
as adding their columns, we have

an
be given in terms of their columns. Since adding two matrices is the same
(cid:3)

b1 b2

a1 a2

· · ·

· · ·

bn

(cid:2)

(cid:2)

(cid:3)

A + B =

a1 + b1 a2 + b2

(cid:2)



Deﬁnition 2.5 gives

an + bn

· · ·

(cid:3)

If we write x = 





x1
x2
...
xn






(A + B)x = x1(a1 + b1) + x2(a2 + b2) +

+ xn(an + bn)
· · ·
+ xnan) + (x1b1 + x2b2 +

+ xnbn)

· · ·

= (x1a1 + x2a2 +
= Ax + Bx

· · ·

Theorem 2.2.2 allows matrix-vector computations to be carried out much as in ordinary arithmetic. For
example, for any m

n matrices A and B and any n-vectors x and y, we have:

×

A(2x

−

5y) = 2Ax

−

5Ay

and

(3A

7B)x = 3Ax

7Bx

−

−

We will use such manipulations throughout the book, often without mention.

Linear Equations

Theorem 2.2.2 also gives a useful way to describe the solutions to a system

of linear equations. There is a related system

Ax = b

Ax = 0

2.2. Matrix-Vector Multiplication

53

called the associated homogeneous system, obtained from the original system Ax = b by replacing all
the constants by zeros. Suppose x1 is a solution to Ax = b and x0 is a solution to Ax = 0 (that is Ax1 = b
and Ax0 = 0). Then x1 + x0 is another solution to Ax = b. Indeed, Theorem 2.2.2 gives

This observation has a useful converse.

A(x1 + x0) = Ax1 + Ax0 = b + 0 = b

Theorem 2.2.3
Supposex1 isanyparticularsolutiontothesystem Ax= boflinearequations. Theneverysolution
x2 to Ax= bhastheform

forsomesolutionx0 oftheassociatedhomogeneoussystem Ax= 0.

x2 = x0 + x1

Proof. Suppose x2 is also a solution to Ax = b, so that Ax2 = b. Write x0 = x2
and, using Theorem 2.2.2, we compute

−

x1. Then x2 = x0 + x1

−
Hence x0 is a solution to the associated homogeneous system Ax = 0.

−

−

Ax0 = A(x2

x1) = Ax2

Ax1 = b

b = 0

Note that gaussian elimination provides one such representation.

Example 2.2.7

Express every solution to the following system as the sum of a speciﬁc solution plus a solution to
the associated homogeneous system.

x2
x2

−
−

x1
2x1
x1

−
−
−

x3 + 3x4 = 2
3x3 + 4x4 = 6
2x3 + x4 = 4

Solution. Gaussian elimination gives x1 = 4 + 2s
and t are arbitrary parameters. Hence the general solution can be written

−

t, x2 = 2 + s + 2t, x3 = s, and x4 = t where s

x1
x2
x3
x4

x = 







= 









4 + 2s
t
−
2 + s + 2t
s
t



= 









4
2
0
0



+ 

s 













2
1
1
0







+ t 

4
2
0
0









Thus x1 = 



is a particular solution (where s = 0 = t), and x0 = s 



gives all

solutions to the associated homogeneous system. (To see why this is so, carry out the gaussian
elimination again but with all the constants set equal to zero.)

The following useful result is included with no proof.

1
−
2
0
1















+ t 




2
1
1
0

1
−
2
0
1

















54

Matrix Algebra

Theorem 2.2.4

Let Ax= bbeasystemofequationswithaugmentedmatrix

A b

. Write rank A = r.

1. rank

A b

iseither r or r + 1.

(cid:2)

(cid:3)

2. Thesystemisconsistentifandonlyif rank

(cid:3)

(cid:2)

A b

= r.

(cid:2)
3. Thesystemisinconsistentifandonlyif rank

(cid:3)
A b

= r + 1.

(cid:2)

(cid:3)

The Dot Product

Deﬁnition 2.5 is not always the easiest way to compute a matrix-vector product Ax because it requires
that the columns of A be explicitly identiﬁed. There is another way to ﬁnd such a product which uses the
matrix A as a whole with no reference to its columns, and hence is useful in practice. The method depends
on the following notion.

Deﬁnition 2.6 Dot Product in Rn

If (a1, a2, . . . , an) and (b1, b2, . . . , bn) aretwoordered n-tuples,theirdotproduct isdeﬁnedto
bethenumber

obtainedbymultiplyingcorrespondingentriesandaddingtheresults.

a1b1 + a2b2 +

+ anbn

· · ·

To see how this relates to matrix products, let A denote a 3

4 matrix and let x be a 4-vector. Writing

×

x = 



and

A =

x1
x2
x3
x4









in the notation of Section 2.1, we compute

a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34









a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34

Ax =





x1
x2
x3
x4











a11
a21
a31

+ x2





a12
a22
a32

+ x3












a11x1 + a12x2 + a13x3 + a14x4
a21x1 + a22x2 + a23x3 + a24x4
a31x1 + a32x2 + a33x3 + a34x4

a14
a24
a34









+ x4





a13
a23
a33





= x1







=







From this we see that each entry of Ax is the dot product of the corresponding row of A with x. This
computation goes through in general, and we record the result in Theorem 2.2.5.

Theorem 2.2.5: Dot Product Rule

2.2. Matrix-Vector Multiplication

55

Let A bean m
productofthecorrespondingrowof A withx.

×

n matrixandletxbean n-vector. Theneachentryofthevector Axisthedot

This result is used extensively throughout linear algebra.

×

If A is m

n and x is an n-vector, the computation of Ax by the dot product rule is simpler than
using Deﬁnition 2.5 because the computation can be carried out directly with no explicit reference to the
columns of A (as in Deﬁnition 2.5). The ﬁrst entry of Ax is the dot product of row 1 of A with x. In
hand calculations this is computed by going across row one of A, going down the column x, multiplying
corresponding entries, and adding the results. The other entries of Ax are computed in the same way using
the other rows of A with the column x.

A

x

Ax





row i













=







entry i

In general, compute entry i of Ax as follows (see the diagram):

Go across row i of A and down column x, multiply corre-
sponding entries, and add the results.

As an illustration, we rework Example 2.2.2 using the dot product rule
instead of Deﬁnition 2.5.

Example 2.2.8

If A =

2
0
3
−





1
−
2
4

−

3 5
3 1
1 2 


and x = 

, compute Ax.



2
1
0
2
−









Solution. The entries of Ax are the dot products of the rows of A with x:

Ax =



1
−
2
4

2
0
3
−

−

3 5
3 1
1 2 




2
1
0
2
−






Of course, this agrees with the outcome in Example 2.2.2.





−





=



(

2 + (
2
·
0
2 +
·
3)2 +

1)1 +
−
2
4

1 + (
1 +

·
·

3
0 + 5(
·
3)0 + 1(
−
0 + 2(
1

·

−
−
−

2)
2)
2) 


=





7
−
0
6 


−

Example 2.2.9

Write the following system of linear equations in the form Ax = b.

x2 + 2x3 + x4

−

5x1
x1 + x2 + 3x3
x1 + x2

−
2x3 +

−
5x4 + 2x5 =

3x5 = 8
2
−
3x5 = 0

−

−

−

56

Matrix Algebra

Solution. Write A =

5
1
1
−





1
−
1
1

2
3
2
−

1
5
−
0

product rule gives Ax =



x2 + 2x3 + x4

−

5x1
x1 + x2 + 3x3
2x3
x1 + x2

−

x1
x2
x3
x4
x5

, b =



−

3
−
2
3 

3x5
−
5x4 + 2x5
3x5



8
2
0 
−


, and x =





. Then the dot








, so the entries of Ax are the left sides of







−
the equations in the linear system. Hence the system becomes Ax = b because matrices are equal if
and only corresponding entries are equal.

−

−





Example 2.2.10

If A is the zero m

×

n matrix, then Ax = 0 for each n-vector x.

Solution. For each k, entry k of Ax is the dot product of row k of A with x, and this is zero because
row k of A consists of zeros.

Deﬁnition 2.7 The Identity Matrix

Foreach n > 2,theidentitymatrix In isthe n
tolowerright),andzeroselsewhere.

×

The ﬁrst few identity matrices are

n matrixwith1sonthemaindiagonal(upperleft

I2 =

1 0
0 1

1 0 0
0 1 0
0 0 1 

In Example 2.2.6 we showed that I3x = x for each 3-vector x using Deﬁnition 2.5. The following result
shows that this holds in general, and is the reason for the name.

I4 = 

I3 =









. . .







(cid:21)

(cid:20)

,

,

,

1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

Example 2.2.11

For each n

≥

2 we have Inx = x for each n-vector x in Rn.

Solution. We verify the case n = 4. Given the 4-vector x = 

x1
x2
x3
x4



the dot product rule gives

1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

x1
x2
x3
x4















= 









I4x = 













x1 + 0 + 0 + 0
0 + x2 + 0 + 0
0 + 0 + x3 + 0
0 + 0 + 0 + x4

x1
x2
x3
x4



= x







= 









In general, Inx = x because entry k of Inx is the dot product of row k of In with x, and row k of In
has 1 in position k and zeros elsewhere.

2.2. Matrix-Vector Multiplication

57

Example 2.2.12

a1 a2
Let A =
column j of the n
(cid:2)

×

Solution. Write e j = 





t1
t2
...
tn

(cid:3)






an

be any m

n matrix with columns a1, a2, . . . , an. If e j denotes

· · ·
n identity matrix In, then Ae j = a j for each j = 1, 2, . . . , n.

×



where t j = 1, but ti = 0 for all i

= j. Then Theorem 2.2.5 gives

Ae j = t1a1 +

+ t ja j +

· · ·

· · ·

+ tnan = 0 +

+ a j +

· · ·

· · ·

+ 0 = a j

Example 2.2.12 will be referred to later; for now we use it to prove:

Theorem 2.2.6

Let A and B be m

×

n matrices. If Ax= Bxforallxin Rn,then A = B.

b1 b2
Proof. Write A =
and in terms of their columns. It is
enough to show that ak = bk holds for all k. But we are assuming that Aek = Bek, which gives ak = bk by
Example 2.2.12.

and B =

a1 a2

· · ·

· · ·

bn

an

(cid:2)

(cid:3)

(cid:3)

(cid:2)

We have introduced matrix-vector multiplication as a new way to think about systems of linear equa-
tions. But it has several other uses as well. It turns out that many geometric operations can be described
using matrix multiplication, and we now investigate how this happens. As a bonus, this description pro-
vides a geometric “picture” of a matrix by revealing the effect on a vector when it is multiplied by A. This
“geometric view” of matrices is a fundamental tool in understanding them.

Transformations

The set R2 has a geometrical interpretation as the euclidean plane where a vector

a1
a2

(cid:20)

(cid:21)

in R2 represents

the point (a1, a2) in the plane (see Figure 2.2.1). In this way we regard R2 as the set of all points in
the plane. Accordingly, we will refer to vectors in R2 as points, and denote their coordinates as a column

rather than a row. To enhance this geometrical interpretation of the vector

by an arrow from the origin

0
0

(cid:20)

(cid:21)

to the vector as in Figure 2.2.1.

a1
a2

(cid:20)

, it is denoted graphically

(cid:21)

6
58

Matrix Algebra

x2

a2

0 =

(cid:20)

0
0

(cid:21)

a1

a1
a2

(cid:20)

(cid:21)

x1

Figure 2.2.1

x3

a3

0

a1

x1

a1
a2
a3

a2









x2

Figure 2.2.2

Similarly we identify R3 with 3-dimensional space by writing a point (a1, a2, a3) as the vector

a1
a2
a3





in R3, again represented by an arrow4 from the origin to the point as in Figure 2.2.2. In this way the terms

“point” and “vector” mean the same thing in the plane or in space.



We begin by describing a particular geometrical transformation of the plane R2.

Example 2.2.13

y

0

Consider the transformation of R2 given by reﬂection in the

x axis. This operation carries the vector

a1
a2

(cid:20)

(cid:21)

to its reﬂection

a1
a2

−

(cid:20)

(cid:21)

a1
a2

(cid:20)

(cid:21)

x

a1
a2

(cid:21)

(cid:20)

−

as in Figure 2.2.3. Now observe that

a1
a2

−

(cid:20)

=

(cid:21)

(cid:20)

1
0

0
1
−

a1
a2

(cid:21)

(cid:21) (cid:20)

Figure 2.2.3

so reﬂecting

(cid:20)
by the matrix

(cid:20)

a1
a2
1
0

in the x axis can be achieved by multiplying

(cid:21)

0
1
−

.
(cid:21)

If we write A =

, Example 2.2.13 shows that reﬂection in the x axis carries each vector x in

(cid:21)
R2 to the vector Ax in R2. It is thus an example of a function

(cid:20)

1
0

0
1
−

T : R2

→

R2 where T (x) = Ax for all x in R2

As such it is a generalization of the familiar functions f : R
number f (x).

→

R that carry a number x to another real

4This “arrow” representation of vectors in R2 and R3 will be used extensively in Chapter 4.

T

x

Rn

T (x)

Rm

Figure 2.2.4

2.2. Matrix-Vector Multiplication

59

More generally, functions T : Rn

Rm are called transformations
from Rn to Rm. Such a transformation T is a rule that assigns to every
vector x in Rn a uniquely determined vector T (x) in Rm called the image
of x under T . We denote this state of affairs by writing

→

→
The transformation T can be visualized as in Figure 2.2.4.

T : Rn

Rm

or Rn T
−→

Rm

To describe a transformation T : Rn

Rm we must specify the vector T (x) in Rm for every x in Rn.
This is referred to as deﬁning T , or as specifying the action of T . Saying that the action deﬁnes the
Rm as equal if they
transformation means that we regard two transformations S : Rn
have the same action; more formally

Rm and T : Rn

→

→

→

S = T

if and only if S(x) = T (x) for all x in Rn.

Again, this what we mean by f = g where f , g : R

R are ordinary functions.

Functions f : R

R are often described by a formula, examples being f (x) = x2 + 1 and f (x) = sin x.

The same is true of transformations; here is an example.

→

→

Example 2.2.14

The formula T 





x1
x2
x3
x4

x1 + x2
x2 + x3
x3 + x4







=









deﬁnes a transformation R4

R3.

→

Example 2.2.13 suggests that matrix multiplication is an important way of deﬁning transformations

Rm. If A is any m

n matrix, multiplication by A gives a transformation

Rn

→

×
TA : Rn

Rm deﬁned by TA(x) = Ax for every x in Rn

→

Deﬁnition 2.8 Matrix Transformation TA

TA iscalledthematrixtransformationinducedby A.

Thus Example 2.2.13 shows that reﬂection in the x axis is the matrix transformation R2

duced by the matrix

1
0

(cid:20)

0
1
−

. Also, the transformation R : R4
(cid:21)

→

transformation induced by the matrix

R3 in Example 2.2.13 is the matrix

R2 in-

→

A =

1 1 0 0
0 1 1 0
0 0 1 1 






because

1 1 0 0
0 1 1 0
0 0 1 1 






x1
x2
x3
x4









=









x1 + x2
x2 + x3
x3 + x4





60

Matrix Algebra

Example 2.2.15

Let Rπ
2

: R2

→

90◦)5. Show that Rπ
2

Solution.

y

(x) =

R π
2

b

0

b
a

−

(cid:20)
q

(cid:21)

a

x =

a

b

p

(cid:20)

a
b

x

(cid:21)

R2 denote counterclockwise rotation about the origin through π

2 radians (that is,

is induced by the matrix

0
1

1
−
0

.
(cid:21)

(cid:20)

a
b

The effect of Rπ
2

is to rotate the vector x =

counterclockwise through π
in Figure 2.2.5. Since triangles 0px and 0qRπ
2

2 to produce the vector Rπ
2

(cid:20)

(cid:21)

(x) are identical,

(x) shown

we obtain Rπ
2

(x) =

so we obtain Rπ
2

b
−
a

b
−
a

0
1

1
−
0

a
b

(cid:20)

(cid:21)

=

. But

(cid:20)
(x) = Ax for all x in R2 where A =

,
(cid:21)
1
−
0
is the matrix transformation induced by A.

(cid:21) (cid:20)
0
1

(cid:20)

(cid:20)

(cid:21)

.
(cid:21)

Figure 2.2.5

In other words, Rπ
2

If A is the m

×

n zero matrix, then A induces the transformation

T : Rn

→

Rm given by T (x) = Ax = 0 for all x in Rn

This is called the zero transformation, and is denoted T = 0.
Another important example is the identity transformation

1Rn : Rn

Rn

given by

1Rn(x) = x for all x in Rn

→
That is, the action of 1Rn on x is to do nothing to it. If In denotes the n
n identity matrix, we showed in
Example 2.2.11 that Inx = x for all x in Rn. Hence 1Rn(x) = Inx for all x in Rn; that is, the identity matrix
In induces the identity transformation.

×

Here are two more examples of matrix transformations with a clear geometric description.

Example 2.2.16

If a > 0, the matrix transformation T

x
y

ax
y

=

induced by the matrix A =

is called

a 0
0 1

(cid:20)
an xxx-expansion of R2 if a > 1, and an xxx-compression if 0 < a < 1. The reason for the names is

(cid:21)

(cid:21)

(cid:20)

(cid:21)

(cid:20)

clear in the diagram below. Similarly, if b > 0 the matrix A =

and yyy-compressions.

1 0
0 b

(cid:21)

(cid:20)

gives rise to yyy-expansions

5Radian measure for angles is based on the fact that 360◦ equals 2π radians. Hence π radians = 180◦ and π

2 radians = 90◦.

2.2. Matrix-Vector Multiplication

61

y

0

y

y

x-compression

x-expansion

x
y

(cid:21)

(cid:20)

x

0

1
2 x
y

(cid:21)

(cid:20)

a = 1
2

3
2 x
y

(cid:21)

(cid:20)

x

x

0

a = 3
2

Example 2.2.17

If a is a number, the matrix transformation T

A =

1 a
0 1

(cid:20)

(cid:21)
illustrated below when a = 1

4 and a =

(cid:20)
is called an xxx-shear of R2 (positive if a > 0 and negative if a < 0). Its effect is

(cid:21)

(cid:21)

(cid:20)

x
y

=

x + ay
y

induced by the matrix

1
4 .

−

y

Positive x-shear

Negative x-shear

y

y

0

y

0

Tw(x) =

x + 2
y + 1

(cid:20)

(cid:21)

x =

(cid:20)

x
y

(cid:21)

x

Figure 2.2.6

up (see Figure 2.2.6).

x
y

(cid:21)

(cid:20)

x + 1
4 y
y

(cid:20)

(cid:21)

x

0

a = 1
4

x

0

1
4 y

x

−
y

(cid:20)

(cid:21)

x

a =

1
4

−

We hasten to note that there are important geometric transformations
that are not matrix transformations. For example, if w is a ﬁxed column in
Rn, deﬁne the transformation Tw : Rn

Rn by

→

Tw(x) = x + w for all x in Rn

Then Tw is called translation by w. In particular, if w =

in R2, the

2
1

(cid:21)

(cid:20)

effect of Tw on

x
y

(cid:20)

(cid:21)

is to translate it two units to the right and one unit

The translation Tw is not a matrix transformation unless w = 0. Indeed, if Tw were induced by a matrix

A, then Ax = Tw(x) = x + w would hold for every x in Rn. In particular, taking x = 0 gives w = A0 = 0.

62

Matrix Algebra

Exercises for 2.2

Exercise 2.2.1 In each case ﬁnd a system of equations
that is equivalent to the given vector equation. (Do not
solve the system.)

a. x1





b. x1 





1
0
1
0

−

2
3
0 


+ x2

−




3
8
2
1

1
1
4 


+ x3




3
−
0
2
2

2
0
1 


−



+ x4 









=




3
2
0
2

−





5
6
3 

5
1
2
0





−



= 









+ x2 











+ x3 









Exercise 2.2.2 In each case ﬁnd a vector equation that
is equivalent to the given system of equations. (Do not
solve the equation.)

a.

b.

−

x1
3x1 + x2 + x3 =
8x2
5x1

x2 + 3x3 = 5
6
−
= 9

−

−
2x2

x1
x1
−
2x1
3x1

−

−
−

−
+ x3
2x2 + 7x3
4x2 + 9x3

x3 + x4 = 5
2x4 =
3
−
= 8
2x4 = 12

−

−

Exercise 2.2.3 In each case compute Ax using: (i) Def-
inition 2.5. (ii) Theorem 2.2.5.

a. A =

b. A =

c. A =

d. A =

(cid:20)

(cid:20)









3
5

1
0

2 0
4 1

−
−

2 3
4 5

−

(cid:21)

(cid:21)

and x =

and x =

x1
x2
x3

x1
x2
x3









.




.




2 0
1 2
5 6

−

−

5 4
0 3
7 8

−





and x = 





x1
x2
x3
x4

4
−
2
7

3
0
8

−

1 6
1 5
3 0

−





and x = 








x1
x2
x3
x4

.






Exercise 2.2.4 Let A =

a1 a2 a3 a4

matrix given in terms of its columns a1 =

(cid:2)

(cid:3)

be the 3
×
1
1
1



4

,

−







a2 =



3
0
2

, a3 =





2
1
−
3

, and a4 =





−

0
3
5

. In each









case either express b as a linear combination of a1, a2, a3,

and a4, or show that it is not such a linear combination.
Explain what your answer means for the corresponding
system Ax = b of linear equations.





a.

b =

0
3
5









b.

b =

4
1
1









Exercise 2.2.5 In each case, express every solution of
the system as a sum of a speciﬁc solution plus a solution
of the associated homogeneous system.

a.

c.

d.

b.

y

−

4z =

x
4
−
−
x + 2y + 5z = 2
x + y + 2z = 0

x + y + z = 2
2x + y
= 3
3z = 0
y
x

−
−
x1 + x2

x3
−
x2 + x3
x2 + x3 + x4

5x5 = 2
−
4x5 =
1
−
−
x5 =
1
−
−
4x3 + x4 + x5 = 6

2x1

−
2x1 + x2
x3
3x1 + x2 + x3
x1
−
2x1

1
−
−
2
−
−
x2 + 2x3 + x4 = 2
+ 2x4 = 3
x2

x4 =
2x4 =

−

−
−

−

Exercise 2.2.6 If x0 and x1 are solutions to the homo-
geneous system of equations Ax = 0, use Theorem 2.2.2
to show that sx0 + tx1 is also a solution for any scalars s
and t (called a linear combination of x0 and x1).

.



Exercise 2.2.7 Assume that A

1
1
2



−

= 0 = A





2
0
3

.


Show that x0 =

2
1
−
3
two-parameter family of solutions to Ax = b.


is a solution to Ax = b. Find a















Exercise 2.2.8 In each case write the system in the form
Ax = b, use the gaussian algorithm to solve the system,
and express the solution as a particular solution plus a
linear combination of basic solutions to the associated
homogeneous system Ax = 0.

a.

b.

−

−
−

−

x1
2x1 + 4x2 + x3
3x1
8x1

2x2 + x3 + 4x4
2x4
6x2 + 8x3 + 4x4
16x2 + 7x3 + 12x4

−

−
−

x5 = 8
4x5 =
1
−
13x5 = 1
6x5 = 11

−
−
−
−

x1
−
3x1 + 6x2
2x1 + 4x2
x1 + 2x2

2x2 + x3 + 2x4 + 3x5 =
2x3
3x4
−
x3 + x4
+ 3x4

4
−
11x5 = 11
8x5 = 7
5x5 = 3

−
−

−
−
−

−

Exercise 2.2.9 Given vectors a1 =

1
0
1

,









1
1
0

a2 =

, and a3 =

0
1
1
not a linear combination of a1, a2, and a3. Justify your
answer. [Hint: Part (2) of Theorem 2.2.1.]

, ﬁnd a vector b that is

−

















Exercise 2.2.10 In each case either show that the state-
ment is true, or give an example showing that it is false.

2.2. Matrix-Vector Multiplication

63

R2 be a transformation.
Exercise 2.2.11 Let T : R2
In each case show that T is induced by a matrix and ﬁnd
the matrix.

→

a. T is a reﬂection in the y axis.

b. T is a reﬂection in the line y = x.

c. T is a reﬂection in the line y =

x.

−

d. T is a clockwise rotation through π
2 .

→

Exercise 2.2.12 The projection P : R3

R2 is deﬁned

(cid:20)





=

x
y

by P

for all

x
y
z 


in R3. Show that P is

(cid:21)
induced by a matrix and ﬁnd the matrix.

x
y
z 


R3 be a transformation.
Exercise 2.2.13 Let T : R3
In each case show that T is induced by a matrix and ﬁnd
the matrix.

→



3
2

a.

is a linear combination of

and

(cid:21)

(cid:20)

(cid:21)
b. If Ax has a zero entry, then A has a row of zeros.

(cid:20)

(cid:21)

(cid:20)

1
0

0
1

.

a. T is a reﬂection in the x

b. T is a reﬂection in the y

y plane.

z plane.

−

−

c. If Ax = 0 where x

= 0, then A = 0.

d. Every linear combination of vectors in Rn can be

written in the form Ax.

e. If A =

if b = 3a1
lution.

(cid:2)

−

a1 a2 a3

in terms of its columns, and
2a2, then the system Ax = b has a so-

(cid:3)

f. If A =

a1 a2 a3

in terms of its columns,
and if the system Ax = b has a solution, then
b = sa1 + ta2 for some s, t.

(cid:2)

(cid:3)

g. If A is m

n and m < n, then Ax = b has a solution

×
for every column b.

h. If Ax = b has a solution for some column b, then

it has a solution for every column b.

i. If x1 and x2 are solutions to Ax = b, then x1

is a solution to Ax = 0.

x2

−

j. Let A =

a1 a2 a3

in terms of its columns. If

(cid:2)

(cid:3)

a3 = sa1 + ta2, then Ax = 0, where x =

s
t

1

−

.









Exercise 2.2.14 Fix a > 0 in R, and deﬁne Ta : R4
R4
by Ta(x) = ax for all x in R4. Show that T is induced by
a matrix and ﬁnd the matrix. [T is called a dilation if
a > 1 and a contraction if a < 1.]

→

Exercise 2.2.15 Let A be m
has a row of zeros, show that Ax has a zero entry.

n and let x be in Rn. If A

×

Exercise 2.2.16 If a vector b is a linear combination of
the columns of A, show that the system Ax = b is consis-
tent (that is, it has at least one solution.)

Exercise 2.2.17 If a system Ax = b is inconsistent (no
solution), show that b is not a linear combination of the
columns of A.

Exercise 2.2.18 Let x1 and x2 be solutions to the homo-
geneous system Ax = 0.

a. Show that x1 + x2 is a solution to Ax = 0.

b. Show that tx1 is a solution to Ax = 0 for any scalar

t.

6
64

Matrix Algebra

Exercise 2.2.19 Suppose x1 is a solution to the system
Ax = b.
If x0 is any nontrivial solution to the associ-
ated homogeneous system Ax = 0, show that x1 + tx0, t a
scalar, is an inﬁnite one parameter family of solutions to
Ax = b. [Hint: Example 2.1.7 Section 2.1.]

Exercise 2.2.20 Let A and B be matrices of the same
size. If x is a solution to both the system Ax = 0 and the
system Bx = 0, show that x is a solution to the system

(A + B)x = 0.

Exercise 2.2.21 If A is m
n and Ax = 0 for every x in
×
Rn, show that A = 0 is the zero matrix. [Hint: Consider
Ae j where e j is the jth column of In; that is, e j is the
vector in Rn with 1 as entry j and every other entry 0.]

Exercise 2.2.22 Prove part (1) of Theorem 2.2.2.

Exercise 2.2.23 Prove part (2) of Theorem 2.2.2.

2.3 Matrix Multiplication

In Section 2.2 matrix-vector products were introduced. If A is an m
for any n-column x in Rn as follows: If A =

a1 a2

n matrix, the product Ax was deﬁned
where the a j are the columns of A, and if

an

×

· · ·

x1
x2
...
xn

x = 







, Deﬁnition 2.5 reads






(cid:2)

(cid:3)

Ax = x1a1 + x2a2 +

+ xnan

· · ·

(2.5)

This was motivated as a way of describing systems of linear equations with coefﬁcient matrix A. Indeed
every such system has the form Ax = b where b is the column of constants.

In this section we extend this matrix-vector multiplication to a way of multiplying matrices in gen-
eral, and then investigate matrix algebra for its own sake. While it shares several properties of ordinary
arithmetic, it will soon become clear that matrix arithmetic is different in a number of ways.

Matrix multiplication is closely related to composition of transformations.

Composition and Matrix Multiplication

Sometimes two transformations “link” together as follows:

T

S

◦

T

S

Rk T
−→

Rn S
−→

Rm

In this case we can apply T ﬁrst and then apply S, and the result is a

Rk

Rn

Rm

S

new transformation

T : Rk

Rm

→

◦

called the composite of S and T , deﬁned by

T )(x) = S [T (x)]

for all x in Rk

(S

◦

The action of S

T can be described as “ﬁrst T then S ” (note the order!)6. This new transformation
is described in the diagram. The reader will have encountered composition of ordinary functions: For
example, consider R

R where f (x) = x2 and g(x) = x + 1 for all x in R. Then

R

◦

g
−→

f
−→

g)(x) = f [g(x)] = f (x + 1) = (x + 1)2

( f

◦

6When reading the notation S

T , we read S ﬁrst and then T even though the action is “ﬁrst T then S ”. This annoying state
of affairs results because we write T (x) for the effect of the transformation T on x, with T on the left. If we wrote this instead
as (x)T , the confusion would not occur. However the notation T (x) is well established.

◦

for all x in R.

f )(x) = g [ f (x)] = g(x2) = x2 + 1

(g

◦

2.3. Matrix Multiplication

65

Our concern here is with matrix transformations. Suppose that A is an m

n matrix and B is an n

k

×
Rm be the matrix transformations induced by B and A respectively, that is:

×

matrix, and let Rk TB
−→

Rn TA
−→

TB(x) = Bx for all x in Rk

and

TA(y) = Ay for all y in Rn

Write B =
(B is n

×

(cid:2)

· · ·

(cid:3)

b1 b2

bk

where b j denotes column j of B for each j. Hence each b j is an n-vector

k) so we can form the matrix-vector product Ab j. In particular, we obtain an m

k matrix

×

Ab1 Ab2

Abk

· · ·

(cid:2)

(cid:3)

with columns Ab1, Ab2,

, Abk. Now compute (TA

◦

· · ·

x1
x2
...
xk








in Rk:

TB)(x) for any x = 




Deﬁnition of TA
A and B induce TA and TB
Equation 2.5 above
Theorem 2.2.2
Theorem 2.2.2
Equation 2.5 above

+ A(xkbk)
+ xk(Abk)
x

TB

◦

(TA

◦

TB)(x) = TA [TB(x)]

= A(Bx)
= A(x1b1 + x2b2 +
= A(x1b1) + A(x2b2) +
= x1(Ab1) + x2(Ab2) +
=

Ab1 Ab2

· · ·

· · ·
· · ·
Abk

+ xkbk)

· · ·

(cid:2)
Because x was an arbitrary vector in Rn, this shows that TA
Abn
the matrix

Ab1 Ab2

(cid:3)

. This motivates the following deﬁnition.

◦

TB is the matrix transformation induced by

(cid:2)

Deﬁnition 2.9 Matrix Multiplication

· · ·

(cid:3)

Let A bean m
n matrix,let B bean n
column j of B foreach j. Theproductmatrix AB isthe m

k matrix,andwrite B =

×

×

b1 b2

bk

wherebj is

k matrixdeﬁnedasfollows:

· · ·

AB = A

b1 b2

bk

=

· · ·

(cid:2)

×
Ab1 Ab2

Abk

· · ·

(cid:3)

(cid:2)

(cid:3)
Thus the product matrix AB is given in terms of its columns Ab1, Ab2, . . . , Abn: Column j of AB is the
matrix-vector product Ab j of A and the corresponding column b j of B. Note that each such product Ab j
n and each b j is in Rn (since B has n rows). Note also that
makes sense by Deﬁnition 2.5 because A is m
if B is a column matrix, this deﬁnition reduces to Deﬁnition 2.5 for matrix-vector multiplication.

×

(cid:3)

(cid:2)

Given matrices A and B, Deﬁnition 2.9 and the above computation give

for all x in Rk. We record this for reference.

(cid:2)

(cid:3)

A(Bx) =

Ab1 Ab2

Abn

x = (AB)x

· · ·

66

Matrix Algebra

Theorem 2.3.1

Let A bean m
satisﬁes

×

n matrixandlet B bean n

×

k matrix. Thentheproductmatrix AB is m

k and

×

A(Bx) = (AB)x forallxin Rk

Here is an example of how to compute the product AB of two matrices using Deﬁnition 2.9.

Example 2.3.1

Compute AB if A =

2 3 5
1 4 7
0 1 8 






and B =



8 9
.
7 2
6 1 


9
2
1 












8
7
6 

67
78
55 


Solution. The columns of B are b1 =

and b2 =

, so Deﬁnition 2.5 gives

Ab1 =

2 3 5
1 4 7
0 1 8 






8
7
6 






=





and Ab2 =



2 3 5
1 4 7
0 1 8 


9
2
1 


=





29
24
10 






Hence Deﬁnition 2.9 above gives AB =

Ab1 Ab2

=

(cid:2)

(cid:3)





Example 2.3.2


67 29
78 24
.
55 10 


If A is m
transformations TA and TB:

n and B is n

×

×

k, Theorem 2.3.1 gives a simple formula for the composite of the matrix

Solution. Given any x in Rk,

TB = TAB

TA

◦

(TA

◦

TB)(x) = TA[TB(x)]

= A[Bx]
= (AB)x
= TAB(x)

While Deﬁnition 2.9 is important, there is another way to compute the matrix product AB that gives
a way to calculate each individual entry. In Section 2.2 we deﬁned the dot product of two n-tuples to be
the sum of the products of corresponding entries. We went on to show (Theorem 2.2.5) that if A is an
n matrix and x is an n-vector, then entry j of the product Ax is the dot product of row j of A with x.
m
This observation was called the “dot product rule” for matrix-vector multiplication, and the next theorem
shows that it extends to matrix multiplication in general.

×

Theorem 2.3.2: Dot Product Rule

Let A and B bematricesofsizes m
n and n
dotproductofrow i of A withcolumn j of B.

×

×

2.3. Matrix Multiplication

67

k,respectively. Thenthe (i, j)-entryof AB isthe

Proof. Write B =
in terms of its columns. Then Ab j is column j of AB for each j.
Hence the (i, j)-entry of AB is entry i of Ab j, which is the dot product of row i of A with b j. This proves
the theorem.

b1 b2

· · ·

bn

(cid:3)

(cid:2)

Thus to compute the (i, j)-entry of AB, proceed as follows (see the diagram):

Go across row i of A, and down column j of B, multiply corresponding entries, and add the results.

A

B

AB





=







row i





column j



(i, j)-entry





Note that this requires that the rows of A must be the same length as the columns of B. The following rule
is useful for remembering this and for deciding the size of the product matrix AB.

CompatibilityRule

A

B

m

n n′ ×

k

×

Let A and B denote matrices. If A is m
k, the product AB
can be formed if and only if n = n′. In this case the size of the product
k, and we say that AB is deﬁned, or that A and B are
matrix AB is m
compatible for multiplication.

n and B is n′ ×

×

×

The diagram provides a useful mnemonic for remembering this. We adopt the following convention:

Convention
Whenever a product of matrices is written, it is tacitly assumed that the sizes of the factors are such that
the product is deﬁned.

To illustrate the dot product rule, we recompute the matrix product in Example 2.3.1.

Example 2.3.3

Compute AB if A =



2 3 5
1 4 7
0 1 8 


and B =



8 9
.
7 2
6 1 




2. Theorem 2.3.2 gives each entry of AB as the dot product of the corresponding row of A with

2, so the product matrix AB is deﬁned and will be of size


Solution. Here A is 3
3
the corresponding column of B j that is,

3 and B is 3

×

×

×

AB =

2 3 5
1 4 7
0 1 8 






8 9
7 2
6 1 


=









2
1
0

·
·
·

8 + 3
8 + 4
8 + 1

7 + 5
7 + 7
7 + 8

6
6
6

·
·
·

·
·
·

9 + 3
9 + 4
9 + 1

2
1
0

·
·
·

·
·
·

2 + 5
2 + 7
2 + 8

1
1
1 


·
·
·

=





67 29
78 24
55 10 


68

Matrix Algebra

Of course, this agrees with Example 2.3.1.

Example 2.3.4

Compute the (1, 3)- and (2, 4)-entries of AB where

A =

(cid:20)

3
0

1 2
1 4

−

(cid:21)

and B =

2 1 6 0
0 2 3 4
1 0 5 8 


−

.





Then compute AB.

Solution. The (1, 3)-entry of AB is the dot product of row 1 of A and column 3 of B (highlighted
in the following display), computed by multiplying corresponding entries and adding the results.

3
0

1 2
1 4

−

(cid:20)



(cid:21)

2 1 6 0
0 2 3 4
1 0 5 8 


−



(1, 3)-entry = 3

6 + (

1)

−

·

·

3 + 2

·

5 = 25

Similarly, the (2, 4)-entry of AB involves row 2 of A and column 4 of B.

(2, 4)-entry = 0

0 + 1

4 + 4

·

·

·

8 = 36

3
0

1 2
1 4

−

(cid:21)

3 and B is 3

×



2 1 6 0
0 2 3 4
1 0 5 8 


4, the product is 2

−

Since A is 2

(cid:20)

×

3
0

1 2
1 4

−

AB =

(cid:20)

(cid:21)





−

4.

×
2 1 6 0
0 2 3 4
1 0 5 8 


=

(cid:20)

4 1 25 12
4 2 23 36

−

(cid:21)

Example 2.3.5

If A =

1 3 2

and B =



(cid:2)

(cid:3)

5
6
4 




Solution. Here, A is a 1
the compatibility rule reads

×

, compute A2, AB, BA, and B2 when they are deﬁned.7

3 matrix and B is a 3

1 matrix, so A2 and B2 are not deﬁned. However,

×

A

×

1

3 3

B

×

1

and

so both AB and BA can be formed and these are 1

×

B

3

1 1

A

×

3

×
1 and 3

3 matrices, respectively.

×

7As for numbers, we write A2 = A

A, A3 = A

A

·

·

·

A, etc. Note that A2 is deﬁned if and only if A is of size n

n for some n.

×

5
6
4 






(cid:3)

AB =

1 3 2

BA =

(cid:2)

5
6
4 


(cid:2)





1 3 2

=

(cid:3)





2.3. Matrix Multiplication

69

=

5
6
4

·
·
·

1

·

5 + 3

6 + 2

4

·

·

=

31

(cid:2)
1 5
1 6
1 4

(cid:3)

(cid:2)

(cid:3)

3 5
3 6
3 4

·
·
·

2
2
2 


·
·
·

=





5 15 10
6 18 12
4 12

8 


Unlike numerical multiplication, matrix products AB and BA need not be equal. In fact they need not
even be the same size, as Example 2.3.5 shows. It turns out to be rare that AB = BA (although it is by no
means impossible), and A and B are said to commute when this happens.

Example 2.3.6

Let A =

6
4
−

(cid:20)

Solution. A2 =

9
6
−

(cid:21)

6
4
−

(cid:20)

and B =

1 2
1 0

−

(cid:20)

. Compute A2, AB, BA.
(cid:21)

9
6
−

(cid:21) (cid:20)

6
4
−

9
6
−

=

(cid:21)

(cid:20)

0 0
0 0

(cid:21)

, so A2 = 0 can occur even if A

= 0. Next,

AB =

BA =

(cid:20)

6
4
−

9
6
−
1 2
1 0

1 2
1 0

9
6
−

(cid:21)

=

=

3
−
2

2
−
6
−

(cid:20)

(cid:20)

12
8
−
3
−
9
−

(cid:21)

(cid:21)

(cid:21) (cid:20)

−
6
4
−

Hence AB

−
= BA, even though AB and BA are the same size.

(cid:21) (cid:20)

(cid:21)

(cid:20)

Example 2.3.7

If A is any matrix, then IA = A and AI = A, and where I denotes an identity matrix of a size so that
the multiplications are deﬁned.

Solution. These both follow from the dot product rule as the reader should verify. For a more
formal proof, write A =
Example 2.2.11 give

where a j is column j of A. Then Deﬁnition 2.9 and

a1 a2

· · ·

an

(cid:3)

Ia1

Ia2

· · ·

Ian

=

a1 a2

an

= A

· · ·

If e j denotes column j of I, then Ae j = a j for each j by Example 2.2.12. Hence Deﬁnition 2.9
gives:

(cid:3)

(cid:2)

(cid:3)

(cid:2)
IA =

(cid:2)

AI = A

e1 e2

en

=

Ae1 Ae2

Aen

=

a1 a2

an

= A

· · ·

· · ·

· · ·

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

The following theorem collects several results about matrix multiplication that are used everywhere in

linear algebra.

6
6
70

Matrix Algebra

Theorem 2.3.3

Assumethat a isanyscalar,andthat A, B,andC arematricesofsizessuchthattheindicated
matrixproductsaredeﬁned. Then:

1. IA = A and AI = A where I denotesan

4. (B +C)A = BA +CA.

identitymatrix.

2. A(BC) = (AB)C.

5. a(AB) = (aA)B = A(aB).

3. A(B +C) = AB + AC.

6. (AB)T = BT AT.

Proof. Condition (1) is Example 2.3.7; we prove (2), (4), and (6) and leave (3) and (5) as exercises.

2. If C =

c1 c2

tion 2.9, so

(cid:2)

ck

in terms of its columns, then BC =

Bc1 Bc2

Bck

by Deﬁni-

· · ·

· · ·

(cid:3)
A(BC) =

A(Bc1) A(Bc2)

(cid:2)

=

(AB)c1

(AB)c2

(cid:2)
= (AB)C

(cid:2)

(cid:3)

A(Bck)

Deﬁnition 2.9

(AB)ck)

(cid:3)

Theorem 2.3.1

· · ·

· · ·

(cid:3)

Deﬁnition 2.9

4. We know (Theorem 2.2.2) that (B +C)x = Bx +Cx holds for every column x. If we write

A =

a1 a2

· · ·

an

in terms of its columns, we get

(cid:2)
(B +C)A =

(cid:3)
(B +C)a1

(B +C)a2

(B +C)an

Ban +Can

(cid:3)

· · ·

· · ·

Ba1 +Ca1 Ba2 +Ca2

(cid:2)

(cid:2)

=

=

Ba1 Ba2

· · ·

Ban

+

Ca1 Ca2

(cid:3)
· · ·

Can

(cid:2)

= BA +CA

(cid:3)

(cid:2)

Deﬁnition 2.9

Theorem 2.2.2

Adding Columns

(cid:3)

Deﬁnition 2.9

6. As in Section 2.1, write A = [ai j] and B = [bi j], so that AT = [a′i j] and BT = [b′i j] where a′i j = a ji and
b′ji = bi j for all i and j. If ci j denotes the (i, j)-entry of BT AT , then ci j is the dot product of row i of
BT with column j of AT . Hence

ci j = b′i1a′1 j + b′i2a′2 j +

· · ·

+ b′ima′m j = b1ia j1 + b2ia j2 +
= a j1b1i + a j2b2i +

+ bmia jm
+ a jmbmi

· · ·

· · ·

But this is the dot product of row j of A with column i of B; that is, the ( j, i)-entry of AB; that is,
the (i, j)-entry of (AB)T . This proves (6).

Property 2 in Theorem 2.3.3 is called the associative law of matrix multiplication. It asserts that the
equation A(BC) = (AB)C holds for all matrices (if the products are deﬁned). Hence this product is the
same no matter how it is formed, and so is written simply as ABC. This extends: The product ABCD of

2.3. Matrix Multiplication

71

four matrices can be formed several ways—for example, (AB)(CD), [A(BC)]D, and A[B(CD)]—but the
associative law implies that they are all equal and so are written as ABCD. A similar remark applies in
general: Matrix products can be written unambiguously with no parentheses.

However, a note of caution about matrix multiplication must be taken: The fact that AB and BA need
not be equal means that the order of the factors is important in a product of matrices. For example ABCD
and ADCB may not be equal.

Warning

Iftheorderofthefactorsinaproductofmatricesischanged,theproductmatrixmaychange
(ormaynotbedeﬁned). Ignoringthiswarningisasourceofmanyerrorsbystudentsoflinear
algebra!

Properties 3 and 4 in Theorem 2.3.3 are called distributive laws. They assert that A(B +C) = AB + AC
and (B +C)A = BA +CA hold whenever the sums and products are deﬁned. These rules extend to more
than two terms and, together with Property 5, ensure that many manipulations familiar from ordinary
algebra extend to matrices. For example

A(2B

3C + D

−
(A + 3C

5E) = 2AB

−
2D)B = AB + 3CB

−

3AC + AD
2DB

5AE

−

−

−

Note again that the warning is in effect: For example A(B
possible a lot of simpliﬁcation of matrix expressions.

−

C) need not equal AB

−

CA. These rules make

Example 2.3.8

Simplify the expression A(BC

CD) + A(C

B)D

AB(C

D).

−

−

−

−

Solution.

A(BC

−

CD) + A(C

B)D

−

−

AB(C

−

D) = A(BC)
= ABC
= 0

−

A(CD) + (AC

AB)D

(AB)C + (AB)D

−
ACD + ACD

−
ABD

−

ABC + ABD

−

−

Example 2.3.9 and Example 2.3.10 below show how we can use the properties in Theorem 2.3.2 to

deduce other facts about matrix multiplication. Matrices A and B are said to commute if AB = BA.

Example 2.3.9

Suppose that A, B, and C are n
AC = CA and BC = CB. Show that AB commutes with C.

×

n matrices and that both A and B commute with C; that is,

Solution. Showing that AB commutes with C means verifying that (AB)C = C(AB). The
computation uses the associative law several times, as well as the given facts that AC = CA and
BC = CB.

(AB)C = A(BC) = A(CB) = (AC)B = (CA)B = C(AB)

72

Matrix Algebra

Example 2.3.10

Show that AB = BA if and only if (A

−
Solution. The following always holds:

B)(A + B) = A2

B2.

−

B)(A + B) = A(A + B)

(A

−

B)(A + B) = A2

B(A + B) = A2 + AB

BA

B2

−
B2 follows. Conversely, if this last equation holds,

−

−

(2.6)

Hence if AB = BA, then (A
−
then equation (2.6) becomes

−
B2 = A2 + AB

A2

−

BA

−

−

B2

This gives 0 = AB

−

BA, and AB = BA follows.

In Section 2.2 we saw (in Theorem 2.2.1) that every system of linear equations has the form

Ax = b

where A is the coefﬁcient matrix, x is the column of variables, and b is the constant matrix. Thus the
system of linear equations becomes a single matrix equation. Matrix multiplication can yield information
about such a system.

Example 2.3.11

Consider a system Ax = b of linear equations where A is an m
n matrix. Assume that a matrix C
exists such that CA = In. If the system Ax = b has a solution, show that this solution must be Cb.
Give a condition guaranteeing that Cb is in fact a solution.

×

Solution. Suppose that x is any solution to the system, so that Ax = b. Multiply both sides of this
matrix equation by C to obtain, successively,

C(Ax) = Cb,

(CA)x = Cb,

Inx = Cb,

x = Cb

This shows that if the system has a solution x, then that solution must be x = Cb, as required. But
it does not guarantee that the system has a solution. However, if we write x1 = Cb, then

Thus x1 = Cb will be a solution if the condition AC = Im is satisﬁed.

Ax1 = A(Cb) = (AC)b

The ideas in Example 2.3.11 lead to important information about matrices; this will be pursued in the

next section.

2.3. Matrix Multiplication

73

Block Multiplication

Deﬁnition 2.10 Block Partition of a Matrix

Itisoftenusefultoconsidermatriceswhoseentriesarethemselvesmatrices(calledblocks). A
matrixviewedinthiswayissaidtobepartitionedintoblocks.

For example, writing a matrix B in the form

B =

b1 b2

· · ·

bk

where the b j are the columns of B

(cid:2)
is such a block partition of B. Here is another example.

(cid:3)

Consider the matrices

1
0
2
3

0
1
1
−
1

0 0 0
0 0 0
4 2 1
1 7 5

−

A = 





I2 023
P Q

(cid:21)



=





(cid:20)

and

B =



4
5
7
1
−
1

2
−
6
3
0
6









=

X
Y

(cid:20)

(cid:21)







where the blocks have been labelled as indicated. This is a natural way to partition A into blocks in view of
the blocks I2 and 023 that occur. This notation is particularly useful when we are multiplying the matrices
A and B because the product AB can be computed in block form as follows:

AB =

I
0
P Q

X
Y

=

(cid:21)

(cid:20)

(cid:21) (cid:20)

(cid:20)

IX + 0Y
PX + QY

=

(cid:21)

(cid:20)

X
PX + QY

(cid:21)

4
5
30
8

2
−
6
8
27







= 





This is easily checked to be the product AB, computed in the conventional manner.

In other words, we can compute the product AB by ordinary matrix multiplication, using blocks as
entries. The only requirement is that the blocks be compatible. That is, the sizes of the blocks must be
such that all (matrix) products of blocks that occur make sense. This means that the number of columns
in each block of A must equal the number of rows in the corresponding block of B.

Theorem 2.3.4: Block Multiplication

Ifmatrices A and B arepartitionedcompatiblyintoblocks,theproduct AB canbecomputedby
matrixmultiplicationusingblocksasentries.

We omit the proof.

We have been using two cases of block multiplication. If B =

· · ·
the b j are the columns of B, and if the matrix product AB is deﬁned, then we have

b1 b2

bk

is a matrix where

(cid:3)

AB = A

b1 b2

(cid:2)

bk

=

· · ·

(cid:3)

(cid:2)

Abk

· · ·

(cid:3)

(cid:2)
Ab1 Ab2

74

Matrix Algebra

This is Deﬁnition 2.9 and is a block multiplication where A = [A] has only one block. As another illustra-
tion,

Bx =

b1 b2

· · ·

bk





= x1b1 + x2b2 +

+ xkbk

· · ·

(cid:2)

(cid:3)

x1
x2
...
xk











where x is any k

1 column matrix (this is Deﬁnition 2.5).

×

It is not our intention to pursue block multiplication in detail here. However, we give one more example

because it will be used below.

Theorem 2.3.5

Supposematrices A =

B X
0 C

(cid:20)

(cid:21)

B1 X1
0 C1

(cid:20)

(cid:21)

and A1 =

arepartitionedasshownwhere B and B1

aresquarematricesofthesamesize,andC andC1 arealsosquareofthesamesize. Theseare
compatiblepartitioningsandblockmultiplicationgives

AA1 =

B X
0 C

(cid:20)

(cid:21) (cid:20)

B1 X1
0 C1

=

(cid:21)

(cid:20)

BB1 BX1 + XC1

0

CC1

(cid:21)

Example 2.3.12

Obtain a formula for Ak where A =

I X
0 0

(cid:20)

(cid:21)

is square and I is an identity matrix.

Solution. We have A2 =

I X
0 0
A3 = AA2 = AA = A2 = A. Continuing in this way, we see that Ak = A for every k

IX + X 0
02

I X
0 0

I X
0 0

(cid:21) (cid:20)

=

=

(cid:20)

(cid:21)

(cid:21)

(cid:21)

(cid:20)

(cid:20)

I2
0

= A. Hence

1.

≥

Block multiplication has theoretical uses as we shall see. However, it is also useful in computing
products of matrices in a computer with limited memory capacity. The matrices are partitioned into blocks
in such a way that each product of blocks can be handled. Then the blocks are stored in auxiliary memory
and their products are computed one by one.

Directed Graphs

The study of directed graphs illustrates how matrix multiplication arises in ways other than the study of
linear equations or matrix transformations.

A directed graph consists of a set of points (called vertices) connected by arrows (called edges). For
example, the vertices could represent cities and the edges available ﬂights. If the graph has n vertices
v1, v2, . . . , vn, the adjacency matrix A =
n matrix whose (i, j)-entry ai j is 1 if there is an
edge from v j to vi (note the order), and zero otherwise. For example, the adjacency matrix of the directed

is the n

ai j

×

(cid:2)

(cid:3)

graph shown is A =

1 1 0
.
1 0 1
1 0 0 






2.3. Matrix Multiplication

75

v1

v2

v3

A path of length r (or an r-path) from vertex j to vertex i is a sequence
of r edges leading from v j to vi. Thus v1
v3 is a 4-path
from v1 to v3 in the given graph. The edges are just the paths of length 1,
so the (i, j)-entry ai j of the adjacency matrix A is the number of 1-paths
from v j to vi. This observation has an important extension:

→

→

→

→

v2

v1

v1

Theorem 2.3.6
If A istheadjacencymatrixofadirectedgraphwith n vertices,thenthe (i, j)-entryof Ar isthe
numberof r-paths v j

vi.

→

As an illustration, consider the adjacency matrix A in the graph shown. Then

1 1 0
1 0 1
1 0 0 


A =





,

A2 =



2 1 1
2 1 0
1 1 0 




,

and

A3 =

4 2 1
3 2 1
2 1 1 

v2 (in fact they are v1





v3

Hence, since the (2, 1)-entry of A2 is 2, there are two 2-paths v1
v2 and
→
→
v1
v2, as the reader
can verify. The fact that no entry of A3 is zero shows that it is possible to go from any vertex to any other
vertex in exactly three steps.

v2). Similarly, the (2, 3)-entry of A2 is zero, so there are no 2-paths v3

→

→

→

→

v1

To see why Theorem 2.3.6 is true, observe that it asserts that

the (i, j)-entry of Ar equals the number of r-paths v j

vi

→

(2.7)

≥

holds for each r
1. We proceed by induction on r (see Appendix C). The case r = 1 is the deﬁnition of
the adjacency matrix. So assume inductively that (2.7) is true for some r
1; we must prove that (2.7)
also holds for r + 1. But every (r + 1)-path v j
vk for some k, followed
→
by a 1-path vk
, there are bk j paths of the former type (by induction)
and aik of the latter type, and so there are aikbk j such paths in all. Summing over k, this shows that there
(cid:3)
are

≥
vi is the result of an r-path v j
bi j

vi. Writing A =

→
and Ar =

ai j

→

(cid:2)

(cid:3)

(cid:2)

ai1b1 j + ai2b2 j +

· · ·

+ ainbn j

(r + 1)-paths v j

vi

→

But this sum is the dot product of the ith row
of Ar. As such, it is the (i,
r + 1, as required.

ai1 ai2

ain

of A with the jth column

b1 j b2 j

j)-entry of the matrix product ArA = Ar+1. This shows that (2.7) holds for
(cid:3)

(cid:3)

(cid:2)

(cid:2)

· · ·

T

bn j

· · ·

Exercises for 2.3

Exercise 2.3.1 Compute the following matrix products.

a.

b.

1
0

1
2

(cid:20)

(cid:20)

3
2

−

(cid:21) (cid:20)

2
0

1
−
1

(cid:21)

−

1 2
0 4

2 3 1
1 9 7
1 0 2





(cid:21)





−

5 0
1 5

−

7
9

c.

(cid:20)

3
1
1

−





(cid:21)





d.

1 3

3

−

(cid:2)

3 0
2 1
0 6

−









(cid:3)

76

Matrix Algebra

1 0 0
0 1 0
0 0 1

3
5
9

−
−

2
7
7













e.





f.

1

1 3

−

2
1
8





(cid:3)

−





(cid:3)

(cid:21)

2
1
7

−





(cid:2)

1

1 3

−

3 1
5 2

1
−
3

2
5

−

(cid:21) (cid:20)

2 3 1
5 7 4

a 0 0
0 b 0
0 0 c 


(cid:21)





a 0 0
0 b 0
0 0 c 






a′
0
0

0
b′
0

0
0
c′





(cid:2)





(cid:20)

(cid:20)





g.

h.

i.

j.

Exercise 2.3.2 In each of the following cases, ﬁnd all
possible products A2, AB, AC, and so on.

Exercise 2.3.3 Find a, b, a1, and b1 if:

a.

b.

(cid:20)

(cid:20)

a
b
a1 b1 (cid:21) (cid:20)
2 1
1 2

(cid:21) (cid:20)

−

3
1

5
−
2

−
a
b
a1 b1 (cid:21)

Exercise 2.3.4 Verify that A2

a.

3
0

(cid:20)

1
2

−
−

(cid:21)

1
2

−

1
0

(cid:20)

(cid:21)

7 2
1 4

−

(cid:21)

6I = 0 if:

−

=

(cid:21)

=

(cid:20)

A

−

b.

2
2

(cid:20)

2
1

−

(cid:21)

Exercise 2.3.5

Given A =

C =



1 0
2 1
5 8

1
0

1
−
1

(cid:20)

, and D =



, B =

(cid:21)

(cid:20)

3
1

(cid:20)
1 2
0 5

−

1 0
3 1

2
−
0

,

(cid:21)

, verify the

(cid:21)

following facts from Theorem 2.3.1.





a.

c.

A(B
D) = AB
(CD)T = DTCT

−

−

AD

b.

A(BC) = (AB)C

Exercise 2.3.6 Let A be a 2

2 matrix.

×

a. If A commutes with

A =

(cid:20)

a b
0 a

(cid:21)

0 1
0 0

(cid:20)

(cid:21)

, show that

for some a and b.

, B =

(cid:21)

(cid:20)

1
1
2

−

2
3

,
(cid:21)

A =

(cid:20)

a 0
c a

(cid:21)

b. If A commutes with

0 0
1 0

(cid:20)

(cid:21)

, show that

for some a and c.

−
−

1 2 3
1 0 0
1 0
2 5
0 3





a. A =

C =

b. A =

(cid:20)





(cid:20)

C =



−



2 0
1 1
1 2





1 2
0 1

4
1

, B =

(cid:21)

(cid:20)

−

1 6
1 0

,

(cid:21)

−

c. Show that A commutes with every 2

2 matrix

×

for some a.

if and only if A =

a 0
0 a

(cid:20)

(cid:21)

Exercise 2.3.7

a. If A2 can be formed, what can be said about the

size of A?

b. If AB and BA can both be formed, describe the

sizes of A and B.

c. If ABC can be formed, A is 3

what size is B?

3, and C is 5

5,

×

×

Exercise 2.3.8

Exercise 2.3.14 Let A denote an m

n matrix.

×

2.3. Matrix Multiplication

77

a. Find two 2

×
b. Find three 2
(ii) A2 = A.

×

2 matrices A such that A2 = 0.

2 matrices A such that (i) A2 = I;

c. Find 2
BA

×
= 0.

2 matrices A and B such that AB = 0 but

Exercise 2.3.9 Write P =

n and B be m

3

×

3.

×

1 0 0
0 0 1
0 1 0





, and let A be





a. Describe PA in terms of the rows of A.

a. If AX = 0 for every n

A = 0.

b. If YA = 0 for every 1

A = 0.

×

×

1 matrix X , show that

m matrix Y , show that

Exercise 2.3.15

a. If U =

1
0

(cid:20)

2
1

−

, and AU = 0, show that A = 0.
(cid:21)

b. Let U be such that AU = 0 implies that A = 0. If

b. Describe BP in terms of the columns of B.

PU = QU , show that P = Q.

Exercise 2.3.10 Let A, B, and C be as in Exercise 2.3.5.
Find the (3, 1)-entry of CAB using exactly six numerical
multiplications.

Exercise 2.3.11 Compute AB, using the indicated block
partitioning.

Exercise 2.3.16
where A, B, and C represent matrices.

Simplify the following expressions

a. A(3B

C) + (A

−

−

2B)C + 2B(C + 2A)

2
1
0
0

−

1 3 1
0 1 2
0 1 0
0 0 1

1
1
0
1

2 0
0 0
5 1
1 0

−









B = 

−









A = 





Exercise 2.3.12 In each case give formulas for all pow-
ers A, A2, A3, . . . of A using the block decomposition
indicated.

a. A =





b. A = 





1
1
1

1
0
0
0

0
1
1

−

−
1
1
0
0

0
1
1

2
0
1
0

−

−




1
−
0
1
1







I X
I
Y

−
I X

X
(cid:3) (cid:2)
I

(cid:21)
n

I
0

−
0 X
I
0

a.

c.

e.

f.

(cid:20)

(cid:2)

(cid:20)

(cid:20)

×

(cid:21)

b.

d.

I 0
I
Y
T

(cid:21) (cid:20)
I X
n

(cid:3)
any n

1

≥

any n

1

≥

(cid:21)

Exercise 2.3.13 Compute the following using block
multiplication (all blocks are k

k).

I X
I
0

I X T

(cid:20)

(cid:2)

(cid:21) (cid:20)

(cid:3) (cid:2)

I
0

−

X
I

X I

−

(cid:21)
T

(cid:3)

D) + B(C

b. A(B +C
+ (A

−
B)D

−

c. AB(BC

−

CB) + (CA

d. (A

B)(C

−

−

A) + (C

Exercise 2.3.17 If A =

(cid:20)

that A factors in the form A =

A + D)

−

(A + B)C

AB)BC +CA(A

B)(A

−

C) + (C

B)C

A)2

−

−

−

−

−

a b
c d

where a

= 0, show

(cid:21)
1 0
x 1

(cid:20)

(cid:21) (cid:20)

y
z
0 w

.

(cid:21)

Exercise 2.3.18 If A and B commute with C, show that
the same is true of:

a.

A + B

b.

kA, k any scalar

Exercise 2.3.19 If A is any matrix, show that both AAT
and AT A are symmetric.

Exercise 2.3.20 If A and B are symmetric, show that AB
is symmetric if and only if AB = BA.

Exercise 2.3.21 If A is a 2
AT A = AAT if and only if A is symmetric or

2 matrix, show that

×

A =

a
b
b a

(cid:20)

(cid:21)
−
Exercise 2.3.22

for some a and b.

6
6
2 matrices A such that

d. If A is symmetric, then I + A is symmetric.

78

Matrix Algebra

a. Find all symmetric 2

A2 = 0.

×

b. Repeat (a) if A is 3

c. Repeat (a) if A is n

3.

n.

×

×

Exercise 2.3.23 Show that there exist no 2
ces A and B such that AB
(1, 1)- and (2, 2)-entries.]

2 matri-
BA = I. [Hint: Examine the

−

×

Exercise 2.3.24 Let B be an n
AB = 0 for some nonzero m
no n

n matrix C exists such that BC = I.

n matrix. Suppose
n matrix A. Show that

×

×

×

Exercise 2.3.25 An autoparts manufacturer makes fend-
ers, doors, and hoods. Each requires assembly and pack-
aging carried out at factories: Plant 1, Plant 2, and Plant
3. Matrix A below gives the number of hours for assem-
bly and packaging, and matrix B gives the hourly rates at
the three plants. Explain the meaning of the (3, 2)-entry
in the matrix AB. Which plant is the most economical to
operate? Give reasons.

e. If AB = AC and A

= 0, then B = C.

f. If A

= 0, then A2

= 0.

g. If A has a row of zeros, so also does BA for all B.

h. If A commutes with A + B, then A commutes with

B.

i. If B has a column of zeros, so also does AB.

j. If AB has a column of zeros, so also does B.

k. If A has a row of zeros, so also does AB.

l. If AB has a row of zeros, so also does A.

Assembly

Fenders
Doors
Hoods



12
21
10

Packaging
2
3
2



Exercise 2.3.28

= A

a. If A and B are 2

1, show that the rows of AB also sum to 1.

×

2 matrices whose rows sum to

Assembly
Packaging



Plant 1 Plant 2
18
10

21
14

(cid:20)



Plant 3
20
13

(cid:21)

b. Repeat part (a) for the case where A and B are

= B

n.

n

×

Exercise 2.3.26 For the directed graph below, ﬁnd the
adjacency matrix A, compute A3, and determine the num-
ber of paths of length 3 from v1 to v4 and from v2 to v3.

Exercise 2.3.29 Let A and B be n
n matrices for which
the systems of equations Ax = 0 and Bx = 0 each have
only the trivial solution x = 0. Show that the system
(AB)x = 0 has only the trivial solution.

×

v1

v4

v2

v3

Exercise 2.3.27 In each case either show the statement
is true, or give an example showing that it is false.

a. If A2 = I, then A = I.

b. If AJ = A, then J = I.

c. If A is square, then (AT )3 = (A3)T .

Exercise 2.3.30 The trace of a square matrix A, denoted
tr A, is the sum of the elements on the main diagonal of
A. Show that, if A and B are n

n matrices:

×

a.

b.

c.

e.

tr (A + B) = tr A + tr B.

tr (kA) = k tr (A) for any number k.

tr (AT ) = tr (A).

d.

tr (AB) = tr (BA).

tr (AAT ) is the sum of the squares of all entries of
A.

Exercise 2.3.31 Show that AB

BA = I is impossible.

[Hint: See the preceding exercise.]

−

6
6
6
2.4. Matrix Inverses

79

Exercise 2.3.32 A square matrix P is called an
idempotent if P2 = P. Show that:

a. Show that AB is diagonal and AB = BA.

b. Formulate a rule for calculating X A if X is m

a. 0 and I are idempotents.

b.

1 1
0 0
(cid:20)
potents.

,

(cid:21)

(cid:20)

1 0
1 0

, and 1
2

(cid:21)

(cid:20)

1 1
1 1

(cid:21)

, are idem-

c. Formulate a rule for calculating AY if Y is n

Exercise 2.3.34 If A and B are n

n matrices, show that:

×

n.

×
k.

×

c. If P is an idempotent, so is I
P) = 0.

that P(I

−

d. If P is an idempotent, so is PT .

e. If P is an idempotent, so is Q = P + AP

any square matrix A (of the same size as P).

−

P. Show further

a. AB = BA if and only if

−

(A + B)2 = A2 + 2AB + B2

PAP for

b. AB = BA if and only if

f. If A is n

m and B is m

×
BA is an idempotent.

n, and if AB = In, then

×

Exercise 2.3.33 Let A and B be n
(all entries off the main diagonal are zero).

×

n diagonal matrices

(A + B)(A

B) = (A

−

−

B)(A + B)

Exercise 2.3.35 In Theorem 2.3.3, prove

a.

part 3;

b.

part 5.

2.4 Matrix Inverses

Three basic operations on matrices, addition, multiplication, and subtraction, are analogs for matrices of
the same operations for numbers. In this section we introduce the matrix analog of numerical division.

To begin, consider how a numerical equation ax = b is solved when a and b are known numbers. If
1 = 1
a
1 is just dividing by a, and the property of
1a = 1. Moreover, we saw in Section 2.2 that the role that 1 plays in

a = 0, there is no solution (unless b = 0). But if a
to obtain the solution x = a−
a−
arithmetic is played in matrix algebra by the identity matrix I. This suggests the following deﬁnition.

= 0, we can multiply both sides by the inverse a−

1b. Of course multiplying by a−

1 that makes this work is that a−

Deﬁnition 2.11 Matrix Inverses

If A isasquarematrix,amatrix B iscalledaninverseof A ifandonlyif

AB = I

and BA = I

Amatrix A thathasaninverseiscalledaninvertiblematrix.8

Example 2.4.1

Show that B =

1 1
1 0

−

(cid:21)

(cid:20)

is an inverse of A =

0 1
1 1

.

(cid:21)

(cid:20)

8Only square matrices have inverses. Even though it is plausible that nonsquare matrices A and B could exist such that
m, we claim that this forces n = m. Indeed, if m < n there exists a nonzero
n.

AB = Im and BA = In, where A is m
column x such that Ax = 0 (by Theorem 1.3.1), so x = Inx = (BA)x = B(Ax) = B(0) = 0, a contradiction. Hence m
Similarly, the condition AB = Im implies that n

m. Hence m = n so A is square.

n and B is n

×

×

≥

≥

6
80

Matrix Algebra

Solution. Compute AB and BA.

AB =

0 1
1 1

(cid:20)

(cid:21) (cid:20)

1 1
1 0

−

=

(cid:21)

(cid:20)

1 0
0 1

BA =

(cid:21)

(cid:20)

1 1
1 0

−

0 1
1 1

=

(cid:21)

(cid:20)

1 0
0 1

(cid:21)

(cid:21) (cid:20)

Hence AB = I = BA, so B is indeed an inverse of A.

Example 2.4.2

Show that A =

0 0
1 3

(cid:21)

(cid:20)

has no inverse.

Solution. Let B =

a b
c d

(cid:20)

(cid:21)

denote an arbitrary 2

2 matrix. Then

×

AB =

0 0
1 3

(cid:20)

(cid:21) (cid:20)

a b
c d

=

(cid:21)

(cid:20)

0

0

a + 3c b + 3d

(cid:21)

so AB has a row of zeros. Hence AB cannot equal I for any B.

The argument in Example 2.4.2 shows that no zero matrix has an inverse. But Example 2.4.2 also
shows that, unlike arithmetic, it is possible for a nonzero matrix to have no inverse. However, if a matrix
does have an inverse, it has only one.

Theorem 2.4.1

If B andC arebothinversesof A,then B = C.

Proof. Since B and C are both inverses of A, we have CA = I = AB. Hence

B = IB = (CA)B = C(AB) = CI = C

If A is an invertible matrix, the (unique) inverse of A is denoted A−

1. Hence A−

1 (when it exists) is a

square matrix of the same size as A with the property that

AA−

1 = I

and

A−

1A = I

These equations characterize A−

1 in the following sense:

Inverse Criterion: Ifsomehowamatrix B canbefoundsuchthat AB = I and BA = I,then A
isinvertibleand B istheinverseof A;insymbols, B = A−

1.

This is a way to verify that the inverse of a matrix exists. Example 2.4.3 and Example 2.4.4 offer illustra-
tions.

2.4. Matrix Inverses

81

Example 2.4.3

If A =

0
1

(cid:20)

1
−
1
−

(cid:21)

, show that A3 = I and so ﬁnd A−

1.

Solution. We have A2 =

0
1

(cid:20)

1
−
1
−

0
1

1
−
1
−

=

(cid:21)

(cid:20)

1 1
1 0

−
−

(cid:21)

(cid:21) (cid:20)

, and so

A3 = A2A =

1 1
1 0

−
−

(cid:20)

(cid:21) (cid:20)

0
1

1
−
1
−

=

(cid:21)

(cid:20)

1 0
0 1

(cid:21)

= I

Hence A3 = I, as asserted. This can be written as A2A = I = AA2, so it shows that A2 is the inverse

of A. That is, A−

1 = A2 =

1 1
1 0

−
−

.

(cid:21)

(cid:20)

×

a b
c d

(cid:20)

(cid:21)

The next example presents a useful formula for the inverse of a 2

2 matrix A =

when it

exists. To state it, we deﬁne the determinant det A and the adjugate adj A of the matrix A as follows:

a b
c d

det

(cid:20)

= ad

−

(cid:21)

bc,

and

adj

a b
c d

=

(cid:21)

(cid:20)

d
c
−

(cid:20)

b
−
a

(cid:21)

, show that A has an inverse if and only if det A

= 0, and in this case

Example 2.4.4

If A =

a b
c d

(cid:20)

(cid:21)

A−

1 = 1

det A adj A

Solution. For convenience, write e = det A = ad

AB = eI = BA as the reader can verify. So if e

bc and B = adj A =

b
d
−
c
a
−
= 0, scalar multiplication by 1
e gives

−

(cid:20)

. Then

(cid:21)

A( 1

e B) = I = ( 1

e B)A

Hence A is invertible and A−
We prove this by showing that assuming e = 0 leads to a contradiction. In fact, if e = 0, then
1 gives A−
AB = eI = 0, so left multiplication by A−
implies that a, b, c, and d are all zero, so A = 0, contrary to the assumption that A−

e B. Thus it remains only to show that if A−

10; that is, IB = 0, so B = 0. But this

1AB = A−

1 exists.

= 0.

1 exists, then e

1 = 1

As an illustration, if A =

then det A = 2

8

·

4

(

·

−

3) = 28

−

= 0. Hence A is invertible and

(cid:21)

A−

1 = 1

det A adj A = 1
28

, as the reader is invited to verify.

The determinant and adjugate will be deﬁned in Chapter 3 for any square matrix, and the conclusions

2 4
3 8
−
4
−
2

(cid:21)

(cid:20)
8
3

(cid:20)

in Example 2.4.4 will be proved in full generality.

6
6
6
6
82

Matrix Algebra

Inverses and Linear Systems

Matrix inverses can be used to solve certain systems of linear equations. Recall that a system of linear
equations can be written as a single matrix equation

where A and b are known and x is to be determined. If A is invertible, we multiply each side of the equation
on the left by A−

1 to get

Ax = b

A−

1Ax = A−
Ix = A−
x = A−

1b
1b
1b

1b really does
This gives the solution to the system of equations (the reader should verify that x = A−
1b, so
satisfy Ax = b). Furthermore, the argument shows that if x is any solution, then necessarily x = A−
the solution is unique. Of course the technique works only when the coefﬁcient matrix A has an inverse.
This proves Theorem 2.4.2.

Theorem 2.4.2

Supposeasystemof n equationsin n variablesiswritteninmatrixformas

Ax= b

Ifthe n

×

n coefﬁcientmatrix A isinvertible,thesystemhastheuniquesolution

x= A−

1b

Example 2.4.5

Use Example 2.4.4 to solve the system

5x1
3x2 =
4
−
7x1 + 4x2 = 8

−

.

(cid:26)

Solution. In matrix form this is Ax = b where A =

(cid:20)

(

3)

7 = 41, so A is invertible and A−

det A = 5

4

·
·
Theorem 2.4.2 gives

−

−

5
7

3
−
4

1 = 1
41

, x =

(cid:21)

(cid:20)

(cid:20)
4 3
7 5

−

x1
x2

(cid:21)

4
−
8

, and b =

. Then

(cid:20)
(cid:21)
by Example 2.4.4. Thus

(cid:21)

x = A−

1b = 1
41

so the solution is x1 = 8

41 and x2 = 68
41.

4 3
7 5

−

(cid:20)

4
−
8

= 1
41

(cid:21)

(cid:20)

8
68

(cid:21)

(cid:21) (cid:20)

An Inversion Method

2.4. Matrix Inverses

83

If a matrix A is n
The following procedure will be justiﬁed in Section 2.5.

×

n and invertible, it is desirable to have an efﬁcient technique for ﬁnding the inverse.

Matrix Inversion Algorithm

If A isaninvertible(square)matrix,thereexistsasequenceofelementaryrowoperationsthatcarry
A totheidentitymatrix I ofthesamesize,written A
carries I to A−

1. Thealgorithmcanbesummarizedasfollows:

I. Thissameseriesofrowoperations

1;thatis, I

A−

→

→

A I

→

1
I A−

wheretherowoperationson A and I arecarriedoutsimultaneously.

(cid:2)

(cid:3)

(cid:2)

(cid:3)

Example 2.4.6

Use the inversion algorithm to ﬁnd the inverse of the matrix

2 7
1 4
1 3

1
1
−
0 


A =





Solution. Apply elementary row operations to the double matrix

2 7
1 4
1 3



A I

=

(cid:2)

(cid:3)



so as to carry A to I. First interchange rows 1 and 2.

−

1 1 0 0
1 0 1 0
0 0 0 1 


Next subtract 2 times row 1 from row 2, and subtract row 1 from row 3.

1 4
2 7
1 3





−

1 0 1 0
1 1 0 0
0 0 0 1 


1
0
0



Continue to reduced row-echelon form.



−

1 0
3 1
1 0

4
1
−
1
−

1 0
2 0
1 1 


−
−

−

7 0
2 0
1 1 


1 0
0 1
0 0

11
3
−
2
−

4
1
−
1
−
3
3
11
1 0 0 −
−
2
2
2
1
3
1
2 −
2
2
1
1
1
−
2 −
2
2

0 1 0

0 0 1



















84

Matrix Algebra

Hence A−

1 = 1

2 



3
−
1
1

3
−
1
1
−

, as is readily veriﬁed.

11
3
−
1 
−


×

Given any n

n matrix A, Theorem 1.2.1 shows that A can be carried by elementary row operations to
a matrix R in reduced row-echelon form. If R = I, the matrix A is invertible (this will be proved in the next
= I, then R has a row of zeros (it is square), so no system of
section), so the algorithm produces A−
linear equations Ax = b can have a unique solution. But then A is not invertible by Theorem 2.4.2. Hence,
the algorithm is effective in the sense conveyed in Theorem 2.4.3.

1. If R

Theorem 2.4.3

If A isan n
theﬁrstcase,thealgorithmproduces A−

×

1;inthesecondcase, A−

1 doesnotexist.

n matrix,either A canbereducedto I byelementaryrowoperationsoritcannot. In

Properties of Inverses

The following properties of an invertible matrix are used everywhere.

Example 2.4.7: Cancellation Laws

Let A be an invertible matrix. Show that:

1. If AB = AC, then B = C.

2. If BA = CA, then B = C.

1 to obtain A−
Solution. Given the equation AB = AC, left multiply both sides by A−
Thus IB = IC, that is B = C. This proves (1) and the proof of (2) is left to the reader.

1AB = A−

1AC.

Properties (1) and (2) in Example 2.4.7 are described by saying that an invertible matrix can be “left
cancelled” and “right cancelled”, respectively. Note however that “mixed” cancellation does not hold in
general: If A is invertible and AB = CA, then B and C may not be equal, even if both are 2
2. Here is a
speciﬁc example:

×

1 1
0 1

A =

, B =

(cid:20)

(cid:21)

(cid:20)

0 0
1 2

, C =

(cid:21)

(cid:20)

1 1
1 1

(cid:21)

Sometimes the inverse of a matrix is given by a formula. Example 2.4.4 is one illustration; Example 2.4.8
and Example 2.4.9 provide two more. The idea is the Inverse Criterion: If a matrix B can be found such
that AB = I = BA, then A is invertible and A−

1 = B.

Example 2.4.8

If A is an invertible matrix, show that the transpose AT is also invertible. Show further that the
inverse of AT is just the transpose of A−

1; in symbols, (AT )−

1 = (A−

1)T .

6
Solution. A−
inverse of AT . Using the inverse criterion, we test it as follows:

1 exists (by assumption). Its transpose (A−

1)T is the candidate proposed for the

2.4. Matrix Inverses

85

AT (A−
(A−

1)T = (A−
1)T AT = (AA−

1A)T = IT = I
1)T = IT = I

Hence (A−

1)T is indeed the inverse of AT ; that is, (AT )−

1 = (A−

1)T .

Example 2.4.9

If A and B are invertible n
(AB)−

1 = B−

1A−

1.

n matrices, show that their product AB is also invertible and

×

Solution. We are given a candidate for the inverse of AB, namely B−

1A−

1. We test it as follows:

1A−
(B−
(AB)(B−

1)(AB) = B−
1A−

1(A−
1) = A(BB−

1A)B = B−
1)A−

1 = AIA−

1IB = B−
1 = AA−

1B = I
1 = I

Hence B−

1A−

1 is the inverse of AB; in symbols, (AB)−

1 = B−

1A−

1.

We now collect several basic properties of matrix inverses for reference.

Theorem 2.4.4

Allthefollowingmatricesaresquarematricesofthesamesize.

1. I isinvertibleand I−

1 = I.

2. If A isinvertible,sois A−

1,and (A−

1)−

1 = A.

3. If A and B areinvertible,sois AB,and (AB)−

1 = B−

1A−

1.

4. If A1, A2, . . . , Ak areallinvertible,soistheirproduct A1A2

Ak,and

(A1A2

· · ·

Ak)−

1
1 = A−
k

· · ·

· · ·
1
1
2 A−
A−
1 .

5. If A isinvertible,sois Ak forany k

≥

1,and (Ak)−

1 = (A−

1)k.

6. If A isinvertibleand a

= 0 isanumber,then aA isinvertibleand (aA)−

1 = 1

aA−

1.

7. If A isinvertible,soisitstranspose AT,and (AT )−

1 = (A−

1)T.

Proof.

1. This is an immediate consequence of the fact that I2 = I.

2. The equations AA−

1 = I = A−

1A show that A is the inverse of A−

1; in symbols, (A−

1)−

1 = A.

6
86

Matrix Algebra

3. This is Example 2.4.9.

4. Use induction on k. If k = 1, there is nothing to prove, and if k = 2, the result is property 3. If
1
1
2 A−
A−
1 . We apply this fact together
1)−

Ak

k > 2, assume inductively that (A1A2
with property 3 as follows:

· · ·

−

1
1 = A−
k
1 · · ·
−

[A1A2

Ak

· · ·

−

1Ak]−

Ak

1 = [(A1A2
1
= A−
k
1
= A−
k

· · ·
(A1A2
· · ·
1
A−
k
1 · · ·
−

1
1) Ak]−
−
1
Ak
1)−
−
1
1
2 A−
A−
1

So the proof by induction is complete.

(cid:0)

(cid:1)

5. This is property 4 with A1 = A2 =

= Ak = A.

· · ·

6. This is left as Exercise 2.4.29.

7. This is Example 2.4.8.

The reversal of the order of the inverses in properties 3 and 4 of Theorem 2.4.4 is a consequence of
the fact that matrix multiplication is not commutative. Another manifestation of this comes when matrix
equations are dealt with. If a matrix equation B = C is given, it can be left-multiplied by a matrix A to yield
AB = AC. Similarly, right-multiplication gives BA = CA. However, we cannot mix the two: If B = C, it

need not be the case that AB = CA even if A is invertible, for example, A =

Part 7 of Theorem 2.4.4 together with the fact that (AT )T = A gives

1 1
0 1

, B =

(cid:21)

(cid:20)

0 0
1 0

(cid:21)

(cid:20)

= C.

Corollary 2.4.1

Asquarematrix A isinvertibleifandonlyif AT isinvertible.

Example 2.4.10

Find A if (AT

2I)−

1 =

−

2 1
1 0

−

.
(cid:21)

(cid:20)

Solution. By Theorem 2.4.4(2) and Example 2.4.4, we have

(AT

−

2I) =

AT

−

1
−

2I

1
−

=

h(cid:0)

=

(cid:21)

(cid:20)

2
1

1
−
4

i

(cid:1)

, so A =

(cid:21)

Hence AT = 2I +

0
1

1
−
2

(cid:20)

2 1
1 0

−

2 1
1 4

−

(cid:20)

(cid:20)

(cid:21)

(cid:21)

1
−

=

0
1

1
−
2

(cid:21)

(cid:20)

by Theorem 2.4.4(7).

The following important theorem collects a number of conditions all equivalent9 to invertibility. It will

be referred to frequently below.

9If p and q are statements, we say that p implies q (written p

q) if q is true whenever p is true. The statements are called

equivalent if both p

q and q

⇒

⇒

p (written p

⇔

q, spoken “p if and only if q”). See Appendix B.

⇒

2.4. Matrix Inverses

87

Theorem 2.4.5: Inverse Theorem

Thefollowingconditionsareequivalentforan n

n matrix A:

×

1. A isinvertible.

2. Thehomogeneoussystem Ax= 0hasonlythetrivialsolutionx= 0.

3. A canbecarriedtotheidentitymatrix In byelementaryrowoperations.

4. Thesystem Ax= bhasatleastonesolutionxforeverychoiceofcolumnb.

5. Thereexistsan n

×

n matrixC suchthat AC = In.

Proof. We show that each of these conditions implies the next, and that (5) implies (1).

(1)
(2)

⇒
⇒

1 exists, then Ax = 0 gives x = Inx = A−

(2). If A−
(3). Assume that (2) is true. Certainly A

1Ax = A−

10 = 0.

R by row operations where R is a reduced, row-
echelon matrix. It sufﬁces to show that R = In. Suppose that this is not the case. Then R has a row
of the system Ax = 0. Then
of zeros (being square). Now consider the augmented matrix
R 0
also has a row of zeros. Since R is square there
must be at least one nonleading variable, and hence at least one parameter. Hence the system Ax = 0 has
(cid:2)
inﬁnitely many solutions, contrary to (2). So R = In after all.

is the reduced form, and

R 0

A 0

A 0

→

→

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

⇒

In by a
→
for some column c.

(3)

(4). Consider the augmented matrix

A b

of the system Ax = b. Using (3), let A

(4)

(5). Write In =

sequence of row operations. Then these same operations carry
(cid:2)
Hence the system Ax = b has a solution (in fact unique) by gaussian elimination. This proves (4).
(cid:3)
where e1, e2, . . . , en are the columns of In. For each

en
j = 1, 2, . . . , n, the system Ax = e j has a solution c j by (4), so Ac j = e j. Now let C =
be the n

n matrix with these matrices c j as its columns. Then Deﬁnition 2.9 gives (5):
(cid:2)
en

Ac1 Ac2

AC = A

e1 e2

c1 c2

e1 e2

= In

Acn

A b

· · ·

⇒

→

cn

=

×

=

In

c

(cid:3)

(cid:2)

(cid:2)

(cid:3)

(cid:3)

(cid:2)

c1 c2

cn

· · ·

(cid:3)

(cid:2)

⇒

· · ·

· · ·
(1). Assume that (5) is true so that AC = In for some matrix C. Then Cx = 0 implies x = 0 (because
(5)
x = Inx = ACx = A0 = 0). Thus condition (2) holds for the matrix C rather than A. Hence the argument
(5) (with A replaced by C) shows that a matrix C′ exists such that CC′ = In.
above that (2)
But then

(3)

(4)

· · ·

⇒

⇒

⇒

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

Thus CA = CC′ = In which, together with AC = In, shows that C is the inverse of A. This proves (1).

A = AIn = A(CC′) = (AC)C′ = InC′ = C′

The proof of (5)

(1) in Theorem 2.4.5 shows that if AC = I for square matrices, then necessarily

CA = I, and hence that C and A are inverses of each other. We record this important fact for reference.

⇒

Corollary 2.4.1

If A andC aresquarematricessuchthat AC = I,thenalsoCA = I. Inparticular,both A andC are
invertible,C = A−

1,and A = C−

1.

Here is a quick way to remember Corollary 2.4.1. If A is a square matrix, then

88

Matrix Algebra

1. If AC = I then C = A−

1.

2. If CA = I then C = A−

1.

Observe that Corollary 2.4.1 is false if A and C are not square matrices. For example, we have

1 2 1
1 1 1

(cid:20)

1
−
1
0



(cid:21)



1
1
−
1 


= I2

but

1
−
1
0





1
1
−
1 


1 2 1
1 1 1

(cid:20)

= I3

(cid:21)

In fact, it is veriﬁed in the footnote on page 79 that if AB = Im and BA = In, where A is m
n

m, then m = n and A and B are (square) inverses of each other.
An n

n matrix A has rank n if and only if (3) of Theorem 2.4.5 holds. Hence

×

n and B is

×

×

Corollary 2.4.2

An n

×

n matrix A isinvertibleifandonlyif rank A = n.

Here is a useful fact about inverses of block matrices.

Example 2.4.11

Let P =

m

= n).

(cid:20)

A X
0 B

(cid:21)

and Q =

A 0
Y B

(cid:20)

(cid:21)

be block matrices where A is m

m and B is n

n (possibly

×

×

a. Show that P is invertible if and only if A and B are both invertible. In this case, show that

P−

1 =

1
A−
0

(cid:20)

A−

1X B−
1
1
B−

−

(cid:21)

b. Show that Q is invertible if and only if A and B are both invertible. In this case, show that

Q−

1 =

(cid:20)

1
A−
1YA−

B−

0
1
1 B−

−

Solution. We do (a.) and leave (b.) for the reader.

(cid:21)

. Using block multiplication, one

a. If A−

1 and B−

1 both exist, write R =

1
A−
0

A−

1X B−
1
1
B−

−

(cid:20)

(cid:21)

veriﬁes that PR = Im+n = RP, so P is invertible, and P−

1 = R. Conversely, suppose that P is

invertible, and write P−

Then the equation PP−

1 =

C V
W D
1 = In+m becomes

(cid:21)

(cid:20)

in block form, where C is m

m and D is n

n.

×

×

A X
0 B

(cid:20)

(cid:21) (cid:20)

C V
W D

=

(cid:21)

(cid:20)

AC + XW AV + X D

BW

BD

= Im+n =

(cid:21)

Im 0
In
0

(cid:20)

(cid:21)

6
6
2.4. Matrix Inverses

89

using block notation. Equating corresponding blocks, we ﬁnd

AC + XW = Im,

BW = 0,

and BD = In

Hence B is invertible because BD = In (by Corollary 2.4.1), then W = 0 because BW = 0,
and ﬁnally, AC = Im (so A is invertible, again by Corollary 2.4.1).

Inverses of Matrix Transformations

Let T = TA : Rn
it may very well be invertible, and this leads to the question:

Rn denote the matrix transformation induced by the n

→

n matrix A. Since A is square,

×

What does it mean geometrically for T that A is invertible?

To answer this, let T ′ = TA−

1 : Rn

Rn denote the transformation induced by A−

1. Then

→
T ′ [T (x)] = A−

1 [Ax] = Ix = x

for all x in Rn

(2.8)

T [T ′(x)] = A

A−

1x

= Ix = x

The ﬁrst of these equations asserts that, if T carries x to a vector T (x), then T ′ carries T (x) right back to
x; that is T ′ “reverses” the action of T . Similarly T “reverses” the action of T ′. Conditions (2.8) can be
stated compactly in terms of composition:

(cid:2)

(cid:3)

T = 1Rn

and

T ′

◦

T ′ = 1Rn

T

◦

(2.9)

When these conditions hold, we say that the matrix transformation T ′ is an inverse of T , and we have
shown that if the matrix A of T is invertible, then T has an inverse (induced by A−

1).

The converse is also true: If T has an inverse, then its matrix A must be invertible. Indeed, suppose
S = 1Rn. It can be shown that S is also a matrix

T = 1Rn and T

S : Rn
Rn is any inverse of T , so that S
transformation. If B is the matrix of S, we have

→

◦

◦

BAx = S [T (x)] = (S

◦

T )(x) = 1Rn(x) = x = Inx

for all x in Rn

It follows by Theorem 2.2.6 that BA = In, and a similar argument shows that AB = In. Hence A is invertible
1, so S = T ′ using the earlier
with A−
notation. This proves the following important theorem.

1 = B. Furthermore, the inverse transformation S has matrix A−

Theorem 2.4.6
Let T : Rn

→

Rn denotethematrixtransformationinducedbyan n

n matrix A. Then

×

A isinvertibleifandonlyif T hasaninverse.

Inthiscase, T hasexactlyoneinverse(whichwedenoteas T −
transformationinducedbythematrix A−

1. Inotherwords

1),and T −

1 : Rn

Rn isthe

→

(TA)−

1 = TA−

1

90

Matrix Algebra

The geometrical relationship between T and T −

1 is embodied in equations (2.8) above:

T −

1 [T (x)] = x

and

T

T −

1(x)

= x

for all x in Rn

These equations are called the fundamental identities relating T and T −
that each of T and T −

1 “reverses” or “undoes” the action of the other.

(cid:2)

(cid:3)

1. Loosely speaking, they assert

This geometric view of the inverse of a linear transformation provides a new way to ﬁnd the inverse of

a matrix A. More precisely, if A is an invertible matrix, we proceed as follows:

1. Let T be the linear transformation induced by A.

2. Obtain the linear transformation T −

1 which “reverses” the action of T .

3. Then A−

1 is the matrix of T −

1.

Here is an example.

Example 2.4.12

Find the inverse of A =

y

x
y

Q1

(cid:20)

=

(cid:21)

(cid:20)

y
x

(cid:21)

transformation R2

(cid:20)
R2.

0 1
1 0

(cid:21)

by viewing it as a linear

→
x
y

x
y

y = x

the vector Ax =

Solution. If x =

y
x
(cid:20)
is the result of reﬂecting x in the line y = x (see the diagram).
Hence, if Q1 : R2
A is the matrix of Q1. Now observe that Q1 reverses itself because
1
reﬂecting a vector x twice results in x. Consequently Q−
1 = Q1.
1 = A. Of course this
Since A−
conclusion is clear by simply observing directly that A2 = I, but the geometric method can often
work where these other methods may be less straightforward.

(cid:21)
(cid:21)
R2 denotes reﬂection in the line y = x, then

1
1 is the matrix of Q−
1 and A is the matrix of Q, it follows that A−

0 1
1 0

(cid:21) (cid:20)

→

x
y

=

(cid:20)

(cid:20)

(cid:21)

x

0

(cid:20)

(cid:21)

Exercises for 2.4

Exercise 2.4.1 In each case, show that the matrices are
inverses of each other.

3 0
0 5

1
3
0

,

(cid:21)

(cid:20)

d.

(cid:20)

0
1
5 (cid:21)

a.

b.

c.

(cid:20)

(cid:20)





3 5
1 2

2
1

−

,

(cid:20)

5
−
3

(cid:21)
0
4

3
1

(cid:21)
−
1 2 0
0 2 3
1 3 1

(cid:21)
0
3

−
7
3
2

(cid:21)
2
1
1

−

4
1

, 1
2

(cid:20)

,




−





Exercise 2.4.2 Find the inverse of each of the following
matrices.

a.

c.

1
1

−

1
3

(cid:20)

−

1
3
1

−

0
2
1

−





(cid:21)

−

1
0
0





b.

d.

4 1
3 2

1
5
2

−
−

(cid:20)





(cid:21)

1
−
7
3

2
11
5

−
−





6
3
2

−

−





3 5 0
3 7 1
1 2 1

2 4 1
3 3 2
4 1 4







3
1
1

1
0
1
1


1 2
1 3
2 4

−





0 7 5
1 3 6
1 5 2
1 5 1

−
−







e.

g.

i.

k.



















f.

h.

j.

l.























3 1
2 1
1 5

3 1
5 2
1 1

−

1
0
1
0

1
0
1

1
0
1








5
0
2
1

−

−

−

−
4
0
2
1

−
−

−
−
1 2 0 0 0
0 1 3 0 0
0 0 1 5 0
0 0 0 1 7
0 0 0 0 1

2
1
−
0
0















Exercise 2.4.3 In each case, solve the systems of equa-
tions by ﬁnding the inverse of the coefﬁcient matrix.

a.

c.

3x
y = 5
2x + 2y = 1

−

x + y + 2z = 5
x + y + z = 0
x + 2y + 4z =
2

−

b.

d.

3y = 0
4y = 1

2x
x

−
−

x + 4y + 2z = 1
2x + 3y + 3z =
1
−
4x + y + 4z = 0

Exercise 2.4.4 Given A−

1 =

1
2
1

−

1 3
0 5
1 0





−

:




1
1
3

−

.








a. Solve the system of equations Ax =

b. Find a matrix B such that

AB =



1
0
1

−

1 2
1 1
0 0

c. Find a matrix C such that
1 2
3 1

CA =

1
1

−

.




.
(cid:21)



(cid:20)

Exercise 2.4.5 Find A when

a.

(3A)−

1 =

c.

(I + 3A)−

(cid:20)
1 =

1
0

1
−
1

2
1

(cid:20)

1
2

−

1
3

(cid:20)

1

−

(cid:21)

b.

(2A)T =

(cid:21)
0
1

−

(cid:21)

2.4. Matrix Inverses

91

d.

(I

−

2AT )−

1 =

e.

A

(cid:18)

1
0

(cid:20)
1 0
2 1

−

1
1

A

(cid:20)

2 1
1 1

1

−

=

(cid:21)

2 3
1 1

(cid:21)

(cid:21)(cid:19)
1
−

=

(cid:20)
1 0
2 2

(cid:21)

(cid:18)(cid:20)
AT

2I

−

(cid:21)
1

−

(cid:19)

= 2

(cid:20)
1 1
2 3

(cid:0)
A−

1

(cid:1)
2I

T

−

(cid:0)

(cid:1)

(cid:20)

=

2

−

(cid:20)

(cid:21)
1 1
1 0

(cid:21)

f.

g.

h.

Exercise 2.4.6 Find A when:

a.

A−

1 =

1
2
0

1
−
1
2





3
1
2

−





b.

A−

1 =

0 1
1 2
1 0

−

1
1
1









−


1 2
0 4
1 0

y1
y2
y3









, express the

y1
y2
y3



Exercise 2.4.7 Given

and



z1
z2
z3

=





1
2
1

−





x1
x2
x3
1
−
3
−
1

3
1
2













=

1
0
2

−

variables x1, x2, and x3 in terms of z1, z2, and z3.













Exercise 2.4.8

a. In the system

3x + 4y = 7
4x + 5y = 1

, substitute the new vari-

ables x′ and y′ given by

x and y.

x =
−
y = 4x′ −

5x′ + 4y′
3y′

. Then ﬁnd

b. Explain part (a) by writing the equations as

x
y

7
1

A

=

and

= B

(cid:20)

(cid:20)
(cid:21)
the relationship between A and B?

(cid:21)

(cid:20)

(cid:21)

(cid:20)

x
y

. What is

x′
y′ (cid:21)

Exercise 2.4.9 In each case either prove the assertion or
give an example showing that it is false.

a. If A

= 0 is a square matrix, then A is invertible.

b. If A and B are both invertible, then A + B is invert-

ible.

c. If A and B are both invertible, then (A−

1B)T is in-

vertible.

d. If A4 = 3I, then A is invertible.

e. If A2 = A and A

= 0, then A is invertible.

6
6
92

Matrix Algebra

f. If AB = B for some B

= 0, then A is invertible.

by computing (a) A6; (b) B4; and (c) C3.

g. If A is invertible and skew symmetric (AT =

the same is true of A−

1.

A),

−

Exercise 2.4.16 Find the inverse of

h. If A2 is invertible, then A is invertible.

terms of c.

1 0 1
c 1 c
3 c 2





in





i. If AB = I, then A and B commute.

Exercise 2.4.10

a. If A, B, and C are square matrices and AB = I,
1.
I = CA, show that A is invertible and B = C = A−

b. If C−

1 = A, ﬁnd the inverse of CT in terms of A.

n and
m. Consider the system Ax = b of n equations in

Exercise 2.4.11 Suppose CA = Im, where C is m
A is n
×
m variables.

×

Exercise 2.4.17

If c

= 0, ﬁnd the inverse of

1
2
0

−
−

1 1
1 2
2 c





in terms of c.


Exercise 2.4.18 Show that A has no inverse when:



a. A has a row of zeros.

b. A has a column of zeros.

c. each row of A sums to 0.
[Hint: Theorem 2.4.5(2).]

d. each column of A sums to 0.

a. Show that this system has a unique solution CB if

[Hint: Corollary 2.4.1, Theorem 2.4.4.]

it is consistent.

b. If C =

0
3

−

5
0

1
1

and A =

(cid:20)

−

(cid:21)
ﬁnd x (if it exists) when
1
0
3

(i) b =





; and (ii) b =





7
4
22





2
1
6





3
−
2
−
10

−

,





Exercise 2.4.19 Let A denote a square matrix.

a. Let YA = 0 for some matrix Y

= 0. Show that
[Hint: Corollary 2.4.1, Theo-

A has no inverse.
rem 2.4.4.]

.





b. Use part (a) to show that (i)

Exercise 2.4.12 Verify that A =

(cid:21)
3A + 2I = 0, and use this fact to show that

(cid:20)

1
0

1
−
2

satisﬁes

(ii)



2 1
1 1
1 0

A2
A−

−
1 = 1

2 (3I

A).

−

have no inverse.

−

−

1
0
1







[Hint: For part (ii) compare row 3 with the differ-
ence between row 1 and row 2.]

1
0
1

−

1 1
1 1
0 2





; and





Exercise 2.4.13 Let Q = 

pute QQT and so ﬁnd Q−

a
b
c
d


1 if Q


c
−
d
−
a
b

−

d
c
b
−
a

−

b
a
d
c
−
= 0.

. Com-







Exercise 2.4.14 Let U =

. Show that each of

U , and

U ,
−
any two of these is the third.

−

(cid:21)
I2 is its own inverse and that the product of

(cid:20)

0 1
1 0

1 1
1 0

−

,

(cid:21)

Exercise 2.4.15 Consider A =

0
1

−

1
0

B =

(cid:20)

, C =

(cid:21)

(cid:20)
0 1 0
0 0 1
5 0 0





. Find the inverses

c. (BA)2 = 0.





Exercise 2.4.20 If A is invertible, show that

a.

A2

= 0.

b.

Ak
= 0 for all
k = 1, 2, . . . .

Exercise 2.4.21 Suppose AB = 0, where A and B are
square matrices. Show that:

a. If one of A and B has an inverse, the other is zero.

b. It is impossible for both A and B to have inverses.

6
6
6
6
6
6
Exercise 2.4.22 Find the inverse of the x-expansion in
Example 2.2.16 and describe it geometrically.

Exercise 2.4.23 Find the inverse of the shear transfor-
mation in Example 2.2.17 and describe it geometrically.

Exercise 2.4.24 In each case assume that A is a square
matrix that satisﬁes the given condition. Show that A is
invertible and ﬁnd a formula for A−

1 in terms of A.

3A + 2I = 0.

a. A3

−
b. A4 + 2A3

A

−

−

4I = 0.

2.4. Matrix Inverses

93

d. If An = 0, ﬁnd the formula for (I

A)−

1.

−

Exercise 2.4.29 Prove property 6 of Theorem 2.4.4:
= 0, then aA is invertible and
If A is invertible and a
a A−
(aA)−

1 = 1

1

Exercise 2.4.30 Let A, B, and C denote n
Using only Theorem 2.4.4, show that:

×

n matrices.

a. If A, C, and ABC are all invertible, B is invertible.

b. If AB and BA are both invertible, A and B are both

invertible.

Exercise 2.4.25 Let A and B denote n

n matrices.

×

a. If A and AB are invertible, show that B is invertible

using only (2) and (3) of Theorem 2.4.4.

b. If AB is invertible, show that both A and B are in-

vertible using Theorem 2.4.5.

Exercise 2.4.31 Let A and B denote invertible n
trices.

×

n ma-

a. If A−

1 = B−

1, does it mean that A = B? Explain.

b. Show that A = B if and only if A−

1B = I.

Exercise 2.4.26 In each case ﬁnd the inverse of the ma-
trix A using Example 2.4.11.

Exercise 2.4.32 Let A, B, and C be n
A and B invertible. Show that

×

n matrices, with

2
1
1

−

1 1
0 2
0 1

3
2
1
3

−



−
−

4 0 0
3 0 0
1 1 3
1 1 4

2 1
1 1
0 0
0 0

−

5
1
1
1

−
−

b.

A =

3 1
5 2
1 3





0
0
1

−







a. If A commutes with C, then A−

1 commutes with

C.

b. If A commutes with B, then A−

B−

1.

1 commutes with




2
0
1
2

Exercise 2.4.33 Let A and B be square matrices of the
same size.

a. Show that (AB)2 = A2B2 if AB = BA.

b. If A and B are invertible and (AB)2 = A2B2, show

that AB = BA.







a.

A =





c.

A = 





d.

A = 





Exercise 2.4.27 If A and B are invertible symmetric ma-
1, and
trices such that AB = BA, show that A−
A−

1 are also invertible and symmetric.

1, AB, AB−

1B−

Exercise 2.4.28 Let A be an n
n

n identity matrix.

×

n matrix and let I be the

×

a. If A2 = 0, verify that (I

A)−

1 = I + A.

−

b. If A3 = 0, verify that (I

A)−

1 = I + A + A2.

1 0
0 0

c. If A =

and B =

(cid:20)

(cid:21)

(AB)2 = A2B2 but AB

(cid:20)

= BA.

1 1
0 0

(cid:21)

, show that

Exercise 2.4.34 Let A and B be n
AB is invertible. Show that A and B are both invertible.

n matrices for which

×

Exercise 2.4.35 Consider A =

1
2
1





−

1
5
13

3
1
7

−

,





c. Find the inverse of





−

1
3
1

.





B =





−

1 1
3 0
2 5

2
3
−
17

.





−
1 2
0 1
0 0

6
6
94

Matrix Algebra

a. Show that A is not invertible by ﬁnding a nonzero

c. U is self-inverse if and only if U = I

1

3 matrix Y such that YA = 0.

idempotent P.

2P for some

−

×

[Hint: Row 3 of A equals 2(row 2)

3(row 1).]

−

b. Show that B is not invertible.

[Hint: Column 3 = 3(column 2)

column 1.]

−

Exercise 2.4.36 Show that a square matrix A is invert-
ible if and only if it can be left-cancelled: AB = AC im-
plies B = C.
Exercise 2.4.37 If U 2 = I, show that I +U is not invert-
ible unless U = I.

Exercise 2.4.38

a. If J is the 4

that I

−

4 matrix with every entry 1, show

×

1
2 J is self-inverse and symmetric.

b. If X is n

m and satisﬁes X T X = Im, show that

×

2X X T is self-inverse and symmetric.

In

−

Exercise 2.4.39 An n
×
tent if P2 = P. Show that:

n matrix P is called an idempo-

a. I is the only invertible idempotent.

b. P is an idempotent if and only if I

inverse.

2P is self-

−

2.5 Elementary Matrices

d. I
(I

−
−

aP is invertible for any a
aP)−

1 = I +

P.

a

a

1
−

= 1, and that

(cid:0)

(cid:1)

Exercise 2.4.40 If A2 = kA, where k
invertible if and only if A = kI.

= 0, show that A is

Exercise 2.4.41 Let A and B denote n
trices.

×

n invertible ma-

a. Show that A−

1 + B−

1 = A−

1(A + B)B−

1.

b. If A + B is also invertible, show that A−
invertible and ﬁnd a formula for (A−

1 + B−

1 + B−
1)−

1 is
1.

Exercise 2.4.42 Let A and B be n
be the n

n identity matrix.

×

n matrices, and let I

×

a. Verify that A(I + BA) = (I + AB)A and that

(I + BA)B = B(I + AB).

b. If I + AB is invertible, verify that I + BA is also in-
1A.

vertible and that (I + BA)−

B(I + AB)−

1 = I

−

It is now clear that elementary row operations are important in linear algebra: They are essential in solving
linear systems (using the gaussian algorithm) and in inverting a matrix (using the matrix inversion algo-
rithm). It turns out that they can be performed by left multiplying by certain invertible matrices. These
matrices are the subject of this section.

Deﬁnition 2.12 Elementary Matrices

×

n matrix E iscalledanelementarymatrixifitcanbeobtainedfromtheidentitymatrix In
An n
byasingleelementaryrowoperation(calledtheoperationcorresponding to E). Wesaythat E is
oftypeI,II,orIIIiftheoperationisofthattype(seeDeﬁnition1.2).

Hence

E1 =

0 1
1 0

(cid:20)

(cid:21)

1 0
0 9

(cid:20)

(cid:21)

,

E2 =

,

and

E3 =

1 5
0 1

(cid:20)

(cid:21)

are elementary of types I, II, and III, respectively, obtained from the 2
rows 1 and 2, multiplying row 2 by 9, and adding 5 times row 2 to row 1.

×

2 identity matrix by interchanging

Suppose now that the matrix A =

a b c
p q r

(cid:21)

(cid:20)

is left multiplied by the above elementary matrices E1,

6
6
2.5. Elementary Matrices

95

E2, and E3. The results are:

E1A =

E2A =

E3A =

(cid:20)

(cid:20)

0 1
1 0

1 0
0 9

1 5
0 1

(cid:21) (cid:20)

(cid:21) (cid:20)

(cid:21) (cid:20)

a b c
p q r

a b c
p q r

a b c
p q r

(cid:21)

(cid:21)

=

=

=

(cid:20)

(cid:20)

(cid:21)

(cid:20)

(cid:20)

p q r
a b c

(cid:21)
c
b
a
9p 9q 9r

(cid:21)

a + 5p b + 5q c + 5r
q

p

r

(cid:21)

In each case, left multiplying A by the elementary matrix has the same effect as doing the corresponding
row operation to A. This works in general.

Lemma 2.5.1: 10
Ifanelementaryrowoperationisperformedonan m
elementarymatrixobtainedbyperformingthesameoperationonthe m

×

n matrix A,theresultis EA where E isthe

m identitymatrix.

×

Proof. We prove it for operations of type III; the proofs for types I and II are left as exercises. Let E be the
= p. The proof depends
elementary matrix corresponding to the operation that adds k times row p to row q
on the fact that each row of EA is equal to the corresponding row of E times A. Let K1, K2, . . . , Km denote
the rows of Im. Then row i of E is Ki if i

= q, while row q of E is Kq + kKp. Hence:

= q then row i of EA = KiA = (row i of A).

If i
Row q of EA = (Kq + kKp)A = KqA + k(KpA)

= (row q of A) plus k (row p of A).

Thus EA is the result of adding k times row p of A to row q, as required.

The effect of an elementary row operation can be reversed by another such operation (called its inverse)
which is also elementary of the same type (see the discussion following (Example 1.1.3). It follows that
In fact, if a row operation on I produces E, then the inverse
each elementary matrix E is invertible.
operation carries E back to I. If F is the elementary matrix corresponding to the inverse operation, this
means FE = I (by Lemma 2.5.1). Thus F = E−

1 and we have proved

Lemma 2.5.2

Everyelementarymatrix E isinvertible,and E−
Moreover, E−

1 correspondstotheinverseoftherowoperationthatproduces E.

1 isalsoaelementarymatrix(ofthesametype).

The following table gives the inverse of each type of elementary row operation:

Type
I
II
III

Operation
Interchange rows p and q
Multiply row p by k
= 0
Add k times row p to row q

Inverse Operation
Interchange rows p and q

Multiply row p by 1/k, k

= 0

= p Subtract k times row p from row q, q

= p

Note that elementary matrices of type I are self-inverse.

10A lemma is an auxiliary theorem used in the proof of other theorems.

6
6
6
6
6
6
6
96

Matrix Algebra

Example 2.5.1

Find the inverse of each of the elementary matrices

E1 =

0 1 0
1 0 0
0 0 1 


,





E2 =

1 0 0
0 1 0
0 0 9 


,





and E3 =

1 0 5
0 1 0
0 0 1 


.





Solution. E1, E2, and E3 are of type I, II, and III respectively, so the table gives

1
E−
1 =

0 1 0
1 0 0
0 0 1 






= E1,

1
E−
2 =

1 0 0
0 1 0
0 0 1
9

,









1
and E−
3 =

1 0
0 1
0 0





.

5
−
0
1 


Inverses and Elementary Matrices

B) by a series of k elementary row
Suppose that an m
operations. Let E1, E2, . . . , Ek denote the corresponding elementary matrices. By Lemma 2.5.1, the
reduction becomes

n matrix A is carried to a matrix B (written A

→

×

A

→

E1A

→

E2E1A

→

E3E2E1A

→ · · · →

EkEk

1 · · ·
−

E2E1A = B

In other words,

A

UA = B where U = EkEk

E2E1

1
−

→

· · ·
The matrix U = EkEk
E2E1 is invertible, being a product of invertible matrices by Lemma 2.5.2.
Moreover, U can be computed without ﬁnding the Ei as follows: If the above series of operations carrying
A
U Im = U . Hence this series of operations carries
the block matrix

B is performed on Im in place of A, the result is Im

. This, together with the above discussion, proves

1 · · ·
−

B U

→

→

A Im

→

(cid:2)
Theorem 2.5.1

Suppose A is m

(cid:3)

(cid:2)

(cid:3)

n and A

×

→

B byelementaryrowoperations.

1. B = UA whereU isan m

2. U canbecomputedby

×
A Im

m invertiblematrix.

B U

usingtheoperationscarrying A

B.

→

→

3. U = EkEk

E2E1 where E1, E2, . . . , Ek aretheelementarymatricescorresponding(in

(cid:3)

(cid:2)

(cid:3)

(cid:2)

order)totheelementaryrowoperationscarrying A to B.

1 · · ·
−

Example 2.5.2

2 3 1
1 2 1

If A =

(cid:20)

, express the reduced row-echelon form R of A as R = UA where U is invertible.

(cid:21)

2.5. Elementary Matrices

97

Solution. Reduce the double matrix

A I

R U

as follows:

→

A I

=

(cid:2)

(cid:20)

(cid:3)

(cid:2)
2 3 1 1 0
1 2 1 0 1

(cid:2)

(cid:3)
(cid:3)
1 2 1 0 1
2 3 1 1 0

→

(cid:20)

(cid:21)

1
0

2
1
−
1 0
0 1

→

(cid:21)

→

(cid:20)

(cid:20)

Hence R =

1 0
0 1

1
−
1

(cid:21)

(cid:20)

and U =

2
1
−

(cid:20)

3
−
2

.

(cid:21)

1 0
1 1

2
1
−

−
1
−
1

1
2
−
3
−
2

(cid:21)

(cid:21)

A I

Now suppose that A is invertible. We know that A

I by Theorem 2.4.5, so taking B = I in Theo-
where I = UA. Thus U = A−
rem 2.5.1 gives
.
This is the matrix inversion algorithm in Section 2.4. However, more is true: Theorem 2.5.1 gives
(cid:3)
(cid:3)
E2E1 where E1, E2, . . . , Ek are the elementary matrices corresponding (in order) to
A−
the row operations carrying A

(cid:2)
1 = U = EkEk
1
−

1, so we have

1
I A−

I. Hence

I U

A I

· · ·

→

→

→

(cid:3)

(cid:2)

(cid:2)

(cid:3)

(cid:2)

1
−

= (EkEk

1
−

· · ·

E2E1)−

1 = E−
1
1
1 E−
2

1
1
1E−
E−
k
k
−

· · ·

(2.10)

By Lemma 2.5.2, this shows that every invertible matrix A is a product of elementary matrices. Since
elementary matrices are invertible (again by Lemma 2.5.2), this proves the following important character-
ization of invertible matrices.

(cid:0)

(cid:1)

→
1
A−

A =

Theorem 2.5.2

Asquarematrixisinvertibleifandonlyifitisaproductofelementarymatrices.

It follows from Theorem 2.5.1 that A

B by row operations if and only if B = UA for some invertible

→
matrix U . In this case we say that A and B are row-equivalent. (See Exercise 2.5.17.)

Example 2.5.3

Express A =

2 3
1 0

−

as a product of elementary matrices.

(cid:21)
Solution. Using Lemma 2.5.1, the reduction of A

(cid:20)

I is as follows:

→

2 3
1 0

−

A =

(cid:20)

→

(cid:21)

E1A =

1 0
2 3

−

(cid:20)

→

(cid:21)

E2E1A =

1 0
0 3

(cid:20)

→

(cid:21)

E3E2E1A =

1 0
0 1

(cid:21)

(cid:20)

where the corresponding elementary matrices are

E1 =

0 1
1 0

,

(cid:21)

(cid:20)

E2 =

1 0
2 1

,

(cid:21)

(cid:20)

E3 =

1 0
0 1

3 (cid:21)

(cid:20)

Hence (E3 E2 E1)A = I, so:

A = (E3E2E1)−

1 = E−
1
1
1
2 E−
1 E−
3 =

0 1
1 0

1 0
2 1

−

(cid:21) (cid:20)

1 0
0 3

(cid:21)

(cid:21) (cid:20)

(cid:20)

98

Matrix Algebra

Smith Normal Form

Let A be an m
that R = UA where U is invertible, and that U can be found from

×

n matrix of rank r, and let R be the reduced row-echelon form of A. Theorem 2.5.1 shows

R U

.

A Im

The matrix R has r leading ones (since rank A = r) so, as R is reduced, the n

(cid:2)
tains each row of Ir in the ﬁrst r columns. Thus row operations will carry RT

(cid:3)

(cid:2)

→

m matrix RT con-
(cid:3)
×
Ir 0
0 0

. Hence

m

(cid:21)n

×

→

(cid:20)

n invertible matrix. Writing

Theorem 2.5.1 (again) shows that

V = U T

1 , we obtain

Ir 0
0 0

(cid:20)

m

(cid:21)n

×

= U1RT where U1 is an n

×

UAV = RV = RU T

1 =

U1RT

T

=

(cid:0)

(cid:1)

Ir 0
0 0

(cid:18)(cid:20)

Moreover, the matrix U1 = V T can be computed by

RT

In

Theorem 2.5.3

(cid:2)

T

=

(cid:20)
Ir 0
0 0

(cid:21)n

×

m(cid:19)

→

(cid:20)(cid:20)

(cid:3)

Ir 0
0 0

(cid:21)m

n
×

V T

. This proves

(cid:21)

m

(cid:21)n

×

Let A bean m
n

×
n,respectively,suchthat

×

n matrixof rank r. ThereexistinvertiblematricesU andV ofsize m

m and

×

n
×
Moreover,if R isthereducedrow-echelonformof A,then:

(cid:21)m

(cid:20)

UAV =

Ir 0
0 0

1. U canbecomputedby

A Im

R U

;

2. V canbecomputedby

(cid:2)

(cid:2)

RT

(cid:3)

In

→

(cid:2)

→

(cid:20)(cid:20)

(cid:3)

(cid:3)
Ir 0
0 0

m

(cid:21)n

×

If A is an m

n matrix of rank r, the matrix

×

Ir 0
0 0

V T

.
(cid:21)

is called the Smith normal form11 of A.

(cid:20)
Whereas the reduced row-echelon form of A is the “nicest” matrix to which A can be carried by row
operations, the Smith canonical form is the “nicest” matrix to which A can be carried by row and column
operations. This is because doing row operations to RT amounts to doing column operations to R and then
transposing.

(cid:21)

Example 2.5.4

Given A =

1
2
1
−
where r = rank A.





1 1
2 1
1 0

−
−

, ﬁnd invertible matrices U and V such that UAV =

2
1
−
3 


Ir 0
0 0

,
(cid:21)

(cid:20)

Solution. The matrix U and the reduced row-echelon form R of A are computed by the row

11Named after Henry John Stephen Smith (1826–83).

reduction

A I3

R U

:

→

2.5. Elementary Matrices

99

(cid:2)

(cid:2)
1 1
−
2 1
−
1 0

(cid:3)
1
2
1
−





(cid:3)
2 1 0 0
1 0 1 0
3 0 0 1 


−

→ 



1
0
0

−

1 0
0 1
0 0

3
−
5
0

1
−
2
1
−

−

1 0
1 0
1 1 


Hence

3
−
5
0 

In particular, r = rank R = 2. Now row-reduce

1 0
0 1
0 0

1
0
0

R =

−





and U =

RT

I4

1 0 0 1 0 0 0
1 0 0 0 1 0 0
0 1 0 0 0 1 0
3 5 0 0 0 0 1

−

−

(cid:2)

→



















whence





1
−
2
1
−

−

1 0
1 0
1 1 

V T

Ir 0
0 0

(cid:21)

:

(cid:21)

→

(cid:20) (cid:20)

(cid:3)

1 0 0 1 0
0 1 0 0 0
0 0 0 1 1
0 0 0 3 0

0 0
1 0
0 0
5 1

−







V T = 

1 0
0 0
1 1
3 0

0
1
0
5
−

0
0
0
1
−







1 0 1
0 0 1
0 1 0
0 0 0

3
0
5
−
1







so V = 









Then UAV =

I2 0
0 0

(cid:21)

(cid:20)

as is easily veriﬁed.

Uniqueness of the Reduced Row-echelon Form

In this short subsection, Theorem 2.5.1 is used to prove the following important theorem.

Theorem 2.5.4

Ifamatrix A iscarriedtoreducedrow-echelonmatrices R and S byrowoperations,then R = S.

Proof. Observe ﬁrst that U R = S for some invertible matrix U (by Theorem 2.5.1 there exist invertible
1). We show that R = S by induction on
matrices P and Q such that R = PA and S = QA; take U = QP−
the number m of rows of R and S. The case m = 1 is left to the reader. If R j and S j denote column j in R
and S respectively, the fact that U R = S gives

U R j = S j

for each j

(2.11)

Since U is invertible, this shows that R and S have the same zero columns. Hence, by passing to the
matrices obtained by deleting the zero columns from R and S, we may assume that R and S have no zero
columns.

100

Matrix Algebra

But then the ﬁrst column of R and S is the ﬁrst column of Im because R and S are row-echelon, so
(2.11) shows that the ﬁrst column of U is column 1 of Im. Now write U , R, and S in block form as follows.

U =

1 X
0 V

1 Y
0 R′ (cid:21)
Since U R = S, block multiplication gives V R′ = S′ so, since V is invertible (U is invertible) and both R′
and S′ are reduced row-echelon, we obtain R′ = S′ by induction. Hence R and S have the same number
(say r) of leading 1s, and so both have m–r zero rows.

1 Z
0 S′ (cid:21)

R =

S =

and

(cid:20)

(cid:20)

(cid:21)

(cid:20)

,

,

In fact, R and S have leading ones in the same columns, say r of them. Applying (2.11) to these
columns shows that the ﬁrst r columns of U are the ﬁrst r columns of Im. Hence we can write U , R, and S
in block form as follows:

U =

Ir M
0 W

,

R =

,

and

S =

R1 R2
0
0

S1 S2
0
0

(cid:21)

(cid:20)
r. Then using U R = S block multiplication gives R1 = S1 and R2 = S2; that is,

(cid:20)

(cid:21)

(cid:21)

(cid:20)

where R1 and S1 are r
S = R. This completes the proof.

×

Exercises for 2.5

Exercise 2.5.1 For each of the following elementary
matrices, describe the corresponding elementary row op-
eration and write the inverse.

Exercise 2.5.2 In each case ﬁnd an elementary matrix
E such that B = EA.

a.

E =

c.

E =

e.

E =

1 0 3
0 1 0
0 0 1

1 0 0
1
0
0
2
0 0 1

0 1 0
1 0 0
0 0 1

























b.

E =

d.

E =

f.

E =

0 0 1
0 1 0
1 0 0





1 0 0
2 1 0
0 0 1

−





1 0 0
0 1 0
0 0 5

















a. A =

b. A =

c. A =

d. A =

e. A =

(cid:20)

(cid:20)

(cid:20)

(cid:20)

(cid:20)

, B =

, B =

, B =

(cid:21)

(cid:21)

(cid:21)

(cid:20)

(cid:20)

(cid:20)
1
3

, B =

(cid:21)

(cid:20)

1
1

, B =

(cid:21)

(cid:20)

2
3

−

1
1

−
1 2
0 1

1 1
1 2

−
4 1
3 2

−

1
1

−
2 1
1 3

2
1

1
0

1
2

2
1

−

−

−

1 2
1 1

(cid:21)

(cid:21)

(cid:21)

1
−
2

(cid:21)
1 1
1 1

−
−
1 3
2 1

(cid:21)

(cid:21)

f. A =

(cid:20)

−

, B =

(cid:21)

(cid:20)

−

Exercise 2.5.3 Let A =

C =

(cid:20)

−

1 1
2 1

.

(cid:21)

1 2
1 1

−

(cid:21)

(cid:20)

and

a. Find elementary matrices E1 and E2 such that

C = E2E1A.

b. Show that there is no elementary matrix E such

that C = EA.

Exercise 2.5.8 In each case factor A as a product of el-
ementary matrices.

2.5. Elementary Matrices

101

Exercise 2.5.4 If E is elementary, show that A and EA
differ in at most two rows.

Exercise 2.5.5

a. Is I an elementary matrix? Explain.

b. Is 0 an elementary matrix? Explain.

Exercise 2.5.6 In each case ﬁnd an invertible matrix U
such that UA = R is in reduced row-echelon form, and
express U as a product of elementary matrices.

a.

A =

1
2

−

1 2
1 0

(cid:21)

(cid:20)

−

b.

A =

1
2
5 12

(cid:20)

1
1

−

(cid:21)

c.

A =

d.

A =

1
3
1

2
3
1









2
1
3

1
1
2

−

−
−

−

1 0
1 2
3 2

−

1 0
2 1
3 1









Exercise 2.5.7 In each case ﬁnd an invertible matrix U
such that UA = B, and express U as a product of elemen-
tary matrices.

a. A =

2 1 3
1 1 2

(cid:20)

−

b. A =

(cid:20)

2
1

−

1 0
1 1

, B =

, B =

(cid:21)

(cid:21)

1
3

3
2

(cid:20)

(cid:20)

−

1
0

−

2
1

(cid:21)

0 1
1 0

−

(cid:21)

a.

A =

c.

A =

1 1
2 1

(cid:21)

1 0 2
0 1 1
2 1 6



(cid:20)



b.

A =

d.

A =

(cid:20)



2 3
1 2

(cid:21)
1 0
0 1
2 2

−


Exercise 2.5.9 Let E be an elementary matrix.





−

3
4
15





a. Show that E T is also elementary of the same type.

b. Show that E T = E if E is of type I or II.

Exercise 2.5.10 Show that every matrix A can be fac-
tored as A = U R where U is invertible and R is in reduced
row-echelon form.

1
1

(cid:20)

2
3

−

(cid:21)

and

Exercise 2.5.11 If A =

B =

(cid:20)
−
AF = B.

5
5

2
3

−

(cid:21)

ﬁnd an elementary matrix F such that

[Hint: See Exercise 2.5.9.]

Exercise 2.5.12 In each case ﬁnd invertible U and V

such that UAV =

(cid:20)

, where r = rank A.

(cid:21)

Ir 0
0 0

a.

A =

c.

A =

d.

A =

(cid:20)







1
2

−
1
2
0

1
2

−
1
1
1

−
−

1 1 0
3 2 1
1 0 1

−

1
4

(cid:21)
2 1
0 3
4 1

1
−
1
3



−

b.

A =

3 2
2 1

(cid:21)

(cid:20)








Exercise 2.5.13 Prove Lemma 2.5.1 for elementary ma-
trices of:

a.

type I;

b.

type II.

Exercise 2.5.14 While trying to invert A,
is carried to
P = QA.

by row operations. Show that
(cid:3)

P Q

A I

(cid:2)

(cid:2)

(cid:3)

Exercise 2.5.15 If A and B are n
n matrices and AB is
a product of elementary matrices, show that the same is
true of A.

×

102

Matrix Algebra

Exercise 2.5.16 If U is invertible, show that the reduced
.
row-echelon form of a matrix

U A

1A

is

I U −

Exercise 2.5.17 Two matrices A and B are called row-
(cid:3)
equivalent (written A r
B) if there is a sequence of ele-
∼
mentary row operations carrying A to B.

(cid:3)

(cid:2)

(cid:2)

a. Show that A r
∼

invertible matrix U .

B if and only if A = U B for some

b. Show that:

i. A r
∼
ii. If A r
∼
iii. If A r
∼

A for all matrices A.

B, then B r
A
∼
C, then A r
B and B r
∼
∼

C.

c. Show that, if A and B are both row-equivalent to

d. Show that

−

some third matrix, then A r
∼
1 3 2
1 4 1
0 8 6
5
8
−
2

4
11
2


1
1
2

1
0
1

1
2
1

−

−







B.

and




are

−
−

[Hint: Consider (c) and Theorem 1.2.1.]




Exercise 2.5.18 If U and V are invertible n
show that U r
∼

V . (See Exercise 2.5.17.)

×

n matrices,

Exercise 2.5.19 (See Exercise 2.5.17.) Find all matrices
that are row-equivalent to:

a.

c.

(cid:20)

(cid:20)

0 0 0
0 0 0

1 0 0
0 1 0

(cid:21)

(cid:21)

b.

d.

(cid:20)

(cid:20)

0 0 0
0 0 1

1 2 0
0 0 1

(cid:21)

(cid:21)

Exercise 2.5.21 Deﬁne an elementary column operation
on a matrix to be one of the following: (I) Interchange
two columns. (II) Multiply a column by a nonzero scalar.
(III) Add a multiple of a column to another column.
Show that:

a. If an elementary column operation is done to an
n matrix A, the result is AF, where F is an
n elementary matrix.

m
n

×
×

b. Given any m

n matrix A, there exist m

×

mentary matrices E1, . . . , Ek and n
matrices F1, . . . , Fp such that, in block form,

×

m ele-
×
n elementary

Ek

· · ·

E1AF1

· · ·

Fp =

Ir 0
0 0

(cid:21)

(cid:20)

Exercise 2.5.22 Suppose B is obtained from A by:

a. interchanging rows i and j;

b. multiplying row i by k

= 0;

In each case describe how to obtain B−
[Hint: See part (a) of the preceding exercise.]

1 from A−

1.

Exercise 2.5.23 Two m
equivalent (written A e
∼
ces U and V (sizes m
×

n matrices A and B are called
×
B) if there exist invertible matri-
n) such that A = U BV .
m and n

×

a. Prove the following the properties of equivalence.

i. A e
∼
ii. If A e
∼
iii. If A e
∼

A for all m

n matrices A.

×
B, then B e
A.
∼
C, then A e
B and B e
∼
∼

C.

row-equivalent.

c. adding k times row i to row j (i

= j).

Exercise 2.5.20 Let A and B be m
m matri-
ces, respectively. If m > n, show that AB is not invertible.
[Hint: Use Theorem 1.3.1 to ﬁnd x

= 0 with Bx = 0.]

n and n

×

×

b. Prove that two m

n matrices are equivalent if
they have the same rank . [Hint: Use part (a) and
Theorem 2.5.3.]

×

6
6
6
2.6 Linear Transformations

If A is an m

×

n matrix, recall that the transformation TA : Rn

TA(x) = Ax

for all x in Rn

→

2.6. Linear Transformations

103

Rm deﬁned by

is called the matrix transformation induced by A. In Section 2.2, we saw that many important geometric
transformations were in fact matrix transformations. These transformations can be characterized in a
different way. The new idea is that of a linear transformation, one of the basic notions in linear algebra. We
deﬁne these transformations in this section, and show that they are really just the matrix transformations
looked at in another way. Having these two ways to view them turns out to be useful because, in a given
situation, one perspective or the other may be preferable.

Linear Transformations

Deﬁnition 2.13 Linear Transformations Rn
Atransformation T : Rn
conditionsforallvectorsxandyin Rn andallscalars a:

Rm

→

→

Rm iscalledalineartransformationifitsatisﬁesthefollowingtwo

T1

T2

T (x+ y) = T (x) + T (y)

T (ax) = aT (x)

Of course, x + y and ax here are computed in Rn, while T (x) + T (y) and aT (x) are in Rm. We say that T
preserves addition if T1 holds, and that T preserves scalar multiplication if T2 holds. Moreover, taking
a = 0 and a =

1 in T2 gives

−

T (0) = 0

and

T (

x) =

−

T (x)

−

for all x

Hence T preserves the zero vector and the negative of a vector. Even more is true.

Recall that a vector y in Rn is called a linear combination of vectors x1, x2, . . . , xk if y has the form

y = a1x1 + a2x2 +

+ akxk

· · ·

for some scalars a1, a2, . . . , ak. Conditions T1 and T2 combine to show that every linear transformation
T preserves linear combinations in the sense of the following theorem. This result is used repeatedly in
linear algebra.

Theorem 2.6.1: Linearity Theorem
If T : Rn

Rm isalineartransformation,thenforeach k = 1, 2, . . .

→

T (a1x1 + a2x2 +

· · ·

+ akxk) = a1T (x1) + a2T (x2) +

+ akT (xk)

· · ·

forallscalars ai andallvectorsxi in Rn.

Proof. If k = 1, it reads T (a1x1) = a1T (x1) which is Condition T1. If k = 2, we have

T (a1x1 + a2x2) = T (a1x1) + T (a2x2)
= a1T (x1) + a2T (x2)

by Condition T1
by Condition T2

104

Matrix Algebra

If k = 3, we use the case k = 2 to obtain

T (a1x1 + a2x2 + a3x3) = T [(a1x1 + a2x2) + a3x3]

= T (a1x1 + a2x2) + T (a3x3)
= [a1T (x1) + a2T (x2)] + T (a3x3)
= [a1T (x1) + a2T (x2)] + a3T (x3)

collect terms
by Condition T1
by the case k = 2
by Condition T2

The proof for any k is similar, using the previous case k

1 and Conditions T1 and T2.

−

The method of proof in Theorem 2.6.1 is called mathematical induction (Appendix C).

Theorem 2.6.1 shows that if T is a linear transformation and T (x1), T (x2), . . . , T (xk) are all known,
then T (y) can be easily computed for any linear combination y of x1, x2, . . . , xk. This is a very useful
property of linear transformations, and is illustrated in the next example.

Example 2.6.1

If T : R2

→

R2 is a linear transformation, T

1
1

(cid:20)

=

(cid:21)

(cid:20)

2
3
−

(cid:21)

and T

1
2
−

(cid:20)

=

(cid:21)

(cid:20)

5
1

, ﬁnd T

(cid:21)

4
3

.
(cid:21)

(cid:20)

4
3

1
1

Solution. Write z =

, x =

, and y =

for convenience. Then we know T (x) and

(cid:20)

(cid:21)

(cid:20)
T (y) and we want T (z), so it is enough by Theorem 2.6.1 to express z as a linear combination of x
and y. That is, we want to ﬁnd numbers a and b such that z = ax + by. Equating entries gives two
equations 4 = a + b and 3 = a
Theorem 2.6.1 gives

2b. The solution is, a = 11

3 and b = 1

3, so z = 11

3y. Thus

3 x + 1

−

(cid:21)

(cid:21)

(cid:20)

1
2
−

T (z) = 11

3 T (x) + 1

3 T (y) = 11
3

2
3
−

(cid:20)

+ 1
3

(cid:21)

(cid:20)

5
1

= 1
3

(cid:21)

(cid:20)

27
32

−

(cid:21)

This is what we wanted.

Example 2.6.2

If A is m

×

n, the matrix transformation TA : Rn

→

Rm, is a linear transformation.

Solution. We have TA(x) = Ax for all x in Rn, so Theorem 2.2.2 gives

TA(x + y) = A(x + y) = Ax + Ay = TA(x) + TA(y)

and

hold for all x and y in Rn and all scalars a. Hence TA satisﬁes T1 and T2, and so is linear.

TA(ax) = A(ax) = a(Ax) = aTA(x)

The remarkable thing is that the converse of Example 2.6.2 is true: Every linear transformation

T : Rn
→
the set of columns

Rm is actually a matrix transformation. To see why, we deﬁne the standard basis of Rn to be

e1, e2, . . . , en

{

}

of the identity matrix In. Then each ei is in Rn and every vector x = 





of the ei. In fact:

x = x1e1 + x2e2 +

+ xnen

· · ·

2.6. Linear Transformations

105

x1
x2
...
xn



in Rn is a linear combination






as the reader can verify. Hence Theorem 2.6.1 shows that

T (x) = T (x1e1 + x2e2 +

· · ·

+ xnen) = x1T (e1) + x2T (e2) +

+ xnT (en)

· · ·

Now observe that each T (ei) is a column in Rm, so

· · ·
n matrix. Hence we can apply Deﬁnition 2.5 to get

(cid:2)

A =

T (e1) T (e2)

T (en)

(cid:3)

is an m

×

x1
x2
...
xn











T (x) = x1T (e1) + x2T (e2) +

· · ·

+ xnT (en) =

T (e1) T (e2)

· · ·

T (en)





= Ax

(cid:2)

(cid:3)

Since this holds for every x in Rn, it shows that T is the matrix transformation induced by A, and so proves
most of the following theorem.

Theorem 2.6.2
Let T : Rn

→

Rm beatransformation.

1. T islinearifandonlyifitisamatrixtransformation.

2. Inthiscase T = TA isthematrixtransformationinducedbyaunique m

intermsofitscolumnsby

n matrix A,given

×

where

e1, e2, . . . , en
{

}

A =

T (e1) T (e2)

T (en)

· · ·

isthestandardbasisof Rn.

(cid:2)

(cid:3)

Proof. It remains to verify that the matrix A is unique. Suppose that T is induced by another matrix B.
Then T (x) = Bx for all x in Rn. But T (x) = Ax for each x, so Bx = Ax for every x. Hence A = B by
Theorem 2.2.6.

Hence we can speak of the matrix of a linear transformation. Because of Theorem 2.6.2 we may (and

shall) use the phrases “linear transformation” and “matrix transformation” interchangeably.

106

Matrix Algebra

Example 2.6.3

Deﬁne T : R3

x1
x2
x3
transformation and use Theorem 2.6.2 to ﬁnd its matrix.

R2 by T

x1
x2
x3

for all

x1
x2

→

=







(cid:21)

(cid:20)







in R3. Show that T is a linear





Solution. Write x =

x1
x2
x3









and y =



y1
y2
y3

, so that x + y =





=


x1 + y1
x2 + y2

x1
x2

+

y1
y2

x1 + y1
x2 + y2
x3 + y3





. Hence





= T (x) + T (y)

T (x + y) =

(cid:20)
Similarly, the reader can verify that T (ax) = aT (x) for all a in R, so T is a linear transformation.
Now the standard basis of R3 is

(cid:21)

(cid:20)

(cid:21)

(cid:21)

(cid:20)

1
0
0 


e1 =





,

e2 =

so, by Theorem 2.6.2, the matrix of T is

,

0
1
0 






and

e3 =

0
0
1 






A =

T (e1) T (e2) T (e3)

=

(cid:2)

1 0 0
0 1 0

(cid:21)

(cid:20)

(cid:3)

Of course, the fact that T

x1
x2
x3
(cid:21)
matrix transformation (hence linear) and reveals the matrix.


1 0 0
0 1 0

x1
x2

=

=











(cid:20)

(cid:21)

(cid:20)

shows directly that T is a

x1
x2
x3





To illustrate how Theorem 2.6.2 is used, we rederive the matrices of the transformations in Exam-

ples 2.2.13 and 2.2.15.

Example 2.6.4

→

Let Q0 : R2
denote counterclockwise rotation through π
Theorem 2.6.2 to ﬁnd the matrices of Q0 and Rπ
2

R2 denote reﬂection in the x axis (as in Example 2.2.13) and let Rπ
2

→
2 about the origin (as in Example 2.2.15). Use

: R2

.

R2

y

0
1

(cid:21)

e2

(cid:20)

1
0

(cid:21)

(cid:20)

x

0

e1

Figure 2.6.1

Solution. Observe that Q0 and Rπ
2
(they are matrix transformations), so Theorem 2.6.2 applies

are linear by Example 2.6.2

to them. The standard basis of R2 is

e1, e2

{

points along the positive x axis, and e2 =

the positive y axis (see Figure 2.6.1).

(cid:20)

where e1 =

1
0

(cid:20)

(cid:21)

points along

(cid:21)

}
0
1

2.6. Linear Transformations

107

The reﬂection of e1 in the x axis is e1 itself because e1 points along the x axis, and the reﬂection
e2 because e2 is perpendicular to the x axis. In other words, Q0(e1) = e1 and

e2. Hence Theorem 2.6.2 shows that the matrix of Q0 is

of e2 in the x axis is
Q0(e2) =

−

−

Q0(e1) Q0(e2)

=

e1

(cid:2)

(cid:3)

(cid:2)

e2

−

=

(cid:20)

(cid:3)

1
0

0
1
−

(cid:21)

which agrees with Example 2.2.13.
Similarly, rotating e1 through π

through π
2 counterclockwise about the origin gives
−
Hence, again by Theorem 2.6.2, the matrix of Rπ
is
2

2 counterclockwise about the origin produces e2, and rotating e2
e2.

(e2) =

e1. That is, Rπ
2

(e1) = e2 and Rπ
2

−

Rπ
2

(e1) Rπ
2

(e2)

=

e2

agreeing with Example 2.2.15.

h

i

(cid:2)

e1

−

=

(cid:20)

(cid:3)

0
1

1
−
0

(cid:21)

Example 2.6.5

x
y

T

(cid:20)

=

(cid:21)

(cid:20)

y
x

(cid:21)

y

e2

0

e1

y = x

x
y

(cid:20)

(cid:21)

x

Figure 2.6.2

R2 denote reﬂection in the line y = x. Show that

Let Q1 : R2
Q1 is a matrix transformation, ﬁnd its matrix, and use it to illustrate
Theorem 2.6.2.

→

Solution. Figure 2.6.2 shows that Q1

Q1

=

x
y

0 1
1 0

y
x

(cid:20)

(cid:21)

(cid:21)
(cid:20)
induced by the matrix A =

(cid:21) (cid:20)

x
y

(cid:20)

=

(cid:21)

(cid:20)

y
x

(cid:21)

. Hence

, so Q1 is the matrix transformation

0 1
1 0

. Hence Q1 is linear (by
(cid:21)

(cid:20)

1
0

0
1

Example 2.6.2) and so Theorem 2.6.2 applies. If e1 =
of R2, then it is clear geometrically that Q1(e1) = e2 and Q1(e2) = e1. Thus (by Theorem 2.6.2)
e2 e1
the matrix of Q1 is

Q1(e1) Q1(e2)

= A as before.

are the standard basis

and e2 =

=

(cid:21)

(cid:21)

(cid:20)

(cid:20)

Recall that, given two “linked” transformations

(cid:2)

(cid:3)

(cid:2)

(cid:3)

we can apply T ﬁrst and then apply S, and so obtain a new transformation

Rk T
−→

Rn S
−→

Rm

T : Rk

S

◦

→

Rm

called the composite of S and T , deﬁned by

If S and T are linear, the action of S

◦

(S

T )(x) = S [T (x)] for all x in Rk

◦
T can be computed by multiplying their matrices.

108

Matrix Algebra

Theorem 2.6.3
Let Rk T
Rn S
−→
−→
respectively. Then S

Rm belineartransformations,andlet A and B bethematricesof S and T

T islinearwithmatrix AB.

◦

Proof. (S

◦

T )(x) = S [T (x)] = A [Bx] = (AB)x for all x in Rk.

Theorem 2.6.3 shows that the action of the composite S

T is determined by the matrices of S and
T . But it also provides a very useful interpretation of matrix multiplication. If A and B are matrices, the
product matrix AB induces the transformation resulting from ﬁrst applying B and then applying A. Thus
the study of matrices can cast light on geometrical transformations and vice-versa. Here is an example.

◦

Example 2.6.6

Show that reﬂection in the x axis followed by rotation through π

2 is reﬂection in the line y = x.

Solution. The composite in question is Rπ

2 ◦

Q0 where Q0 is reﬂection in the x axis and Rπ
2

is

rotation through π

2 . By Example 2.6.4, Rπ
2

has matrix A =

and Q0 has matrix

B =

1
0

0
1
−

(cid:20)
AB =

(cid:21)
1
−
0
(cid:20)
Example 2.6.3.

0
1

. Hence Theorem 2.6.3 shows that the matrix of Rπ

1
0

0
1
−

=

(cid:21)

(cid:20)

0 1
1 0

(cid:21) (cid:20)

, which is the matrix of reﬂection in the line y = x by

(cid:21)

0
1

1
−
0

(cid:20)

(cid:21)
Q0 is

2 ◦

This conclusion can also be seen geometrically. Let x be a typical point in R2, and assume that x
is shown

makes an angle α with the positive x axis. The effect of ﬁrst applying Q0 and then applying Rπ
2
in Figure 2.6.3. The fact that Rπ
2
is the reﬂection of x in the line y = x.

[Q0(x)] makes the angle α with the positive y axis shows that Rπ
2

[Q0(x)]

y

0

x

α

y

y

[Q0(x)]

R π
2

y = x

x

α

x

x

0

α

x

0

α

Q0(x)

Figure 2.6.3

x

Q0(x)

In Theorem 2.6.3, we saw that the matrix of the composite of two linear transformations is the product
of their matrices (in fact, matrix products were deﬁned so that this is the case). We are going to apply
this fact to rotations, reﬂections, and projections in the plane. Before proceeding, we pause to present
useful geometrical descriptions of vector addition and scalar multiplication in the plane, and to give a
short review of angles and the trigonometric functions.

x2

(cid:20)

(cid:21)

2x =

1
2

(cid:21)

(cid:20)
1
2
1

2
4

(cid:21)

x1

x =

1
2 x =

(cid:20)

0

1
2 x =

−

1
2
1

−
−

(cid:20)

(cid:21)

Figure 2.6.4

x2

1
3

y =

(cid:20)

(cid:21)

x + y =

3
4

(cid:20)

(cid:21)

0

x2

0

x =

2
1

(cid:20)

(cid:21)
x1

Figure 2.6.5

y

x + y

x

x1

Figure 2.6.6

y

p

1

θ

0

Radian
measure
of θ

x

2.6. Linear Transformations

109

Some Geometry
As we have seen, it is convenient to view a vector x in R2 as an arrow
from the origin to the point x (see Section 2.2). This enables us to visualize
what sums and scalar multiples mean geometrically. For example consider
, 1
2 x =
(cid:21)
these are shown as arrows in Figure 2.6.4.

in R2. Then 2x =

1
2
−
1
−

1
2 x =

, and

x =

and

1
2
1

2
4

1
2

−

(cid:20)

(cid:20)

(cid:21)

(cid:20)

(cid:21)

(cid:20)

(cid:21)

Observe that the arrow for 2x is twice as long as the arrow for x and in
the same direction, and that the arrows for 1
2x is also in the same direction
as the arrow for x, but only half as long. On the other hand, the arrow
1
2x is half as long as the arrow for x, but in the opposite direction.
for
More generally, we have the following geometrical description of scalar
multiplication in R2:

−

Scalar Multiple Law

times12aslongas
Letxbeavectorin R2. Thearrowfor kxis
thearrowforx,andisinthesamedirectionasthearrowforxif
k > 0,andintheoppositedirectionif k < 0.

k

|

|

Now consider two vectors x =

and y =

(cid:20)
plotted in Figure 2.6.5 along with their sum x + y =

(cid:20)

(cid:21)

2
1

in R2. They are

. It is a routine

1
3

(cid:21)
3
4

(cid:20)

(cid:21)

matter to verify that the four points 0, x, y, and x + y form the vertices of a
parallelogram–that is opposite sides are parallel and of the same length.
(The reader should verify that the side from 0 to x has slope of 1
2 , as does
the side from y to x + y, so these sides are parallel.) We state this as
follows:

Parallelogram Law

Considervectorsxandyin R2. Ifthearrowsforxandyaredrawn
(seeFigure2.6.6),thearrowforx+ ycorrespondstothefourth
vertexoftheparallelogramdeterminedbythepointsx,y,and0.

Figure 2.6.7

We will have more to say about this in Chapter 4.

Before proceeding we turn to a brief review of angles and the trigono-
metric functions. Recall that an angle θ is said to be in standard position if it is measured counterclock-
wise from the positive x axis (as in Figure 2.6.7). Then θ uniquely determines a point p on the unit circle
(radius 1, centre at the origin). The radian measure of θ is the length of the arc on the unit circle from the
positive x axis to p. Thus 360◦ = 2π radians, 180◦ = π, 90◦ = π

2 , and so on.

12If k is a real number,

k

|

|

denotes the absolute value of k; that is,

= k if k

k

|

|

0 and

=

k

|

|

−

≥

k if k < 0.

110

Matrix Algebra

The point p in Figure 2.6.7 is also closely linked to the trigonometric functions cosine and sine, written
cosθ and sinθ respectively. In fact these functions are deﬁned to be the x and y coordinates of p; that is

p =

cosθ
sinθ

(cid:20)

(cid:21)

. This deﬁnes cosθ and sinθ for the arbitrary angle θ (possibly negative), and agrees with

the usual values when θ is an acute angle
of this, see Appendix A.

θ

0

≤

≤

π
2

(cid:0)

(cid:1)

as the reader should verify. For more discussion

Rotations

y

0

Rθ(x)

θ

x

x

Figure 2.6.8

y

y

θ

Rθ(x + y)

Rθ(x)

Rθ(y)

0

x + y

x

x

Figure 2.6.9

y
sin θ

Rθ(e2)

e2

cos θ

θ

1

0

1

θ
cos θ

Rθ(e1)

sin θ
x
e1

Figure 2.6.10

We record this as

We can now describe rotations in the plane. Given an angle θ, let

Rθ : R2

R2

→

denote counterclockwise rotation of R2 about the origin through the angle
θ. The action of Rθ is depicted in Figure 2.6.8. We have already looked
at Rπ
(in Example 2.2.15) and found it to be a matrix transformation.
2
It turns out that Rθ is a matrix transformation for every angle θ (with a
simple formula for the matrix), but it is not clear how to ﬁnd the matrix.
Our approach is to ﬁrst establish the (somewhat surprising) fact that Rθ is
linear, and then obtain the matrix from Theorem 2.6.2.

Let x and y be two vectors in R2. Then x + y is the diagonal of the

parallelogram determined by x and y as in Figure 2.6.9.
The effect of Rθ is to rotate the entire parallelogram to obtain the new
parallelogram determined by Rθ(x) and Rθ(y), with diagonal Rθ(x + y).
But this diagonal is Rθ(x) + Rθ(y) by the parallelogram law (applied to
the new parallelogram). It follows that

Rθ(x + y) = Rθ(x) + Rθ(y)

A similar argument shows that Rθ(ax) = aRθ(x) for any scalar a, so

Rθ : R2

→

R2 is indeed a linear transformation.

With linearity established we can ﬁnd the matrix of Rθ. Let e1 =

1
0

(cid:21)
denote the standard basis of R2. By Figure 2.6.10 we see

(cid:20)

and e2 =

that

0
1

(cid:20)

(cid:21)

Rθ(e1) =

and

Rθ(e2) =

cosθ
sinθ

sinθ
cosθ

−

(cid:21)
Hence Theorem 2.6.2 shows that Rθ is induced by the matrix

(cid:20)

(cid:20)

(cid:21)

Rθ(e1) Rθ(e2)

=

(cid:2)

(cid:20)

(cid:3)

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

2.6. Linear Transformations

111

Theorem 2.6.4

Therotation Rθ : R2

→

R2 isthelineartransformationwithmatrix

cosθ
sinθ

sinθ
cosθ

−

.
(cid:21)

(cid:20)

For example, Rπ
2

and Rπ have matrices

0
1

1
−
0

and

(cid:21)

(cid:20)

1
−
0

0
1
−

(cid:20)

(cid:21)

The ﬁrst of these conﬁrms the result in Example 2.2.15. The second shows that rotating a vector x =

, respectively, by Theorem 2.6.4.

x
y

(cid:21)
x. Thus applying Rπ is the same

(cid:20)

through the angle π results in Rπ(x) =

(cid:21)
as negating x, a fact that is evident without Theorem 2.6.4.

(cid:21) (cid:20)

(cid:20)

1
−
0

0
1
−

x
y

=

x
−
y
−

=

(cid:21)

(cid:20)

−

Example 2.6.7

y

Rθ

Rφ(x)

(cid:2)

(cid:3)

θ

φ

Rφ(x)
x

x

0

Figure 2.6.11

Let θ and φ be angles. By ﬁnding the matrix of the composite
Rθ ◦
Rφ, obtain expressions for cos(θ +φ) and sin(θ +φ).
Solution. Consider the transformations R2
composite Rθ ◦
plane through φ and then rotates it through θ, and so is the rotation
through the angle θ +φ (see Figure 2.6.11).
In other words

Rφ is the transformation that ﬁrst rotates the

R2 Rθ
−→

R2. Their

Rφ
−→

Rθ+φ = Rθ ◦
Theorem 2.6.3 shows that the corresponding equation holds
for the matrices of these transformations, so Theorem 2.6.4 gives:

Rφ

cos(θ +φ)
sin(θ +φ)

sin(θ +φ)
cos(θ +φ)

−

=

cosθ
sinθ

sinθ
cosθ

−

cosφ
sinφ

sinφ
cosφ

−

(cid:20)

(cid:20)
If we perform the matrix multiplication on the right, and then compare ﬁrst column entries, we
obtain

(cid:21) (cid:20)

(cid:21)

(cid:21)

cos(θ +φ) = cosθ cosφ
sinθ sinφ
sin(θ +φ) = sinθcosφ + cosθ sinφ

−

These are the two basic identities from which most of trigonometry can be derived.

112

Matrix Algebra

Reﬂections

y

0

Qm(x)

y = mx

x

x

Figure 2.6.12

→

The line through the origin with slope m has equation y = mx, and we let
Qm : R2

R2 denote reﬂection in the line y = mx.

In
This transformation is described geometrically in Figure 2.6.12.
words, Qm(x) is the “mirror image” of x in the line y = mx. If m = 0 then
Q0 is reﬂection in the x axis, so we already know Q0 is linear. While we
could show directly that Qm is linear (with an argument like that for Rθ),
we prefer to do it another way that is instructive and derives the matrix of
Qm directly without using Theorem 2.6.2.

Let θ denote the angle between the positive x axis and the line y = mx.
The key observation is that the transformation Qm can be accomplished in
θ (so our line coincides with the x axis), then reﬂect in the x axis, and

three steps: First rotate through
ﬁnally rotate back through θ. In other words:

−

◦
θ, Q0, and Rθ are all linear, this (with Theorem 2.6.3) shows that Qm is linear and that its matrix
−
θ. If we write c = cosθ and s = sinθ for simplicity, then
−

Since R
is the product of the matrices of Rθ, Q0, and R
the matrices of Rθ, R

−

Qm = Rθ ◦

Q0

R

θ

θ, and Q0 are
−
c
s
−
s
c
Hence, by Theorem 2.6.3, the matrix of Qm = Rθ ◦
c
s

c
s
s c

s
−
c

1
0

−

(cid:21)

(cid:20)

(cid:21)

(cid:20)

,

,

(cid:20)

(cid:21) (cid:20)

(cid:21) (cid:20)

−

0
1
−

and

Q0
◦
c s
s c

1
0

(cid:20)
θ is
−

R

=

(cid:21)

(cid:20)

respectively.13

0
1
−

(cid:21)

c2

s2

−
2sc

2sc

s2

c2

−

(cid:21)

We can obtain this matrix in terms of m alone. Figure 2.6.13 shows

y

that

1
m

(cid:20)

(cid:21)

m

y = mx

x

√1 + m2

θ
1

0

Figure 2.6.13

so the matrix

(cid:20)

c2

s2

−
2sc

cosθ = 1

√1+m2 and sinθ = m
2sc

of Qm becomes

√1+m2

1
1+m2

s2

c2

−

(cid:21)

1

m2

−
2m

(cid:20)

2m

m2

1

−

.
(cid:21)

Theorem 2.6.5
Let Qm denotereﬂectionintheline y = mx. Then Qm isalinear
transformationwithmatrix 1

m2

2m

1

1+m2

(cid:20)

−
2m

m2

1

−

.
(cid:21)

Note that if m = 0, the matrix in Theorem 2.6.5 becomes

, as expected. Of course this

analysis fails for reﬂection in the y axis because vertical lines have no slope. However it is an easy

1
0

(cid:20)

0
1
−

(cid:21)

exercise to verify directly that reﬂection in the y axis is indeed linear with matrix

−

1 0
0 1

.14
(cid:21)
θ) = cosθ and

(cid:20)

13The matrix of R
θ comes from the matrix of Rθ using the fact that, for all angles θ, cos(
−
sin(θ).

θ) =

sin(

−

−
−
14Note that

1 0
0 1

−

(cid:20)

= lim
m
∞
→

(cid:21)

1
1+m2

(cid:20)

1

m2

−
2m

2m

m2

1

−

.

(cid:21)

Example 2.6.8

2.6. Linear Transformations

113

Let T : R2
reﬂection in a line through the origin and ﬁnd the line.

R2 be rotation through

→

−

π
2 followed by reﬂection in the y axis. Show that T is a

Solution. The matrix of R

π
2

−

is



cos(

sin(

π
2 )
π
2 )

−

−

sin(

−

cos(

−

−

π
2 )
2 ) 
π


=

(cid:20)

0 1
1 0

−

(cid:21)

and the matrix of

reﬂection in the y axis is

. Hence the matrix of T is

−

1 0

0 1
0
1
−

(cid:21)
1
−
0

and this is reﬂection in the line y =

x (take m =

−

1 in

−

(cid:21)

1 0
0 1

−

(cid:20)
−
Theorem 2.6.5).

(cid:21) (cid:20)

0 1
1 0

(cid:20)
=

(cid:21)

(cid:20)

Projections

y

Pm(x)

0

y = mx

x

x

Figure 2.6.14

The method in the proof of Theorem 2.6.5 works more generally. Let
R2 denote projection on the line y = mx. This transformation is
Pm : R2
described geometrically in Figure 2.6.14.

→

If m = 0, then P0

x
y

(cid:20)

=

(cid:21)

(cid:20)

x
0

(cid:21)

for all

x
y

(cid:20)

(cid:21)

in R2, so P0 is linear with

matrix

. Hence the argument above for Qm goes through for Pm.
(cid:20)
(cid:21)
First observe that

1 0
0 0

Pm = Rθ ◦
as before. So, Pm is linear with matrix

P0

R

θ

−

◦

c
s

s
−
c

1 0
0 0

c s
s c

−

=

(cid:21)

(cid:20)

(cid:21) (cid:20)

(cid:21) (cid:20)

c2
sc
sc s2

(cid:21)

where c = cosθ = 1

(cid:20)
√1+m2 and s = sinθ = m
√1+m2 .

This gives:

Theorem 2.6.6
Let Pm : R2

1
1+m2

(cid:20)

→
1 m
m m2

.
(cid:21)

R2 beprojectionontheline y = mx. Then Pm isalineartransformationwithmatrix

Again, if m = 0, then the matrix in Theorem 2.6.6 reduces to

as expected. As the y axis has

no slope, the analysis fails for projection on the y axis, but this transformation is indeed linear with matrix

(cid:21)

(cid:20)

as is easily veriﬁed directly.

0 0
0 1
Note that the formula for the matrix of Qm in Theorem 2.6.5 can be derived from the above formula
x.

for the matrix of Pm. Using Figure 2.6.12, observe that Qm(x) = x + 2[Pm(x)
Substituting the matrices for Pm(x) and 1R2(x) gives the desired formula.

x] so Qm(x) = 2Pm(x)

−

−

1 0
0 0

(cid:20)

(cid:21)

114

Matrix Algebra

Example 2.6.9

Given x in R2, write y = Pm(x). The fact that y lies on the line y = mx means that Pm(y) = y. But
then

(Pm

◦

Pm)(x) = Pm(y) = y = Pm(x) for all x in R2,
1 m
m m2

1+m2

In particular, if we write the matrix of Pm as A = 1
verify this directly.

(cid:20)

that is, Pm

Pm = Pm.

◦

, then A2 = A. The reader should
(cid:21)

Exercises for 2.6

Exercise 2.6.1 Let T : R3
tion.

→

R2 be a linear transforma-

Exercise 2.6.2 Let T : R4
tion.

→

R3 be a linear transforma-

a. Find T





and T





b. Find T





and T





a. Find T 





and T 





b. Find T 

2
3

=

(cid:20)

(cid:21)

1
1
0
1

2
3
1 


−



=











if T 







=











−




5
0
.
1 

1
1
1
1

if T 



=



5
1
3 


1
3
2
−
3
−
0
1
−
1
1

−

5
1
2
4
−
1
−
1
0
2





−

=













and T 




2
0
.
1 

Exercise 2.6.3 In each case assume that the transfor-
mation T is linear, and use Theorem 2.6.2 to obtain the
matrix A of T .













if T

8
3
7 
2

1
=
3 


(cid:20)

1
0
1 


−
1
0

.

(cid:21)




−

5
6
13 


=

(cid:20)

−
2
0
5 


3
5

=

(cid:20)

(cid:21)

if T

1
2

−

3
2
1 


−





.

(cid:21)

a. T : R2

b. T : R2

c. T : R2

d. T : R2

→

→

→

→

R2 is reﬂection in the line y =

x.

−

R2 is given by T (x) =

x for each x in R2.

−

R2 is clockwise rotation through π
4 .

R2 is counterclockwise rotation through π
4 .

Exercise 2.6.4 In each case use Theorem 2.6.2 to obtain
the matrix A of the transformation T . You may assume
that T is linear in each case.

a. T : R3

b. T : R3

→

→

R3 is reﬂection in the x

R3 is reﬂection in the y

z plane.

z plane.

−

−

Exercise 2.6.5 Let T : Rn
tion.

→

Rm be a linear transforma-

a. If x is in Rn, we say that x is in the kernel of T if
T (x) = 0. If x1 and x2 are both in the kernel of T ,
show that ax1 + bx2 is also in the kernel of T for
all scalars a and b.

b. If y is in Rn, we say that y is in the image of T if
y = T (x) for some x in Rn. If y1 and y2 are both
in the image of T , show that ay1 + by2 is also in
the image of T for all scalars a and b.

Exercise 2.6.6 Use Theorem 2.6.2 to ﬁnd the matrix of
Rn deﬁned by
the identity transformation 1Rn : Rn
1Rn (x) = x for each x in Rn.
Exercise 2.6.7 In each case show that T : R2
not a linear transformation.

R2 is

→

→

a.

T

x
y

(cid:20)

=

(cid:21)

(cid:20)

xy
0

(cid:21)

b.

T

x
y

(cid:20)

=

(cid:21)

(cid:20)

0
y2

(cid:21)

Exercise 2.6.8 In each case show that T is either reﬂec-
tion in a line or rotation through an angle, and ﬁnd the
line or angle.

a. T

b. T

c. T

d. T

(cid:20)

(cid:20)

(cid:20)

(cid:20)

x
y

x
y

x
y

x
y

(cid:21)

(cid:21)

(cid:21)

(cid:21)

= 1
5

(cid:20)

= 1
√2

= 1
√3

3x + 4y
−
4x + 3y

(cid:21)

x + y
x + y

(cid:21)
−
√3y
x
√3x + y

−

(cid:20)

(cid:20)

=

1
10

−

8x + 6y
8y
6x

−

(cid:20)

(cid:21)

(cid:21)

Exercise 2.6.9 Express reﬂection in the line y =
x as
the composition of a rotation followed by reﬂection in
the line y = x.
Exercise 2.6.10 Find the matrix of T : R3
case:

R3 in each

→

−

a. T is rotation through θ about the x axis (from the

y axis to the z axis).

b. T is rotation through θ about the y axis (from the

x axis to the z axis).

2.6. Linear Transformations

115

Exercise 2.6.11 Let Tθ : R2
the line making an angle θ with the positive x axis.

R2 denote reﬂection in

→

a. Show that the matrix of Tθ is

for all θ.

cos 2θ
sin 2θ

(cid:20)

sin 2θ
cos 2θ

−

(cid:21)

b. Show that Tθ

R2φ = Tθ

φ for all θ and φ.
−

◦

Exercise 2.6.12 In each case ﬁnd a rotation or reﬂection
that equals the given transformation.

a. Reﬂection in the y axis followed by rotation

through π
2 .

b. Rotation through π followed by reﬂection in the x

axis.

c. Rotation through π

2 followed by reﬂection in the

line y = x.

d. Reﬂection in the x axis followed by rotation

through π
2 .

e. Reﬂection in the line y = x followed by reﬂection

in the x axis.

f. Reﬂection in the x axis followed by reﬂection in

the line y = x.

116

Matrix Algebra

Exercise 2.6.13 Let R and S be matrix transformations
Rn
Rm induced by matrices A and B respectively. In
each case, show that T is a matrix transformation and
describe its matrix in terms of A and B.

→

Exercise 2.6.18 Let Q0 : R2
axis, let Q1 : R2
Q
−
R π
2

R2 be reﬂection in the x
R2 be reﬂection in the line y = x, let
R2 be reﬂection in the line y =
x, and let
R2 be counterclockwise rotation through π
2 .

1 : R2
: R2

→
→

→

→

−

a. T (x) = R(x) + S(x) for all x in Rn.

b. T (x) = aR(x) for all x in Rn (where a is a ﬁxed

real number).

a. Show that Q1

b. Show that Q1

◦

◦

= Q0.

R π
2

.

Q0 = R π
2
Q0 = Q1.

c. Show that R π

2 ◦

Exercise 2.6.14 Show that the following hold for all lin-
ear transformations T : Rn

Rm:

→

a.

T (0) = 0

x) =

b.

T (
Rn

−

−

T (x) for all x in

Exercise 2.6.15 The transformation T : Rn
Rm de-
ﬁned by T (x) = 0 for all x in Rn is called the zero trans-
formation.

→

a. Show that the zero transformation is linear and

ﬁnd its matrix.

b. Let e1, e2, . . . , en denote the columns of the n

n
Rm is linear and
identity matrix.
T (ei) = 0 for each i, show that T is the zero trans-
formation. [Hint: Theorem 2.6.1.]

If T : Rn

→

×

Exercise 2.6.16 Write the elements of Rn and Rm as
Rn by
rows. If A is an m
×
T (y) = yA for all rows y in Rm. Show that:

n matrix, deﬁne T : Rm

→

a. T is a linear transformation.

b. the rows of A are T (f1), T (f2), . . . , T (fm) where
fi denotes row i of Im. [Hint: Show that fiA is row
i of A.]

Exercise 2.6.17 Let S : Rn
→
ear transformations with matrices A and B respectively.

Rn and T : Rn

Rn be lin-

→

a. Show that B2 = B if and only if T 2 = T (where T 2

means T

T ).

◦

b. Show that B2 = I if and only if T 2 = 1Rn .

c. Show that AB = BA if and only if S

T = T

S.

◦

◦

[Hint: Theorem 2.6.3.]

d. Show that Q0

R π
2

◦

= Q

1.

−

Exercise 2.6.19 For any slope m, show that:

a.

Qm

◦

Pm = Pm

b.

Pm

Qm = Pm

◦
T : Rn

· · ·

R by
→
+ xn. Show that T

Exercise
2.6.20
Deﬁne
T (x1, x2, . . . , xn) = x1 + x2 +
is a linear transformation and ﬁnd its matrix.
Exercise 2.6.21 Given c in R, deﬁne Tc : Rn
R by
Tc(x) = cx for all x in Rn. Show that Tc is a linear trans-
formation and ﬁnd its matrix.
Exercise 2.6.22 Given vectors w and x in Rn, denote
their dot product by w

→

x.

·

a. Given w in Rn, deﬁne Tw : Rn

R by Tw(x) =
x for all x in Rn. Show that Tw is a linear trans-

→

w
formation.

·

b. Show that every linear transformation T : Rn

R
is given as in (a); that is T = Tw for some w in Rn.

→

= 0 and y are vectors in Rn, show
Exercise 2.6.23 If x
Rn such that
that there is a linear transformation T : Rn
T (x) = y. [Hint: By Deﬁnition 2.5, ﬁnd a matrix A such
that Ax = y.]
Rm S
Exercise 2.6.24 Let Rn T
−→
−→
formations. Show directly that S
◦

Rk be two linear trans-
T is linear. That is:

→

a. Show that (S
◦
all x, y in Rn.

T )(x + y) = (S

T )x + (S

T )y for

◦

◦

b. Show that (S

◦
and all a in R.

T )(ax) = a[(S

◦

T )x] for all x in Rn

Exercise 2.6.25 Let Rn T
−→
Show that R
T ) = (R
◦
that [R
S)
(S
◦
tor x in Rn.

◦
◦
T )](x) = [(R

(S

◦

◦

Rk R
−→

Rm S
Rk be linear.
−→
S)
T by showing directly
T )](x) holds for each vec-

◦

◦

6
2.7. LU-Factorization

117

2.7

LU-Factorization15

The solution to a system Ax = b of linear equations can be solved quickly if A can be factored as A = LU
where L and U are of a particularly nice form. In this section we show that gaussian elimination can be
used to ﬁnd such factorizations.

Triangular Matrices

n matrix, the elements a11, a22, a33, . . . form the main
As for square matrices, if A =
diagonal of A. Then A is called upper triangular if every entry below and to the left of the main diagonal
(cid:3)
is zero. Every row-echelon matrix is upper triangular, as are the matrices

is an m

ai j

×

(cid:2)

1
0
0

1
−
2
0





0 3
1 1
3 0 


−

0 2 1 0 5
0 0 0 3 1
0 0 1 0 1 






1
0
0
0

1 1
1 1
0 0
0 0

−













By analogy, a matrix A is called lower triangular if its transpose is upper triangular, that is if each entry
above and to the right of the main diagonal is zero. A matrix is called triangular if it is upper or lower
triangular.

Example 2.7.1

Solve the system

x1 + 2x2

−

−

3x3
x4 + 5x5 = 3
5x3 + x4 + x5 = 8
2x5 = 6

where the coefﬁcient matrix is upper triangular.

Solution. As in gaussian elimination, let the “non-leading” variables be parameters: x2 = s and
x4 = t. Then solve for x5, x3, and x1 in that order as follows. The last equation gives

Substitution into the second last equation gives

x5 = 6

2 = 3

Finally, substitution of both x5 and x3 into the ﬁrst equation gives

x3 = 1

1
5t

−

x1 =

9
−

−

2s + 2
5t

The method used in Example 2.7.1 is called back substitution because later variables are substituted
into earlier equations. It works because the coefﬁcient matrix is upper triangular. Similarly, if the coefﬁ-
cient matrix is lower triangular the system can be solved by forward substitution where earlier variables
are substituted into later equations. As observed in Section 1.2, these procedures are more numerically
efﬁcient than gaussian elimination.

15This section is not used later and so may be omitted with no loss of continuity.

118

Matrix Algebra

Now consider a system Ax = b where A can be factored as A = LU where L is lower triangular and U

is upper triangular. Then the system Ax = b can be solved in two stages as follows:

1. First solve Ly = b for y by forward substitution.

2. Then solve U x = y for x by back substitution.

Then x is a solution to Ax = b because Ax = LU x = Ly = b. Moreover, every solution x arises this way
(take y = U x). Furthermore the method adapts easily for use in a computer.

This focuses attention on efﬁciently obtaining such factorizations A = LU . The following result will

be needed; the proof is straightforward and is left as Exercises 2.7.7 and 2.7.8.

Lemma 2.7.1

Let A and B denotematrices.

1. If A and B arebothlower(upper)triangular,thesameistrueof AB.

2. If A is n

n andlower(upper)triangular,thenAisinvertibleifandonlyifeverymain

diagonalentryisnonzero. Inthiscase A−

1 isalsolower(upper)triangular.

×

LU-Factorization

Let A be an m
×
in Section 2.5, the reduction is

n matrix. Then A can be carried to a row-echelon matrix U (that is, upper triangular). As

A

→

E1A

→

E2E1A

→

E3E2E1A

→ · · · →

EkEk

1
−

· · ·

E2E1A = U

where E1, E2, . . . , Ek are elementary matrices corresponding to the row operations used. Hence

A = LU

· · ·

1
−

E2E1)−

1 = E−
1
1
1 E−
2

1
1
. If we do not insist that U is reduced then, except
1E−
E−
where L = (EkEk
k
k
−
for row interchanges, none of these row operations involve adding a row to a row above it. Thus, if no
row interchanges are used, all the Ei are lower triangular, and so L is lower triangular (and invertible) by
Lemma 2.7.1. This proves the following theorem. For convenience, let us say that A can be lower reduced
if it can be carried to row-echelon form using no row interchanges.

· · ·

2.7. LU-Factorization

119

Theorem 2.7.1

If A canbelowerreducedtoarow-echelonmatrixU,then

A = LU

where L islowertriangularandinvertibleandU isuppertriangularandrow-echelon.

Deﬁnition 2.14 LU-factorization

Afactorization A = LU asinTheorem2.7.1iscalledanLU-factorizationof A.

Such a factorization may not exist (Exercise 2.7.4) because A cannot be carried to row-echelon form
using no row interchange. A procedure for dealing with this situation will be outlined later. However, if
an LU-factorization A = LU does exist, then the gaussian algorithm gives U and also leads to a procedure
for ﬁnding L. Example 2.7.2 provides an illustration. For convenience, the ﬁrst nonzero column from the
left in a matrix A is called the leading column of A.

Example 2.7.2

Find an LU-factorization of A =

0
0
0



6
−
3
3

2
1
−
1
−

2
4
−
.
2
3
7 10 




Solution. We lower reduce A to row-echelon form as follows:

0
0
0

2
1
−
1
−

6
−
3
3

4
2
−
2
3
7 10 


→ 



A =





0 1
0 0
0 0

3
−
0
0

2
1
−
4
2
6 12 


→ 



0 1
0 0
0 0

3
−
0
0

−

1 2
1 2
0 0 


= U

The circled columns are determined as follows: The ﬁrst is the leading column of A, and is used
(by lower reduction) to create the ﬁrst leading 1 and create zeros below it. This completes the work
on row 1, and we repeat the procedure on the matrix consisting of the remaining rows. Thus the
second circled column is the leading column of this smaller matrix, which we use to create the
second leading 1 and the zeros below it. As the remaining row is zero here, we are ﬁnished. Then
A = LU where

−
−
This matrix L is obtained from I3 by replacing the bottom of the ﬁrst two columns by the circled
columns in the reduction. Note that the rank of A is 2 here, and this is the number of circled
columns.





L =

2 0 0
1 2 0
1 6 1 


The calculation in Example 2.7.2 works in general. There is no need to calculate the elementary

120

Matrix Algebra

matrices Ei, and the method is suitable for use in a computer because the circled columns can be stored in
memory as they are created. The procedure can be formally stated as follows:

LU-Algorithm

Let A bean m
matrixU. Then A = LU wherethelowertriangular,invertiblematrix L isconstructedasfollows:

n matrixof rank r,andsupposethat A canbelowerreducedtoarow-echelon

×

1. If A = 0,take L = Im andU = 0.

2. If A

= 0,write A1 = A andletc1 betheleadingcolumnof A1. Usec1 tocreatetheﬁrst
leading 1 andcreatezerosbelowit(usinglowerreduction). Whenthisiscompleted,let A2
denotethematrixconsistingofrows2to m ofthematrixjustcreated.

3. If A2

= 0,letc2 betheleadingcolumnof A2 andrepeatStep2on A2 tocreate A3.

4. ContinueinthiswayuntilU isreached,whereallrowsbelowthelastleading 1 consistof

zeros. Thiswillhappenafter r steps.

5. Create L byplacingc1, c2, . . . , cr atthebottomoftheﬁrst r columnsof Im.

A proof of the LU-algorithm is given at the end of this section.

LU-factorization is particularly important if, as often happens in business and industry, a series of
equations Ax = B1, Ax = B2, . . . , Ax = Bk, must be solved, each with the same coefﬁcient matrix A. It is
very efﬁcient to solve the ﬁrst system by gaussian elimination, simultaneously creating an LU-factorization
of A, and then using the factorization to solve the remaining systems by forward and back substitution.

Example 2.7.3

Find an LU-factorization for A = 

5
3
−
2
−
1

5 10
−
2
3
2
0
1 10

−

0 5
2 1
1 0
2 5

−

.











Solution. The reduction to row-echelon form is

6
6
2.7. LU-Factorization

121

















5
3
−
2
−
1







5 10
2
3
0
2
1 10

−

−

0 5
2 1
1 0
2 5

−

→







→

→

1
0
0
0

1

0

0

0

1

0

0

0



























−

1 2
0 8
0 4
0 8

1 2

−

0 1

0 0

0 0

0 1
2 4
1 2
2 4

−

0 1

1
4

1
2

2 0

−

0 0

−

1 2 0 1

0 1 1
4

1
2

0 0 1 0

0 0 0 0

= U











If U denotes this row-echelon matrix, then A = LU , where

5 0
3 8
2 4
1 8

−
−

0 0
0 0
2 0
0 1

−







L = 





The next example deals with a case where no row of zeros is present in U (in fact, A is invertible).

Example 2.7.4

Find an LU-factorization for A =



2 4 2
1 1 2
.
1 0 2 


−



Solution. The reduction to row-echelon form is

2 4 2
1 1 2
1 0 2 


−





→ 



1
0
0

−

2 1
1 1
2 3 


→ 



1 2
0 1
0 0

1
1
5 
−


→ 



1 2
0 1
0 0

1
1
1 
−


= U

Hence A = LU where L =

2
1
1
−





−

0 0
1 0
.
2 5 


122

Matrix Algebra

There are matrices (for example

0 1
1 0

(cid:20)

(cid:21)

row interchange when being carried to row-echelon form via the gaussian algorithm. However, it turns
out that, if all the row interchanges encountered in the algorithm are carried out ﬁrst, the resulting matrix
requires no interchanges and so has an LU-factorization. Here is the precise result.

) that have no LU-factorization and so require at least one

Theorem 2.7.2

n matrix A iscarriedtoarow-echelonmatrixU viathegaussianalgorithm. Let
Supposean m
P1, P2, . . . , Ps betheelementarymatricescorresponding(inorder)totherowinterchangesused,
andwrite P = Ps

P2P1. (Ifnointerchangesareusedtake P = Im.) Then:

×

· · ·

1. PA isthematrixobtainedfrom A bydoingtheseinterchanges(inorder)to A.

2. PA hasanLU-factorization.

The proof is given at the end of this section.

A matrix P that is the product of elementary matrices corresponding to row interchanges is called
a permutation matrix. Such a matrix is obtained from the identity matrix by arranging the rows in a
different order, so it has exactly one 1 in each row and each column, and has zeros elsewhere. We regard
the identity matrix as a permutation matrix. The elementary permutation matrices are those obtained from
I by a single row interchange, and every permutation matrix is a product of elementary ones.

Example 2.7.5

If A = 

, ﬁnd a permutation matrix P such that PA has an LU-factorization,



0
1
−
2
0

0
1
−
1
1

1 2
1 2
3 6
1 4

−

−
−



and then ﬁnd the factorization.






Solution. Apply the gaussian algorithm to A:

1
−
0
2
0

1
−
0
1
1

1 2
1 2
3 6
1 4

−
−
−

A ∗
−→







→







→









1
0
0
0

1
0
1
−
1

1 1
0 1
0 0
0 0











∗
−→

1
0
0
0

2
−
10
2
4

2
−
2
10
4

1
1
−
0
1

1
−
1
−
1
−
1
−
1
−
1
1
−
2
−
), ﬁrst rows 1 and 2 and then rows 2 and 3.

1
−
1
−
1
−
1
−
1
−
1
1
0

1 1
0 1
0 0
0 0

2
−
10
−
2
−
10

2
−
10
2
14





















→

−











Two row interchanges were needed (marked with
Hence, as in Theorem 2.7.2,

∗

1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1

0 1 0 0
1 0 0 0
0 0 1 0
0 0 0 1

0 1 0 0
0 0 1 0
1 0 0 0
0 0 0 1









= 





















P = 





If we do these interchanges (in order) to A, the result is PA. Now apply the LU-algorithm to PA:

2.7. LU-Factorization

123

1
−
2
0
0

1
−
1
0
1

1 2
3 6
1 2
1 4

−
−
−

PA = 





1
0
0
0

1
1
−
0
1

1 1
0 1
0 0
0 0

1
−
1
−
1
−
1
−
1
−
1
1
0

2
−
10
2
4

2
−
10
−
2
−
10

1 1
0 1
0 0
0 0

1 1
0 1
0 0
0 0

1
−
1
1
−
2
−
1
−
1
1
0

2
−
10
2
14

−

2
−
10
2
−
1

−







= U







→



















→







→







→













Hence, PA = LU , where L = 





1
−
2
0
0

0
1
−
0
1

0
0
0
0
1
0
−
2 10
−



and U = 









1 1
0 1
0 0
0 0

1
−
1
1
0

2
−
10
−
2
−
1

.






Theorem 2.7.2 provides an important general factorization theorem for matrices. If A is any m
n
matrix, it asserts that there exists a permutation matrix P and an LU-factorization PA = LU . Moreover,
P2P1, where P1, P2, . . . , Ps are the elementary permutation matri-
it shows that either P = I or P = Ps
1
i = Pi for each i (they are
ces arising in the reduction of A to row-echelon form. Now observe that P−
elementary row interchanges). Thus, P−

Ps, so the matrix A can be factored as

1 = P1P2

· · ·

×

· · ·
A = P−

1LU

1 is a permutation matrix, L is lower triangular and invertible, and U is a row-echelon matrix.

where P−
This is called a PLU-factorization of A.

The LU-factorization in Theorem 2.7.1 is not unique. For example,

1 0
3 2

(cid:20)

(cid:21) (cid:20)

1
0

2 3
0 0

−

1 0
3 1

=

(cid:21)

(cid:20)

(cid:21) (cid:20)

1
0

2 3
0 0

−

(cid:21)

However, it is necessary here that the row-echelon matrix has a row of zeros. Recall that the rank of a
matrix A is the number of nonzero rows in any row-echelon matrix U to which A can be carried by row
operations. Thus, if A is m

n, the matrix U has no row of zeros if and only if A has rank m.

×

Theorem 2.7.3

Let A bean m

×

n matrixthathasanLU-factorization

A = LU

If A has rank m (thatis,U hasnorowofzeros),then L andU areuniquelydeterminedby A.

Proof. Suppose A = MV is another LU-factorization of A, so M is lower triangular and invertible and V is
1L. Then N
row-echelon. Hence LU = MV , and we must show that L = M and U = V . We write N = M−

124

Matrix Algebra

is lower triangular and invertible (Lemma 2.7.1) and NU = V , so it sufﬁces to prove that N = I. If N is
m
m, we use induction on m. The case m = 1 is left to the reader. If m > 1, observe ﬁrst that column 1
of V is N times column 1 of U . Thus if either column is zero, so is the other (N is invertible). Hence, we
can assume (by deleting zero columns) that the (1, 1)-entry is 1 in both U and V .

×

a
0
X N1

1 Y
0 U1

Now we write N =

, U =

, and V =

in block form. Then NU = V

becomes

(cid:20)
(cid:21)
. Hence a = 1, Y = Z, X = 0, and N1U1 = V1. But N1U1 = V1
(cid:21)
implies N1 = I by induction, whence N = I.

a
X XY + N1U1

(cid:21)
1 Z
0 V1

aY

=

(cid:20)

(cid:21)

(cid:20)

(cid:21)

(cid:20)

(cid:20)

1 Z
0 V1

If A is an m

m invertible matrix, then A has rank m by Theorem 2.4.5. Hence, we get the following

important special case of Theorem 2.7.3.

×

Corollary 2.7.1

Ifaninvertiblematrix A hasanLU-factorization A = LU,then L andU areuniquelydeterminedby
A.

Of course, in this case U is an upper triangular matrix with 1s along the main diagonal.

Proofs of Theorems

Proof of the LU-Algorithm. If c1, c2, . . . , cr are columns of lengths m, m
write L(m)(c1, c2, . . . , cr) for the lower triangular m
at the bottom of the ﬁrst r columns of Im.

r +1, respectively,
1, . . . , m
m matrix obtained from Im by placing c1, c2, . . . , cr

×

−

−

Proceed by induction on n. If A = 0 or n = 1, it is left to the reader. If n > 1, let c1 denote the leading
m identity matrix. There exist elementary

column of A and let k1 denote the ﬁrst column of the m
matrices E1, . . . , Ek such that, in block form,

×

(Ek

E2E1)A =

0 k1

where (Ek

E2E1)c1 = k1

· · ·

· · ·

(cid:20)
Moreover, each E j can be taken to be lower triangular (by assumption). Write

(cid:21)

X1
A1

G = (Ek · · ·

E2E1)−

1
1
1 = E−
1 E−
2

1
E−
k

· · ·

1
Then G is lower triangular, and Gk1 = c1. Also, each E j (and so each E−
j
ing row 1 of Im by a constant or adding a multiple of row 1 to another row. Hence,

) is the result of either multiply-

1
1
1 E−
G = (E−
2

· · ·

1
k )Im =
E−

c1

(cid:20)

0
Im

1
−

(cid:21)

in block form. Now, by induction, let A1 = L1U1 be an LU-factorization of A1, where L1 = L(m
and U1 is row-echelon. Then block multiplication gives

1) [c2, . . . , cr]
−

G−

1A =

0 k1

(cid:20)

X1
L1U1

=

(cid:21)

(cid:20)

1
0
0 L1 (cid:21) (cid:20)

0 1 X1
0 0 U1 (cid:21)

2.7. LU-Factorization

125

Hence A = LU , where U =

(cid:20)

L =

c1

(cid:20)
This completes the proof.

0 1 X1
0 0 U1 (cid:21)
1
0
0 L1 (cid:21)

0
Im

1
−

(cid:21) (cid:20)

is row-echelon and

=

c1

(cid:20)

0
L

(cid:21)

= L(m) [c1, c2, . . . , cr]

n matrix and let k j denote column j of Im. There is a
Proof of Theorem 2.7.2. Let A be a nonzero m
permutation matrix P1 (where either P1 is elementary or P1 = Im) such that the ﬁrst nonzero column c1 of
P1A has a nonzero entry on top. Hence, as in the LU-algorithm,

×

1
L(m) [c1]−

A =

P1

·

·

0 1 X1
0 0 A1 (cid:21)

(cid:20)

in block form. Then let P2 be a permutation matrix (either elementary or Im) such that

0 1 X1
0 0 A′1 (cid:21)
and the ﬁrst nonzero column c2 of A′1 has a nonzero entry on top. Thus,

1
L(m) [c1]−

A =

P1

P2

(cid:20)

·

·

·

1
L(m) [k1, c2]−

1
L(m) [c1]−

P2

·

·

0 1

A =

P1

·

·



0 0

X1
0 1 X2
0 0 A2



in block form. Continue to obtain elementary permutation matrices P1, P2, . . . , Pr and columns c1, c2, . . . , cr
of lengths m, m

1, . . . , such that





−

(LrPrLr

1Pr

−

1
−

· · ·

L2P2L1P1)A = U

where U is a row-echelon matrix and L j = L(m)
means the ﬁrst j
L(m)
k1, . . . , k j
matrix Pk can be “moved past” each matrix L j to the right of it, in the sense that
h

1 for each j, where the notation
−
−
1 columns are those of Im. It is not hard to verify that each L j has the form L j =
j + 1. We now claim that each permutation

where c′j is a column of length m

−
1, c′j
−

k1, . . . , k j

1, c j

−

i

(cid:2)

(cid:3)

PkL j = L′jPk

where L′j = L(m)
k1, . . . , k j
obtain a factorization of the form
h

1, c′′j
−

i

for some column c′′j of length m

−

j + 1. Given that this is true, we

If we write P = PrPr
triangular and invertible. All that remains is to prove the following rather technical result.

1
−
P2P1, this shows that PA has an LU-factorization because LrL′r

1 · · ·
−

1
−

· · ·

· · ·

1 · · ·
−

(LrL′r

L′2L′1)(PrPr

P2P1)A = U

L′2L′1 is lower

126

Matrix Algebra

Lemma 2.7.2
Let Pk resultfrominterchangingrow k of Im witharowbelowit. If j < k,let c j beacolumnof
length m

j + 1. Thenthereisanothercolumnc′j oflength m

j + 1 suchthat

−

−

Pk

·

L(m)

k1, . . . , kj

1, cj

−

= L(m)

k1, . . . , kj

(cid:2)

(cid:3)

(cid:2)

1, c′j
−

Pk

·

(cid:3)

The proof is left as Exercise 2.7.11.

Exercises for 2.7

Exercise 2.7.1 Find an LU-factorization of the follow-
ing matrices.

Exercise 2.7.2 Find a permutation matrix P and an LU-
factorization of PA if A is:

2
3
1

2
1
1

2
1
3
1

1
1
1
0

2
1
2
0
2

−

−

−

−

−

−

2
1
3
1

a.

b.









c. 





d. 





e. 






f. 





−

−

−

−

−

−

6
9
3

4
1
7

6
5
7
1

3
4
2
2

2
1
2
2
4

−
−

2
3
1

2
3
7

−

−
−
−

2
1
3
1

1
1
3
4

4
2
4
0
4

−
−

−

−

0 2
3 1
3 1

−









0 2
2 5
2 5
2 3

−







0
1
1
2

6
1
1
3
1

−
−

−

−

1
1
1
0







0 2
3 1
1 6
4 8
2 6

−









−

2
1
1
3

2 4 2
0 2 1
2 6 3
2 2 1

−

−
−

0
0
1

−

1 2
0 4
2 1









−

b.







a.

c.











d.



0
0
3

0 2
1 4
5 1

−



0
1
−
1
2

1
−
2
1
2

−

−
−

−

1
1
1
2

2
4
1
5

2 1 3
3 1 4
3 6 2
4 1 0



−
−

3 0
6 5
−
1 3
−
10 1





−

Exercise 2.7.3
In each case use the given LU-
decomposition of A to solve the system Ax = b by ﬁnding
y such that Ly = b, and then x such that U x = y:





1 0 0 1
0 0 1 2
0 0 0 1

;




1 1 0
0 1 0
0 0 0

1
−
1
0

;





















2
0
1

0 0
1 0
1 3

−

a. A =





b =



−

1
1
2



b. A =

b =












2 0 0
1 3 0
1 2 1
−
2
−
1
−
1











2
1
1
0

−

−

0 0
1 0
0 2
1 0

−

0
0
0
2

c. A = 





1

0

0

0

















1 2

−
1

1

1

0

0

0

1

4

1
2

−

−

1


;








2.8. An Application to Input-Output Economic Models

127

1
1
2
0

2
1
1
−
3
4
6
4
5

−

b = 

−





d. A = 





b = 











0
0
1 0
−
2
1
1
0







1
0
0
0

1
−
1
0
0

0
2
−
1
0

1
1
−
1
0

0
0
0
1

−













;







Exercise 2.7.4 Show that

= LU is impossible

(cid:20)
where L is lower triangular and U is upper triangular.

(cid:21)

0 1
1 0

Exercise 2.7.5 Show that we can accomplish any row
interchange by using only row operations of other types.

Exercise 2.7.6

a. Let L and L1 be invertible lower triangular matri-
ces, and let U and U1 be invertible upper triangu-
lar matrices. Show that LU = L1U1 if and only if
there exists an invertible diagonal matrix D such
1U . [Hint: Scrutinize
that L1 = LD and U1 = D−
1
L−
1 .]

1L1 = UU −

b. Use part (a) to prove Theorem 2.7.3 in the case

that A is invertible.

Exercise 2.7.7 Prove Lemma 2.7.1(1). [Hint: Use block
multiplication and induction.]

Exercise 2.7.8 Prove Lemma 2.7.1(2). [Hint: Use block
multiplication and induction.]

Exercise 2.7.9 A triangular matrix is called unit trian-
gular if it is square and every main diagonal element is a
1.

a. If A can be carried by the gaussian algorithm
to row-echelon form using no row interchanges,
show that A = LU where L is unit lower triangular
and U is upper triangular.

b. Show that the factorization in (a.) is unique.

−

. . . , cr be columns
Let c1, c2,
Exercise 2.7.10
If k j de-
of lengths m, m
r + 1.
. . . , m
1,
−
notes column j of Im, show that L(m) [c1, c2, . . . , cr] =
L(m) [c1] L(m) [k1, c2] L(m) [k1, k2, c3]
· · ·
L(m) [k1, k2, . . . , kr
1, cr]. The notation is as in the
proof of Theorem 2.7.2. [Hint: Use induction on m and
block multiplication.]

−

Exercise 2.7.11 Prove Lemma 2.7.2. [Hint: P−

1

k = Pk.

Write Pk =

(m

k)

×

−

(cid:20)
(m
−

Ik
0
0 P0 (cid:21)
k) permutation matrix.]

in block form where P0 is an

2.8 An Application to Input-Output Economic Models16

In 1973 Wassily Leontief was awarded the Nobel prize in economics for his work on mathematical mod-
els.17 Roughly speaking, an economic system in this model consists of several industries, each of which
produces a product and each of which uses some of the production of the other industries. The following
example is typical.

Example 2.8.1

A primitive society has three basic needs: food, shelter, and clothing. There are thus three
industries in the society—the farming, housing, and garment industries—that produce these
commodities. Each of these industries consumes a certain proportion of the total output of each

16The applications in this section and the next are independent and may be taken in any order.
17See W. W. Leontief, “The world economy of the year 2000,” Scientiﬁc American, Sept. 1980.

128

Matrix Algebra

commodity according to the following table.

Farming
CONSUMPTION Housing
Garment

OUTPUT
Farming Housing Garment
0.2
0.6
0.2

0.3
0.4
0.3

0.4
0.2
0.4

Find the annual prices that each industry must charge for its income to equal its expenditures.

Solution. Let p1, p2, and p3 be the prices charged per year by the farming, housing, and garment
industries, respectively, for their total output. To see how these prices are determined, consider the
farming industry. It receives p1 for its production in any year. But it consumes products from all
these industries in the following amounts (from row 1 of the table): 40% of the food, 20% of the
housing, and 30% of the clothing. Hence, the expenditures of the farming industry are
0.4p1 + 0.2p2 + 0.3p3, so

0.4p1 + 0.2p2 + 0.3p3 = p1

A similar analysis of the other two industries leads to the following system of equations.

0.4p1 + 0.2p2 + 0.3p3 = p1
0.2p1 + 0.6p2 + 0.4p3 = p2
0.4p1 + 0.2p2 + 0.3p3 = p3

This has the matrix form Ep = p, where

0.4 0.2 0.3
0.2 0.6 0.4
0.4 0.2 0.3 


E =





and

p =

p1
p2
p3









The equations can be written as the homogeneous system

where I is the 3

×

3 identity matrix, and the solutions are

E)p = 0

(I

−

p =



2t
3t
2t 



where t is a parameter. Thus, the pricing must be such that the total output of the farming industry
has the same value as the total output of the garment industry, whereas the total value of the
housing industry must be 3

2 as much.

In general, suppose an economy has n industries, each of which uses some (possibly none) of the
production of every industry. We assume ﬁrst that the economy is closed (that is, no product is exported
or imported) and that all product is used. Given two industries i and j, let ei j denote the proportion of the

2.8. An Application to Input-Output Economic Models

129

total annual output of industry j that is consumed by industry i. Then E =
matrix for the economy. Clearly,

ei j

0

≤

≤

1

for all i and j

(cid:2)

(cid:3)

Moreover, all the output from industry j is used by some industry (the model is closed), so

e1 j + e2 j +

· · ·

+ ei j = 1

for each j

ei j

is called the input-output

(2.12)

(2.13)

This condition asserts that each column of E sums to 1. Matrices satisfying conditions (2.12) and (2.13)
are called stochastic matrices.

As in Example 2.8.1, let pi denote the price of the total annual production of industry i. Then pi is the
+ ein pn annually for
annual revenue of industry i. On the other hand, industry i spends ei1 p1 + ei2 p2 +
the product it uses (ei j p j is the cost for product from industry j). The closed economic system is said to
be in equilibrium if the annual expenditure equals the annual revenue for each industry—that is, if

· · ·

e1 j p1 + e2 j p2 +

· · ·

+ ei j pn = pi

for each i = 1, 2, . . . , n

If we write p = 





p1
p2
...
pn








, these equations can be written as the matrix equation

Ep = p

This is called the equilibrium condition, and the solutions p are called equilibrium price structures.
The equilibrium condition can be written as

E)p = 0

(I

−

which is a system of homogeneous equations for p. Moreover, there is always a nontrivial solution p.
Indeed, the column sums of I
E has
a row of zeros. In fact, more is true:

E are all 0 (because E is stochastic), so the row-echelon form of I

−

−

Theorem 2.8.1

n stochasticmatrix. Thenthereisanonzero n

Let E beany n
entriessuchthat Ep= p. Ifalltheentriesof E arepositive,thematrixpcanbechosenwithall
entriespositive.

1 vectorpwithnonnegative

×

×

Theorem 2.8.1 guarantees the existence of an equilibrium price structure for any closed input-output

system of the type discussed here. The proof is beyond the scope of this book.18

18The interested reader is referred to P. Lancaster’s Theory of Matrices (New York: Academic Press, 1969) or to E. Seneta’s

Non-negative Matrices (New York: Wiley, 1973).

130

Matrix Algebra

Example 2.8.2

Find the equilibrium price structures for four industries if the input-output matrix is

0.6 0.2 0.1 0.1
0.3 0.4 0.2
0
0.1 0.3 0.5 0.2
0 0.1 0.2 0.7







E = 





Find the prices if the total value of business is $1000.

p1
p2
p3
p4

Solution. If p = 



is the equilibrium price structure, then the equilibrium condition reads



Ep = p. When we write this as (I

of solutions:





E)p = 0, the methods of Chapter 1 yield the following family

−

p = 

44t
39t
51t
47t











where t is a parameter. If we insist that p1 + p2 + p3 + p4 = 1000, then t = 5.525. Hence

243.09
215.47
281.76
259.67







p = 





to ﬁve ﬁgures.

The Open Model

We now assume that there is a demand for products in the open sector of the economy, which is the part of
the economy other than the producing industries (for example, consumers). Let di denote the total value of
the demand for product i in the open sector. If pi and ei j are as before, the value of the annual demand for
+ ein pn, so the total annual revenue
product i by the producing industries themselves is ei1 p1 + ei2 p2 +
pi of industry i breaks down as follows:

· · ·

pi = (ei1 p1 + ei2 p2 +

· · ·

+ ein pn) + di

for each i = 1, 2, . . . , n

The column d = 




d1
...
dn






or

is called the demand matrix, and this gives a matrix equation

p = Ep + d

E)p = d

(I

−

(2.14)

2.8. An Application to Input-Output Economic Models

131

Before proceeding, it is convenient to introduce a useful notation.

This is a system of linear equations for p, and we ask for a solution p with every entry nonnegative. Note
that every entry of E is between 0 and 1, but the column sums of E need not equal 1 as in the closed model.
are
bi j for all
(cid:2)
(cid:3)
0 implies that

and B =
B if ai j
matrices of the same size, we write A > B if ai j > bi j for all i and j, and we write A
(cid:3)
(cid:2)
≥
i and j. Thus P
0 and B
AB
0.

0 means that every entry of P is nonnegative. Note that A

If A =

bi j

ai j

≥

≥

≥

≥

≥
Now, given a demand matrix d

This certainly exists if I
any solution p to equation (2.14) satisﬁes p

−

≥
E is invertible and (I

0, we look for a production matrix p
1
E)−

≥
0. On the other hand, the fact that d

0 satisfying equation (2.14).
0 means

−

≥

Ep. Hence, the following theorem is not too surprising.

≥

≥

Theorem 2.8.2

0 beasquarematrix. Then I

Let E
acolumnp> 0suchthatp> Ep.

≥

E isinvertibleand (I

−

1
E)−

−

≥

0 ifandonlyifthereexists

Heuristic Proof.
1
E)−
If (I
a column p exists. Observe that

≥

−

0, the existence of p > 0 with p > Ep is left as Exercise 2.8.11. Conversely, suppose such

(I

−

E)(I + E + E2 +

+ Ek

1) = I
−

Ek

−

· · ·

2. If we can show that every entry of Ek approaches 0 as k becomes large then, intuitively,

holds for all k
the inﬁnite matrix sum

≥

U = I + E + E2 +

· · ·

E)U = I. Since U

exists and (I
≥
EP < µP for some number µ with 0 < µ < 1 (then EkP < µkP for all k
of µ is left as Exercise 2.8.12.

0, this does it. To show that Ek approaches 0, it sufﬁces to show that
1 by induction). The existence

≥

−

The condition p > Ep in Theorem 2.8.2 has a simple economic interpretation. If p is a production
matrix, entry i of Ep is the total value of all product used by industry i in a year. Hence, the condition
p > Ep means that, for each i, the value of product produced by industry i exceeds the value of the product
it uses. In other words, each industry runs at a proﬁt.

Example 2.8.3

If E =

0.6 0.2 0.3
0.1 0.4 0.2
0.2 0.5 0.1 

Solution. Use p = (3, 2, 2)T in Theorem 2.8.2.

, show that I

−





E is invertible and (I

1
E)−

0.

≥

−

If p0 = (1, 1, 1)T , the entries of Ep0 are the row sums of E. Hence p0 > Ep0 holds if the row sums of

E are all less than 1. This proves the ﬁrst of the following useful facts (the second is Exercise 2.8.10).

132

Matrix Algebra

Corollary 2.8.1

Let E

≥

0 beasquarematrix. Ineachcase, I

E isinvertibleand (I

−

1
E)−

0:

≥

−

1. Allrowsumsof E arelessthan 1.

2. Allcolumnsumsof E arelessthan 1.

Exercises for 2.8

Exercise 2.8.1 Find the possible equilibrium price struc-
tures when the input-output matrices are:

b. Use part (a.) to deduce that, if E and F are both
stochastic matrices, then EF is also stochastic.

0.1 0.2 0.3
0.6 0.2 0.3
0.3 0.6 0.4





0.3 0.1 0.1 0.2
0.2 0.3 0.1
0
0.3 0.3 0.2 0.3
0.2 0.3 0.6 0.5

0 0.1 0.1
0.5
0.2 0.7
0 0.1
0.1 0.2 0.8 0.2
0.2 0.1 0.1 0.6

a.

c.











d.











0.5
0 0.5
0.1 0.9 0.2
0.4 0.1 0.3





b.













Exercise 2.8.2 Three industries A, B, and C are such
that all the output of A is used by B, all the output of B is
used by C, and all the output of C is used by A. Find the
possible equilibrium price structures.

−
E =

≥
a b
c d

(cid:20)

(cid:21)

Exercise 2.8.3 Find the possible equilibrium price struc-
tures for three industries where the input-output matrix

1 0 0
0 0 1
0 1 0

is



here.


. Discuss why there are two parameters





Exercise 2.8.4
2
stochastic matrix E by ﬁrst writing it in the form E =

Prove Theorem 2.8.1 for a 2

×

a

b

, where 0

a

1 and 0

b

1.

b

1

a 1

(cid:21)

−

−

≤
(cid:20)
Exercise 2.8.5 If E is an n
is an n
equals the sum of the entries of the n

≤
≤
n stochastic matrix and c
1 matrix, show that the sum of the entries of c

1 matrix Ec.

×

×

≤

Exercise 2.8.6 Let W =
and F denote n

×

(cid:2)

1 1 1

1

. Let E

n matrices with nonnegative entries.

(cid:3)

a. Show that E is a stochastic matrix if and only if

W E = W .

×

· · ·

Exercise 2.8.7 Find a 2
tween 0 and 1 such that:

×

2 matrix E with entries be-

a. I

−

E has no inverse.

b. I

E has an inverse but not all entries of (I

−
are nonnegative.

1

E)−

−

If E is a 2

Exercise 2.8.8
×
between 0 and 1, show that I
(I

2 matrix with entries
E is invertible and
−
0 if and only if tr E < 1 + det E. Here, if

1

E)−

, then tr E = a + d and det E = ad

bc.

−

Exercise 2.8.9 In each case show that I
and (I

0.

E)−

1

E is invertible

−

−

≥

0.6 0.5 0.1
0.1 0.3 0.3
0.2 0.1 0.4

0.6 0.2 0.1
0.3 0.4 0.2
0.2 0.5 0.1









a.

c.









0.7 0.1 0.3
0.2 0.5 0.2
0.1 0.1 0.4

0.8 0.1 0.1
0.3 0.1 0.2
0.3 0.3 0.2









b.

d.









Exercise 2.8.10 Prove that (1) implies (2) in the Corol-
lary to Theorem 2.8.2.

Exercise 2.8.11 If (I
p > Ep.

−

1

E)−

≥

0, ﬁnd p > 0 such that

Exercise 2.8.12 If Ep < p where E
a number µ such that Ep < µp and 0 < µ < 1.

≥

0 and p > 0, ﬁnd

[Hint: If Ep = (q1, . . . , qn)T and p = (p1, . . . , pn)T ,
< µ < 1.]

take any number µ where max

q1
p1

, . . . , qn
pn

n

o

2.9. An Application to Markov Chains

133

2.9 An Application to Markov Chains

Many natural phenomena progress through various stages and can be in a variety of states at each stage.
For example, the weather in a given city progresses day by day and, on any given day, may be sunny or
rainy. Here the states are “sun” and “rain,” and the weather progresses from one state to another in daily
stages. Another example might be a football team: The stages of its evolution are the games it plays, and
the possible states are “win,” “draw,” and “loss.”

The general setup is as follows: A real conceptual “system” is run generating a sequence of outcomes.
The system evolves through a series of “stages,” and at any stage it can be in any one of a ﬁnite number of
“states.” At any given stage, the state to which it will go at the next stage depends on the past and present
history of the system—that is, on the sequence of states it has occupied to date.

Deﬁnition 2.15 Markov Chain

AMarkovchainissuchanevolvingsystemwhereinthestatetowhichitwillgonextdepends
onlyonitspresentstateanddoesnotdependontheearlierhistoryofthesystem.19

Even in the case of a Markov chain, the state the system will occupy at any stage is determined only
in terms of probabilities. In other words, chance plays a role. For example, if a football team wins a
particular game, we do not know whether it will win, draw, or lose the next game. On the other hand, we
may know that the team tends to persist in winning streaks; for example, if it wins one game it may win
the next game 1
10 of the time. These fractions are called the
probabilities of these various possibilities. Similarly, if the team loses, it may lose the next game with
probability 1
4. The probabilities
of the various outcomes after a drawn game will also be known.

2 (that is, half the time), win with probability 1

4, and draw with probability 1

10 of the time, and draw 1

2 of the time, lose 4

We shall treat probabilities informally here: The probability that a given event will occur is the long-
run proportion of the time that the event does indeed occur. Hence, all probabilities are numbers between
0 and 1. A probability of 0 means the event is impossible and never occurs; events with probability 1 are
certain to occur.

If a Markov chain is in a particular state, the probabilities that it goes to the various states at the next
stage of its evolution are called the transition probabilities for the chain, and they are assumed to be
known quantities. To motivate the general conditions that follow, consider the following simple example.
Here the system is a man, the stages are his successive lunches, and the states are the two restaurants he
chooses.

Example 2.9.1

A man always eats lunch at one of two restaurants, A and B. He never eats at A twice in a row.
However, if he eats at B, he is three times as likely to eat at B next time as at A. Initially, he is
equally likely to eat at either restaurant.

a. What is the probability that he eats at A on the third day after the initial one?

b. What proportion of his lunches does he eat at A?

19The name honours Andrei Andreyevich Markov (1856–1922) who was a professor at the university in St. Petersburg,

Russia.

134

Matrix Algebra

Solution. The table of transition probabilities follows. The A column indicates that if he eats at A
on one day, he never eats there again on the next day and so is certain to go to B.

Next
Lunch

A
B

Present Lunch

A
0
1

B
0.25
0.75

The B column shows that, if he eats at B on one day, he will eat there on the next day 3
and switches to A only 1
The restaurant he visits on a given day is not determined. The most that we can expect is to know
the probability that he will visit A or B on that day.

4 of the time.

4 of the time

s(m)
1
s(m)
2

Let sm =

denote the state vector for day m. Here s(m)

1

denotes the probability that he




eats at A on day m, and s(m)

correspond to the initial day. Because he is equally likely to eat at A or B on that initial day,
1 = 0.5 and s(0)
s(0)

is the probability that he eats at B on day m. It is convenient to let s0

2 = 0.5, so s0 =

. Now let



2

0.5
0.5

(cid:20)

(cid:21)

P =

(cid:20)

0 0.25
1 0.75

(cid:21)

denote the transition matrix. We claim that the relationship

sm+1 = Psm

holds for all integers m
compute s1, s2, s3, . . . .

≥

0. This will be derived later; for now, we use it as follows to successively

s1 = Ps0 =

s2 = Ps1 =

s3 = Ps2 =

(cid:20)

(cid:20)

0 0.25
1 0.75

0 0.25
1 0.75

0 0.25
1 0.75

(cid:21) (cid:20)

(cid:21) (cid:20)

0.5
0.5

(cid:21)
0.125
0.875

=

(cid:20)
=

(cid:21)
0.21875
0.78125

0.125
0.875

(cid:21)
0.21875
0.78125

(cid:20)
=

(cid:21)

0.1953125
0.8046875

(cid:21) (cid:20)
Hence, the probability that his third lunch (after the initial one) is at A is approximately 0.195,
whereas the probability that it is at B is 0.805. If we carry these calculations on, the next state
vectors are (to ﬁve ﬁgures):

(cid:21)

(cid:20)

(cid:21)

(cid:20)

s4 =

s6 =

(cid:20)

0.20117
0.79883

0.20007
0.79993

(cid:21)

s5 =

s7 =

(cid:20)

0.19971
0.80029

0.19998
0.80002

(cid:21)

(cid:21)
Moreover, as m increases the entries of sm get closer and closer to the corresponding entries of

(cid:20)

(cid:21)

(cid:20)

0.2
0.8

(cid:21)

(cid:20)

. Hence, in the long run, he eats 20% of his lunches at A and 80% at B.

2.9. An Application to Markov Chains

135

Present
State

p1 j

state
j

p2 j

pn j

Next
State

state
1

state
2

state
n

Example 2.9.1 incorporates most of the essential features of all Markov
chains. The general model is as follows: The system evolves through
various stages and at each stage can be in exactly one of n distinct states. It
progresses through a sequence of states as time goes on. If a Markov chain
is in state j at a particular stage of its development, the probability pi j that
it goes to state i at the next stage is called the transition probability. The
is called the transition matrix for the Markov
n
chain. The situation is depicted graphically in the diagram.

n matrix P =

pi j

×

We make one important assumption about the transition matrix P =
: It does not depend on which stage the process is in. This assumption
pi j
means that the transition probabilities are independent of time—that is,
(cid:2)
they do not change as time goes on. It is this assumption that distinguishes
Markov chains in the literature of this subject.

(cid:3)

(cid:2)

(cid:3)

Example 2.9.2

Suppose the transition matrix of a three-state Markov chain is

P =

p11 p12 p13
p21 p22 p23
p31 p32 p33



=











Present state
1
3
2
0.3 0.1 0.6
0.5 0.9 0.2
0.2 0.0 0.2 


1
2
3

Next state

If, for example, the system is in state 2, then column 2 lists the probabilities of where it goes next.
Thus, the probability is p12 = 0.1 that it goes from state 2 to state 1, and the probability is
p22 = 0.9 that it goes from state 2 to state 2. The fact that p32 = 0 means that it is impossible for it
to go from state 2 to state 3 at the next stage.

Consider the jth column of the transition matrix P.

p1 j
p2 j
...
pn j















If the system is in state j at some stage of its evolution, the transition probabilities p1 j, p2 j, . . . , pn j
represent the fraction of the time that the system will move to state 1, state 2, . . . , state n, respectively, at
the next stage. We assume that it has to go to some state at each transition, so the sum of these probabilities
is 1:

p1 j + p2 j +

+ pn j = 1

for each j

· · ·

Thus, the columns of P all sum to 1 and the entries of P lie between 0 and 1. Hence P is called a stochastic
matrix.

As in Example 2.9.1, we introduce the following notation: Let s(m)

i

denote the probability that the

136

Matrix Algebra

system is in state i after m transitions. The n



sm =

m = 0, 1, 2, . . .

1 matrices

×
s(m)
1
s(m)
2
...
s(m)
n



















are called the state vectors for the Markov chain. Note that the sum of the entries of sm must equal 1
because the system must be in some state after m transitions. The matrix s0 is called the initial state
vector for the Markov chain and is given as part of the data of the particular chain. For example, if the

chain has only two states, then an initial vector s0 =

state 2, the initial vector would be s0 =

in state 1 or in state 2.

0
1

(cid:20)
. If s0 =
(cid:21)

(cid:20)

1
0

(cid:20)

means that it started in state 1. If it started in

(cid:21)
0.5
0.5

, it is equally likely that the system started

(cid:21)

Theorem 2.9.1
Let P bethetransitionmatrixforan n-stateMarkovchain. Ifsm isthestatevectoratstage m,then

foreach m = 0, 1, 2, . . ..

sm+1 = Psm

Heuristic Proof. Suppose that the Markov chain has been run N times, each time starting with the same
initial state vector. Recall that pi j is the proportion of the time the system goes from state j at some stage
to state i at the next stage, whereas s(m)

is the proportion of the time it is in state i at stage m. Hence

i

sm+1
i N

is (approximately) the number of times the system is in state i at stage m + 1. We are going to calculate
this number another way. The system got to state i at stage m + 1 through some other state (say state j)
at stage m. The number of times it was in state j at that stage is (approximately) s(m)
j N, so the number of
times it got to state i via state j is pi j(s(m)
j N). Summing over j gives the number of times the system is in
state i (at stage m + 1). This is the number we calculated before, so

Dividing by N gives s(m+1)
matrix equation sm+1 = Psm.

i

= pi1s(m)

s(m+1)
i

N = pi1s(m)
1 + pi2s(m)

2 +

1 N + pi2s(m)

2 N +
+ pins(m)

n

+ pins(m)
n N

· · ·
for each i, and this can be expressed as the

· · ·

If the initial probability vector s0 and the transition matrix P are given, Theorem 2.9.1 gives s1, s2, s3, . . . ,

one after the other, as follows:

s1 = Ps0
s2 = Ps1
s3 = Ps2
...

2.9. An Application to Markov Chains

137

Hence, the state vector sm is completely determined for each m = 0, 1, 2, . . . by P and s0.

Example 2.9.3

A wolf pack always hunts in one of three regions R1, R2, and R3. Its hunting habits are as follows:

1. If it hunts in some region one day, it is as likely as not to hunt there again the next day.

2. If it hunts in R1, it never hunts in R2 the next day.

3. If it hunts in R2 or R3, it is equally likely to hunt in each of the other regions the next day.

If the pack hunts in R1 on Monday, ﬁnd the probability that it hunts there on Thursday.

Solution. The stages of this process are the successive days; the states are the three regions. The
transition matrix P is determined as follows (see the table): The ﬁrst habit asserts that
p11 = p22 = p33 = 1
goes to state 2, so p21 = 0 and, because the column must sum to 1, p31 = 1
what happens if it starts in R2: p22 = 1
because the column sum must equal 1. Column 3 is ﬁlled in a similar way.

2 . Now column 1 displays what happens when the pack starts in R1: It never
2. Column 2 describes
2 and p12 and p32 are equal (by habit 3), so p12 = p32 = 1
2

R1 R2 R3
1
1
1
4
4
2
1
1
4
2
1
1
2
4

1
2

0

R1

R2

R3

Now let Monday be the initial stage. Then s0 =



because the pack hunts in R1 on that day.

Then s1, s2, and s3 describe Tuesday, Wednesday, and Thursday, respectively, and we compute
them using Theorem 2.9.1.



1
2



s3 = Ps2 = 








Hence, the probability that the pack hunts in Region R1 on Thursday is 11
32 .

s1 = Ps0 = 





s2 = Ps1 = 












1
2

0

11
32
6
32
15
32








1
0
0 


3
8
1
8
4
8

138

Matrix Algebra

Steady State Vector

Another phenomenon that was observed in Example 2.9.1 can be expressed in general terms. The state

0.2
0.8

(cid:20)

(cid:21)

vectors s0, s1, s2, . . . were calculated in that example and were found to “approach” s =

. This

means that the ﬁrst component of sm becomes and remains very close to 0.2 as m becomes large, whereas
the second component gets close to 0.8 as m increases. When this is the case, we say that sm converges to
s. For large m, then, there is very little error in taking sm = s, so the long-term probability that the system
is in state 1 is 0.2, whereas the probability that it is in state 2 is 0.8. In Example 2.9.1, enough state vectors
were computed for the limiting vector s to be apparent. However, there is a better way to do this that works
in most cases.

Suppose P is the transition matrix of a Markov chain, and assume that the state vectors sm converge to
a limiting vector s. Then sm is very close to s for sufﬁciently large m, so sm+1 is also very close to s. Thus,
the equation sm+1 = Psm from Theorem 2.9.1 is closely approximated by

s = Ps

so it is not surprising that s should be a solution to this matrix equation. Moreover, it is easily solved
because it can be written as a system of homogeneous linear equations

P)s = 0

(I

−

, the general solution to (I

P)s = 0 is s =

, where t

−

t
4t

(cid:20)

(cid:21)

with the entries of s as variables.

In Example 2.9.1, where P =

0 0.25
1 0.75

(cid:20)

(cid:21)

t = 0.2 and so s =

as before.

is a parameter. But if we insist that the entries of S sum to 1 (as must be true of all state vectors), we ﬁnd

0.2
0.8

(cid:20)

(cid:21)

0 0.25
1 0.75

All this is predicated on the existence of a limiting vector for the sequence of state vectors of the
Markov chain, and such a vector may not always exist. However, it does exist in one commonly occurring
situation. A stochastic matrix P is called regular if some power Pm of P has every entry greater than zero.

The matrix P =

of Example 2.9.1 is regular (in this case, each entry of P2 is positive), and

(cid:20)
the general theorem is as follows:

(cid:21)

Theorem 2.9.2

Let P bethetransitionmatrixofaMarkovchainandassumethat P isregular. Thenthereisa
uniquecolumnmatrixssatisfyingthefollowingconditions:

1. Ps= s.

2. Theentriesofsarepositiveandsumto 1.

Moreover,condition1canbewrittenas

andsogivesahomogeneoussystemoflinearequationsfors. Finally,thesequenceofstatevectors
s0, s1, s2, . . . convergestosinthesensethatif m islargeenough,eachentryofsm isclosely
approximatedbythecorrespondingentryofs.

P)s= 0

(I

−

2.9. An Application to Markov Chains

139

This theorem will not be proved here.20

If P is the regular transition matrix of a Markov chain, the column s satisfying conditions 1 and 2 of
Theorem 2.9.2 is called the steady-state vector for the Markov chain. The entries of s are the long-term
probabilities that the chain will be in each of the various states.

Example 2.9.4

A man eats one of three soups—beef, chicken, and vegetable—each day. He never eats the same
soup two days in a row. If he eats beef soup on a certain day, he is equally likely to eat each of the
others the next day; if he does not eat beef soup, he is twice as likely to eat it the next day as the
alternative.

a. If he has beef soup one day, what is the probability that he has it again two days later?

b. What are the long-run probabilities that he eats each of the three soups?

Solution. The states here are B, C, and V , the three soups. The transition matrix P is given in the
table. (Recall that, for each state, the corresponding column lists the probabilities for the next
state.)

B C V

B 0

C 1
2
V 1
2

2
3

0

1
3

2
3
1
3

0

If he has beef soup initially, then the initial state vector is

Then two days later the state vector is s2. If P is the transition matrix, then

1
0
0 


s0 =





s1 = Ps0 = 1

2 

,

s2 = Ps1 = 1

6 

0
1
1 




4
1
1 




so he eats beef soup two days later with probability 2
eats chicken and vegetable soup each with probability 1
6.

3. This answers (a.) and also shows that he

To ﬁnd the long-run probabilities, we must ﬁnd the steady-state vector s. Theorem 2.9.2 applies

because P is regular (P2 has positive entries), so s satisﬁes Ps = s. That is, (I

P)s = 0 where

−

I

−

P = 1

6 



6
3
−
3
−

4
−
6
2
−

4
−
2
6 
−


20The interested reader can ﬁnd an elementary proof in J. Kemeny, H. Mirkil, J. Snell, and G. Thompson, Finite Mathematical

Structures (Englewood Cliffs, N.J.: Prentice-Hall, 1958).

140

Matrix Algebra

The solution is s =



4t
3t
3t 


, where t is a parameter, and we use s =

because the entries of

s must sum to 1. Hence, in the long run, he eats beef soup 40% of the time and eats chicken soup

and vegetable soup each 30% of the time.





0.4
0.3
0.3 







(cid:21)








Exercises for 2.9

Exercise 2.9.1 Which of the following stochastic matri-
ces is regular?

0 0

a.



1 0

1
2

1
2



b.



0 1 0






1
2

1
4

1
4

0

1

0

1
3

1
3

1
3













Exercise 2.9.2 In each case ﬁnd the steady-state vector
and, assuming that it starts in state 1, ﬁnd the probability
that it is in state 2 after 3 transitions.

0.5 0.3
0.5 0.7

0

1
2

1 0

0

1
2

1
4

1
4

1
2

0.8 0.0 0.2
0.1 0.6 0.1
0.1 0.4 0.7

a.

c.

e.

(cid:20)












b.

d.

f.













1
2

1
2

1
0 

0.4 0.1 0.5
0.2 0.6 0.2
0.4 0.3 0.3

0.1 0.3 0.3
0.3 0.1 0.6
0.6 0.6 0.1













Exercise 2.9.3 A fox hunts in three territories A, B, and
C. He never hunts in the same territory on two successive
days. If he hunts in A, then he hunts in C the next day. If
he hunts in B or C, he is twice as likely to hunt in A the
next day as in the other territory.

a. What proportion of his time does he spend in A, in

B, and in C?

b. If he hunts in A on Monday (C on Monday), what
is the probability that he will hunt in B on Thurs-
day?

Exercise 2.9.4 Assume that
there are three social
classes—upper, middle, and lower—and that social mo-
bility behaves as follows:

1. Of the children of upper-class parents, 70% re-
main upper-class, whereas 10% become middle-
class and 20% become lower-class.

2. Of the children of middle-class parents, 80% re-
main middle-class, whereas the others are evenly
split between the upper class and the lower class.

3. For the children of lower-class parents, 60% re-
main lower-class, whereas 30% become middle-
class and 10% upper-class.

a. Find the probability that the grandchild of
lower-class parents becomes upper-class.

b. Find the long-term breakdown of society

into classes.

Exercise 2.9.5 The prime minister says she will call
an election. This gossip is passed from person to person
with a probability p
= 0 that the information is passed in-
correctly at any stage. Assume that when a person hears
the gossip he or she passes it to one person who does not
know. Find the long-term probability that a person will
hear that there is going to be an election.

Exercise 2.9.6 John makes it to work on time one Mon-
day out of four. On other work days his behaviour is as
follows: If he is late one day, he is twice as likely to come
to work on time the next day as to be late. If he is on time
one day, he is as likely to be late as not the next day. Find
the probability of his being late and that of his being on
time Wednesdays.

Exercise 2.9.7 Suppose you have 1¢ and match coins
with a friend. At each match you either win or lose 1¢
with equal probability. If you go broke or ever get 4¢,
you quit. Assume your friend never quits. If the states
are 0, 1, 2, 3, and 4 representing your wealth, show that
the corresponding transition matrix P is not regular. Find
the probability that you will go broke after 3 matches.

6
Exercise 2.9.8 A mouse is put into a maze of compart-
ments, as in the diagram. Assume that he always leaves
any compartment he enters and that he is equally likely
to take any tunnel entry.

1

3

2

4

5

a. If he starts in compartment 1, ﬁnd the probability
that he is in compartment 1 again after 3 moves.

b. Find the compartment in which he spends most of

his time if he is left for a long time.

2.9. An Application to Markov Chains

141

Exercise 2.9.9 If a stochastic matrix has a 1 on its main
diagonal, show that it cannot be regular. Assume it is not
1

1.

Exercise 2.9.10 If sm is the stage-m state vector for a
Markov chain, show that sm+k = Pksm holds for all m
1
1 (where P is the transition matrix).
and k

≥

×

≥

Exercise 2.9.11 A stochastic matrix is doubly stochas-
tic if all the row sums also equal 1. Find the steady-state
vector for a doubly stochastic matrix.

Exercise 2.9.12 Consider the 2
p

q

1

P =

(cid:20)

−
p

1

q

−

,

(cid:21)

where 0 < p < 1 and 0 < q < 1.

2 stochastic matrix

×

a. Show that

P.

1
p+q

(cid:20)

q
p

(cid:21)

is the steady-state vector for

b. Show that Pm

converges

to

the matrix

by ﬁrst verifying inductively that

1
p+q

q q
p p

(cid:20)
Pm = 1
p+q

(cid:21)
q q
p p

p
+ (1
−
−
p+q

q)m

p
p

q
−
q

for

(cid:21)

(cid:20)

(cid:21)

m = 1, 2, . . . . (It can be shown that the sequence
of powers P, P2, P3, . . . of any regular transi-
tion matrix converges to the matrix each of whose
columns equals the steady-state vector for P.)

(cid:20)

−

Supplementary Exercises for Chapter 2

Exercise 2.1 Solve for the matrix X if:

b. If p(U ) = 0 where U is n

a.

PX Q = R;

b.

X P = S;

U .

n, ﬁnd U −

1 in terms of

×

where P =

R =



−
−



1
2
0

1 1
4 0
6 6

0
1
3

−

, Q =



(cid:20)

1 1
2 0

1
−
3

,

(cid:21)


, S =



4
6
6

(cid:20)

1 6
3 1

(cid:21)

−
−
−


Exercise 2.2 Consider



p(X ) = X 3

5X 2 + 11X

4I.

−

−

a. If p(U ) =

1 3
1 0

(cid:21)

(cid:20)

−

compute p(U T ).

Exercise 2.3
Show that, if a (possibly nonhomoge-
neous) system of equations is consistent and has more
variables than equations,
then it must have inﬁnitely
[Hint: Use Theorem 2.2.2 and Theo-
many solutions.
rem 1.3.1.]

Exercise 2.4 Assume that a system Ax = b of linear
equations has at least two distinct solutions y and z.

a. Show that xk = y + k(y

k.

z) is a solution for every

−

b. Show that xk = xm implies k = m. [Hint: See Ex-

ample 2.1.7.]

142

Matrix Algebra

c. Deduce that Ax = b has inﬁnitely many solutions.

Exercise 2.9 If A is 2

Exercise 2.5

a. Let A be a 3

3 matrix with all entries on and be-

A =

low the main diagonal zero. Show that A3 = 0.

×

2, show that A−

1 = AT if and

only if A =

for some θ or

×

cosθ sinθ
sinθ cosθ
sinθ
cosθ

(cid:21)
for some θ.

−

(cid:20)
cosθ
sinθ

(cid:20)
[Hint: If a2 + b2 = 1, then a = cosθ, b = sinθ for

−

(cid:21)

some θ. Use

cos(θ

−

φ) = cosθcosφ + sinθsinφ.]

Exercise 2.10

a. If A =

0 1
1 0

(cid:21)

(cid:20)

, show that A2 = I.

b. What is wrong with the following argument? If
I)(A + I) = 0,

I = 0, so (A

A2 = I, then A2
whence A = I or A =

−

−

I.

−

Exercise 2.11 Let E and F be elementary matrices ob-
tained from the identity matrix by adding multiples of
row k to rows p and q. If k
= q, show that
EF = FE.

= p and k

0 0
0 0

2 real matrix, A2 = A and

Exercise 2.12 If A is a 2
AT = A, show that either A is one of

×

,

1 0
0 0

0 0
0 1
(cid:20)
(cid:21)
where a2 + b2 = a,

(cid:20)

1 0
0 1
b

,

(cid:21)
−

(cid:20)
1
2 ≤

(cid:20)
, or A =

(cid:21)
1
2 and b

(cid:20)

= 0.

≤
Exercise 2.13 Show that the following are equivalent
for matrices P, Q:

,

(cid:21)
a
b 1

b

−

a

(cid:21)

1. P, Q, and P + Q are all invertible and

(P + Q)−

1 = P−

1 + Q−

1

b. Generalize to the n

swer.

n case and prove your an-

×

Exercise 2.6 Let Ipq denote the n
entry equal to 1 and all other entries 0. Show that:

n matrix with (p, q)-

×

a. In = I11 + I22 +

b. IpqIrs =

Ips
0

(cid:26)

+ Inn.

· · ·
if q = r
= r
if q

.

c. If A = [ai j] is n

×

n, then A = ∑n

i=1 ∑n

j=1 ai jIi j.

d. If A = [ai j], then IpqAIrs = aqrIps for all p, q, r, and

s.

Exercise 2.7 A matrix of the form aIn, where a is a
number, is called an n

n scalar matrix.

×

a. Show that each n

n scalar matrix commutes with

every n

×

×
n matrix.

b. Show that A is a scalar matrix if it commutes with
n matrix. [Hint: See part (d.) of Exer-

every n
×
cise 2.6.]

A B
C D

Exercise 2.8 Let M =

, where A, B, C, and

(cid:21)
n and each commutes with all the others. If
D are all n
M2 = 0, show that (A + D)3 = 0. [Hint: First show that
A2 =

BC = D2 and that

×

(cid:20)

−

B(A + D) = 0 = C(A + D).]

2. P is invertible and Q = PG where G2 + G + I = 0.

6
6
6
6
Chapter 3

Determinants and Diagonalization

With each square matrix we can calculate a number, called the determinant of the matrix, which tells us
whether or not the matrix is invertible. In fact, determinants can be used to give a formula for the inverse
of a matrix. They also arise in calculating certain numbers (called eigenvalues) associated with the matrix.
These eigenvalues are essential to a technique called diagonalization that is used in many applications
where it is desired to predict the future behaviour of a system. For example, we use it to predict whether a
species will become extinct.

Determinants were ﬁrst studied by Leibnitz in 1696, and the term “determinant” was ﬁrst used in
1801 by Gauss is his Disquisitiones Arithmeticae. Determinants are much older than matrices (which
were introduced by Cayley in 1878) and were used extensively in the eighteenth and nineteenth centuries,
primarily because of their signiﬁcance in geometry (see Section 4.4). Although they are somewhat less
important today, determinants still play a role in the theory and application of matrix algebra.

3.1 The Cofactor Expansion

In Section 2.4 we deﬁned the determinant of a 2

2 matrix A =

×

as follows:1

a b
c d

(cid:21)

(cid:20)

bc

det A =

a b
c d

= ad

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

and showed (in Example 2.4.4) that A has an inverse if and only if det A
is to do this for any square matrix A. There is no difﬁculty for 1
det A = det [a] = a and note that A is invertible if and only if a
= 0.

×

= 0. One objective of this chapter
1 matrices: If A = [a], we deﬁne

If A is 3

3 and invertible, we look for a suitable deﬁnition of det A by trying to carry A to the identity
matrix by row operations. The ﬁrst column is not zero (A is invertible); suppose the (1, 1)-entry a is not
zero. Then row operations give

×

A =




where u = ae
suppose that u

→ 

a b c
d e
f
g h i 


bd and v = ah
−
−
= 0. Then the reduction proceeds

c
b
a
ad ae a f
ag ah ai 


→ 

−
−
bg. Since A is invertible, one of u and v is nonzero (by Example 2.4.11);

−
−

−
−







=

cd
cg 


c

a b
0 u a f
ai
0 v

cd
cg 


a
0 ae
0 ah

b

c

bd a f
bg ai

A

→ 

a b
0 u a f
ai
0 v

c

b
u

a
a f
0
0 uv u(ai

cd
cg 


→ 

−
−

cd) = a(aei + b f g + cdh

c

cd
cg)

−
−
ceg

a b
0 u a f
0 0

c

−
w

cd



→ 






a f h

−


bdi). We deﬁne

−

−

where w = u(ai


cg)

−

v(a f

−

−

1Determinants are commonly written

A

|

|

= det A using vertical bars. We will use both notations.

det A = aei + b f g + cdh

ceg

a f h

bdi

−

−

−

(3.1)

143

6
6
6
144

Determinants and Diagonalization

and observe that det A

= 0 because a det A = w

= 0 (is invertible).

To motivate the deﬁnition below, collect the terms in Equation 3.1 involving the entries a, b, and c in

row 1 of A:

det A =

a b c
d e
f
g h i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= aei + b f g + cdh

ceg

a f h

bdi

−

−

−

f h)
= a(ei
−
e
f
h i

= a

b(di
d
g

b

−

−

−

f g) + c(dh
f
i

−
d e
g h

+ c

eg)

(cid:12)
(cid:12)
(cid:12)
3 matrix A, multiply
This last expression can be described as follows: To compute the determinant of a 3
(cid:12)
×
each entry in row 1 by a sign times the determinant of the 2
2 matrix obtained by deleting the row and
column of that entry, and add the results. The signs alternate down row 1, starting with +. It is this
observation that we generalize below.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

×

Example 3.1.1

det

−

2 3 7
4 0 6
1 5 0 






+ 7

0 6
5 0

3

−

4 6
1 0

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

(cid:12)
(cid:12)
(cid:12)
6) + 7(
(cid:12)
−

(cid:12)
(cid:12)
(cid:12)
20)
(cid:12)
−

30)

3(

= 2

(cid:12)
(cid:12)
(cid:12)
= 2(
(cid:12)
−
182
=

−

4 0
1 5

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

This suggests an inductive method of deﬁning the determinant of any square matrix in terms of de-
3 matrices in terms of

terminants of matrices one size smaller. The idea is to deﬁne determinants of 3
determinants of 2

2 matrices, then we do 4

4 matrices in terms of 3

3 matrices, and so on.

×

To describe this, we need some terminology.

×

×

×

Deﬁnition 3.1 Cofactors of a Matrix

Assumethatdeterminantsof (n
A,let

−

1)

×

(n

−

1) matriceshavebeendeﬁned. Giventhe n

n matrix

×

Ai j denotethe (n

1)

(n

1) matrixobtainedfrom A bydeletingrow i andcolumn j.

×
Thenthe (i, j)-cofactor ci j(A) isthescalardeﬁnedby

−

−

Here (

1)i+ j iscalledthesignofthe (i, j)-position.

−

ci j(A) = (

1)i+ j det (Ai j)

−

6
6
The sign of a position is clearly 1 or

1, and the following diagram is useful for remembering it:

−

3.1. The Cofactor Expansion

145

+

−
+

−
...

−
+

−
+
...

+

−
+

−
...

− · · ·
+
· · ·
− · · ·
+
· · ·
...



















Note that the signs alternate along each row and column with + in the upper left corner.

Example 3.1.2

Find the cofactors of positions (1, 2), (3, 1), and (2, 3) in the following matrix.

A =

3
5
8





−

1 6
2 7
9 4 


Solution. Here A12 is the matrix

that remains when row 1 and column 2 are deleted. The

1 (this is also the (1, 2)-entry in the sign diagram), so the

sign of position (1, 2) is (
(1, 2)-cofactor is

−

5 7
8 4

−

(cid:20)
1)1+2 =

(cid:21)

c12(A) = (

1)1+2

−

(cid:12)
(cid:12)
(cid:12)
Turning to position (3, 1), we ﬁnd
(cid:12)

= (

1)(5

−

4

7

·

−

·

8) = (

1)(

−

36) = 36

−

5 7
8 4

(cid:12)
(cid:12)
(cid:12)
(cid:12)

c31(A) = (

1)3+1A31 = (

−

1)3+1

−

Finally, the (2, 3)-cofactor is

= (+1)(

7
−

−

12) =

19

−

1 6
2 7

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

c23(A) = (

1)2+3A23 = (

−

1)2+3

−

3
8

1
−
9

= (

1)(27 + 8) =

−

35

−

Clearly other cofactors can be found—there are nine in all, one for each position in the matrix.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

We can now deﬁne det A for any square matrix A

Deﬁnition 3.2 Cofactor expansion of a Matrix

Assumethatdeterminantsof (n
deﬁne

−

1)

×

(n

−

1) matriceshavebeendeﬁned. If A =

ai j

is n

n

×

det A = a11c11(A) + a12c12(A) +

+ a1nc1n(A)

(cid:2)

(cid:3)

· · ·

Thisiscalledthecofactorexpansionof det A alongrow 1.

146

Determinants and Diagonalization

It asserts that det A can be computed by multiplying the entries of row 1 by the corresponding cofac-
tors, and adding the results. The astonishing thing is that det A can be computed by taking the cofactor
expansion along any row or column: Simply multiply each entry of that row or column by the correspond-
ing cofactor and add.

Theorem 3.1.1: Cofactor Expansion Theorem2

Thedeterminantofan n
roworcolumnof A. Thatis det A canbecomputedbymultiplyingeachentryoftherowor
columnbythecorrespondingcofactorandaddingtheresults.

n matrix A canbecomputedbyusingthecofactorexpansionalongany

×

The proof will be given in Section 3.6.

Example 3.1.3

Compute the determinant of A =

3 4
1 7
9 8



.

5
2
6 


−


Solution. The cofactor expansion along the ﬁrst row is as follows:

det A = 3c11(A) + 4c12(A) + 5c13(A)
2
6
−
58)

2
6
−
24) + 5(

= 3

7
8

1
9

−

4

+ 5

(cid:12)
(cid:12)
4(
(cid:12)
(cid:12)

−

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
55)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
= 3(
(cid:12)
−
(cid:12)
353
=
−

1 7
9 8

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Note that the signs alternate along the row (indeed along any row or column). Now we compute
det A by expanding along the ﬁrst column.

= 3

det A = 3c11(A) + 1c21(A) + 9c31(A)
2
6
−
58)

5
6
−
(cid:12)
−
(cid:12)
64) + 9(
(cid:12)
(cid:12)

(cid:12)
(cid:12)
27)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(
(cid:12)
−
(cid:12)

+ 9

7
8

4
8

−

−

(cid:12)
(cid:12)
= 3(
(cid:12)
−
(cid:12)
353
=
−

4 5
7 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

The reader is invited to verify that det A can be computed by expanding along any other row or
column.

The fact that the cofactor expansion along any row or column of a matrix A always gives the same
result (the determinant of A) is remarkable, to say the least. The choice of a particular row or column can
simplify the calculation.

2The cofactor expansion is due to Pierre Simon de Laplace (1749–1827), who discovered it in 1772 as part of a study of

linear differential equations. Laplace is primarily remembered for his work in astronomy and applied mathematics.

Example 3.1.4

Compute det A where A = 

3 0 0
5 1 2
2 6 0
6 3 1

−

0
0
1
−
0









3.1. The Cofactor Expansion

147

.


Solution. The ﬁrst choice we must make is which row or column to use in the cofactor expansion.
The expansion involves multiplying entries by cofactors, so the work is minimized when the row
or column contains as many zero entries as possible. Row 1 is a best choice in this matrix (column
4 would do as well), and the expansion is

det A = 3c11(A) + 0c12(A) + 0c13(A) + 0c14(A)

= 3

1 2
6 0
3 1

0
1
−
0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
This is the ﬁrst stage of the calculation, and we have succeeded in expressing the determinant of
(cid:12)
3 matrix. The next stage involves this 3
the 4
matrix. Again, we can use any row or column for the cofactor expansion. The third column is
preferred (with two zeros), so

4 matrix A in terms of the determinant of a 3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

×

×

×

3

det A = 3

0
(cid:18)
= 3[0 + 1(
15
=

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

6 0
3 1

(

1 2
3 1

1)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 2
6 0

(cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)

−
(cid:12)
(cid:12)
5) + 0]
(cid:12)
(cid:12)

−

This completes the calculation.

Computing the determinant of a matrix A can be tedious. For example, if A is a 4

4 matrix, the
cofactor expansion along any row or column involves calculating four cofactors, each of which involves
3 matrix. And if A is 5
4
the determinant of a 3
matrices! There is a clear need for some techniques to cut down the work.3

5, the expansion involves ﬁve determinants of 4

×

×

×

×

The motivation for the method is the observation (see Example 3.1.4) that calculating a determinant
is simpliﬁed a great deal when a row or column consists mostly of zeros. (In fact, when a row or column
consists entirely of zeros, the determinant is zero—simply expand along that row or column.)

Recall next that one method of creating zeros in a matrix is to apply elementary row operations to it.
Hence, a natural question to ask is what effect such a row operation has on the determinant of the matrix.
It turns out that the effect is easy to determine and that elementary column operations can be used in the
same way. These observations lead to a technique for evaluating determinants that greatly reduces the



b
e
h

a
d
g

3If A =

a
d
g
1 and 2 on the right. Then det A = aei + b f g + cdh
bdi, where the positive terms aei, b f g, and cdh are the
a f h

products down and to the right starting at a, b, and c, and the negative terms ceg, a f h, and bdi are the products down and to the
left starting at c, a, and b. Warning: This rule does not apply to n

obtained from A by adjoining columns

we can calculate det A by considering

n matrices where n > 3 or n = 2.

b
e
h 


c
f
i 


a
d
g

b
e
h

c
f
i

ceg

−

−

−





×

148

Determinants and Diagonalization

labour involved. The necessary information is given in Theorem 3.1.2.

Theorem 3.1.2

Let A denotean n

n matrix.

×

1. IfAhasaroworcolumnofzeros, det A = 0.

2. Iftwodistinctrows(orcolumns)of A areinterchanged,thedeterminantoftheresulting

matrixis

det A.

−

3. Ifarow(orcolumn)of A ismultipliedbyaconstant u,thedeterminantoftheresulting

matrixis u( det A).

4. Iftwodistinctrows(orcolumns)of A areidentical, det A = 0.

5. Ifamultipleofonerowof A isaddedtoadifferentrow(orifamultipleofacolumnisadded

toadifferentcolumn),thedeterminantoftheresultingmatrixis det A.

Proof. We prove properties 2, 4, and 5 and leave the rest as exercises.

×

Property 2. If A is n

n, this follows by induction on n. If n = 2, the veriﬁcation is left to the reader.
If n > 2 and two rows are interchanged, let B denote the resulting matrix. Expand det A and det B along a
row other than the two that were interchanged. The entries in this row are the same for both A and B, but
the cofactors in B are the negatives of those in A (by induction) because the corresponding (n
1)
matrices have two rows interchanged. Hence, det B =
det A, as required. A similar argument works if
two columns are interchanged.

(n

1)

−

×

−

−

Property 4. If two rows of A are equal, let B be the matrix obtained by interchanging them. Then
det A by property 2, so det A = det B = 0. Again, the same

B = A, so det B = detA. But det B =
argument works for columns.

−

Property 5. Let B be obtained from A =

ai j

by adding u times row p to row q. Then row q of B is

(aq1 + uap1, aq2 + uap2, . . . , aqn + uapn)

(cid:2)

(cid:3)

The cofactors of these elements in B are the same as in A (they do not involve row q):
cq j(B) = cq j(A) for each j. Hence, expanding B along row q gives

in symbols,

det B = (aq1 + uap1)cq1(A) + (aq2 + uap2)cq2(A) +

+ (aqn + uapn)cqn(A)

= [aq1cq1(A) + aq2cq2(A) +
= det A + u det C

· · ·

+ aqncqn(A)] + u[ap1cq1(A) + ap2cq2(A) +

· · ·

+ apncqn(A)]

· · ·

where C is the matrix obtained from A by replacing row q by row p (and both expansions are along row
q). Because rows p and q of C are equal, det C = 0 by property 4. Hence, det B = det A, as required. As
before, a similar proof holds for columns.

To illustrate Theorem 3.1.2, consider the following determinants.

3.1. The Cofactor Expansion

149

= 0

(because the last row consists of zeros)

3
2
0

3
2
1

−

1 2
5 1
0 0

1
−
8
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
5
(cid:12)
7
1
−

8 1
3 0
1 2

2
9
1
−

2 1 2
4 0 4
1 3 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 5 2
(cid:12)
1 2 9
3 1 1

−

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= 3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= 0

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

5
7
1
−

− (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
8 1
(cid:12)
1 0
1 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

1 3
8 2
2 1

2
3
1
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 9 20
9
1 2
1
3 1

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(because two columns are interchanged)

(because the second row of the matrix on the left is 3 times
the second row of the matrix on the right)

(because two columns are identical)

(because twice the second row of the matrix on the left was
added to the ﬁrst row)

The following four examples illustrate how Theorem 3.1.2 is used to evaluate determinants.

Example 3.1.5

Evaluate det A when A =

1
1
2

1
−
0
1





.

3
1
−
6 


Solution. The matrix does have zero entries, so expansion along (say) the second row would
involve somewhat less work. However, a column operation can be used to get a zero in position
(2, 3)—namely, add column 1 to column 3. Because this does not change the value of the
determinant, we obtain

det A =

1
1
2

1
−
0
1

3
1
−
6

=

1
1
2

−

1 4
0 0
1 8

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
3 matrix along row 2.
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

×

1 4
1 8

−

= 12

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where we expanded the second 3

Example 3.1.6

If det

a b c
p q r
x

y z 






= 6, evaluate det A where A =





a + x b + y c + z
3z
3y
.
r 
q
−


3x
p

−

−

150

Determinants and Diagonalization

Solution. First take common factors out of rows 2 and 3.

det A = 3(

1) det

−



a + x b + y c + z
y
q

x
p

z
r 




Now subtract the second row from the ﬁrst and interchange the last two rows.

det A =

3 det

−

a b c
x
z
y
p q r 






= 3 det

a b c
p q r
y
x

z 






6 = 18

= 3

·

The determinant of a matrix is a sum of products of its entries.

In particular, if these entries are
polynomials in x, then the determinant itself is a polynomial in x. It is often of interest to determine which
values of x make the determinant zero, so it is very useful if the determinant is given in factored form.
Theorem 3.1.2 can help.

Example 3.1.7

Find the values of x for which det A = 0, where A =



1 x
x
x 1 x
x

.
x 1 




Solution. To evaluate det A, ﬁrst subtract x times row 1 from rows 2 and 3.

det A =

1 x x
x 1 x
x 1
x

=

1
0 1
0 x

x

x

x2
x
x2 1

1
x

−
−

x2
x
x2 1

x2
x2

−
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
At this stage we could simply evaluate the determinant (the result is 2x3
(cid:12)
would have to factor this polynomial to ﬁnd the values of x that make it zero. However, this
factorization can be obtained directly by ﬁrst factoring each entry in the determinant and taking a
common factor of (1

3x2 + 1). But then we

x) from each row.

−
−

−
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

=

x2
x2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

det A =

(1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x)(1 + x)
x)

−
x(1

−

x(1

x)
x)(1 + x)

−

(1

−

−
x)2(2x + 1) = 0, that is x = 1 or x =

= (1

x)2

−

1 + x
x

= (1

x)2(2x + 1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x
1 + x

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
2.

−

Hence, det A = 0 means (1

−

3.1. The Cofactor Expansion

151

Example 3.1.8

If a1, a2, and a3 are given show that

1 a1 a2
1
1 a2 a2
2
1 a3 a2
3





det





= (a3

a1)(a3

a2)(a2

a1)

−

−

−

Solution. Begin by subtracting row 1 from rows 2 and 3, and then expand along column 1:

det



1 a1 a2
1
1 a2 a2
2
1 a3 a2
3



a1

= det

1
0 a2
0 a3

a2
1
2 −
3 −
a1) are common factors in rows 1 and 2, respectively, so

a1 a2
a1 a2

a1 a2
a1 a2

= det

a2
1
a2
1

a2
a3

−
−

−
−











(cid:20)

a2
1
a2
1 (cid:21)

2 −
3 −



a1) and (a3

Now (a2

−

det

−





1 a1 a2
1
1 a2 a2
2
1 a3 a2
3

= (a2

= (a2





−

−

a1)(a3

a1)(a3

−

−

a1) det

a1)(a3

(cid:20)

−

1 a2 + a1
1 a3 + a1

(cid:21)

a2)

The matrix in Example 3.1.8 is called a Vandermonde matrix, and the formula for its determinant can be
generalized to the n
If A is an n

n matrix, forming uA means multiplying every row of A by u. Applying property 3 of
Theorem 3.1.2, we can take the common factor u out of each row and so obtain the following useful result.

n case (see Theorem 3.2.7).

×

×

Theorem 3.1.3

IfAisan n

×

n matrix,then det (uA) = un det A foranynumber u.

The next example displays a type of matrix whose determinant is easy to compute.

Example 3.1.9

Evaluate det A if A = 





a 0 0 0
u b 0 0
v w c 0
z d
x

y

.







Solution. Expand along row 1 to get det A = a

. Now expand this along the top row to

b 0 0
w c 0
z d
y

(cid:12)
(cid:12)
(cid:12)
(cid:12)
= abcd, the product of the main diagonal entries.
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

get det A = ab

c 0
z d

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

152

Determinants and Diagonalization

A square matrix is called a lower triangular matrix if all entries above the main diagonal are zero
(as in Example 3.1.9). Similarly, an upper triangular matrix is one for which all entries below the main
diagonal are zero. A triangular matrix is one that is either upper or lower triangular. Theorem 3.1.4
gives an easy rule for calculating the determinant of any triangular matrix. The proof is like the solution
to Example 3.1.9.

Theorem 3.1.4

IfAisasquaretriangularmatrix,thendetAistheproductoftheentriesonthemaindiagonal.

Theorem 3.1.4 is useful in computer calculations because it is a routine matter to carry a matrix to trian-
gular form using row operations.

Block matrices such as those in the next theorem arise frequently in practice, and the theorem gives an

easy method for computing their determinants. This dovetails with Example 2.4.11.

A X
0 B

and

A 0
Y B

(cid:20)

(cid:21)

inblockform,where A and B aresquarematrices.

= det A det B and det

(cid:21)

A 0
Y B

(cid:20)

(cid:21)

= det A det B

Theorem 3.1.5

Considermatrices

Then

(cid:20)

(cid:21)
A X
0 B

det

(cid:20)

A X
0 B

(cid:20)

(cid:21)

Proof. Write T = det

and proceed by induction on k where A is k

k. If k = 1, it is the cofactor

×

expansion along column 1. In general let Si(T ) denote the matrix obtained from T by deleting row i and
column 1. Then the cofactor expansion of det T along the ﬁrst column is

(3.2)

for each

Si(A) Xi
B

0

det T = a11 det (S1(T ))

a21 det (S2(T )) +

−

· · · ±

ak1 det (Sk(T ))

where a11, a21,

· · ·

i = 1, 2,

· · ·

, ak1 are the entries in the ﬁrst column of A. But Si(T ) =

, k, so det (Si(T )) = det (Si(A))

(cid:21)
det B by induction. Hence, Equation 3.2 becomes

(cid:20)

·
a21 det (S2(T )) +

ak1 det (Sk(T ))

det B

}

· · · ±

det T =
=

{
{

a11 det (S1(T ))
det A

det B

}

−

as required. The lower triangular case is similar.

Example 3.1.10

2
1
0
0

3
2
−
1
4

1 3
1 1
0 1
0 1

−

det 







=

−





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2
1
0
0

1
1
−
0
0

3 3
2 1
1 1
4 1

−

2
1

1
1
−

1 1
4 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(
−

3)(

−

3) =

−

9
−

The next result shows that det A is a linear transformation when regarded as a function of a ﬁxed

column of A. The proof is Exercise 3.1.21.

3.1. The Cofactor Expansion

153

Theorem 3.1.6
Givencolumnsc1,

, cj

· · ·

1, cj+1,

−

T (x) = det

c1

, cn in Rn,deﬁne T : Rn

R by

→

cj

1 x cj+1

−

· · ·

cn

forallxin Rn

· · ·

· · ·

Then,forallxandyin Rn andall a in R,

(cid:2)

(cid:3)

T (x+ y) = T (x) + T (y) and T (ax) = aT (x)

Exercises for 3.1

Exercise 3.1.1 Compute the determinants of the follow-
ing matrices.

Exercise 3.1.3 Show that the sign of the position in the
last row and the last column of A is always +1.

b.

d.

f.

h.

j.

l.



n.

a.

c.

e.

g.

i.

k.

m.

(cid:20)

(cid:20)

(cid:20)





















o.



−

2
1
3
2
a2 ab
b2
ab

(cid:21)

(cid:21)

cosθ
sinθ

−

sinθ
cosθ

(cid:21)

1 2 3
4 5 6
7 8 9





1 b c
b c 1
c 1 b 

1 0
0 1
0 2
3 0
2 1
0 1
0 7
5 0

−

3 1
1 3
1 0
1 1

−

5
0
5
2

1
3
1
1

−

−

−

1 5
1 2
3 8
1 2






2
1
2
1

−




5
4
0
1



p.



(cid:20)

(cid:20)

























6
9
8 12

a + 1
a

2 0
1 2
0 3

(cid:21)

a

a

−
3
−
5
0

0 a 0
b c d
0 e 0

0 a b
a 0 c
b c 0







Exercise 3.1.4 Show that det I = 1 for any identity ma-
trix I.

Exercise 3.1.5 Evaluate the determinant of each matrix
by reducing it to upper triangular form.

1

(cid:21)





−

−

1
3
2

1
−
2
0
1

a.

c.











1 2
1 1
1 3





−

1
1
1
3

1 0
1 3
1 2
1 2

−

−

1
2
1

2 3
0 2
0 5
1 1

b.

d.

















3 1
5 3
2 1



−

−


1 1
1 3
1 1
2 5







Exercise 3.1.6 Evaluate by cursory inspection:

1 0
2 2
1 0
4 1

−



3 1
6 0
3 1
−
12 0

3
0
2
1

−



4
3
0
1

−

1
1
1
2

−
0 0 0 a
0 0 b p
k
0 c q
u
t
s
d

a. det

b. det














1
2
2
1







c

a

b
a + 1 b + 1 c + 1
a
1

1 b

1 c

−

−
a

−
b
a + b 2b c + b
2

2

2

c








a b c
p q r
y
x

Exercise 3.1.7 If det





x

−

y
−
3p + a 3q + b 3r + c
2q

2p

2r

−

a. det





z 

z

1 compute:

=

−












Exercise 3.1.2 Show that det A = 0 if A has a row or
column consisting of zeros.









−

154

Determinants and Diagonalization

b. det





2a

2c
2b
−
−
2p + x 2q + y 2r + z
3y

3x

3z

−





Exercise 3.1.8 Show that:

a. det

b. det









r + z
c + z

p + x q + y
a + x b + y
a + p b + q c + r 


= 2 det

a b c
p q r
y
x

z 






2a + p 2b + q 2c + r
2p + x 2q + y 2r + z
2x + a 2y + b 2z + c 


= 9 det

a b c
p q r
y
x

z 






Exercise 3.1.9 In each case either prove the statement
or give an example showing that it is false:

a. det (A + B) = det A + det B.

b. If det A = 0, then A has two equal rows.

c. If A is 2

×

2, then det (AT ) = det A.

d. If R is the reduced row-echelon form of A, then

det A = det R.

e. If A is 2

×

2, then det (7A) = 49 det A.

f. det (AT ) =

det A.

−

g. det (

−

A) =

−

det A.

Exercise 3.1.10 Compute the determinant of each ma-
trix, using Theorem 3.1.5.

1
0
1
0
0

−

1 2 0
1 0 4
1 5 0
0 0 3
0 0 1

−

−

2
1
0
1
1

−

1 2
1 3
0 0
0 0
0 0

0 3 0
1 4 0
2 1 1
1 0 2
3 0 1

−

















a. 






b. 






Exercise 3.1.11 If det A = 2, det B =
3, ﬁnd:

−

1, and det C =

a.

det

c.

det









A X Y
0 B Z
0

0 C 


A X Y
0 B 0
0 Z C 


b.

det

d.

det

A 0
0
X B 0
Y Z C 


A X 0
0 B 0
Y Z C 










Exercise 3.1.12 If A has three columns with only the
top two entries nonzero, show that det A = 0.

Exercise 3.1.13

a. Find det A if A is 3

3 and det (2A) = 6.

×

b. Under what conditions is det (

−

A) = det A?

Exercise 3.1.14 Evaluate by ﬁrst adding all other rows
to the ﬁrst row.

x

1

−
2
2

−
x

1

−
2
3

−

a. det

b. det









2
3 x
−
x

3

2
−
2
−





3
−
1
−
x + 2

1

x

1
−
2
−





h. If det A = det B where A and B are the same size,

Exercise 3.1.15

then A = B.

a. Find b if det

5
2
5

−

1 x
6 y
4 z









−

= ax + by + cz.

b. Find c if det

2 x
1 y
3 z

−

−

1
3
4









= ax + by + cz.

Exercise 3.1.16 Find the real numbers x and y such that
det A = 0 if:

1
x
x

−
−

x
2
x

−
−

x
x
3

−









0 x
y
y 0 x
y 0
x

1
x
x2
x3

x
x2
x3
1




x2
x3
1
x

x
y 0 0
0 x
y 0
y
0 0 x
y 0 0 x

a.

A =





c.

A = 





d.

A = 





b.

A =







x3
1
x
x2







3.1. The Cofactor Expansion

155

Exercise 3.1.22 Show that

0
0
...
0
an

0
0
...
an

−
∗

1

0
a2
...

∗
∗

a1

∗
...

∗
∗

· · ·
· · ·

· · ·
· · ·










det










1)ka1a2

= (

−

an

· · ·

where either n = 2k or n = 2k + 1, and
trary.

-entries are arbi-
∗

Exercise 3.1.23 By expanding along the ﬁrst column,
show that:

1 1 0 0
0 1 1 0
0 0 1 1
...
...
...
...
0 0 0 0
1 0 0 0

· · ·
· · ·
· · ·

· · ·
· · ·
n, n

0 0
0 0
0 0
...
...
1 1
0 1

2.

det












= 1 + (

−

1)n+1












if the matrix is n

×

≥
Exercise 3.1.24 Form matrix B from a matrix A by writ-
ing the columns of A in reverse order. Express det B in
terms of det A.

Exercise 3.1.25 Prove property 3 of Theorem 3.1.2 by
expanding along the row (or column) in question.

Exercise 3.1.26 Show that the line through two distinct
points (x1, y1) and (x2, y2) in the plane has equation

Exercise 3.1.17 Show that

det 

0 1 1 1
1 0 x
x
1 x 0 x
x 0
1 x



=

3x2

−

Exercise 3.1.18 Show that
x2
x
1
c

1 x
a 1
p b
q r

det 



= (1




x3
x2
x
1













ax)(1

bx)(1

cx).

−

−

−

det





x
x1
x2

y
1
y1 1
y2 1

= 0



Exercise 3.1.19
Given the polynomial p(x) = a + bx + cx2 + dx3 + x4, the
0
1
0
c

is called the com-

matrix C = 

0
0
1
d

1
0
0
b

0
0
0
a



−
panion matrix of p(x). Show that det (xI

−

−

−









C) = p(x).

−

Exercise 3.1.27 Let A be an n
nomial p(x) = a0 + a1x +
p(A) = a0I + a1A +

×
· · ·
+ amAm.

· · ·


n matrix. Given a poly-

+ amxm, we write

For example, if p(x) = 2

3x + 5x2, then

−

−

3A + 5A2. The characteristic polynomial of
p(A) = 2I
A is deﬁned to be cA(x) = det [xI
A], and the Cayley-
Hamilton theorem asserts that cA(A) = 0 for any matrix
A.

−

det

Exercise 3.1.20 Show that
a + x b + x
b + x
c + x a + x b + x 

= (a + b + c + 3x)[(ab + ac + bc)

c + x
c + x a + x





a. Verify the theorem for

i. A =

3
1

(cid:20)

2
1

−

(cid:21)

ii. A =

(a2 + b2 + c2)]

−

Exercise 3.1.21 . Prove Theorem 3.1.6. [Hint: Expand
the determinant along column j.]

b. Prove the theorem for A =

a b
c d

(cid:20)

(cid:21)

1
0
8

−

1 1
1 0
2 2









156

Determinants and Diagonalization

3.2 Determinants and Matrix Inverses

In this section, several theorems about determinants are derived. One consequence of these theorems is
that a square matrix A is invertible if and only if det A
= 0. Moreover, determinants are used to give a
1 which, in turn, yields a formula (called Cramer’s rule) for the solution of any system of
formula for A−
linear equations with an invertible coefﬁcient matrix.

We begin with a remarkable theorem (due to Cauchy in 1812) about the determinant of a product of

matrices. The proof is given at the end of this section.

Theorem 3.2.1: Product Theorem

If A and B are n

×

n matrices,then det (AB) = det A det B.

The complexity of matrix multiplication makes the product theorem quite unexpected. Here is an

example where it reveals an important numerical identity.

Example 3.2.1

a b
b a

If A =

and B =

then AB =

(cid:20)

−

(cid:21)

(cid:20)

−

(cid:21)

(cid:20)

−

Hence det A det B = det (AB) gives the identity

c d
d c

bd
ac
(ad + bc) ac

ad + bc
bd

−

−

.
(cid:21)

(a2 + b2)(c2 + d2) = (ac

bd)2 + (ad + bc)2

−

Theorem 3.2.1 extends easily to det (ABC) = det A det B det C. In fact, induction gives

det (A1A2

Ak

· · ·

1Ak) = det A1 det A2

−

det Ak

1 det Ak

−

· · ·

for any square matrices A1, . . . , Ak of the same size. In particular, if each Ai = A, we obtain

det (Ak) = (detA)k, for any k

1

≥

We can now give the invertibility condition.

Theorem 3.2.2

An n

×

n matrix A isinvertibleifandonlyif det A

= 0. Whenthisisthecase, det (A−

1) = 1

det A

Proof. If A is invertible, then AA−

1 = I; so the product theorem gives

1 = det I = det (AA−

1
1) = det A det A−

Hence, det A

= 0 and also det A−

1 = 1

det A.

Conversely, if det A

Theorem 2.4.5). Certainly, A can be carried to its reduced row-echelon form R, so R = Ek
the Ei are elementary matrices (Theorem 2.5.1). Hence the product theorem gives

= 0, we show that A can be carried to I by elementary row operations (and invoke
E2E1A where

· · ·

det R = det Ek

· · ·

det E2 det E1 det A

6
6
6
6
Since det E
so R = I because R is square and reduced row-echelon. This is what we wanted.

= 0 for all elementary matrices E, this shows det R

= 0. In particular, R has no row of zeros,

3.2. Determinants and Matrix Inverses

157

Example 3.2.2

For which values of c does A =

c
−
1
4 

Solution. Compute det A by ﬁrst adding c times column 1 to column 3 and then expanding along
row 1.

0
1
1
3
−
0 2c

have an inverse?

−





det A = det




−

0
1
1
3
−
0 2c

c
−
1
4 


−

= det





0
1
1
3
−
0 2c

= 2(c + 2)(c

3)

−

0

1

c
−
4 
−

2 and c
=

−

Hence, det A = 0 if c =

2 or c = 3, and A has an inverse if c

= 3.

Example 3.2.3

If a product A1A2

· · ·

Ak of square matrices is invertible, show that each Ai is invertible.

Solution. We have det A1 det A2
det (A1A2

· · ·
= 0 by Theorem 3.2.2 because A1A2

det Ak = det (A1A2

Ak)

Ak) by the product theorem, and

· · ·
Ak is invertible. Hence

· · ·

det A1 det A2

· · ·

· · ·
det Ak

= 0

so det Ai

= 0 for each i. This shows that each Ai is invertible, again by Theorem 3.2.2.

Theorem 3.2.3
If A isanysquarematrix, det AT = det A.

Proof. Consider ﬁrst the case of an elementary matrix E. If E is of type I or II, then ET = E; so certainly
det ET = det E. If E is of type III, then ET is also of type III; so det ET = 1 = det E by Theorem 3.1.2.
Hence, det ET = det E for every elementary matrix E.

Now let A be any square matrix. If A is not invertible, then neither is AT ; so det AT = 0 = det A by
E2E1, where the Ei are elementary

Theorem 3.2.2. On the other hand, if A is invertible, then A = Ek
matrices (Theorem 2.5.2). Hence, AT = ET

ET
k so the product theorem gives

1 ET

· · ·

2 · · ·

6
6
6
6
6
6
6
158

Determinants and Diagonalization

det AT = det ET

1 det ET

2 · · ·

det ET

k = det E1 det E2
= det Ek · · ·
= det A

det Ek
det E2 det E1

· · ·

This completes the proof.

Example 3.2.4

If det A = 2 and det B = 5, calculate det (A3B−

1AT B2).

Solution. We use several of the facts just derived.

det (A3B−

1) det (AT ) det (B2)

1AT B2) = det (A3) det (B−
= ( det A)3
= 23
1
2
5 ·
= 80

·

·

1

det B det A( det B)2
52

Example 3.2.5

A square matrix is called orthogonal if A−
orthogonal?

1 = AT . What are the possible values of det A if A is

Solution. If A is orthogonal, we have I = AAT . Take determinants to obtain

1 = det I = det (AAT ) = det A det AT = ( det A)2

Since det A is a number, this means det A =

1.

±

Hence Theorems 2.6.4 and 2.6.5 imply that rotation about the origin and reﬂection about a line through
1 respectively. In fact they are the

the origin in R2 have orthogonal matrices with determinants 1 and
only such transformations of R2. We have more to say about this in Section 8.2.

−

Adjugates

In Section 2.4 we deﬁned the adjugate of a 2

b
d
−
a
c
(cid:20)
−
1 = 1
det A adj A. We are
we veriﬁed that A( adj A) = ( det A)I = ( adj A)A and hence that, if det A
now able to deﬁne the adjugate of an arbitrary square matrix and to show that this formula for the inverse
remains valid (when the inverse exists).

to be adj (A) =

2 matrix A =

a b
c d

= 0, A−

. Then

×

(cid:21)

(cid:20)

(cid:21)

Recall that the (i, j)-cofactor ci j(A) of a square matrix A is a number deﬁned for each position (i, j)
whose

in the matrix. If A is a square matrix, the cofactor matrix of A is deﬁned to be the matrix
(i, j)-entry is the (i, j)-cofactor of A.

ci j(A)

(cid:2)

(cid:3)

6
3.2. Determinants and Matrix Inverses

159

Deﬁnition 3.3 Adjugate of a Matrix

Theadjugate4of A,denoted adj (A),isthetransposeofthiscofactormatrix;insymbols,

adj (A) =

ci j(A)

(cid:2)

T

(cid:3)

This agrees with the earlier deﬁnition for a 2

2 matrix A as the reader can verify.

×

Example 3.2.6

Compute the adjugate of A =



Solution. We ﬁrst ﬁnd the cofactor matrix.

1
0
2
−

3
1
6
−

and calculate A( adj A) and ( adj A)A.

2
−
5
7 




c11(A) c12(A) c13(A)
c21(A) c22(A) c23(A)
c31(A) c32(A) c33(A)





















(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
37
(cid:12)
(cid:12)
9
−
17

=





=

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

0 5
2 7

−

2
−
7

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2
−

1
0

2
−
5

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0
2
−

1
2
−

1
6
−

(cid:12)
(cid:12)
(cid:12)
3
(cid:12)
6
−

1 3
0 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)















(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 5
6 7

(cid:12)
(cid:12)
(cid:12)
2
(cid:12)
−
7

−

3
6
−

3
1

2
−
5

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:12)
(cid:12)
10 2
(cid:12)
(cid:12)
3 0
5 1 


−

Then the adjugate of A is the transpose of this cofactor matrix.



adj A =

37
9
−
17



The computation of A( adj A) gives



T

−

10 2
3 0
5 1 


−

37
10
2

−

9
−
3
0

17
5
−
1 


=





A( adj A) =



1
0
2
−

3
1
6
−

37
10
2

−

9
−
3
0



2
−
5
7 




17
5
1 
−


=







= 3I

3 0 0
0 3 0
0 0 3 

2 case would

and the reader can verify that also ( adj A)A = 3I. Hence, analogy with the 2
indicate that det A = 3; this is, in fact, the case.

×

The relationship A( adj A) = ( det A)I holds for any square matrix A. To see why this is so, consider

4This is also called the classical adjoint of A, but the term “adjoint” has another meaning.

160

Determinants and Diagonalization

the general 3

×

3 case. Writing ci j(A) = ci j for short, we have

adj A =



c11
c21
c31

c12 c13
c22 c23
c32 c33

T

=





c11
c12
c13

c21 c31
c22 c32
c23 c33



If A =

ai j

(cid:2)

(cid:3)

in the usual notation, we are to verify that A( adj A) = ( det A)I. That is,









A( adj A) =

a11 a12 a13
a21 a22 a23
a31 a32 a33



c11
c12
c13

c21 c31
c22 c32
c23 c33





=





det A
0
0

0
det A
0

0
0
det A 












Consider the (1, 1)-entry in the product. It is given by a11c11 + a12c12 + a13c13, and this is just the cofactor
expansion of det A along the ﬁrst row of A. Similarly, the (2, 2)-entry and the (3, 3)-entry are the cofactor
expansions of det A along rows 2 and 3, respectively.

So it remains to be seen why the off-diagonal elements in the matrix product A( adj A) are all zero.
It is given by a11c21 + a12c22 + a13c23. This looks like the
Consider the (1, 2)-entry of the product.
cofactor expansion of the determinant of some matrix. To see which, observe that c21, c22, and c23 are
all computed by deleting row 2 of A (and one of the columns), so they remain the same if row 2 of A is
changed. In particular, if row 2 of A is replaced by row 1, we obtain

a11c21 + a12c22 + a13c23 = det

a11 a12 a13
a11 a12 a13
a31 a32 a33



= 0



where the expansion is along row 2 and where the determinant is zero because two rows are identical. A
similar argument shows that the other off-diagonal entries are zero.





This argument works in general and yields the ﬁrst part of Theorem 3.2.4. The second assertion follows
1
det A .

from the ﬁrst by multiplying through by the scalar

Theorem 3.2.4: Adjugate Formula

IfAisanysquarematrix,then

Inparticular,ifdetA

= 0,theinverseofAisgivenby

A( adj A) = ( det A)I = ( adj A)A

A−

1 = 1

det A adj A

It is important to note that this theorem is not an efﬁcient way to ﬁnd the inverse of the matrix A. For
10, the calculation of adj A would require computing 102 = 100 determinants of
example, if A were 10
1 with about the same
9
effort as ﬁnding det A. Clearly, Theorem 3.2.4 is not a practical result: its virtue is that it gives a formula
for A−

9 matrices! On the other hand, the matrix inversion algorithm would ﬁnd A−

1 that is useful for theoretical purposes.

×

×

6
3.2. Determinants and Matrix Inverses

161

Example 3.2.7

Find the (2, 3)-entry of A−

1 if A =

Solution. First compute

2
5
3

1
7
−
0





3
1
.
6 


−

det A =

2
5
3

1
7
−
0

3
1
6
−

=

2
5
3

7
1
7 11
−
0
0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
det A adj A = 1
ci j(A)
(cid:12)
180
180 c32(A) = 1
; that is, it equals 1
(cid:2)
(cid:3)
180

1 = 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
T , the (2, 3)-entry of A−
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 3

= 180

1
7
7 11

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1 is the (3, 2)-entry of the matrix

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 13
180 .

2 3
5 1

(cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)

−

(cid:18)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Since A−
1
180

ci j(A)

(cid:2)

(cid:3)

Example 3.2.8

If A is n

n, n

×

≥

2, show that det ( adj A) = ( det A)n
1.
−

Solution. Write d = det A; we must show that det ( adj A) = dn
1. We have A( adj A) = dI by
−
Theorem 3.2.4, so taking determinants gives d det ( adj A) = dn. Hence we are done if d
= 0.
Assume d = 0; we must show that det ( adj A) = 0, that is, adj A is not invertible. If A
= 0, this
follows from A( adj A) = dI = 0; if A = 0, it follows because then adj A = 0.

Cramer’s Rule

Theorem 3.2.4 has a nice application to linear equations. Suppose

Ax = b

is a system of n equations in n variables x1, x2, . . . , xn. Here A is the n
are the columns

n coefﬁcient matrix, and x and b

×

and b = 




1b. When we use the adjugate formula, this becomes

x = 








If det A

of variables and constants, respectively.
x = A−







= 0, we left multiply by A−

x1
x2
...
xn

b1
b2
...
bn



1 to obtain the solution

= 1

det A( adj A)b

x1
x2
...
xn















6
6
6
162

Determinants and Diagonalization

= 1

det A



c11(A) c21(A)
c12(A) c22(A)

...

...

c1n(A) c2n(A)






· · ·
· · ·

· · ·

cn1(A)
cn2(A)
...
cnn(A)

b1
b2
...
bn






















Hence, the variables x1, x2, . . . , xn are given by

x1 = 1
x2 = 1

det A [b1c11(A) + b2c21(A) +
det A [b1c12(A) + b2c22(A) +

...

...

xn = 1

det A [b1c1n(A) + b2c2n(A) +

+ bncn1(A)]
+ bncn2(A)]

+ bncnn(A)]

· · ·

· · ·

· · ·

Now the quantity b1c11(A) + b2c21(A) +

+ bncn1(A) occurring in the formula for x1 looks like the
cofactor expansion of the determinant of a matrix. The cofactors involved are c11(A), c21(A), . . . , cn1(A),
corresponding to the ﬁrst column of A. If A1 is obtained from A by replacing the ﬁrst column of A by b,
then ci1(A1) = ci1(A) for each i because column 1 is deleted when computing them. Hence, expanding
det (A1) by the ﬁrst column gives

· · ·

det A1 = b1c11(A1) + b2c21(A1) +

= b1c11(A) + b2c21(A) +
= ( det A)x1

· · ·

+ bncn1(A1)

· · ·
+ bncn1(A)

Hence, x1 = det A1

det A and similar results hold for the other variables.

Theorem 3.2.5: Cramer’s Rule5
If A isaninvertible n

×

n matrix,thesolutiontothesystem

Ax= b

of n equationsinthevariables x1, x2, . . . , xn isgivenby

x1 = det A1

det A , x2 = det A2
det A ,

, xn = det An
det A

· · ·

where,foreach k, Ak isthematrixobtainedfrom A byreplacingcolumn k byb.

Example 3.2.9

Find x1, given the following system of equations.

5x1 + x2
9x1 + x2
x1

x3 = 4
x3 = 1
x2 + 5x3 = 2

−
−

−

5Gabriel Cramer (1704–1752) was a Swiss mathematician who wrote an introductory work on algebraic curves. He popu-

larized the rule that bears his name, but the idea was known earlier.

3.2. Determinants and Matrix Inverses

163

Solution. Compute the determinants of the coefﬁcient matrix A and the matrix A1 obtained from it
by replacing the ﬁrst column by the column of constants.

det A = det

det A1 = det

Hence, x1 = det A1

det A =

3
4 by Cramer’s rule.

−

5
9
1

4
1
2









1
1
1
−

1
1
1
−

1
−
1
5 
−

1
−
1
−
5 


=

16

−

= 12

Cramer’s rule is not an efﬁcient way to solve linear systems or invert matrices. True, it enabled us to
calculate x1 here without computing x2 or x3. Although this might seem an advantage, the truth of the
matter is that, for large systems of equations, the number of computations needed to ﬁnd all the variables
by the gaussian algorithm is comparable to the number required to ﬁnd one of the determinants involved in
Cramer’s rule. Furthermore, the algorithm works when the matrix of the system is not invertible and even
when the coefﬁcient matrix is not square. Like the adjugate formula, then, Cramer’s rule is not a practical
numerical technique; its virtue is theoretical.

Polynomial Interpolation

Example 3.2.10

Age

(10, 5)

(15, 6)

(5, 3)

6

4

2

Diameter

A forester wants to estimate the age (in years) of a tree by measuring
the diameter of the trunk (in cm). She obtains the following data:

Trunk Diameter
Age

Tree 1 Tree 2 Tree 3
10
5

15
6

5
3

Estimate the age of a tree with a trunk diameter of 12 cm.

0

10 12 15

5
Solution.
The forester decides to “ﬁt” a quadratic polynomial

to the data, that is choose the coefﬁcients r0, r1, and r2 so that p(5) = 3, p(10) = 5, and p(15) = 6,
and then use p(12) as the estimate. These conditions give three linear equations:

p(x) = r0 + r1x + r2x2

r0 + 5r1 + 25r2 = 3
r0 + 10r1 + 100r2 = 5
r0 + 15r1 + 225r2 = 6

The (unique) solution is r0 = 0, r1 = 7

10 , and r2 =

1
50 , so

p(x) = 7

10 x

−

−
50x2 = 1

1

50 x(35

x)

−

164

Determinants and Diagonalization

Hence the estimate is p(12) = 5.52.

As in Example 3.2.10, it often happens that two variables x and y are related but the actual functional
form y = f (x) of the relationship is unknown. Suppose that for certain values x1, x2, . . . , xn of x the
corresponding values y1, y2, . . . , yn are known (say from experimental measurements). One way to
estimate the value of y corresponding to some other value a of x is to ﬁnd a polynomial6
p(x) = r0 + r1x + r2x2 +

+ rn

1xn
1
−
−

· · ·

that “ﬁts” the data, that is p(xi) = yi holds for each i = 1, 2, . . . , n. Then the estimate for y is p(a). As we
will see, such a polynomial always exists if the xi are distinct.

The conditions that p(xi) = yi are

r0 + r1x1 + r2x2
r0 + r1x2 + r2x2

...

...

1 +
2 +
...
n +

· · ·

· · ·

· · ·

+ rn
+ rn

1xn
1
1 = y1
−
−
1xn
1
2 = y2
−
−
...
...
1xn
1
n = yn
−
−

r0 + r1xn + r2x2

+ rn

1 x1 x2
1
1 x2 x2
2
...
...
...
1 xn x2
n

· · ·
· · ·

· · ·

xn
1
−
1
xn
1
−
2
...
xn
1
−
n

r0
r1
...
rn

1
−















y1
y2
...
yn













= 










−

In matrix form, this is





(3.3)

It can be shown (see Theorem 3.2.7) that the determinant of the coefﬁcient matrix equals the product of
x j) with i > j and so is nonzero (because the xi are distinct). Hence the equations have a
all terms (xi
−
unique solution r0, r1, . . . , rn

1. This proves

Theorem 3.2.6

Let n datapairs (x1, y1), (x2, y2), . . . , (xn, yn) begiven,andassumethatthe xi aredistinct. Then
thereexistsauniquepolynomial

suchthat p(xi) = yi foreach i = 1, 2, . . . , n.

p(x) = r0 + r1x + r2x2 +

+ rn

1xn
1
−
−

· · ·

The polynomial in Theorem 3.2.6 is called the interpolating polynomial for the data.

We conclude by evaluating the determinant of the coefﬁcient matrix in Equation 3.3. If a1, a2, . . . , an

are numbers, the determinant

det

1 a1 a2
1
1 a2 a2
2
1 a3 a2
3
...
...
...
1 an a2
n

an
1
−
1
an
1
−
2
an
1
−
3
...
an
1
−
n

· · ·
· · ·
· · ·

· · ·



















6A polynomial is an expression of the form a0 + a1x + a2x2 +
= 0, the integer n is called the degree of the polynomial, and an is called the leading coefﬁcient. See Appendix D.

+ anxn where the ai are numbers and x is a variable. If

· · ·

an

6
3.2. Determinants and Matrix Inverses

165

is called a Vandermonde determinant.7 There is a simple formula for this determinant. If n = 2, it equals
(a2

a1) by Example 3.1.8. The general result is the product

a1); if n = 3, it is (a3

a2)(a3

a1)(a2

−

−

−

−
∏
(ai
n
j<i
1
≤
≤

a j)

−

of all factors (ai

a j) where 1

−

(a4

−

j < i

≤
a3)(a4

−

≤
a2)(a4

n. For example, if n = 4, it is

a1)(a3

a2)(a3

a1)(a2

a1)

−

−

−

−

Theorem 3.2.7
Let a1, a2, . . . , an benumberswhere n
givenby

≥
1 a1 a2
1
1 a2 a2
2
1 a3 a2
3
...
...
...
1 an a2
n

2. ThenthecorrespondingVandermondedeterminantis

· · ·
· · ·
· · ·

· · ·

an
1
−
1
an
1
−
2
an
1
−
3
...
an
1
−
n










= ∏
(ai
n
j<i
1
≤
≤

a j)

−

det










Proof. We may assume that the ai are distinct; otherwise both sides are zero. We proceed by induction on
n
1. The trick is to replace an by a variable x, and
consider the determinant

2; we have it for n = 2, 3. So assume it holds for n

≥

p(x) = det

−
a2
a1
1
1
a2
a2
1
2
...
...
...
1 a2
1 an
n
1
−
−
x2
x
1










· · ·
· · ·

· · ·
· · ·

an
1
−
1
an
1
−
2
...
an
1
−
n
1
xn
−
1
−










Then p(x) is a polynomial of degree at most n
i = 1, 2, . . . , n
−
p(a1) = 0, so we have p(x) = (x
−
obtain p1(a2) = 0, and so p1(x) = (x
this process continues to obtain

1 (expand along the last row), and p(ai) = 0 for each
1 because in each case there are two identical rows in the determinant. In particular,
= a1, we
a2)p2(x). As the ai are distinct,

a1)p1(x) by the factor theorem (see Appendix D). Since a2

a2)p2(x). Thus p(x) = (x

a1)(x

−

−

−

−

−
where d is the coefﬁcient of xn
1 in p(x). By the cofactor expansion of p(x) along the last row we get
−

−

· · ·

−

−

p(x) = (x

a1)(x

a2)

(x

an

1)d

(3.4)

a2
1
a2
2
...
1 a2
n
1
−
−
7Alexandre Théophile Vandermonde (1735–1796) was a French mathematician who made contributions to the theory of

1)n+n det 





a1
1
a2
1
...
...
1 an

an
2
−
1
an
2
−
2
...
an
2
−
n
1
−

· · ·
· · ·

d = (






· · ·

−



equations.

6
166

Determinants and Diagonalization

Because (
j < i
1

≤

−
≤

−

1)n+n = 1 the induction hypothesis shows that d is the product of all factors (ai
1. The result now follows from Equation 3.4 by substituting an for x in p(x).
n

Proof of Theorem 3.2.1. If A and B are n

n matrices we must show that

×
det (AB) = det A det B

a j) where

−

(3.5)

Recall that if E is an elementary matrix obtained by doing one row operation to In, then doing that operation
to a matrix C (Lemma 2.5.1) results in EC. By looking at the three types of elementary matrices separately,
Theorem 3.1.2 shows that

Thus if E1, E2, . . . , Ek are all elementary matrices, it follows by induction that

det (EC) = det E det C for any matrix C

det (Ek · · ·

E2E1C) = det Ek · · ·

det E2 det E1 det C for any matrix C

Lemma. If A has no inverse, then det A = 0.

(3.6)

(3.7)

Proof. Let A

R where R is reduced row-echelon, say En

→

Part (4) of Theorem 2.4.5, and hence det R = 0. But then Equation 3.7 gives det A = 0 because det E
for any elementary matrix E. This proves the Lemma.

· · ·

E2E1A = R. Then R has a row of zeros by
= 0

Now we can prove Equation 3.5 by considering two cases.

Case 1. A has no inverse. Then AB also has no inverse (otherwise A[B(AB)−
Corollary 2.4.1 to Theorem 2.4.5). Hence the above Lemma (twice) gives

1] = I so A is invertible by

det (AB) = 0 = 0 det B = det A det B

proving Equation 3.5 in this case.
Case 2. A has an inverse. Then A is a product of elementary matrices by Theorem 2.5.2, say A =
E1E2

Ek. Then Equation 3.7 with C = I gives

· · ·

det A = det (E1E2

Ek) = det E1 det E2

det Ek

· · ·

· · ·

But then Equation 3.7 with C = B gives

det (AB) = det [(E1E2

· · ·
and Equation 3.5 holds in this case too.

Ek)B] = det E1 det E2

· · ·

det Ek det B = det A det B

Exercises for 3.2

Exercise 3.2.1 Find the adjugate of each of the follow-
ing matrices.

5 1 3
1 2 3
1 4 8





a.



−



b.

1
3
0





1 2
1 0
1 1

−

−





1
1
−
0

−

1
0
1

0
1
1

−





c.





−

1
2
2

2
1
2

−

2
2
1

−





d.

1
3 



Exercise 3.2.2 Use determinants to ﬁnd which real val-
ues of c make each of the following matrices invertible.

6
1
3
2

0 3
4 c
5 8

−

c 1 0
0 2 c
1 c 5

−

2
1
c

−

−
1
0
2








1
c
1





a.

c.

e.













b.

d.

f.













0
1
c

−

−
4 c 3
c 2 c
5 c 4

c
2
c

c
−
1
c







1 c
c 1
0 1


1
−
1
c





Exercise 3.2.3 Let A, B, and C denote n
and assume that det A =
Evaluate:

n matrices
1, det B = 2, and det C = 3.

×

−

a.

det (A3BCT B−

1)

b.

det (B2C−

1AB−

1CT )

Exercise 3.2.4 Let A and B be invertible n
Evaluate:

×

n matrices.

a.

det (B−

1AB)

b.

det (A−

1B−

1AB)

a. det (2B−

1) where B =

b. det (2C−

1) where C =







Exercise 3.2.7 If det


a b
c d

(cid:21)

(cid:20)

4u 2a
4v 2b
4w 2c

p
q
r



−
−
−
a + u
−
b + v
c + w 3w 
−
−



3u
3v

2p
2q
2r

2 calculate:

=

−

2
c + 1
d
2

2
0
−
1 2a
−
2b
2

−
2b
1

4d
2

0
2
−
a + 1 2 2(c
−

a. det

b. det









c. det (3A−

1) where A =

(cid:20)







1)


a + c
3c
3d b + d

(cid:21)

3.2. Determinants and Matrix Inverses

167

Exercise 3.2.8 Solve each of the following by Cramer’s
rule:

a.

c.

2x + y = 1
3x + 7y =
2

5x + y
2x
y
3x

−

−
z =
7
−
2z = 6
7

−
−
+ 2z =

−

b.

d.

3x + 4y = 9
2x
1

y =

−

−
y + 3z = 1
z = 0
1

4x
−
6x + 2y
3x + 3y + 2z =

−

−

Exercise 3.2.9 Use Theorem 3.2.4 to ﬁnd the (2, 3)-
entry of A−

1 if:

a.

A =

3 2 1
1 1 2
1 2 1









−

b.

A =





1 2
3 1
0 4

1
−
1
7





Exercise 3.2.10 Explain what can be said about det A
if:

a.

c.

A2 = A
A3 = A

b.

d.

f.

A2 = I
PA = P and P is
invertible

A =
n

−

AT and A is n

×

Exercise 3.2.11 Let A be n
n. Show that uA = (uI)A,
and use this with Theorem 3.2.1 to deduce the result in
Theorem 3.1.3: det (uA) = un det A.

×

Exercise 3.2.12 If A and B are n

n matrices, if AB =
BA, and if n is odd, show that either A or B has no in-

×

−
verse.

n matrices A and B.

Exercise 3.2.13 Show that det AB = det BA holds for
any two n
Exercise 3.2.14 If Ak = 0 for some k
is not invertible.

1, show that A

×

≥

Exercise 3.2.15 If A−
trix of A in terms of A.

1 = AT , describe the cofactor ma-

Exercise 3.2.16 Show that no 3
3 matrix A exists such
that A2 + I = 0. Find a 2
2 matrix A with this property.
Exercise 3.2.17 Show that det (A + BT ) = det (AT + B)
for any n

n matrices A and B.

×

×

Exercise 3.2.18 Let A and B be invertible n
n matrices.
Show that det A = det B if and only if A = U B where U
is a matrix with det U = 1.

×

×

Exercise 3.2.5 If A is 3
1)T ) =
det (A3(B−

3 and det (2A−
4, ﬁnd det A and det B.

×

1) =

4 and

−

e.

A2 = uA and A is n

n

×

−

Exercise 3.2.6 Let A =

det A = 3. Compute:

c
r

a b
p q
u v w 






and assume that

g.

A2 + I = 0 and A is
n

n

×

g. If A is invertible, then adj A is invertible.

a. Show that A =

168

Determinants and Diagonalization

Exercise 3.2.19 For each of the matrices in Exercise 2,
ﬁnd the inverse for those values of c for which it exists.

Exercise 3.2.20 In each case either prove the statement
or give an example showing that it is false:

a. If adj A exists, then A is invertible.

b. If A is invertible and adj A = A−

1, then det A = 1.

c. det (AB) = det (BT A).

d. If det A

= 0 and AB = AC, then B = C.

e. If AT =

−

A, then det A =

1.

−

f. If adj A = 0, then A = 0.

h. If A has a row of zeros, so also does adj A.

i. det (AT A) > 0 for all square matrices A.

j. det (I + A) = 1 + det A.

k. If AB is invertible, then A and B are invertible.

l. If det A = 1, then adj A = A.

m. If A is invertible and det A = d, then adj A =

dA−

1.

Exercise 3.2.21 If A is 2
2 and det A = 0, show that
×
one column of A is a scalar multiple of the other. [Hint:
Deﬁnition 2.5 and Part (2) of Theorem 2.4.5.]

Exercise 3.2.22 Find a polynomial p(x) of degree 2
such that:

a. p(0) = 2, p(1) = 3, p(3) = 8

b. p(0) = 5, p(1) = 3, p(2) = 5

Exercise 3.2.23 Find a polynomial p(x) of degree 3
such that:

a. p(0) = p(1) = 1, p(

−

1) = 4, p(2) =

5

b. p(0) = p(1) = 1, p(

−

1) = 2, p(

−

−
2) =

3

−

Exercise 3.2.24 Given the following data pairs, ﬁnd the
interpolating polynomial of degree at most 3 and estimate
the value of y corresponding to x = 1.5.

a. (0, 1), (1, 2), (2, 5), (3, 10)

b. (0, 1), (1, 1.49), (2,

c. (0, 2), (1, 2.03), (2,

0.42), (3,

11.33)

−

0.40), (

−

1, 0.89)

−

−

Exercise 3.2.25

a b
1 c
c 1
det A = 1 + a2 + b2 + c2. Hence, ﬁnd A−
and c.

If A =

1
a
b

−
−

−





show that



1 for any a, b,



Exercise 3.2.26

a p q
0 b r
0 0 c





has an inverse if and

only if abc

1 in that case.

= 0, and ﬁnd A−



b. Show that if an upper triangular matrix is invert-

ible, the inverse is also upper triangular.

Exercise 3.2.27 Let A be a matrix each of whose entries
are integers. Show that each of the following conditions
implies the other.

1. A is invertible and A−

1 has integer entries.

2. det A = 1 or

1.

−

Exercise 3.2.28 If A−

1 =

3 0
0 2
3 1



Exercise 3.2.29
det (A−

1 + 4 adj A).


If A is 3

×

Exercise 3.2.30 Show that det

1
3
1

−

ﬁnd adj A.



3 and det A = 2, ﬁnd



= det A det B

0 A
B X

I
0
I 0

.]

when A and B are 2

×

[Hint: Block multiply by

2. What if A and B are 3

(cid:20)

(cid:21)

3?

×

(cid:21)
Exercise 3.2.31 Let A be n
2, and assume one
column of A consists of zeros. Find the possible values
of rank ( adj A).

n, n

×

≥

(cid:20)

Exercise 3.2.32 If A is 3
det (

A2( adj A)−

1).

−

3 and invertible, compute

×

6
6
3.3. Diagonalization and Eigenvalues

169

1 adj A for all

b. adj (A−

1) = ( adj A)−

1

c. adj (AT ) = ( adj A)T

d. adj (AB) = ( adj B)( adj A) [Hint:
AB adj (AB) = AB adj B adj A.]

Show that

Exercise 3.2.33 Show that adj (uA) = un
−
n

n matrices A.

×

Exercise 3.2.34 Let A and B denote invertible n
trices. Show that:

n ma-

×

a. adj ( adj A) = ( det A)n
−

2A (here n

Example 3.2.8.]

2) [Hint: See

≥

3.3 Diagonalization and Eigenvalues

The world is ﬁlled with examples of systems that evolve in time—the weather in a region, the economy
of a nation, the diversity of an ecosystem, etc. Describing such systems is difﬁcult in general and various
methods have been developed in special cases. In this section we describe one such method, called diag-
onalization, which is one of the most important techniques in linear algebra. A very fertile example of
this procedure is in modelling the growth of the population of an animal species. This has attracted more
attention in recent years with the ever increasing awareness that many species are endangered. To motivate
the technique, we begin by setting up a simple model of a bird population in which we make assumptions
about survival and reproduction rates.

Example 3.3.1

Consider the evolution of the population of a species of birds. Because the number of males and
females are nearly equal, we count only females. We assume that each female remains a juvenile
for one year and then becomes an adult, and that only adults have offspring. We make three
assumptions about reproduction and survival rates:

1. The number of juvenile females hatched in any year is twice the number of adult females

alive the year before (we say the reproduction rate is 2).

2. Half of the adult females in any year survive to the next year (the adult survival rate is 1

2).

3. One quarter of the juvenile females in any year survive into adulthood (the juvenile survival

rate is 1

4).

If there were 100 adult females and 40 juvenile females alive initially, compute the population of
females k years later.

Solution. Let ak and jk denote, respectively, the number of adult and juvenile females after k years,
so that the total female population is the sum ak + jk. Assumption 1 shows that jk+1 = 2ak, while
assumptions 2 and 3 show that ak+1 = 1
4 jk. Hence the numbers ak and jk in successive years
are related by the following equations:

2 ak + 1

2ak + 1

4 jk

ak+1 = 1
jk+1 = 2ak

If we write vk =

ak
jk

(cid:20)

(cid:21)

and A =

1
1
2
4
2 0

(cid:21)

(cid:20)

these equations take the matrix form

vk+1 = Avk, for each k = 0, 1, 2, . . .

170

Determinants and Diagonalization

Taking k = 0 gives v1 = Av0, then taking k = 1 gives v2 = Av1 = A2v0, and taking k = 2 gives
v3 = Av2 = A3v0. Continuing in this way, we get

vk = Akv0, for each k = 0, 1, 2, . . .

a0
j0

Since v0 =

for all k
been developed.

≥

=

is known, ﬁnding the population proﬁle vk amounts to computing Ak
(cid:20)
0. We will complete this calculation in Example 3.3.12 after some new techniques have

(cid:20)

(cid:21)

(cid:21)

100
40

Let A be a ﬁxed n

n matrix. A sequence v0, v1, v2, . . . of column vectors in Rn is called a linear
dynamical system8 if v0 is known and the other vk are determined (as in Example 3.3.1) by the conditions

×

These conditions are called a matrix recurrence for the vectors vk. As in Example 3.3.1, they imply that

vk+1 = Avk for each k = 0, 1, 2, . . .

vk = Akv0 for all k

0

≥
0.

≥

so ﬁnding the columns vk amounts to calculating Ak for k

Direct computation of the powers Ak of a square matrix A can be time-consuming, so we adopt an
indirect method that is commonly used. The idea is to ﬁrst diagonalize the matrix A, that is, to ﬁnd an
invertible matrix P such that

1AP = D is a diagonal matrix
(3.8)
This works because the powers Dk of the diagonal matrix D are easy to compute, and Equation 3.8 enables
us to compute powers Ak of the matrix A in terms of powers Dk of D. Indeed, we can solve Equation 3.8
for A to get A = PDP−

1. Squaring this gives

P−

A2 = (PDP−

1)(PDP−

1
1) = PD2P−

Using this we can compute A3 as follows:

A3 = AA2 = (PDP−

1)(PD2P−

1
1) = PD3P−

Continuing in this way we obtain Theorem 3.3.1 (even if D is not diagonal).

Theorem 3.3.1

If A = PDP−

1 then Ak = PDkP−

1 foreach k = 1, 2, . . ..

Hence computing Ak comes down to ﬁnding an invertible matrix P as in equation Equation 3.8. To do

this it is necessary to ﬁrst compute certain numbers (called eigenvalues) associated with the matrix A.

8More precisely, this is a linear discrete dynamical system. Many models regard vt as a continuous function of the time t,

and replace our condition between bk+1 and Avk with a differential relationship viewed as functions of time.

3.3. Diagonalization and Eigenvalues

171

Eigenvalues and Eigenvectors

Deﬁnition 3.4 Eigenvalues and Eigenvectors of a Matrix

If A isan n

×

n matrix,anumberλ iscalledaneigenvalueof A if

Ax= λxforsomecolumnx

= 0in Rn

Inthiscase,xiscalledaneigenvectorof A correspondingtotheeigenvalueλ,oraλ-eigenvector
forshort.

Example 3.3.2

If A =

5
1
(cid:20)
−
eigenvector x.

3
1

and x =

(cid:21)

5
1

(cid:21)

(cid:20)

then Ax = 4x so λ = 4 is an eigenvalue of A with corresponding

The matrix A in Example 3.3.2 has another eigenvalue in addition to λ = 4. To ﬁnd it, we develop a

general procedure for any n

n matrix A.

×
By deﬁnition a number λ is an eigenvalue of the n

x

= 0. This is equivalent to asking that the homogeneous system

×

n matrix A if and only if Ax = λx for some column

= 0. By Theorem 2.4.5 this happens if and only if the matrix
A is not invertible and this, in turn, holds if and only if the determinant of the coefﬁcient matrix is

of linear equations has a nontrivial solution x
λI
−
zero:

(λI

−

A)x = 0

This last condition prompts the following deﬁnition:

det (λI

A) = 0

−

Deﬁnition 3.5 Characteristic Polynomial of a Matrix

If A isan n

×

n matrix,thecharacteristicpolynomial cA(x) of A isdeﬁnedby

cA(x) = det (xI

A)

−

Note that cA(x) is indeed a polynomial in the variable x, and it has degree n when A is an n
n matrix (this
is illustrated in the examples below). The above discussion shows that a number λ is an eigenvalue of A if
and only if cA(λ) = 0, that is if and only if λ is a root of the characteristic polynomial cA(x). We record
these observations in

×

Theorem 3.3.2

Let A bean n

n matrix.

×

1. Theeigenvaluesλ of A aretherootsofthecharacteristicpolynomial cA(x) of A.

6
6
6
172

Determinants and Diagonalization

2. Theλ-eigenvectorsxarethenonzerosolutionstothehomogeneoussystem

oflinearequationswithλI

A ascoefﬁcientmatrix.

−

(λI

−

A)x= 0

In practice, solving the equations in part 2 of Theorem 3.3.2 is a routine application of gaussian elimina-
tion, but ﬁnding the eigenvalues can be difﬁcult, often requiring computers (see Section 8.5). For now,
the examples and exercises will be constructed so that the roots of the characteristic polynomials are rela-
tively easy to ﬁnd (usually integers). However, the reader should not be misled by this into thinking that
eigenvalues are so easily obtained for the matrices that occur in practical applications!

Example 3.3.3

Find the characteristic polynomial of the matrix A =

then ﬁnd all the eigenvalues and their eigenvectors.

3
1

(cid:20)

5
1
−

(cid:21)

discussed in Example 3.3.2, and

Solution. Since xI

x 0
0 x

A =

−

(cid:20)

cA(x) = det

(cid:20)

−

(cid:21)

(cid:20)

x

3
−
1
−

3
1

5
1
−

=

(cid:21)

(cid:20)

x

3
−
1
−

5
−
x + 1

(cid:21)

we get

5
−
x + 1

= x2

2x

8 = (x

4)(x + 2)

−

−

−

(cid:21)
2, so these are the eigenvalues of A. Note that

Hence, the roots of cA(x) are λ1 = 4 and λ2 =
λ1 = 4 was the eigenvalue mentioned in Example 3.3.2, but we have found a new one: λ2 =
To ﬁnd the eigenvectors corresponding to λ2 =

2, observe that in this case

−

2.

−

−

(λ2I

−

A)x =

(cid:20)

λ2

3

5
−

−
1 λ2 + 1
−

so the general solution to (λ2I

A)x = 0 is x = t

−

1
−
1

(cid:20)

Hence, the eigenvectors x corresponding to λ2 are x = t

=

(cid:21)

(cid:20)

5
−
1
−

5
−
1
−

(cid:21)

where t is an arbitrary real number.

(cid:21)

(cid:20)

1
−
1

(cid:21)

where t

= 0 is arbitrary. Similarly,

, t

= 0 which includes the observation in

λ1 = 4 gives rise to the eigenvectors x = t

Example 3.3.2.

5
1

(cid:21)

(cid:20)

Note that a square matrix A has many eigenvectors associated with any given eigenvalue λ. In fact
A)x = 0 is an eigenvector. Recall that these solutions are all linear com-
every nonzero solution x of (λI
binations of certain basic solutions determined by the gaussian algorithm (see Theorem 1.3.2). Observe
that any nonzero multiple of an eigenvector is again an eigenvector,9 and such multiples are often more
convenient.10 Any set of nonzero multiples of the basic solutions of (λI
A)x = 0 will be called a set of

−

−

9In fact, any nonzero linear combination of λ-eigenvectors is again a λ-eigenvector.
10Allowing nonzero multiples helps eliminate round-off error when the eigenvectors involve fractions.

6
6
3.3. Diagonalization and Eigenvalues

173

basic eigenvectors corresponding to λ.

Example 3.3.4

Find the characteristic polynomial, eigenvalues, and basic eigenvectors for

2 0
1 2
1 3

0
1
−
2 
−


A =





Solution. Here the characteristic polynomial is given by

cA(x) = det



x

2
−
1
−
1
−

0

x

2
−
3
−



0
1
x + 2 


−

= (x

2)(x

−

−

1)(x + 1)

so the eigenvalues are λ1 = 2, λ2 = 1, and λ3 =

1. To ﬁnd all eigenvectors for λ1 = 2, compute

λ1I

−

A =



λ1

2

−
1 λ1
−
1
−

0

0
2
1
3 λ1 + 2 
−
−


A)x = 0. The augmented matrix becomes

0 0
0 1
3 4 


0
1
−
1
−

=

−





We want the (nonzero) solutions to (λ1I

−
0 0 0
0 1 0
3 4 0 


−

0
1
−
1
−





1 0
0 1
0 0

−
−

1 0
1 0
0 0 


→ 



using row operations. Hence, the general solution x to (λ1I

A)x = 0 is x = t

−

arbitrary, so we can use x1 =

1
1
1 

reader can verify, the gaussian algorithm gives basic eigenvectors x2 =





as the basic eigenvector corresponding to λ1 = 2. As the

where t is

1
1
1 






and x3 =



0
1
3
1



0
1
1 






corresponding to λ2 = 1 and λ3 =

1, respectively. Note that to eliminate fractions, we could





instead use 3x3 =

0
1
3 






−

as the basic λ3-eigenvector.

174

Determinants and Diagonalization

Example 3.3.5

If A is a square matrix, show that A and AT have the same characteristic polynomial, and hence the
same eigenvalues.

Solution. We use the fact that xI

−

cAT (x) = det

xI

AT

−

AT = (xI

−
= det

A)T . Then

A)T

= det (xI

(xI

−

A) = cA(x)

−

(cid:0)
(cid:2)
by Theorem 3.2.3. Hence cAT (x) and cA(x) have the same roots, and so AT and A have the same
eigenvalues (by Theorem 3.3.2).

(cid:1)

(cid:3)

The eigenvalues of a matrix need not be distinct. For example, if A =

the characteristic poly-

1)2 so the eigenvalue 1 occurs twice. Furthermore, eigenvalues are usually not computed
nomial is (x
as the roots of the characteristic polynomial. There are iterative, numerical methods (for example the
QR-algorithm in Section 8.5) that are much more efﬁcient for large matrices.

−

1 1
0 1

(cid:20)

(cid:21)

A-Invariance

If A is a 2
2 matrix, we can describe the eigenvectors of A geometrically using the following concept. A
×
line L through the origin in R2 is called A-invariant if Ax is in L whenever x is in L. If we think of A as a
R2, this asks that A carries L into itself, that is the image Ax of each vector x
linear transformation R2
in L is again in L.

→

Example 3.3.6

The x axis L =

x
0

|

(cid:21)

(cid:26)(cid:20)

A =

(cid:20)

a b
0 c

(cid:21)

y

Lx

x

0

x

x in R

is A-invariant for any matrix of the form

(cid:27)

because

a b
0 c

x
0

=

(cid:21)

(cid:20)

ax
0

(cid:21)

(cid:21) (cid:20)

(cid:20)

is L for all x =

x
0

(cid:21)

(cid:20)

in L

To see the connection with eigenvectors, let x

= 0 be any nonzero vec-
tor in R2 and let Lx denote the unique line through the origin containing x
(see the diagram). By the deﬁnition of scalar multiplication in Section 2.6,
we see that Lx consists of all scalar multiples of x, that is

tx
{
Now suppose that x is an eigenvector of A, say Ax = λx for some λ in R.
Then if tx is in Lx then

Lx = Rx =

t in R

}

|

That is, Lx is A-invariant. On the other hand, if Lx is A-invariant then Ax is in Lx (since x is in Lx). Hence
Ax = tx for some t in R, so x is an eigenvector for A (with eigenvalue t). This proves:

A(tx) = t (Ax) = t(λx) = (tλ)x is again in Lx

6
3.3. Diagonalization and Eigenvalues

175

= 0beavectorin R2,andlet Lx bethelinethroughtheoriginin R2

Theorem 3.3.3

Let A bea 2
containingx. Then

×

2 matrix,letx

xisaneigenvectorof A ifandonlyif

Lx is A-invariant

Example 3.3.7

1. If θ is not a multiple of π, show that A =

sinθ
cosθ
sinθ cosθ

−

(cid:21)

(cid:20)

has no real eigenvalue.

2. If m is real show that B = 1

1+m2

1

m2

−
2m

(cid:20)

2m

m2

1

−

(cid:21)

has a 1 as an eigenvalue.

Solution.

1. A induces rotation about the origin through the angle θ (Theorem 2.6.4). Since θ is not a
multiple of π, this shows that no line through the origin is A-invariant. Hence A has no
eigenvector by Theorem 3.3.3, and so has no eigenvalue.

2. B induces reﬂection Qm in the line through the origin with slope m by Theorem 2.6.5. If x is
any nonzero point on this line then it is clear that Qmx = x, that is Qmx = 1x. Hence 1 is an
eigenvalue (with eigenvector x).

If θ = π

1
so cA(x) = x2 + 1. This polynomial has no root
−
0
(cid:21)
in R, so A has no (real) eigenvalue, and hence no eigenvector. In fact its eigenvalues are the complex

2 in Example 3.3.7, then A =

0
1

(cid:20)

numbers i and

i, with corresponding eigenvectors

and eigenvectors, just not real ones.

−

1
i
−

(cid:20)

and

(cid:21)

(cid:20)

1
i

(cid:21)

In other words, A has eigenvalues

Note that every polynomial has complex roots,11 so every matrix has complex eigenvalues. While
these eigenvalues may very well be real, this suggests that we really should be doing linear algebra over the
complex numbers. Indeed, everything we have done (gaussian elimination, matrix algebra, determinants,
etc.) works if all the scalars are complex.

11This is called the Fundamental Theorem of Algebra and was ﬁrst proved by Gauss in his doctoral dissertation.

6
176

Determinants and Diagonalization

Diagonalization

n matrix D is called a diagonal matrix if all its entries off the main diagonal are zero, that is if D

An n
×
has the form

0
λ1
0 λ2
...
...
0
0

· · ·
· · ·
. . .

0
0
...
λn

D = 







= diag (λ1, λ2,

, λn)

· · ·





where λ1, λ2, . . . , λn are numbers. Calculations with diagonal matrices are very easy. Indeed, if
D = diag (λ1, λ2, . . . , λn) and E = diag (µ1, µ2, . . . , µn) are two diagonal matrices, their product DE and
sum D + E are again diagonal, and are obtained by doing the same operations to corresponding diagonal
elements:

· · ·

DE = diag (λ1µ1, λ2µ2, . . . , λnµn)

D + E = diag (λ1 + µ1, λ2 + µ2, . . . , λn + µn)

Because of the simplicity of these formulas, and with an eye on Theorem 3.3.1 and the discussion preced-
ing it, we make another deﬁnition:

Deﬁnition 3.6 Diagonalizable Matrices

An n

×

n matrix A iscalleddiagonalizableif

P−

1AP isdiagonalforsomeinvertiblen

n matrix P

×

Heretheinvertiblematrix P iscalledadiagonalizingmatrixfor A.

To discover when such a matrix P exists, we let x1, x2, . . . , xn denote the columns of P and look
for ways to determine when such xi exist and how to compute them. To this end, write P in terms of its
columns as follows:

Observe that P−

1AP = D for some diagonal matrix D holds if and only if

P = [x1, x2,

, xn]

· · ·

If we write D = diag (λ1, λ2, . . . , λn), where the λi are numbers to be determined, the equation AP = PD
becomes

AP = PD

A [x1, x2,

· · ·

, xn] = [x1, x2,

· · ·

0
λ1
0 λ2
...
...
0
0

· · ·
· · ·
. . .

· · ·

0
0
...
λn








, xn] 





By the deﬁnition of matrix multiplication, each side simpliﬁes as follows

Comparing columns shows that Axi = λixi for each i, so

(cid:2)

(cid:2)

(cid:3)

Ax1 Ax2

· · ·

Axn

=

λ1x1 λ2x2

λnxn

· · ·

(cid:3)

P−

1AP = D if and only if Axi = λixi for each i

In other words, P−
columns of P are corresponding eigenvectors. This proves the following fundamental result.

1AP = D holds if and only if the diagonal entries of D are eigenvalues of A and the

3.3. Diagonalization and Eigenvalues

177

Theorem 3.3.4

Let A bean n

n matrix.

×

1. A isdiagonalizableifandonlyifithaseigenvectorsx1, x2, . . . , xn suchthatthematrix

P =

x1 x2

. . . xn

isinvertible.

(cid:2)

(cid:3)
2. Whenthisisthecase, P−
of A correspondingtoxi.

1AP = diag (λ1, λ2, . . . , λn) where,foreach i,λi istheeigenvalue

Example 3.3.8

Diagonalize the matrix A =

0
1
2 
−
−

Solution. By Example 3.3.4, the eigenvalues of A are λ1 = 2, λ2 = 1, and λ3 =

in Example 3.3.4.

2 0
1 2
1 3





corresponding basic eigenvectors x1 =

, x2 =

1, with

−

respectively. Since

, and x3 =

0
1
1 


0
1
3 










is invertible, Theorem 3.3.4 guarantees that

the matrix P =

x1 x2 x3

=

(cid:2)

(cid:3)





P−

1AP =





1
1
1 
1 0 0


1 1 1
1 1 3 

0
0
0 λ3

0
λ1
0 λ2
0



=





2 0
0 1
0 0





= D

0
0
1 


−

The reader can verify this directly—easier to check AP = PD.

In Example 3.3.8, suppose we let Q =

be the matrix formed from the eigenvectors x1,
1AQ = diag (λ2, λ1, λ3) is diag-
x2, and x3 of A, but in a different order than that used to form P. Then Q−
onal by Theorem 3.3.4, but the eigenvalues are in the new order. Hence we can choose the diagonalizing
matrix P so that the eigenvalues λi appear in any order we want along the main diagonal of D.

x2 x1 x3

(cid:2)

(cid:3)

In every example above each eigenvalue has had only one basic eigenvector. Here is a diagonalizable

matrix where this is not the case.

Example 3.3.9

Diagonalize the matrix A =



0 1 1
1 0 1
1 1 0 




Solution. To compute the characteristic polynomial of A ﬁrst add rows 2 and 3 of xI

A to row 1:

−

178

Determinants and Diagonalization

cA(x) = det

= det







x
1
−
1
−
x

2
−
1
−
1
−

1
−
x
1
−

= det

1
−
1
x 
−

0
0
x + 1 


0
x + 1
0




= (x



2 x

x

2 x
−
1
−
1
−

−
x
1
−

2
−
1
x 
−


2)(x + 1)2

−

Hence the eigenvalues are λ1 = 2 and λ2 =
multiplicity two). However, A is diagonalizable. For λ1 = 2, the system of equations

1, with λ2 repeated twice (we say that λ2 has

−

(λ1I

−

A)x = 0 has general solution x = t

as the reader can verify, so a basic λ1-eigenvector

1
1
1 






is x1 =



1
1
.
1 


Turning to the repeated eigenvalue λ2 =



1, we must solve (λ2I

elimination, the general solution is x = s





−

+ t

1
−
1
0 


1
−
0
1 






the gaussian algorithm produces two basic λ2-eigenvectors x2 =

A)x = 0. By gaussian

−

where s and t are arbitrary. Hence

1
−
1
0 






and y2 =

If we

1
−
0
1 






we ﬁnd that P is invertible. Hence

take P =

x1 x2 y2

(cid:2)
1AP = diag (2,

P−

1,

−

(cid:3)
−

=

1
1
1

1
−
1
0
1) by Theorem 3.3.4.

1
−
0
1 






Example 3.3.9 typiﬁes every diagonalizable matrix. To describe the general case, we need some ter-

minology.

Deﬁnition 3.7 Multiplicity of an Eigenvalue

Aneigenvalueλ ofasquarematrix A issaidtohavemultiplicity m ifitoccurs m timesasarootof
thecharacteristicpolynomialcA(x).

For example, the eigenvalue λ2 =
algorithm yields two basic λ2-eigenvectors, the same number as the multiplicity. This works in general.

1 in Example 3.3.9 has multiplicity 2. In that example the gaussian

−

Theorem 3.3.5

Asquarematrix A isdiagonalizableifandonlyifeveryeigenvalueλ ofmultiplicitym yields
exactly m basiceigenvectors;thatis,ifandonlyifthegeneralsolutionofthesystem (λI
hasexactly m parameters.

−

A)x= 0

3.3. Diagonalization and Eigenvalues

179

One case of Theorem 3.3.5 deserves mention.

Theorem 3.3.6

An n

×

n matrixwith n distincteigenvaluesisdiagonalizable.

The proofs of Theorem 3.3.5 and Theorem 3.3.6 require more advanced techniques and are given in Chap-
ter 5. The following procedure summarizes the method.

Diagonalization Algorithm

Todiagonalizean n

n matrix A:

×

Step1. Findthedistincteigenvaluesλ of A.

Step2. Computeasetofbasiceigenvectorscorrespondingtoeachoftheseeigenvaluesλ as
basicsolutionsofthehomogeneoussystem (λI

A)x= 0.

−

Step3. ThematrixAisdiagonalizableifandonlyiftherearenbasiceigenvectorsinall.

Step4. If A isdiagonalizable,the n
adiagonalizingmatrixfor A,thatis, P isinvertibleand P−

n matrix P withthesebasiceigenvectorsasitscolumnsis
1AP isdiagonal.

×

The diagonalization algorithm is valid even if the eigenvalues are nonreal complex numbers. In this case
the eigenvectors will also have complex entries, but we will not pursue this here.

Example 3.3.10

Show that A =

1 1
0 1

(cid:21)

(cid:20)

is not diagonalizable.

Solution 1. The characteristic polynomial is cA(x) = (x

−

of multiplicity 2. But the system of equations (λ1I

A)x = 0 has general solution t

−

is only one parameter, and so only one basic eigenvector

. Hence A is not diagonalizable.

1)2, so A has only one eigenvalue λ1 = 1

1
0

(cid:21)

(cid:20)

, so there

Solution 2. We have cA(x) = (x

−

(cid:21)
1)2 so the only eigenvalue of A is λ = 1. Hence, if A were

(cid:20)

diagonalizable, Theorem 3.3.4 would give P−

1AP =

= I for some invertible matrix P.

But then A = PIP−

1 = I, which is not the case. So A cannot be diagonalizable.

1
2

1 0
0 1

(cid:20)

(cid:21)

Diagonalizable matrices share many properties of their eigenvalues. The following example illustrates

why.

180

Determinants and Diagonalization

Example 3.3.11

If λ3 = 5λ for every eigenvalue of the diagonalizable matrix A, show that A3 = 5A.

Solution. Let P−

1AP = D = diag (λ1, . . . , λn). Because λ3

i = 5λi for each i, we obtain

D3 = diag (λ3

1 , . . . , λ3

n ) = diag (5λ1, . . . , 5λn) = 5D

Hence A3 = (PDP−
what we wanted.

1)3 = PD3P−

1 = P(5D)P−

1 = 5(PDP−

1) = 5A using Theorem 3.3.1. This is

If p(x) is any polynomial and p(λ) = 0 for every eigenvalue of the diagonalizable matrix A, an argu-
ment similar to that in Example 3.3.11 shows that p(A) = 0. Thus Example 3.3.11 deals with the case
p(x) = x3
5x. In general, p(A) is called the evaluation of the polynomial p(x) at the matrix A. For
−
example, if p(x) = 2x3

3A + 5I—note the use of the identity matrix.

3x + 5, then p(A) = 2A3

In particular, if cA(x) denotes the characteristic polynomial of A, we certainly have cA(λ) = 0 for each
eigenvalue λ of A (Theorem 3.3.2). Hence cA(A) = 0 for every diagonalizable matrix A. This is, in fact,
true for any square matrix, diagonalizable or not, and the general result is called the Cayley-Hamilton
theorem. It is proved in Section 8.7 and again in Section 11.1.

−

−

Linear Dynamical Systems

We began Section 3.3 with an example from ecology which models the evolution of the population of a
species of birds as time goes on. As promised, we now complete the example—Example 3.3.12 below.

The bird population was described by computing the female population proﬁle vk =

of the

species, where ak and jk represent the number of adult and juvenile females present k years after the initial
values a0 and j0 were observed. The model assumes that these numbers are related by the following
equations:

ak
jk

(cid:20)

(cid:21)

2ak + 1

4 jk

ak+1 = 1
jk+1 = 2ak

the columns vk satisfy vk+1 = Avk for each k = 0, 1, 2, . . . .

If we write A =

1
1
2
4
2 0

(cid:20)

(cid:21)

Hence vk = Akv0 for each k = 1, 2, . . . . We can now use our diagonalization techniques to determine the
population proﬁle vk for all values of k in terms of the initial values.

Example 3.3.12

Assuming that the initial values were a0 = 100 adult females and j0 = 40 juvenile females,
compute ak and jk for k = 1, 2, . . . .

Solution. The characteristic polynomial of the matrix A =

cA(x) = x2

1
2x

−

−

1
2 = (x

−

1)(x + 1

2), so the eigenvalues are λ1 = 1 and λ2 =

1
2 and gaussian

−

1
1
2
4
2 0

is

(cid:21)

(cid:20)

3.3. Diagonalization and Eigenvalues

181

1
2
1

(cid:20)

and

(cid:21)

(cid:20)

1
4
1

−

. For convenience, we can

(cid:21)

use multiples x1 =

and x2 =

respectively. Hence a diagonalizing matrix is

elimination gives corresponding basic eigenvectors

1
2

(cid:20)

(cid:21)
and we obtain

1
−
4

(cid:20)

(cid:21)

P =

(cid:20)

1
2

1
−
4

(cid:21)

P−

1AP = D where D =

1
0

0
1
2 (cid:21)

(cid:20)
0, we can compute Ak explicitly:

−

1
2

1
−
4

4 + 2(
8(
8

−

−

"

0
1
1
2 )k
0 (
(cid:21) (cid:20)
−
1
2)k
1
(
−
1
2)k 2 + 4(

−

−

1
6

(cid:21)
(cid:20)
1
2)k
1
2)k

−

4 1
2 1

−

(cid:21)

#

This gives A = PDP−

1 so, for each k

≥

Ak = PDkP−

1 =

(cid:20)
= 1
6

Hence we obtain

ak
jk

(cid:20)

(cid:21)

= vk = Akv0 = 1
6

= 1
6

"

"

1
2)k
1
2)k

100
40

(cid:21)

# (cid:20)

4 + 2(
8(
8

−

−

−

440 + 160(
640(
880

−

−

1
2)k
1
(
−
1
2)k 2 + 4(
1
2)k
1
2)k

−

#

−

−

Equating top and bottom entries, we obtain exact formulas for ak and jk:

ak = 220

3 + 80
3

k

1
2

and jk = 440

3 + 320
3

k

1
2

−

for k = 1, 2,

· · ·

−
(cid:0)

(cid:1)

In practice, the exact values of ak and jk are not usually required. What is needed is a measure of
1
2 )k is nearly
how these numbers behave for large values of k. This is easy to obtain here. Since (
zero for large k, we have the following approximate values

−

(cid:0)

(cid:1)

3 and jk ≈
Hence, in the long term, the female population stabilizes with approximately twice as many
juveniles as adults.

ak ≈

440
3 if k is large

220

Deﬁnition 3.8 Linear Dynamical System

n matrix,asequencev0, v1, v2, . . . ofcolumnsin Rn iscalledalineardynamical
If A isan n
systemifv0 isspeciﬁedandv1, v2, . . . aregivenbythematrixrecurrencevk+1 = Avk foreach
k

0. Wecall A themigrationmatrixofthesystem.

×

≥

182

Determinants and Diagonalization

We have v1 = Av0, then v2 = Av1 = A2v0, and continuing we ﬁnd

vk = Akv0 for each k = 1, 2,

· · ·

(3.9)

Hence the columns vk are determined by the powers Ak of the matrix A and, as we have seen, these powers
can be efﬁciently computed if A is diagonalizable. In fact Equation 3.9 can be used to give a nice “formula”
for the columns vk in this case.

Assume that A is diagonalizable with eigenvalues λ1, λ2, . . . , λn and corresponding basic eigenvectors
is a diagonalizing matrix with the xi as columns, then P is

. . . xn

x1 x2

x1, x2, . . . , xn. If P =
invertible and

(cid:2)

by Theorem 3.3.4. Hence A = PDP−

(cid:3)

P−

1AP = D = diag (λ1, λ2,
1 so Equation 3.9 and Theorem 3.3.1 give

, λn)

· · ·

vk = Akv0 = (PDP−

1)kv0 = (PDkP−

1)v0 = PDk(P−

1v0)

for each k = 1, 2, . . . . For convenience, we denote the column P−

1v0 arising here as follows:

b = P−

1v0 = 





b1
b2
...
bn








Then matrix multiplication gives

vk = PDk(P−

1v0)

=

x1 x2

xn

· · ·

(cid:2)

=

x1 x2

xn

· · ·

(cid:3)










λk
0
1
0 λk
2
...
...
0
0
b1λk
1
b2λk
2
...

b3λk

n


+ bnλk
n xn



· · ·
· · ·
. . .

· · ·

0
0
...
λk
n

b1
b2
...
bn


























· · ·
0. This is a useful exact formula for the columns vk. Note that, in particular,

(cid:2)
= b1λk

1 x1 + b2λk

2 x2 +

(cid:3)

(3.10)

for each k

≥

v0 = b1x1 + b2x2 +

+ bnxn

· · ·

However, such an exact formula for vk is often not required in practice; all that is needed is to estimate
vk for large values of k (as was done in Example 3.3.12). This can be easily done if A has a largest
eigenvalue. An eigenvalue λ of a matrix A is called a dominant eigenvalue of A if it has multiplicity 1
and

denotes the absolute value of the number λ. For example, λ1 = 1 is dominant in Example 3.3.12.

λ
|
|

>

µ
|
|

for all eigenvalues µ

= λ

where

λ
|
|

6
3.3. Diagonalization and Eigenvalues

183

Returning to the above discussion, suppose that A has a dominant eigenvalue. By choosing the order
in which the columns xi are placed in P, we may assume that λ1 is dominant among the eigenvalues
λ1, λ2, . . . , λn of A (see the discussion following Example 3.3.8). Now recall the exact expression for vk
in Equation 3.10 above:

1 x1 + b2λk
1 out as a common factor in this equation to get

vk = b1λk

2 x2 +

· · ·

Take λk

+ bnλk

n xn

vk = λk
1

b1x1 + b2
(cid:20)

k

x2 +

λ2
λ1

+ bn

k

xn

λn
λ1

· · ·

(cid:21)

0. Since λ1 is dominant, we have

(cid:16)
for each k
λi
|
|
become small in absolute value as k increases. Hence vk is approximately equal to the ﬁrst term λk
and we write this as vk
with the above exact formula for vk).

(cid:17)
2, so each of the numbers (λi/λ1)k
1 b1x1,
λk
1 b1x1. These observations are summarized in the following theorem (together

(cid:16)
for each i

λ1
|

(cid:17)
<

≥

≥

≈

|

Theorem 3.3.7
Considerthedynamicalsystemv0, v1, v2, . . . withmatrixrecurrence

vk+1 = Avk for k

0

≥

where A andv0 aregiven. Assumethat A isadiagonalizable n
λ1, λ2, . . . , λn andcorrespondingbasiceigenvectorsx1, x2, . . . , xn,andlet
P =

bethediagonalizingmatrix. Thenanexactformulaforvk is

. . . xn

x1 x2

n matrixwitheigenvalues

×

(cid:2)

(cid:3)
vk = b1λk

1x1 + b2λk

2x2 +

+ bnλk

n xn foreach k

0

≥

· · ·

wherethecoefﬁcients bi comefrom

b= P−

b1
b2
...
bn








1v0 = 





Moreover,if A hasdominant12eigenvalueλ1,thenvk isapproximatedby

vk = b1λk

1 x1 forsufﬁcientlylarge k.

Example 3.3.13

Returning to Example 3.3.12, we see that λ1 = 1 is the dominant eigenvalue, with eigenvector
. Hence b1 = 220
(cid:21)

. Here P =

1v0 = 1
3

and v0 =

220
80

100
40

1
−
4

so P−

x1 =

1
2

1
2

(cid:20)

(cid:20)

(cid:21)

(cid:21)

(cid:20)

(cid:20)

(cid:21)

−

3 in

12Similar results can be found in other situations. If for example, eigenvalues λ1 and λ2 (possibly equal) satisfy
for all i > 2, then we obtain vk
2 x2 for large k.

1 x1 + b2λk

b1λk

λ1
|

|

=

>

λ2
|

|

λi
|

|

≈

184

Determinants and Diagonalization

the notation of Theorem 3.3.7, so

where k is large. Hence ak ≈

ak
jk

= vk

≈

b1λk

1 x1 = 220

3 1k

(cid:20)
(cid:21)
220
3 and jk ≈

(cid:21)
440
3 as in Example 3.3.12.

(cid:20)

1
2

This next example uses Theorem 3.3.7 to solve a “linear recurrence.” See also Section 3.4.

Example 3.3.14

Suppose a sequence x0, x1, x2, . . . is determined by insisting that

x0 = 1, x1 =

1, and xk+2 = 2xk

−
Find a formula for xk in terms of k.

xk+1 for every k

0

≥

−

Solution. Using the linear recurrence xk+2 = 2xk

xk+1 repeatedly gives

−
x2 =

x2 = 2x0

−

x1 = 3,

x3 = 2x1

−

5,

−

x4 = 11,

x5 =

21, . . .

−

so the xi are determined but no pattern is apparent. The idea is to ﬁnd vk =

for each k

instead, and then retrieve xk as the top component of vk. The reason this works is that the linear
recurrence guarantees that these vk are a dynamical system:

xk
xk+1 (cid:21)

(cid:20)

vk+1 =

xk+1
xk+2 (cid:21)

=

(cid:20)

xk+1

2xk

xk+1 (cid:21)

−

(cid:20)

= Avk where A =

0
2

(cid:20)

1
1
−

(cid:21)

The eigenvalues of A are λ1 =

2 and λ2 = 1 with eigenvectors x1 =

−

the diagonalizing matrix is P =

1
0 v0 = 1
Moreover, b = P−
3

2
1

(cid:20)

(cid:21)

1 1
2 1

.

(cid:20)
−
so the exact formula for vk is

(cid:21)

1
2
−

(cid:21)

(cid:20)

and x2 =

1
1

(cid:20)

, so

(cid:21)

xk
xk+1 (cid:21)

(cid:20)

= vk = b1λk

1 x1 + b2λk

2 x2 = 2
3 (

2)k

−

1
2
−

(cid:20)

(cid:21)

+ 1

3 1k

1
1

(cid:21)

(cid:20)

Equating top entries gives the desired formula for xk:

The reader should check this for the ﬁrst few values of k.

xk = 1
3

2(

2)k + 1

−

h

i

for all k = 0, 1, 2, . . .

Graphical Description of Dynamical Systems

If a dynamical system vk+1 = Avk is given, the sequence v0, v1, v2, . . . is called the trajectory of the

3.3. Diagonalization and Eigenvalues

185

system starting at v0. It is instructive to obtain a graphical plot of the system by writing vk =

plotting the successive values as points in the plane, identifying vk with the point (xk, yk) in the plane. We
give several examples which illustrate properties of dynamical systems. For ease of calculation we assume
that the matrix A is simple, usually diagonal.

xk
yk

(cid:20)

(cid:21)

and

Example 3.3.15

Let A =

1
2 0
0 1

(cid:20)

3 (cid:21)
corresponding eigenvectors x1 =

Then the eigenvalues are 1

3, with

2 and 1
0
1

(cid:20)

.
(cid:21)

and x2 =

1
0

(cid:21)

(cid:20)

The exact formula is

x

vk = b1

k

1
2

1
0

+ b2

k

1
3

0
1

(cid:1)

(cid:0)

(cid:0)

(cid:1)

(cid:21)

(cid:20)

(cid:21)

(cid:20)
for k = 0, 1, 2, . . . by Theorem 3.3.7, where the coefﬁcients
b1 and b2 depend on the initial point v0. Several trajectories are
plotted in the diagram and, for each choice of v0, the trajectories
converge toward the origin because both eigenvalues are less
than 1 in absolute value. For this reason, the origin is called
an attractor for the system.

y

O

Example 3.3.16

y

3
2 0
0 4

Let A =

. Here the eigenvalues are 3

(cid:20)

3 (cid:21)
corresponding eigenvectors x1 =

2 and 4
3 , with
0
1

(cid:20)

(cid:21)

and x2 =

as before.

1
0

(cid:20)

(cid:21)

The exact formula is

O

x

vk = b1

k

3
2

1
0

+ b2

k

4
3

0
1

(cid:1)

(cid:0)

(cid:21)

(cid:20)
for k = 0, 1, 2, . . . . Since both eigenvalues are greater than
1 in absolute value, the trajectories diverge away from the origin
for every choice of initial point V0. For this reason, the origin
is called a repellor for the system.

(cid:20)

(cid:21)

(cid:0)

(cid:1)

186

Determinants and Diagonalization

Example 3.3.17

y

O

Let A =

1
2
1

−

1
1
2

(cid:20)

−

. Now the eigenvalues are 3
(cid:21)

2, with

2 and 1
1
1

(cid:20)

(cid:21)

and x2 =

The

1
−
1

(cid:21)

corresponding eigenvectors x1 =

exact formula is

(cid:20)

x

vk = b1

k

3
2

1
−
1

+ b2

k

1
2

1
1

(cid:20)

(cid:0)

(cid:1)

for k = 0, 1, 2, . . . . In this case 3

so, if b1

= 0, we have vk

≈
is approaching the line y =

x.
(cid:0)
However, if b1 = 0, then vk = b2

−

b1

3
2

(cid:20)

(cid:1)

k

1
2

(cid:0)

(cid:1)

(cid:21)

(cid:21)
(cid:20)
2 is the dominant eigenvalue
k
for large k and vk

1
−
1

(cid:21)

and so approaches

1
1

(cid:20)

(cid:21)

the origin along the line y = x. In general the trajectories appear
as in the diagram, and the origin is called a saddle point for the

(cid:0)

(cid:1)

dynamical system in this case.

Example 3.3.18

0 1
2
1
0
2

Let A =

. Now the characteristic polynomial is cA(x) = x2 + 1
(cid:21)

−
the complex numbers i

i
2 where i2 =
However, the trajectories are not difﬁcult to describe. If we start with v0 =

2 and

−

−

(cid:20)

1. Hence A is not diagonalizable as a real matrix.

4, so the eigenvalues are

1
1

(cid:20)

(cid:21)

then the

trajectory begins as

v1 =

1
2
1
2 #

"

−

, v2 =

1
4
1
4 #

−

−

"

, v3 =

−

1
8
1
8 #

"

, v4 =

1
16
1
16 #

"

, v5 =

1
32
1
32 #

"

−

, v6 =

1
64
1
64 #

−

−

"

, . . .

The ﬁrst ﬁve of these points are plotted in the diagram. Here
each trajectory spirals in toward the origin, so the origin is an
attractor. Note that the two (complex) eigenvalues have absolute
value less than 1 here. If they had absolute value greater than
1, the trajectories would spiral out from the origin.

y

1

v3

O

v2

v0

x

1

v1

6
3.3. Diagonalization and Eigenvalues

187

Google PageRank

Dominant eigenvalues are useful to the Google search engine for ﬁnding information on the Web. If an
information query comes in from a client, Google has a sophisticated method of establishing the “rele-
vance” of each site to that query. When the relevant sites have been determined, they are placed in order of
importance using a ranking of all sites called the PageRank. The relevant sites with the highest PageRank
are the ones presented to the client. It is the construction of the PageRank that is our interest here.

The Web contains many links from one site to another. Google interprets a link from site j to site
i as a “vote” for the importance of site i. Hence if site i has more links to it than does site j, then i is
regarded as more “important” and assigned a higher PageRank. One way to look at this is to view the sites
as vertices in a huge directed graph (see Section 2.2). Then if site j links to site i there is an edge from j
to i, and hence the (i, j)-entry is a 1 in the associated adjacency matrix (called the connectivity matrix in
this context). Thus a large number of 1s in row i of this matrix is a measure of the PageRank of site i.13

However this does not take into account the PageRank of the sites that link to i. Intuitively, the higher
the rank of these sites, the higher the rank of site i. One approach is to compute a dominant eigenvector x
for the connectivity matrix. In most cases the entries of x can be chosen to be positive with sum 1. Each
site corresponds to an entry of x, so the sum of the entries of sites linking to a given site i is a measure of
the rank of site i. In fact, Google chooses the PageRank of a site so that it is proportional to this sum.14

Exercises for 3.3

Exercise 3.3.1 In each case ﬁnd the characteristic poly-
nomial, eigenvalues, eigenvectors, and (if possible) an in-
vertible matrix P such that P−

1AP is diagonal.

vk+1 = Avk for k
≥
ing Theorem 3.3.7.

0. In each case approximate vk us-

2
1

−
1
2
1

4
1

−
−
1
0
1

−
0 1 0
3 0 1
2 0 0

a. A =

b. A =

c. A =

(cid:20)

(cid:20)





(cid:21)
3
−
6
5







2
4

3
2

1
1

2
2

−

−
−

(cid:21)

(cid:21)

, v0 =

, v0 =

1
2

(cid:20)

(cid:20)

−

1 0 0
1 2 3
1 4 1

, v0 =





(cid:21)

3
1

1
1
1

(cid:21)









2
0
1


1 1
1 0
1 2

−





d. A =



−



1
1
4

3
2
1

−

2
1
1





−

, v0 =





2
0
1





1 2
3 2

7 0
0 5
5 0

(cid:21)

4
0
2

−

−

1
2
1

−

2
6
2

−
−

3
4
−
2

1
2
2

−

λ 0
0
0 λ 0
0 µ
0

a.

A =

c.

A =

e.

A =

g.

A =

i.

A =

(cid:20)

















b.

A =

d.

A =

f.

A =

h.

A =





(cid:20)
















3
6
1




1
5
5

−

= µ

, λ





Exercise 3.3.3 Show that A has λ = 0 as an eigenvalue
if and only if A is not invertible.

Exercise 3.3.4 Let A denote an n
A1 = A
−
A if and only if λ

n matrix and put
αI, α in R. Show that λ is an eigenvalue of
α is an eigenvalue of A1. (Hence,

×

−

Exercise 3.3.2 Consider a linear dynamical system

13For more on PageRank, visit https://en.wikipedia.org/wiki/PageRank.
14See the articles “Searching the web with eigenvectors” by Herbert S. Wilf, UMAP Journal 23(2), 2002, pages 101–103,
and “The worlds largest matrix computation: Google’s PageRank is an eigenvector of a matrix of order 2.7 billion” by Cleve
Moler, Matlab News and Notes, October 2002, pages 12–13.

6
188

Determinants and Diagonalization

the eigenvalues of A1 are just those of A “shifted” by α.)
How do the eigenvectors compare?

Exercise 3.3.5
sinθ
cosθ
sinθ cosθ
(cid:20)
(cid:21)
(See Appendix A)

−

Show that

the

eigenvalues of

are eiθ and e−

iθ.

Exercise 3.3.6 Find the characteristic polynomial of the
n identity matrix I. Show that I has exactly one eigen-
n
value and ﬁnd the eigenvectors.

×

Exercise 3.3.7 Given A =

a b
c d

(cid:21)

(cid:20)

show that:

a. cA(x) = x2

called the trace of A.

−

tr Ax + det A, where tr A = a + d is

b. The eigenvalues are 1
2

(a + d)

h

±

(a

−

Exercise 3.3.8 In each case, ﬁnd P−
pute An.

p

i
1AP and then com-

a. A =

6
2

(cid:20)

5
1

−
−

b. A =

−

(cid:20)
[Hint:
1, 2, . . . .]

, P =

1 5
1 2

(cid:21)
12
10

(cid:20)

(cid:21)
3
−
2
(cid:21)
1)n = PDnP−

, P =

(cid:20)

1

7
6

−
−
(PDP−

4
3

(cid:21)

−
for each n =

Exercise 3.3.9

a. If A =

and B =

verify that A

2 0
0 1

and B are diagonalizable, but AB is not.

(cid:20)

(cid:21)

(cid:20)

1 3
0 2

1
0

(cid:21)

0
1

d. U −

1AU for any invertible matrix U .

e. kI + A for any scalar k.

Exercise 3.3.12 Give an example of two diagonalizable
matrices A and B whose sum A + B is not diagonalizable.

Exercise 3.3.13 If A is diagonalizable and 1 and
the only eigenvalues, show that A−

1 = A.

−

1 are

Exercise 3.3.14 If A is diagonalizable and 0 and 1 are
the only eigenvalues, show that A2 = A.

Exercise 3.3.15
0 for
each eigenvalue of A, show that A = B2 for some matrix
B.

If A is diagonalizable and λ

≥

1BP are both diago-
Exercise 3.3.16 If P−
nal, show that AB = BA. [Hint: Diagonal matrices com-
mute.]

1AP and P−

b)2 + 4bc

.

Exercise 3.3.17 A square matrix A is called nilpotent if
An = 0 for some n
1. Find all nilpotent diagonalizable
matrices. [Hint: Theorem 3.3.1.]

≥

Exercise 3.3.18 Let A be any n
real number.

×

n matrix and r

= 0 a

a. Show that the eigenvalues of rA are precisely the

numbers rλ, where λ is an eigenvalue of A.

b. Show that crA(x) = rncA

Exercise 3.3.19

x
r

.

(cid:0)

(cid:1)

a. If all rows of A have the same sum s, show that s

is an eigenvalue.

b. If all columns of A have the same sum s, show that

s is an eigenvalue.

b. If D =

ﬁnd a diagonalizable matrix A

Exercise 3.3.20 Let A be an invertible n

n matrix.

×

(cid:21)
such that D + A is not diagonalizable.

−

(cid:20)

Exercise 3.3.10 If A is an n
diagonalizable if and only if AT is diagonalizable.

n matrix, show that A is

×

a. Show that the eigenvalues of A are nonzero.

b. Show that the eigenvalues of A−

1 are precisely the

numbers 1/λ, where λ is an eigenvalue of A.

Exercise 3.3.11 If A is diagonalizable, show that each
of the following is also diagonalizable.

c. Show that cA−

1(x) = (

x)n
det A cA
−

1
x

.

a. An, n

1

≥
b. kA, k any scalar.

Exercise 3.3.21 Suppose λ is an eigenvalue of a square
matrix A with eigenvector x

= 0.

(cid:0)

(cid:1)

a. Show that λ2 is an eigenvalue of A2 (with the same

c. p(A), p(x) any polynomial (Theorem 3.3.1)

x).

6
6
b. Show that λ3
2A + 3I.

A3

−

2λ + 3 is an eigenvalue of

−

c. Show that p(λ) is an eigenvalue of p(A) for any

nonzero polynomial p(x).

3.3. Diagonalization and Eigenvalues

189

Exercise 3.3.25 Let A2 = I, and assume that A
A

I.

=

= I and

−
a. Show that the only eigenvalues of A are λ = 1 and

λ =

1.

−

b. Show that A is diagonalizable. [Hint: Verify that
I), and then

A(A+I) = A+I and A(A
look at nonzero columns of A + I and of A

I) =

(A

−

−

−

I.]

−

Exercise 3.3.22
cA2(x2) = (

−

1)ncA(x)cA(

x).

−

If A is an n

n matrix, show that

×

c. If Qm : R2
where m
Qm is diagonalizable for each m.

R2 is reﬂection in the line y = mx
= 0, use (b) to show that the matrix of

→

d. Now prove (c) geometrically using Theorem 3.3.3.

Exercise 3.3.23 An n
Am = 0 for some m

×
1.

≥

n matrix A is called nilpotent if

Exercise 3.3.26

Let A =

2 3
1 0
1 1





3
1
2

−
−
−





and B =

0 1 0
3 0 1
2 0 0



. Show that cA(x) = cB(x) = (x + 1)2(x

−

2), but A is diagonalizable and B is not.




Exercise 3.3.27

a. Show that every triangular matrix with zeros on



the main diagonal is nilpotent.

b. If A is nilpotent, show that λ = 0 is the only eigen-

value (even complex) of A.

a. Show that the only diagonalizable matrix A that
has only one eigenvalue λ is the scalar matrix
A = λI.

c. Deduce that cA(x) = xn, if A is n

n and nilpotent.

×

Exercise 3.3.24 Let A be diagonalizable with real eigen-
values and assume that Am = I for some m

1.

≥

b. Is

3
2

(cid:20)

2
1

−
−

(cid:21)

diagonalizable?

Exercise 3.3.28 Characterize the diagonalizable n
matrices A such that A2
eigenvalues. [Hint: Theorem 3.3.1.]

n
3A + 2I = 0 in terms of their

−

×

Exercise 3.3.29 Let A =

square matrices.

B 0
0 C

(cid:20)

(cid:21)

where B and C are

a. If B and C are diagonalizable via Q and R (that is,
1CR are diagonal), show that A is

1BQ and R−

Q−

a. Show that A2 = I.

b. If m is odd, show that A = I.

[Hint: Theorem A.3]

diagonalizable via

Q 0
0 R

(cid:21)

(cid:20)

b. Use (a) to diagonalize A if B =

C =

7
1

−

1
7

.

(cid:21)

(cid:20)

−

5 3
3 5

(cid:20)

and

(cid:21)

Exercise 3.3.30 Let A =

square matrices.

B 0
0 C

(cid:20)

(cid:21)

where B and C are

6
6
6
190

Determinants and Diagonalization

a. Show that cA(x) = cB(x)cC(x).

b. If x and y are eigenvectors of B and C, respec-

x
0

0
y

tively, show that

and

are eigenvec-

tors of A, and show how every eigenvector of A
arises from such eigenvectors.

(cid:20)

(cid:21)

(cid:20)

(cid:21)

Referring to the model

Exercise 3.3.31
in Exam-
ple 3.3.1, determine if the population stabilizes, becomes
extinct, or becomes large in each case. Denote the adult
and juvenile survival rates as A and J, and the reproduc-
tion rate as R.

R

2

3

2

3

a.

b.

c.

d.

A

1
2

1
4

1
4

3
5

J

1
2

1
4

1
3

1
5

Exercise 3.3.32 In the model of Example 3.3.1, does the
ﬁnal outcome depend on the initial population of adult
and juvenile females? Support your answer.

Exercise 3.3.33 In Example 3.3.1, keep the same repro-
duction rate of 2 and the same adult survival rate of 1
2 ,
but suppose that the juvenile survival rate is ρ. Deter-
mine which values of ρ cause the population to become
extinct or to become large.

Exercise 3.3.34 In Example 3.3.1, let the juvenile sur-
vival rate be 2
5 and let the reproduction rate be 2. What
values of the adult survival rate α will ensure that the
population stabilizes?

3.4 An Application to Linear Recurrences

It often happens that a problem can be solved by ﬁnding a sequence of numbers x0, x1, x2, . . . where the
ﬁrst few are known, and subsequent numbers are given in terms of earlier ones. Here is a combinatorial
example where the object is to count the number of ways to do something.

Example 3.4.1

An urban planner wants to determine the number xk of ways that a row of k parking spaces can be
ﬁlled with cars and trucks if trucks take up two spaces each. Find the ﬁrst few values of xk.

Solution. Clearly, x0 = 1 and x1 = 1, while x2 = 2 since there can be two cars or one truck. We
have x3 = 3 (the 3 conﬁgurations are ccc, cT, and Tc) and x4 = 5 (cccc, ccT, cTc, Tcc, and TT). The
key to this method is to ﬁnd a way to express each subsequent xk in terms of earlier values. In this
case we claim that

xk+2 = xk + xk+1 for every k

0

≥

(3.11)

Indeed, every way to ﬁll k + 2 spaces falls into one of two categories: Either a car is parked in the
ﬁrst space (and the remaining k + 1 spaces are ﬁlled in xk+1 ways), or a truck is parked in the ﬁrst
two spaces (with the other k spaces ﬁlled in xk ways). Hence, there are xk+1 + xk ways to ﬁll the
k + 2 spaces. This is Equation 3.11.

3.4. An Application to Linear Recurrences

191

The recurrence in Equation 3.11 determines xk for every k
the ﬁrst few values are

2 since x0 and x1 are given. In fact,

≥

x0 = 1
x1 = 1
x2 = x0 + x1 = 2
x3 = x1 + x2 = 3
x4 = x2 + x3 = 5
x5 = x3 + x4 = 8
...
...

...

Clearly, we can ﬁnd xk for any value of k, but one wishes for a “formula” for xk as a function of k.
It turns out that such a formula can be found using diagonalization. We will return to this example
later.

A sequence x0, x1, x2, . . . of numbers is said to be given recursively if each number in the sequence is
completely determined by those that come before it. Such sequences arise frequently in mathematics and
computer science, and also occur in other parts of science. The formula xk+2 = xk+1 + xk in Example 3.4.1
is an example of a linear recurrence relation of length 2 because xk+2 is the sum of the two preceding
terms xk+1 and xk; in general, the length is m if xk+m is a sum of multiples of xk, xk+1, . . . , xk+m

1.

The simplest linear recursive sequences are of length 1, that is xk+1 is a ﬁxed multiple of xk for each k,
say xk+1 = axk. If x0 is speciﬁed, then x1 = ax0, x2 = ax1 = a2x0, and x3 = ax2 = a3x0, . . . . Continuing,
we obtain xk = akx0 for each k
0, which is an explicit formula for xk as a function of k (when x0 is given).
Such formulas are not always so easy to ﬁnd for all choices of the initial values. Here is an example

≥

−

where diagonalization helps.

Example 3.4.2

Suppose the numbers x0, x1, x2, . . . are given by the linear recurrence relation

where x0 and x1 are speciﬁed. Find a formula for xk when x0 = 1 and x1 = 3, and also when x0 = 1
and x1 = 1.

xk+2 = xk+1 + 6xk for k

0

≥

Solution. If x0 = 1 and x1 = 3, then

x2 = x1 + 6x0 = 9,

x3 = x2 + 6x1 = 27,

x4 = x3 + 6x2 = 81

and it is apparent that

xk = 3k for k = 0, 1, 2, 3, and 4

This formula holds for all k because it is true for k = 0 and k = 1, and it satisﬁes the recurrence
xk+2 = xk+1 + 6xk for each k as is readily checked.
However, if we begin instead with x0 = 1 and x1 = 1, the sequence continues

x2 = 7,

x3 = 13,

x4 = 55,

x5 = 133,

. . .

In this case, the sequence is uniquely determined but no formula is apparent. Nonetheless, a simple
device transforms the recurrence into a matrix recurrence to which our diagonalization techniques
apply.

192

Determinants and Diagonalization

The idea is to compute the sequence v0, v1, v2, . . . of columns instead of the numbers
x0, x1, x2, . . . , where

vk =

(cid:20)

xk
xk+1 (cid:21)

for each k

0

≥

Then v0 =

=

is speciﬁed, and the numerical recurrence xk+2 = xk+1 + 6xk transforms

x0
x1

1
1

(cid:21)
into a matrix recurrence as follows:

(cid:20)

(cid:21)

(cid:20)

vk+1 =

xk+1
xk+2 (cid:21)

=

(cid:20)

xk+1
6xk + xk+1 (cid:21)

=

(cid:20)

(cid:20)

0 1
6 1

(cid:21) (cid:20)

xk
xk+1 (cid:21)

= Avk

0 1
6 1

(cid:20)

(cid:21)

where A =

. Thus these columns vk are a linear dynamical system, so Theorem 3.3.7

applies provided the matrix A is diagonalizable.
We have cA(x) = (x

3)(x + 2) so the eigenvalues are λ1 = 3 and λ2 =

2 with corresponding

eigenvectors x1 =

P =

x1 x2

=

(cid:20)

−
1
3
1
3

(cid:20)

(cid:21)
1
−
2

(cid:2)

(cid:3)
Theorem 3.3.7 are given by

and x2 =

1
−
2

(cid:21)

(cid:20)

−
as the reader can check. Since

is invertible, it is a diagonalizing matrix for A. The coefﬁcients bi in

(cid:21)

(cid:20)

b1
b2

(cid:21)

= P−

1v0 =

"

3
5
2
5 #
−

, so that the theorem gives

xk
xk+1 (cid:21)
Equating top entries yields

(cid:20)

= vk = b1λk

1 x1 + b2λk

2 x2 = 3

5 3k

1
3

(cid:20)

(cid:21)

2
5 (
+ −

2)k

−

1
−
2

(cid:21)

(cid:20)

−
This gives x0 = 1 = x1, and it satisﬁes the recurrence xk+2 = xk+1 + 6xk as is easily veriﬁed.
Hence, it is the desired formula for the xk.

−

≥

h

i

xk = 1
5

3k+1

(

2)k+1

for k

0

Returning to Example 3.4.1, these methods give an exact formula and a good approximation for the num-
bers xk in that problem.

Example 3.4.3

In Example 3.4.1, an urban planner wants to determine xk, the number of ways that a row of k
parking spaces can be ﬁlled with cars and trucks if trucks take up two spaces each. Find a formula
for xk and estimate it for large k.

Solution. We saw in Example 3.4.1 that the numbers xk satisfy a linear recurrence

xk+2 = xk + xk+1 for every k

0

≥

3.4. An Application to Linear Recurrences

193

as before, this recurrence becomes a matrix recurrence for the vk:

If we write vk =

xk
xk+1 (cid:21)

(cid:20)

vk+1 =

xk+1
xk + xk+1 (cid:21)

=

(cid:20)

=

(cid:20)

0 1
1 1

(cid:21) (cid:20)

xk
xk+1 (cid:21)

= Avk

(cid:20)

xk+1
xk+2 (cid:21)
0 1
1 1

for all k

0 where A =

. Moreover, A is diagonalizable here. The characteristic

≥

(cid:20)
polynomial is cA(x) = x2
eigenvalues

x

−

−

(cid:21)
1 with roots 1
2

1

±

√5

by the quadratic formula, so A has

(cid:16)
Corresponding eigenvectors are x1 =

h

i
and λ2 = 1
2

(cid:17)
and x2 =

(cid:21)

(cid:20)

(cid:16)

1
λ2

1

√5

−

(cid:17)

λ1 = 1
2

1 + √5

1
λ1
1
1
λ1 λ2

(cid:20)

respectively as the reader can verify.

(cid:21)

As the matrix P =

x1 x2

=

is invertible, it is a diagonalizing matrix for A. We

(cid:20)
compute the coefﬁcients b1 and b2 (in Theorem 3.3.7) as follows:

(cid:21)

(cid:2)

(cid:3)

b1
b2

(cid:20)

(cid:21)

= P−

1v0 = 1
√5
−

λ2
λ1

−

(cid:20)

1
−
1

1
1

(cid:21)

= 1
√5

λ1
λ2

−

(cid:20)

(cid:21)

(cid:21) (cid:20)

where we used the fact that λ1 +λ2 = 1. Thus Theorem 3.3.7 gives

xk
xk+1 (cid:21)

(cid:20)

= vk = b1λk

1 x1 + b2λk

2 x2 = λ1
√5

1
λ1

λk
1

(cid:20)

λ2
√5

λk
2

1
λ2

(cid:20)

(cid:21)

−

(cid:21)

Comparing top entries gives an exact formula for the numbers xk:

xk = 1
√5

λk+1
1 −

λk+1
2

for k

0

≥

h

i

Finally, observe that λ1 is dominant here (in fact, λ1 = 1.618 and λ2 =
places) so λk+1

is negligible compared with λk+1

is large. Thus,

0.618 to three decimal

−

1

2

xk

≈

1
√5

λk+1
1

for each k

0.

≥

This is a good approximation, even for as small a value as k = 12. Indeed, repeated use of the
recurrence xk+2 = xk + xk+1 gives the exact value x12 = 233, while the approximation is
x12

= 232.94.

(1.618)13
√5

≈

The sequence x0, x1, x2, . . . in Example 3.4.3 was ﬁrst discussed in 1202 by Leonardo Pisano of Pisa,
also known as Fibonacci,15 and is now called the Fibonacci sequence. It is completely determined by
the conditions x0 = 1, x1 = 1 and the recurrence xk+2 = xk + xk+1 for each k
0. These numbers have

≥

15Fibonacci was born in Italy. As a young man he travelled to India where he encountered the “Fibonacci” sequence. He
returned to Italy and published this in his book Liber Abaci in 1202. In the book he is the ﬁrst to bring the Hindu decimal
system for representing numbers to Europe.

194

Determinants and Diagonalization

been studied for centuries and have many interesting properties (there is even a journal, the Fibonacci
Quarterly, devoted exclusively to them). For example, biologists have discovered that the arrangement of
λk+1
leaves around the stems of some plants follow a Fibonacci pattern. The formula xk = 1
2
√5
in Example 3.4.3 is called the Binet formula. It is remarkable in that the xk are integers but λ1 and λ2 are
i
not. This phenomenon can occur even if the eigenvalues λi are nonreal complex numbers.

λk+1
1 −

h

We conclude with an example showing that nonlinear recurrences can be very complicated.

Example 3.4.4

Suppose a sequence x0, x1, x2, . . . satisﬁes the following recurrence:

xk+1 =

(cid:26)

1
2xk
if xk is even
3xk + 1 if xk is odd

If x0 = 1, the sequence is 1, 4, 2, 1, 4, 2, 1, . . . and so continues to cycle indeﬁnitely. The same
thing happens if x0 = 7. Then the sequence is

7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1, . . .

and it again cycles. However, it is not known whether every choice of x0 will lead eventually to 1.
It is quite possible that, for some x0, the sequence will continue to produce different values
indeﬁnitely, or will repeat a value and cycle without reaching 1. No one knows for sure.

Exercises for 3.4

Exercise 3.4.1 Solve the following linear recurrences.

b. xk+3 =

2xk+2 + xk+1 + 2xk, where x0 = 1, x1 = 0,

−

and x2 = 1.

a. xk+2 = 3xk + 2xk+1, where x0 = 1 and x1 = 1.

b. xk+2 = 2xk

−

xk+1, where x0 = 1 and x1 = 2.

c. xk+2 = 2xk + xk+1, where x0 = 0 and x1 = 1.

d. xk+2 = 6xk

−

xk+1, where x0 = 1 and x1 = 1.

[Hint: Use vk =

xk
xk+1
xk+2



.]







Exercise 3.4.3 In Example 3.4.1 suppose buses are also
allowed to park, and let xk denote the number of ways a
row of k parking spaces can be ﬁlled with cars, trucks,
and buses.

a. If trucks and buses take up 2 and 3 spaces respec-
tively, show that xk+3 = xk + xk+1 + xk+2 for each
k, and use this recurrence to compute x10. [Hint:
The eigenvalues are of little use.]

Exercise 3.4.2 Solve the following linear recurrences.

b. If buses take up 4 spaces, ﬁnd a recurrence for the

xk and compute x10.

a. xk+3 = 6xk+2
and x2 = 1.

−

11xk+1 + 6xk, where x0 = 1, x1 = 0,

Exercise 3.4.4 A man must climb a ﬂight of k steps.
He always takes one or two steps at a time. Thus he can

climb 3 steps in the following ways: 1, 1, 1; 1, 2; or 2, 1.
Find sk, the number of ways he can climb the ﬂight of k
steps. [Hint: Fibonacci.]

Exercise 3.4.5 How many “words” of k letters can be
if there are no adjacent a’s?
made from the letters

a, b
}
{
Exercise 3.4.6 How many sequences of k ﬂips of a coin
are there with no HH?

Exercise 3.4.7 Find xk, the number of ways to make
a stack of k poker chips if only red, blue, and gold chips
are used and no two gold chips are adjacent. [Hint: Show
that xk+2 = 2xk+1 + 2xk by considering how many stacks
have a red, blue, or gold chip on top.]

Exercise 3.4.8 A nuclear reactor contains α- and β-
particles. In every second each α-particle splits into three
β-particles, and each β-particle splits into an α-particle
and two β-particles. If there is a single α-particle in the
reactor at time t = 0, how many α-particles are there at
t = 20 seconds? [Hint: Let xk and yk denote the number
of α- and β-particles at time t = k seconds. Find xk+1
and yk+1 in terms of xk and yk.]
Exercise 3.4.9 The annual yield of wheat in a certain
country has been found to equal the average of the yield
in the previous two years. If the yields in 1990 and 1991
were 10 and 12 million tons respectively, ﬁnd a formula
for the yield k years after 1990. What is the long-term
average yield?

Exercise 3.4.10 Find the general solution to the recur-
rence xk+1 = rxk + c where r and c are constants. [Hint:
Consider the cases r = 1 and r
= 1,
rn
you will need the identity 1 + r + r2 +
r
for n

= 1 separately. If r

1 = 1
−
1
−

+ rn
−

1.]

· · ·

≥

Exercise 3.4.11 Consider the length 3 recurrence
xk+3 = axk + bxk+1 + cxk+2.

3.4. An Application to Linear Recurrences

195

is a λ-eigenvector.

[Hint: Show directly that Ax = λx.]

c. Generalize (a) and (b) to a recurrence

xk+4 = axk + bxk+1 + cxk+2 + dxk+3

of length 4.

Exercise 3.4.12 Consider the recurrence

xk+2 = axk+1 + bxk + c

where c may not be zero.

a. If a + b

= 1 show that p can be found such that,
if we set yk = xk + p, then yk+2 = ayk+1 + byk.
[Hence, the sequence xk can be found provided yk
can be found by the methods of this section (or
otherwise).]

b. Use (a) to solve xk+2 = xk+1 +6xk +5 where x0 = 1

and x1 = 1.

Exercise 3.4.13 Consider the recurrence

xk+2 = axk+1 + bxk + c(k)

(3.12)

where c(k) is a function of k, and consider the related
recurrence

xk+2 = axk+1 + bxk

(3.13)

Suppose that xk = pk is a particular solution of Equation
3.12.

a. If vk =



xk
xk+1
xk+2

vk+1 = Avk.



and A =





0 1 0
0 0 1
a b c 






b. If λ is any eigenvalue of A, show that x =

1
λ
λ2 






show that

a. If qk is any solution of Equation 3.13, show that

qk + pk is a solution of Equation 3.12.

b. Show that every solution of Equation 3.12 arises
as in (a) as the sum of a solution of Equation 3.13
plus the particular solution pk of Equation 3.12.

6
6
6
196

Determinants and Diagonalization

3.5 An Application to Systems of Differential Equations

A function f of a real variable is said to be differentiable if its derivative exists and, in this case, we let f ′
denote the derivative. If f and g are differentiable functions, a system

f ′ = 3 f + 5g
f + 2g
g′ =

−
is called a system of ﬁrst order differential equations, or a differential system for short. Solving many
practical problems often comes down to ﬁnding sets of functions that satisfy such a system (often in-
volving more than two functions). In this section we show how diagonalization can help. Of course an
acquaintance with calculus is required.

The Exponential Function

The simplest differential system is the following single equation:

f ′ = a f where a is constant

(3.14)

It is easily veriﬁed that f (x) = eax is one solution; in fact, Equation 3.14 is simple enough for us to ﬁnd
all solutions. Suppose that f is any solution, so that f ′(x) = a f (x) for all x. Consider the new function g
given by g(x) = f (x)e−

ax. Then the product rule of differentiation gives

g′(x) = f (x)
=
−
= 0

ae−
−
a f (x)e−
(cid:2)

ax

+ f ′(x)e−
ax + [a f (x)] e−

ax

ax

(cid:3)

ax, that is

for all x. Hence the function g(x) has zero derivative and so must be a constant, say g(x) = c. Thus
c = g(x) = f (x)e−

f (x) = ceax
In other words, every solution f (x) of Equation 3.14 is just a scalar multiple of eax. Since every such
scalar multiple is easily seen to be a solution of Equation 3.14, we have proved

Theorem 3.5.1

Thesetofsolutionsto f ′ = a f is

ceax

{

c anyconstant
}

|

= Reax.

Remarkably, this result together with diagonalization enables us to solve a wide variety of differential
systems.

Example 3.5.1

Assume that the number n(t) of bacteria in a culture at time t has the property that the rate of
change of n is proportional to n itself. If there are n0 bacteria present when t = 0, ﬁnd the number
at time t.

Solution. Let k denote the proportionality constant. The rate of change of n(t) is its time-derivative

3.5. An Application to Systems of Differential Equations

197

n′(t), so the given relationship is n′(t) = kn(t). Thus Theorem 3.5.1 shows that all solutions n are
given by n(t) = cekt , where c is a constant. In this case, the constant c is determined by the
requirement that there be n0 bacteria present when t = 0. Hence n0 = n(0) = cek0 = c, so

gives the number at time t. Of course the constant k depends on the strain of bacteria.

n(t) = n0ekt

The condition that n(0) = n0 in Example 3.5.1 is called an initial condition or a boundary condition

and serves to select one solution from the available solutions.

General Differential Systems

Solving a variety of problems, particularly in science and engineering, comes down to solving a system
of linear differential equations. Diagonalization enters into this as follows. The general problem is to ﬁnd
differentiable functions f1, f2, . . . , fn that satisfy a system of equations of the form

f ′1 = a11 f1 + a12 f2 +
f ′2 = a21 f1 + a22 f2 +
...
f ′n = an1 f1 + an2 f2 +

...

...

+ a1n fn
+ a2n fn
...
+ ann fn

· · ·
· · ·

· · ·

where the ai j are constants. This is called a linear system of differential equations or simply a differen-
tial system. The ﬁrst step is to put it in matrix form. Write



f1
f2
...
fn



f ′1
f ′2
...
f ′n

a11 a12
a21 a22
...
...
an1 an2

· · ·
· · ·

· · ·

a1n
a2n
...
ann








A = 





f′ = 









Then the system can be written compactly using matrix multiplication:






f = 





Hence, given the matrix A, the problem is to ﬁnd a column f of differentiable functions that satisﬁes this
condition. This can be done if A is diagonalizable. Here is an example.

f′ = Af

Example 3.5.2

Find a solution to the system

that satisﬁes f1(0) = 0, f2(0) = 5.

f ′1 = f1 + 3 f2
f ′2 = 2 f1 + 2 f2

198

Determinants and Diagonalization

Solution. This is f′ = Af, where f =

(cid:20)
4)(x + 1), and that x1 =

and A =

(cid:21)

(cid:20)
and x2 =

f1
f2
1
1

(cid:20)

(cid:21)

1 3
2 2
3
2
−

(cid:20)

(cid:21)

. The reader can verify that

(cid:21)

are eigenvectors corresponding to

cA(x) = (x

−
the eigenvalues 4 and

P−

1AP =

4
0

(cid:20)

0
1
−

(cid:21)

given by f = Pg (equivalently, g = P−

(cid:2)

(cid:3)
1f ), where g =

1, respectively. Hence the diagonalization algorithm gives

−
, where P =

x1 x2

=

1
1

(cid:20)

3
. Now consider new functions g1 and g2
2
−
(cid:21)
g1
g2

Then

(cid:20)

(cid:21)

(cid:20)
Hence f ′1 = g′1 + 3g′2 and f ′2 = g′1 −

(cid:20)

(cid:21)

f1
f2

=

1
1

3
2
−

(cid:21) (cid:20)

g1
g2

(cid:21)

2g′2 so that

that is,

f1 = g1 + 3g2
2g2
f2 = g1

−

g′1
g′2 (cid:21)
If this is substituted in f′ = Af, the result is Pg′ = APg, whence

f ′1
f ′2 (cid:21)

3
2
−

f′ =

(cid:21) (cid:20)

1
1

=

(cid:20)

(cid:20)

= Pg′

g′ = P−

1APg

But this means that

(cid:20)
(cid:21)
Hence Theorem 3.5.1 gives g1(x) = ce4x, g2(x) = de−

(cid:21) (cid:20)

(cid:20)

x, where c and d are constants. Finally, then,

g′1
g′2 (cid:21)

=

4
0

0
1
−

g1
g2

,

so

g′1 = 4g1
g2
g′2 =

−

f1(x)
f2(x)

(cid:21)
so the general solution is

(cid:20)

= P

g1(x)
g2(x)

=

(cid:21)

(cid:20)

1
1

3
2
−

(cid:20)

(cid:21) (cid:20)

ce4x
x
de−

=

(cid:21)

(cid:20)

ce4x + 3de−
x
x
ce4x
2de−

−

(cid:21)

x
f1(x) = ce4x + 3de−
x
f2(x) = ce4x
2de−

−

c and d constants

It is worth observing that this can be written in matrix form as

That is,

f1(x)
f2(x)

= c

(cid:21)

(cid:20)

1
1

(cid:21)

(cid:20)

e4x + d

3
2
−

(cid:21)

(cid:20)

x
e−

x
f(x) = cx1e4x + dx2e−

This form of the solution works more generally, as will be shown.
Finally, the requirement that f1(0) = 0 and f2(0) = 5 in this example determines the constants c
and d:

0 = f1(0) = ce0 + 3de0 = c + 3d

3.5. An Application to Systems of Differential Equations

199

5 = f2(0) = ce0

2de0 = c

2d

−

−

These equations give c = 3 and d =

1, so

−

f1(x) = 3e4x
x
3e−
x
f2(x) = 3e4x + 2e−

−

satisfy all the requirements.

The technique in this example works in general.

Theorem 3.5.2

Consideralinearsystem

f′ = Af

ofdifferentialequations,where A isan n
P isgivenintermsofitscolumns

×

n diagonalizablematrix. Let P−

1AP bediagonal,where

x1, x2, . . . , xn
{

and
}
everysolutionfoff′ = Afhastheform

areeigenvectorsofA.Ifxi correspondstotheeigenvalueλi foreachi,then

P = [x1, x2,

, xn]

· · ·

f(x) = c1x1eλ1x + c2x2eλ2x +

+ cnxneλnx

· · ·

where c1, c2, . . . , cn arearbitraryconstants.

Proof. By Theorem 3.3.4, the matrix P =

x1 x2

. . . xn

is invertible and

(cid:2)

P−

1AP = 





0
λ1
0 λ2
...
...
0
0

(cid:3)
· · ·
· · ·

· · ·

0
0
...
λn








As in Example 3.5.2, write f = 





, this gives

P =

pi j

(cid:2)

(cid:3)

f1
f2
...
fn








and deﬁne g = 





fi = pi1g1 + pi2g2 +

· · ·

g1
g2
...
gn





+ pingn



by g = P−

1f; equivalently, f = Pg. If

Since the pi j are constants, differentiation preserves this relationship:

f ′i = pi1g′1 + pi2g′2 +

+ ping′n

· · ·

so f′ = Pg′. Substituting this into f′ = Af gives Pg′ = APg. But then left multiplication by P−

1 gives

200

Determinants and Diagonalization

g′ = P−

1APg, so the original system of equations f′ = Af for f becomes much simpler in terms of g:



= 




Hence g′i = λigi holds for each i, and Theorem 3.5.1 implies that the only solutions are


























· · ·









0
λ1
0 λ2
...
...
0
0

· · ·
· · ·

0
0
...
λn

g1
g2
...
gn

g′1
g′2
...
g′n

Then the relationship f = Pg gives the functions f1, f2, . . . , fn as follows:

gi(x) = cieλix

ci some constant

f(x) = [x1, x2,

This is what we wanted.

c1eλ1x
c2eλ2x
...
cneλnx








· · ·

, xn] 





= c1x1eλ1x + c2x2eλ2x +

+ cnxneλnx

· · ·

The theorem shows that every solution to f′ = Af is a linear combination

f(x) = c1x1eλ1x + c2x2eλ2x +

+ cnxneλnx

· · ·

where the coefﬁcients ci are arbitrary. Hence this is called the general solution to the system of differential
equations. In most cases the solution functions fi(x) are required to satisfy boundary conditions, often of
the form fi(a) = bi, where a, b1, . . . , bn are prescribed numbers. These conditions determine the constants
ci. The following example illustrates this and displays a situation where one eigenvalue has multiplicity
greater than 1.

Example 3.5.3

Find the general solution to the system

f ′1 = 5 f1 + 8 f2 + 16 f3
f ′2 = 4 f1 + f2 + 8 f3
11 f3
f ′3 =

4 f2

4 f1

−

−

−

Then ﬁnd a solution satisfying the boundary conditions f1(0) = f2(0) = f3(0) = 1.

Solution. The system has the form f′ = Af, where A =

. In this case

16
8
11 
1) and eigenvectors corresponding to the eigenvalues


8
1
4
−

5
4
4
−

−





cA(x) = (x + 3)2(x
respectively,

−

3,

3, and 1 are,

−

−

x1 =

1
−
1
0 






x2 =

2
−
0
1 






x3 =

2
1
1 


−





3.5. An Application to Systems of Differential Equations

201

Hence, by Theorem 3.5.2, the general solution is

The boundary conditions f1(0) = f2(0) = f3(0) = 1 determine the constants ci.

f(x) = c1



e−

3x + c2



e−

3x + c3



ex,

ci constants.

2
1
1 


−



1
−
1
0 




1
1
1 






2
−
0
1 




1
−
1
0 

2
−
0
1


1
−
1
0

=





= f(0) = c1



+ c2



2
−
0
1 

c1
c2
c3



+ c3

2
1
1 


−












2
1
1 


−

The solution is c1 =

3, c2 = 5, c3 = 4, so the required speciﬁc solution is

−

f1(x) =
7e−
3e−
f2(x) =
f3(x) = 5e−

−
−

3x + 8ex
3x + 4ex
4ex
3x

−

Exercises for 3.5

Exercise 3.5.1 Use Theorem 3.5.1 to ﬁnd the general
solution to each of the following systems. Then ﬁnd a
speciﬁc solution satisfying the given boundary condition.

Exercise 3.5.3 A radioactive element decays at a rate
proportional to the amount present. Suppose an initial
mass of 10 g decays to 8 g in 3 hours.

a.

b.

c.

d.

f ′1 = 2 f1 + 4 f2, f1(0) = 0
f ′2 = 3 f1 + 3 f2, f2(0) = 1

f ′1 =
f ′2 = f1 + 3 f2, f2(0) =

f1 + 5 f2, f1(0) = 1
1

−

−

4 f2 + 4 f3
2 f3
f1 + f2 + 4 f3

f ′1 =
f ′2 = f1 + f2
f ′3 =
f1(0) = f2(0) = f3(0) = 1

−

−

f ′1 = 2 f1+ f2+ 2 f3
f ′2 = 2 f1+ 2 f2
2 f3
f ′3 = 3 f1+ f2+ f3
f1(0) = f2(0) = f3(0) = 1

−

a. Find the mass t hours later.

b. Find the half-life of the element—the time taken

to decay to half its mass.

Exercise 3.5.4 The population N(t) of a region at time
t increases at a rate proportional to the population.
If
the population doubles every 5 years and is 3 million ini-
tially, ﬁnd N(t).

Exercise 3.5.5 Let A be an invertible diagonalizable
n matrix and let b be an n-column of constant func-
n
tions. We can solve the system f′ = Af + b as follows:

×

a. If g satisﬁes g′ = Ag (using Theorem 3.5.2), show

that f = g

A−

−

1b is a solution to f′ = Af + b.

Exercise 3.5.2 Show that the solution to f ′ = a f satis-
fying f (x0) = k is f (x) = kea(x
−

x0).

b. Show that every solution to f′ = Af + b arises as in

(a) for some solution g to g′ = Ag.

202

Determinants and Diagonalization

Exercise 3.5.6 Denote the second derivative of f by
f ′′ = ( f ′)′. Consider the second order differential equa-
tion

a. Show that

f1
f2
f3





is a solution to the system

f ′′

a1 f ′

a2 f = 0,

a1 and a2 real numbers (3.15)

−

−
a. If f is a solution to Equation 3.15 let f1 = f and

a1 f . Show that

f2 = f ′ −
f ′1 = a1 f1 + f2
f ′2 = a2 f1
f ′1
f ′2 (cid:21)

(cid:26)
that is

=

(cid:20)

,

a1 1
a2 0

f1
f2 (cid:21)

(cid:21) (cid:20)

(cid:20)
f1
f2 (cid:21)

(cid:20)

b. Conversely, if

is a solution to the system in

(a), show that f1 is a solution to Equation 3.15.



,







that is

f ′1 = a1 f1 + f2
f ′2 = a2 f1 + f3
f ′3 = a3 f1
f ′1
f ′2
f ′3





=





a1 1 0
a2 0 1
a3 0 0

f1
f2
f3

















b. Show further that if

f1
f2
f3
system, then f = f1 is a solution to Equation 3.15.

is any solution to this









Exercise 3.5.7 Writing f ′′′ = ( f ′′)′, consider the third
order differential equation

f ′′′

−

a1 f ′′

−

a2 f ′

−

a3 f = 0

where a1, a2, and a3 are real numbers. Let
a1 f ′ −
f1 = f , f2 = f ′ −

a1 f and f3 = f ′′ −

a2 f ′′.

Remark. A similar construction casts every linear differ-
ential equation of order n (with constant coefﬁcients) as
an n
n linear system of ﬁrst order equations. However,
the matrix need not be diagonalizable, so other methods
have been developed.

×

3.6 Proof of the Cofactor Expansion Theorem

Recall that our deﬁnition of the term determinant is inductive: The determinant of any 1
deﬁned ﬁrst; then it is used to deﬁne the determinants of 2
case, and so on. The case of a 1

1 matrix [a] poses no problem. We simply deﬁne

2 matrices. Then that is used for the 3

1 matrix is
3

×

×

×

×

det [a] = a

as in Section 3.1. Given an n
(n
deleting row i and column j. Now assume that the determinant of any (n
deﬁned. Then the determinant of A is deﬁned to be

n matrix A, deﬁne Ai j to be the (n

1)

−

×

×

−
−

1) matrix obtained from A by
1) matrix has been
1)

(n

×

−

a21 det A21 +

det A = a11 det A11
n
∑
(
i=1

−

=

−
1)i+1ai1 det Ai1

+ (

· · ·

1)n+1an1 det An1

−

where summation notation has been introduced for convenience.16 Observe that, in the terminology of
1)i+ j det Ai j is
Section 3.1, this is just the cofactor expansion of det A along the ﬁrst column, and that (
the (i, j)-cofactor (previously denoted as ci j(A)).17 To illustrate the deﬁnition, consider the 2
2 matrix

−

×

∑4

16Summation notation is a convenient shorthand way to write sums of similar expressions. For example a1 + a2 + a3 + a4 =
i=1 ai, a5b5 + a6b6 + a7b7 + a8b8 = ∑8
17Note that we used the expansion along row 1 at the beginning of Section 3.1. The column 1 expansion deﬁnition is more

k=5 akbk, and 12 + 22 + 32 + 42 + 52 = ∑5

j=1 j2.

convenient here.

A =

(cid:20)

a11 a12
a21 a22

(cid:21)

. Then the deﬁnition gives

det

a11 a12
a21 a22

= a11 det [a22]

(cid:20)
and this is the same as the deﬁnition in Section 3.1.

(cid:21)

3.6. Proof of the Cofactor Expansion Theorem

203

a21 det [a12] = a11a22

a21a12

−

−

Of course, the task now is to use this deﬁnition to prove that the cofactor expansion along any row
or column yields det A (this is Theorem 3.1.1). The proof proceeds by ﬁrst establishing the properties of
determinants stated in Theorem 3.1.2 but for rows only (see Lemma 3.6.2). This being done, the full proof
of Theorem 3.1.1 is not difﬁcult. The proof of Lemma 3.6.2 requires the following preliminary result.

Lemma 3.6.1

Let A, B,andC be n
pthrowsof B andC. Then

×

n matricesthatareidenticalexceptthatthe pthrowof A isthesumofthe

det A = det B + det C

Proof. We proceed by induction on n, the cases n = 1 and n = 2 being easily checked. Consider ai1 and
Ai1:

Case 1: If i

= p,

ai1 = bi1 = ci1

and

det Ai1 = det Bi1 = det Ci1

by induction because Ai1, Bi1, Ci1 are identical except that one row of Ai1 is the sum of the corresponding
rows of Bi1 and Ci1.
Case 2: If i = p,

ap1 = bp1 + cp1

and

Ap1 = Bp1 = Cp1

Now write out the deﬁning sum for det A, splitting off the pth term for special attention.

det A = ∑
i
=p
= ∑
=p
i

ai1(

ai1(

1)i+1 det Ai1 + ap1(

1)p+1 det Ap1

−
1)i+1 [ det Bi1 + det Bi1] + (bp1 + cp1)(

−

−

1)p+1 det Ap1

−

where det Ai1 = det Bi1 + det Ci1 by induction. But the terms here involving Bi1 and bp1 add up to det B
because ai1 = bi1 if i
= p and Ap1 = Bp1. Similarly, the terms involving Ci1 and cp1 add up to det C. Hence
det A = det B + det C, as required.

Lemma 3.6.2

Let A =

ai j

denotean n

n matrix.

×

1. If B =
(cid:3)

(cid:2)

bi j

isformedfrom A bymultiplyingarowof A byanumberu,then det B = u det A.

2. If A containsarowofzeros,then det A = 0.

(cid:2)

(cid:3)

3. If B =

bi j

isformedbyinterchangingtworowsof A,then det B =

det A.

−

4. If A containstwoidenticalrows,then det A = 0.

(cid:3)

(cid:2)

6
6
6
6
204

Determinants and Diagonalization

5. If B =

bi j

isformedbyaddingamultipleofonerowof A toadifferentrow,then

det B = det A.
(cid:2)

(cid:3)

Proof. For later reference the deﬁning sums for det A and det B are as follows:

det A =

det B =

n
∑
i=1
n
∑
i=1

ai1(

bi1(

1)i+1 det Ai1

−

1)i+1 det Bi1

−

(3.16)

(3.17)

Property 1. The proof is by induction on n, the cases n = 1 and n = 2 being easily veriﬁed. Consider

the ith term in the sum 3.17 for det B where B is the result of multiplying row p of A by u.

a. If i

= p, then bi1 = ai1 and det Bi1 = u det Ai1 by induction because Bi1 comes from Ai1 by multi-

plying a row by u.

b. If i = p, then bp1 = uap1 and Bp1 = Ap1.

In either case, each term in Equation 3.17 is u times the corresponding term in Equation 3.16, so it is clear
that det B = u det A.

Property 2. This is clear by property 1 because the row of zeros has a common factor u = 0.
Property 3. Observe ﬁrst that it sufﬁces to prove property 3 for interchanges of adjacent rows. (Rows
p and q (q > p) can be interchanged by carrying out 2(q
1 adjacent changes, which results in an
odd number of sign changes in the determinant.) So suppose that rows p and p + 1 of A are interchanged
to obtain B. Again consider the ith term in Equation 3.17.

p)

−

−

a. If i

= p and i

= p + 1, then bi1 = ai1 and det Bi1 =

det Ai1 by induction because Bi1 results from
interchanging adjacent rows in Ai1. Hence the ith term in Equation 3.17 is the negative of the ith
det A in this case.
term in Equation 3.16. Hence det B =

−

−

b. If i = p or i = p + 1, then bp1 = ap+1, 1 and Bp1 = Ap+1, 1, whereas bp+1, 1 = ap1 and Bp+1, 1 = Ap1.

Hence terms p and p + 1 in Equation 3.17 are

1)p+1 det Bp1 =

ap+1, 1(

1)(p+1)+1 det (Ap+1, 1)

bp1(

−
bp+1, 1(

−

−
1)(p+1)+1 det Bp+1, 1 =

−

ap1(

1)p+1 det (Ap1)

−

−

This means that terms p and p + 1 in Equation 3.17 are the same as these terms in Equation 3.16,
except that the order is reversed and the signs are changed. Thus the sum 3.17 is the negative of the sum
3.16; that is, det B =

det A.

Property 4. If rows p and q in A are identical, let B be obtained from A by interchanging these rows.
det A. This implies that

det A by property 3 so det A =

Then B = A so det A = det B. But det B =
det A = 0.

−

−

−

Property 5. Suppose B results from adding u times row q of A to row p. Then Lemma 3.6.1 applies to
B to show that det B = det A + det C, where C is obtained from A by replacing row p by u times row q. It
now follows from properties 1 and 4 that det C = 0 so det B = det A, as asserted.

6
6
6
3.6. Proof of the Cofactor Expansion Theorem

205

These facts are enough to enable us to prove Theorem 3.1.1. For convenience, it is restated here in the
notation of the foregoing lemmas. The only difference between the notations is that the (i, j)-cofactor of
an n

n matrix A was denoted earlier by

×

ci j(A) = (

1)i+ j det Ai j

−

Theorem 3.6.1

If A =

ai j

isan n

n matrix,then

(cid:3)
1. det A = ∑n

(cid:2)

×
i=1 ai j(

1)i+ j det Ai j

−

(cofactorexpansionalongcolumn j).

2. det A = ∑n

j=1 ai j(

1)i+ j det Ai j

−

(cofactorexpansionalongrow i).

Here Ai j denotesthematrixobtainedfrom A bydeletingrow i andcolumn j.

Proof. Lemma 3.6.2 establishes the truth of Theorem 3.1.2 for rows. With this information, the arguments
in Section 3.2 proceed exactly as written to establish that det A = det AT holds for any n
n matrix A.
Now suppose B is obtained from A by interchanging two columns. Then BT is obtained from AT by
interchanging two rows so, by property 3 of Lemma 3.6.2,

×

Hence property 3 of Lemma 3.6.2 holds for columns too.

det B = det BT =

det AT =

det A

−

−

This enables us to prove the cofactor expansion for columns. Given an n

be obtained by moving column j to the left side, using j

, let
1 interchanges of adjacent columns.

n matrix A =

ai j

×

(cid:2)

(cid:3)

−

1 det A and, because Bi1 = Ai j and bi1 = ai j for all i, we obtain
−

B =
bi j
Then det B = (
(cid:3)

(cid:2)

1) j

−

det A = (

1) j

1 det B = (
−

1) j

−

1
−

n
∑
i=1

bi1(

1)i+1 det Bi1

−

−
n
∑
i=1

=

ai j(

1)i+ j det Ai j

−

This is the cofactor expansion of det A along column j.

Finally, to prove the row expansion, write B = AT . Then Bi j = (AT

i j) and bi j = a ji for all i and j.

Expanding det B along column j gives

det A = det AT = det B =

n
∑
i=1

bi j(

1)i+ j det Bi j

−

=

n
∑
i=1

a ji(

1) j+i det

−

(AT
ji)

=

(cid:2)
This is the required expansion of det A along row j.

(cid:3)

n
∑
i=1

a ji(

1) j+i det A ji

−

206

Determinants and Diagonalization

Exercises for 3.6

Exercise 3.6.1 Prove Lemma 3.6.1 for columns.

Exercise 3.6.2 Verify that interchanging rows p and q
(q > p) can be accomplished using 2(q
1 adjacent
interchanges.

p)

−

−

Exercise 3.6.3 If u is a number and A is an n
n matrix,
prove that det (uA) = un det A by induction on n, using
only the deﬁnition of det A.

×

Supplementary Exercises for Chapter 3

det

Exercise 3.1 Show that
a + px b + qx
q + vx
p + ux
v + bx w + cx 
u + ax


c + rx
r + wx





= (1+x3) det

a
p
u





c
b
q
r
v w 


Exercise 3.2

a. Show that (Ai j)T = (AT ) ji for all i,

j, and all

square matrices A.

b. Use (a) to prove that det AT = det A. [Hint: In-

duction on n where A is n

n.]

×

Exercise 3.3 Show that det

1 and m

n

≥

1.

≥

Exercise 3.4 Show that

0
Im

In
0

(cid:20)

= (

−

(cid:21)

1)nm for all

1 a a3
1 b b3
1 c

c3 


det





= (b

a)(c

−

−

a)(c

−

b)(a + b + c)

Exercise 3.5 Let A =

R1
R2 (cid:21)

(cid:20)

be a 2

×

2 matrix with

rows R1 and R2. If det A = 5, ﬁnd det B where

B =

(cid:20)

(cid:20)

Exercise 3.6 Let A =

each k

0.

≥

3R1 + 2R3
2R1 + 5R2 (cid:21)

3
2

4
3

−
−

(cid:21)

and let vk = Akv0 for

a. Show that A has no dominant eigenvalue.

b. Find vk if v0 equals:

i.

ii.

iii.

1
1

2
1

x
y

(cid:20)

(cid:20)

(cid:20)

(cid:21)

(cid:21)

=

(cid:21)

(cid:20)

1
1

or

(cid:21)

(cid:20)

2
1

(cid:21)

6
Chapter 4

Vector Geometry

4.1 Vectors and Lines

In this chapter we study the geometry of 3-dimensional space. We view a point in 3-space as an arrow from
the origin to that point. Doing so provides a “picture” of the point that is truly worth a thousand words.
We used this idea earlier, in Section 2.6, to describe rotations, reﬂections, and projections of the plane R2.
We now apply the same techniques to 3-space to examine similar transformations of R3. Moreover, the
method enables us to completely describe all lines and planes in space.

Vectors in R3

Introduce a coordinate system in 3-dimensional space in the usual way. First choose a point O called the
origin, then choose three mutually perpendicular lines through O, called the x, y, and z axes, and establish
a number scale on each axis with zero at the origin. Given a point P in 3-space we associate three numbers
x, y, and z with P, as described in Figure 4.1.1. These numbers are called the coordinates of P, and we
denote the point as (x, y, z), or P(x, y, z) to emphasize the label P. The result is called a cartesian1
coordinate system for 3-space, and the resulting description of 3-space is called cartesian geometry.

As in the plane, we introduce vectors by identifying each point

z

O

x

P(x, y, z)

v =





x
y
z 

y

P0(x, y, 0)



P(x, y, z) with the vector v =

in R3, represented by the arrow

x
y
z 
from the origin to P as in Figure 4.1.1. Informally, we say that the point P

has vector v, and that vector v has point P. In this way 3-space is identi-
ﬁed with R3, and this identiﬁcation will be made throughout this chapter,
often without comment. In particular, the terms “vector” and “point” are
interchangeable.2 The resulting description of 3-space is called vector



Figure 4.1.1

geometry. Note that the origin is 0 =

Length and Direction

0
0
.
0 






We are going to discuss two fundamental geometric properties of vectors in R3: length and direction. First,
if v is a vector with point P, the length
of vector v is deﬁned to be the distance from the origin to P,
that is the length of the arrow representing v. The following properties of length will be used frequently.

k

v

k

1Named after René Descartes who introduced the idea in 1637.
2Recall that we deﬁned Rn as the set of all ordered n-tuples of real numbers, and reserved the right to denote them as rows

or as columns.

207

208

Vector Geometry

Theorem 4.1.1

beavector.

x
y
z 


=

x2 + y2 + z2. 3

Letv=




v
k
k

1.

p
2. v= 0ifandonlyif

= 0

v
k
k
forallscalars a. 4

3.

av
k

k

=

a

v
k

|k

|

z

v

h

P

z

Q

O
x

y

R

i

x

Figure 4.1.2

y

Proof. Let v have point P(x, y, z).

v

1. In Figure 4.1.2,

is the hypotenuse of the right triangle OQP, and
2 = h2 + z2 by Pythagoras’ theorem.5 But h is the hypotenuse
so
of the right triangle ORQ, so h2 = x2 + y2. Now (1) follows by
eliminating h2 and taking positive square roots.

v
k

k

k

k

2. If

k

v

k

= 0, then x2 + y2 + z2 = 0 by (1). Because squares of real
numbers are nonnegative, it follows that x = y = z = 0, and hence
that v = 0. The converse is because

= 0.

0

3. We have av =

ax ay az

T

k

k
so (1) gives

Hence

av

= √a2

v

, and we are done because √a2 =

k
k
Of course the R2-version of Theorem 4.1.1 also holds.

k

k

for any real number a.

a

|

|

2 = (ax)2 + (ay)2 + (az)2 = a2

(cid:3)

(cid:2)
av
k
k

2

v

k

k

3When we write √p we mean the positive square root of p.
4Recall that the absolute value

of a real number is deﬁned by

a

|

|

a if a
0
≥
a if a < 0

.

=

a

|

|

(cid:26)

−

5Pythagoras’ theorem states that if a and b are sides of right triangle with hypotenuse c, then a2 + b2 = c2. A proof is given

at the end of this section.

Example 4.1.1

4.1. Vectors and Lines

209

2
1
3 
−
= √9 + 16 = 5.


then





If v =

v

k

k

= √4 + 1 + 9 = √14. Similarly if v =

v

k

k

3
4
−

(cid:20)

(cid:21)

in 2-space then

When we view two nonzero vectors as arrows emanating from the origin, it is clear geometrically
what we mean by saying that they have the same or opposite direction. This leads to a fundamental new
description of vectors.

Theorem 4.1.2

= 0andw

Letv
directionandthesamelength.6

= 0bevectorsin R3. Thenv= wasmatricesifandonlyifvandwhavethesame

z

O

v

P

w

Q

y

Proof. If v = w, they clearly have the same direction and length. Conversely,
let v and w be vectors with points P(x, y, z) and Q(x1, y1, z1) respectively. If
v and w have the same length and direction then, geometrically, P and Q must
be the same point (see Figure 4.1.3). Hence x = x1, y = y1, and z = z1, that is

x

Figure 4.1.3

v =



=



x1
y1
z1

= w.



x
y
z 








A characterization of a vector in terms of its length and direction only is called an intrinsic description
of the vector. The point to note is that such a description does not depend on the choice of coordinate
system in R3. Such descriptions are important in applications because physical laws are often stated in
terms of vectors, and these laws cannot depend on the particular coordinate system used to describe the
situation.

Geometric Vectors

If A and B are distinct points in space, the arrow from A to B has length and direction.

z

−→AB

O

B

y

A

x

Figure 4.1.4

6It is Theorem 4.1.2 that gives vectors their power in science and engineering because many physical quantities are deter-
mined by their length and magnitude (and are called vector quantities). For example, saying that an airplane is ﬂying at 200
km/h does not describe where it is going; the direction must also be speciﬁed. The speed and direction comprise the velocity
of the airplane, a vector quantity.

6
6
210

Vector Geometry

Hence:

Deﬁnition 4.1 Geometric Vectors
Supposethat A and B areanytwopointsin R3. InFigure4.1.4thelinesegmentfrom A to B is
denoted −→AB andiscalledthegeometricvectorfrom A to B. Point A iscalledthetail of −→AB, B is
calledthetipof −→AB,andthelengthof −→AB isdenoted

−→AB

.
k

k

y

O

B(2, 3)

Q(0, 2)

P(1, 0)

A(3, 1)

x

Figure 4.1.5

k

−→AB
k

Note that if v is any vector in R3 with point P then v = −→OP is itself
a geometric vector where O is the origin. Referring to −→AB as a “vector”
seems justiﬁed by Theorem 4.1.2 because it has a direction (from A to B)
and a length
. However there appears to be a problem because two
geometric vectors can have the same length and direction even if the tips
and tails are different. For example −→AB and −→PQ in Figure 4.1.5 have the
same length √5 and the same direction (1 unit left and 2 units up) so, by
Theorem 4.1.2, they are the same vector! The best way to understand this
apparent paradox is to see −→AB and −→PQ as different representations of the
same7 underlying vector

. Once it is clariﬁed, this phenomenon is

1
−
2

(cid:20)

(cid:21)

a great beneﬁt because, thanks to Theorem 4.1.2, it means that the same
geometric vector can be positioned anywhere in space; what is important is the length and direction, not
the location of the tip and tail. This ability to move geometric vectors about is very useful as we shall soon
see.

The Parallelogram Law

P

v
w

A

P
v + w

Q

Figure 4.1.6

We now give an intrinsic description of the sum of two vectors v and w in R3,
that is a description that depends only on the lengths and directions of v and w
and not on the choice of coordinate system. Using Theorem 4.1.2 we can think
of these vectors as having a common tail A. If their tips are P and Q respectively,
containing A, P, and Q, as shown in Figure 4.1.6.
then they both lie in a plane
The vectors v and w create a parallelogram8 in
, shaded in Figure 4.1.6, called
the parallelogram determined by v and w.

P

P

If we now choose a coordinate system in the plane

with A as origin, then the parallelogram law in
the plane (Section 2.6) shows that their sum v + w is the diagonal of the parallelogram they determine with
tail A. This is an intrinsic description of the sum v + w because it makes no reference to coordinates. This
discussion proves:

P

The Parallelogram Law

Intheparallelogramdeterminedbytwovectorsvandw,thevectorv+ wisthediagonalwiththe
sametailasvandw.

7Fractions provide another example of quantities that can be the same but look different. For example 6

appear different, but they are equal fractions—both equal 2

3 in “lowest terms”.

8Recall that a parallelogram is a four-sided ﬁgure whose opposite sides are parallel and of equal length.

9 and 14

21 certainly

4.1. Vectors and Lines

211

Because a vector can be positioned with its tail at any point, the parallelo-
gram law leads to another way to view vector addition. In Figure 4.1.7(a) the
sum v + w of two vectors v and w is shown as given by the parallelogram law. If
w is moved so its tail coincides with the tip of v (Figure 4.1.7(b)) then the sum
v + w is seen as “ﬁrst v and then w. Similarly, moving the tail of v to the tip of w
shows in Figure 4.1.7(c) that v + w is “ﬁrst w and then v.” This will be referred
to as the tip-to-tail rule, and it gives a graphic illustration of why v +w = w +v.
Since −→AB denotes the vector from a point A to a point B, the tip-to-tail rule

takes the easily remembered form

−→AB + −→BC = −→AC

for any points A, B, and C. The next example uses this to derive a theorem in
geometry without using coordinates.

v

v

P
(a)

(b)

v + w
w

w

v + w

w + v

v

(c)

w

Figure 4.1.7

Example 4.1.2

Show that the diagonals of a parallelogram bisect each other.

A

B

EM

D

C

Solution. Let the parallelogram have vertices A, B, C, and D,
as shown; let E denote the intersection of the two diagonals;
and let M denote the midpoint of diagonal AC. We must show
that M = E and that this is the midpoint of diagonal BD. This
is accomplished by showing that −→BM = −−→MD. (Then the fact
that these vectors have the same direction means that M = E,
and the fact that they have the same length means that M = E
is the midpoint of BD.) Now −→AM = −→MC because M is the midpoint
of AC, and −→BA = −→CD because the ﬁgure is a parallelogram. Hence

−→BM = −→BA + −→AM = −→CD + −→MC = −→MC + −→CD = −−→MD

where the ﬁrst and last equalities use the tip-to-tail rule of vector addition.

w

u

v

u + v + w

w

u

v

Figure 4.1.8

One reason for the importance of the tip-to-tail rule is that it means two
or more vectors can be added by placing them tip-to-tail in sequence. This
gives a useful “picture” of the sum of several vectors, and is illustrated for
three vectors in Figure 4.1.8 where u + v + w is viewed as ﬁrst u, then v,
then w.

−

There is a simple geometrical way to visualize the (matrix) difference
v
w of two vectors. If v and w are positioned so that they have a common
tail A (see Figure 4.1.9), and if B and C are their respective tips, then the
w = −→CB is the vector from the
tip-to-tail rule gives w + −→CB = v. Hence v
w and v + w appear as diagonals in the parallelogram determined by

−

tip of w to the tip of v. Thus both v
v and w (see Figure 4.1.9). We record this for reference.

−

212

Vector Geometry

−→CB

C

w

v

−

v + w

A

v

v

B

w

w

Figure 4.1.9

Theorem 4.1.3

Ifvandwhaveacommontail,thenv
ofwtothetipofv.

−

wisthevectorfromthetip

One of the most useful applications of vector subtraction is that it gives
a simple formula for the vector from one point to another, and for the
distance between the points.

Theorem 4.1.4

Let P1(x1, y1, z1) and P2(x2, y2, z2) betwopoints. Then:

1. −→P1P2 =



x2
y2
z2

x1
y1
z1

−
−
−

.






2. Thedistancebetween P1 and P2 is

(x2

−

x1)2 + (y2

y1)2 + (z2

z1)2.

−

−

O

v1

P1

v2

−−→P1P2

P2

Figure 4.1.10

p

Proof. If O is the origin, write

v1 = −→OP1 =

as in Figure 4.1.10.

x1
y1
z1









and v2 = −→OP2 =

x2
y2
z2









Then Theorem 4.1.3 gives −→P1P2 = v2

distance between P1 and P2 is
rem 4.1.1.

k

−→P1P2

k

v1, and (1) follows. But the
, so (2) follows from (1) and Theo-

−

Of course the R2-version of Theorem 4.1.4 is also valid: If P1(x1, y1) and P2(x2, y2) are points in R2,

then −→P1P2 =

x2
y2

x1
y1

−
−

(cid:20)

Example 4.1.3

, and the distance between P1 and P2 is
(cid:21)

p

(x2

−

x1)2 + (y2

y1)2.

−

The distance between P1(2,

1, 3) and P2(1, 1, 4) is

from P1 to P2 is −→P1P2 =





−
1
−
2
.
1 


1)2 + (2)2 + (1)2 = √6, and the vector

−

(

p

As for the parallelogram law, the intrinsic rule for ﬁnding the length and direction of a scalar multiple

of a vector in R3 follows easily from the same situation in R2.

4.1. Vectors and Lines

213

Scalar Multiple Law

Ifaisarealnumberandv

= 0isavectorthen:

1. Thelengthof avis

av
k

k

=

a

v
.
k

|k

|

2. If9av

= 0,thedirectionof avis

thesameasvif a > 0,
oppositetovif a < 0.

(cid:26)

Proof.

1. This is part of Theorem 4.1.1.

2. Let O denote the origin in R3, let v have point P, and choose any plane containing O and P. If we
set up a coordinate system in this plane with O as origin, then v = −→OP so the result in (2) follows
from the scalar multiple law in the plane (Section 2.6).

Figure 4.1.11 gives several examples of scalar multiples of a vector v.

v

2v

1
2 v

2)v

(

−

1
2 )v

(

−

Figure 4.1.11

Consider a line L through the origin, let P be any point on L other than
the origin O, and let p = −→OP. If t
= 0, then tp is a point on L because it
has direction the same or opposite as that of p. Moreover t > 0 or t < 0
according as the point tp lies on the same or opposite side of the origin as
P. This is illustrated in Figure 4.1.12.

L

P
p

3
2 p

O

1
2 p

1
2 p

−

Figure 4.1.12

Example 4.1.4

If v

= 0 show that 1
v
k
k

A vector u is called a unit vector if

= 1. Then i =

u
k

k

j =

0
0
1 
We discuss them in more detail in Section 4.2.


0
1
0 


, and k =









are unit vectors, called the coordinate vectors.

1
0
,
0 






v is the unique unit vector in the same direction as v.

Solution. The vectors in the same direction as v are the scalar multiples av where a > 0. But
av
k

when a > 0, so av is a unit vector if and only if a = 1
v
k
k

v
k

= a

|k

=

k

v

k

k

a

|

.

The next example shows how to ﬁnd the coordinates of a point on the line segment between two given

points. The technique is important and will be used again below.

9Since the zero vector has no direction, we deal only with the case av

= 0.

6
6
6
6
6
214

Vector Geometry

Example 4.1.5

Let p1 and p2 be the vectors of two points P1 and P2. If M is the point one third the way from P1 to
P2, show that the vector m of M is given by

Conclude that if P1 = P1(x1, y1, z1) and P2 = P2(x2, y2, z2), then M has coordinates

m = 2

3p1 + 1

3p2

2
3 x1 + 1

3 x2, 2

3 y1 + 1

3 y2, 2

3 z1 + 1

3z2

M = M

(cid:0)

(cid:1)

Solution. The vectors p1, p2, and m are shown in the diagram. We
have −−→P1M = 1
1
3 as long. By Theorem 4.1.3 we have −→P1P2 = p2 −
addition gives

3−→P1P2 because −−→P1M is in the same direction as −→P1P2 and
p1, so tip-to-tail

m = p1 + −−→P1M = p1 + 1

3 (p2 −

p1) = 2

3 p1 + 1

3p2

P1

M

p1

m

O

p2

P2

as required. For the coordinates, we have p1 =

so

m = 2

3 



x1
y1
z1

x2
y2
z2

+ 1

3 







by matrix addition. The last statement follows.

3x1 + 1
2
3y1 + 1
2
3 z1 + 1
2

3x2
3y2
3z2












= 





x1
y1
z1









and p2 =

x2
y2
z2





,




Note that in Example 4.1.5 m = 2
because m is closer to p1.

3p1 + 1

3p2 is a “weighted average” of p1 and p2 with more weight on p1

The point M halfway between points P1 and P2 is called the midpoint between these points. In the

same way, the vector m of M is

m = 1

2p1 + 1

2 p2 = 1

2(p1 + p2)

as the reader can verify, so m is the “average” of p1 and p2 in this case.

Example 4.1.6

Show that the midpoints of the four sides of any quadrilateral are the vertices of a parallelogram.
Here a quadrilateral is any ﬁgure with four vertices and straight sides.

Solution. Suppose that the vertices of the quadrilateral are A, B, C, and D (in that order) and that
E, F, G, and H are the midpoints of the sides as shown in the diagram. It sufﬁces to show
−→EF = −→HG (because then sides EF and HG are parallel and of equal length).

4.1. Vectors and Lines

215

Now the fact that E is the midpoint of AB means that −→EB = 1
Similarly, −→BF = 1

2−→BC, so

2 −→AB.

−→EF = −→EB + −→BF = 1

2−→AB + 1

2−→BC = 1

2(−→AB + −→BC) = 1

2−→AC

A similar argument shows that −→HG = 1
as required.

2−→AC too, so −→EF = −→HG

B

E

A

F

C

G

H

D

Deﬁnition 4.2 Parallel Vectors in R3
Twononzerovectorsarecalledparalleliftheyhavethesameoroppositedirection.

Many geometrical propositions involve this notion, so the following theorem will be referred to repeat-

edly.

Theorem 4.1.5

Twononzerovectorsvandwareparallelifandonlyifoneisascalarmultipleoftheother.

Proof. If one of them is a scalar multiple of the other, they are parallel by the scalar multiple law.

v
Conversely, assume that v and w are parallel and write d = k
k
w
k
k

for convenience. Then v and w have
the same or opposite direction. If they have the same direction we show that v = dw by showing that v
and dw have the same length and direction. In fact,
by Theorem 4.1.1; as to the
direction, dw and w have the same direction because d > 0, and this is the direction of v by assumption.
Hence v = dw in this case by Theorem 4.1.2. In the other case, v and w have opposite direction and a
similar argument shows that v =

dw. We leave the details to the reader.

w
k

v
k

dw

|k

=

=

d

k

k

k

|

−

Example 4.1.7

Given points P(2,
parallel.

1, 4), Q(3,

−

1, 3), A(0, 2, 1), and B(1, 3, 0), determine if −→PQ and −→AB are

−

Solution. By Theorem 4.1.3, −→PQ = (1, 0,
(1, 0,
of −→AB, so these vectors are not parallel by Theorem 4.1.5.

1) = (t, t,

−

−

−

1) and −→AB = (1, 1,

−

t), so 1 = t and 0 = t, which is impossible. Hence −→PQ is not a scalar multiple

1). If −→PQ = t−→AB then

216

Vector Geometry

Lines in Space

These vector techniques can be used to give a very simple way of describing straight lines in space. In
order to do this, we ﬁrst need a way to specify the orientation of such a line, much as the slope does in the
plane.

Deﬁnition 4.3 Direction Vector of a Line

Withthisinmind,wecallanonzerovectord
−→AB forsomepairofdistinctpoints A and B ontheline.

= 0adirectionvectorforthelineifitisparallelto

P0

d

p0

Origin

P0P

p

Figure 4.1.13

Of course it is then parallel to −→CD for any distinct points C and D on the line.
In particular, any nonzero scalar multiple of d will also serve as a direction
vector of the line.

P

We use the fact that there is exactly one line that passes through a par-

ticular point P0(x0, y0, z0) and has a given direction vector d =



. We

want to describe this line by giving a condition on x, y, and z that the point
x0
y0
z0

P(x, y, z) lies on this line. Let p0 =

denote the vectors

and p =









a
b
c 


x
y
z 


of P0 and P, respectively (see Figure 4.1.13). Then







p = p0 + −→P0P

Hence P lies on the line if and only if −→P0P is parallel to d—that is, if and only if −→P0P = td for some scalar
t by Theorem 4.1.5. Thus p is the vector of a point on the line if and only if p = p0 + td for some scalar t.
This discussion is summed up as follows.

Vector Equation of a Line

Thelineparalleltod

= 0throughthepointwithvectorp0 isgivenby

p= p0 + td t anyscalar

Inotherwords,thepoint P withvectorpisonthislineifandonlyifarealnumbertexistssuch
thatp= p0 + td.

In component form the vector equation becomes


Equating components gives a different description of the line.







=



x0
y0
z0

+ t







x
y
z 


a
b
c 


6
6
4.1. Vectors and Lines

217

= 0isgivenby

Parametric Equations of a Line

Thelinethrough P0(x0, y0, z0) withdirectionvectord=



a
b
c 




x = x0 + ta
y = y0 + tb t anyscalar
z = z0 + tc

Inotherwords,thepoint P(x, y, z) isonthislineifandonlyifarealnumbert existssuchthat
x = x0 + ta, y = y0 + tb,and z = z0 + tc.

Example 4.1.8

Find the equations of the line through the points P0(2, 0, 1) and P1(4,

1, 1).

−

Solution. Let d = −→P0P1 =



denote the vector from P0 to P1. Then d is parallel to the line

(P0 and P1 are on the line), so d serves as a direction vector for the line. Using P0 as the point on
the line leads to the parametric equations



2
1
0 
−


x = 2 + 2t
t
y =
−
z = 1

t a parameter

Note that if P1 is used (rather than P0), the equations are

x = 4 + 2s
y =
s
1
−
z = 1

−

s a parameter

These are different from the preceding equations, but this is merely the result of a change of
parameter. In fact, s = t

1.

−

Example 4.1.9

Find the equations of the line through P0(3,

1, 2) parallel to the line with equations

−
x =
−
y = 1 + t
z =

−

1 + 2t

3 + 4t

6
218

Vector Geometry

Solution. The coefﬁcients of t give a direction vector d =



of the given line. Because the

2
1
4 


line we seek is parallel to this line, d also serves as a direction vector for the new line. It passes
through P0, so the parametric equations are



x = 3 + 2t
y =
1 + t
−
z = 2 + 4t

Example 4.1.10

Determine whether the following lines intersect and, if so, ﬁnd the point of intersection.

−

x = 1
3t
y = 2 + 5t
z = 1 + t

x =
−
y = 3
z = 1

1 + s
4s
s

−
−

Solution. Suppose P(x, y, z) with vector p lies on both lines. Then



3t
1
−
2 + 5t
1 + t 


=



=



x
y
z 


−
3
−
1
−

1 + s
4s
s 


for some t and s,


where the ﬁrst (second) equation is because P lies on the ﬁrst (second) line. Hence the lines
intersect if and only if the three equations





−

3t =
1
−
2 + 5t = 3
1 + t = 1

1 + s
4s
s

−
−

have a solution. In this case, t = 1 and s =
and the point of intersection is

1 satisfy all three equations, so the lines do intersect

−

p =

3t
1
−
2 + 5t
1 + t 


=





2
−
7
2 






using t = 1. Of course, this point can also be found from p =





−
3
−
1
−

1 + s
4s
s 


using s =

1.

−

4.1. Vectors and Lines

219

Example 4.1.11

Show that the line through P0(x0, y0) with slope m has direction vector d =

y0 = m(x

y

−

−

x0). This equation is called the point-slope formula.

1
m

(cid:20)

(cid:21)

and equation

y

P1(x1, y1)

P0(x0, y0)

Solution. Let P1(x1, y1) be the point on the line one unit
to the right of P0 (see the diagram). Hence x1 = x0 + 1.
Then d = −→P0P1 serves as direction vector of the line, and

x1
y1

d =

−
(cid:20)
−
as follows:

x0
y0

=

(cid:21)

1

y1

(cid:20)
−
m = y1
x1

y0

y0
x0

−
−

. But the slope m can be computed

(cid:21)
= y1

y0
1 = y1
−

y0

−

and the parametric equations are x = x0 + t,

O

x0

x1 = x0 + 1

x

Hence d =

1
m

y = y0 + mt. Eliminating t gives y

−

(cid:20)
(cid:21)
y0 = mt = m(x

x0), as asserted.

−

0
1

(cid:21)

(cid:20)

that is not of the form

Note that the vertical line through P0(x0, y0) has a direction vector d =

for any m. This result conﬁrms that the notion of slope makes no sense in this case. However, the

1
m

(cid:20)
vector method gives parametric equations for the line:

(cid:21)

Because y is arbitrary here (t is arbitrary), this is usually written simply as x = x0.

x = x0
y = y0 + t

Pythagoras’ Theorem

Dp

B

a

C

q

c

b

A

Figure 4.1.14

The Pythagorean theorem was known earlier, but Pythagoras (c. 550 B.C.)
is credited with giving the ﬁrst rigorous, logical, deductive proof of the
result. The proof we give depends on a basic property of similar triangles:
ratios of corresponding sides are equal.

Theorem 4.1.6: Pythagoras’ Theorem

Givenaright-angledtrianglewithhypotenusec andsides a and b,then a2 + b2 = c2.

Proof. Let A, B, and C be the vertices of the triangle as in Figure 4.1.14. Draw a perpendicular line from
C to the point D on the hypotenuse, and let p and q be the lengths of BD and DA respectively. Then DBC
and CBA are similar triangles so p
c . This means a2 = pc. In the same way, the similarity of DCA and
CBA gives q

c , whence b2 = qc. But then

a = a

b = b

a2 + b2 = pc + qc = (p + q)c = c2

220

Vector Geometry

because p + q = c. This proves Pythagoras’ theorem10.

Exercises for 4.1

Exercise 4.1.1 Compute

if v equals:

v
k

k

a. If E is the midpoint of side BC, show that

2
1
2

1
0
1







b.

d.

a.

c.



−







−


1
1
2





e.

2



−



1
1
2

1
0
2









−

−








1
1
2









f.

3

−

Exercise 4.1.2 Find a unit vector in the direction of:

a.



−

7
1
5





Exercise 4.1.3

−
−

2
1
2





b.





a. Find a unit vector in the direction from

3
1
4

1
3
5

.





to











−


b. If u

= 0, for which values of a is au a unit vector?

Exercise 4.1.4 Find the distance between the following
pairs of points.

a.

c.

3
1
0

3
5
2



−

−















2
1
1





and



−

and

1
3
3











b.

d.

2
1
2

4
0
2



−







−









and

and

2
0
1

3
2
0

















Exercise 4.1.5 Use vectors to show that the line joining
the midpoints of two sides of a triangle is parallel to the
third side and half as long.

Exercise 4.1.6 Let A, B, and C denote the three vertices
of a triangle.

−→AE = 1

2 (−→AB + −→AC)

b. If F is the midpoint of side AC, show that

−→FE = 1

2 −→AB

Exercise 4.1.7 Determine whether u and v are parallel
in each of the following cases.

a. u =

−
−





b. u =



−

3
6
3

3
6
3

c. u =

d. u =











1
0
1

−




2
0
1









−

; v =





; v =





; v =





; v =









5
10
5

−

1
−
2
1

−
1
0
1




8
−
0
4













Exercise 4.1.8 Let p and q be the vectors of points P
and Q, respectively, and let R be the point whose vector
is p + q. Express the following in terms of p and q.

a.

c.

−→QP

−→RP

b.

−→QR

d.

−→RO where O is the origin

Exercise 4.1.9 In each case, ﬁnd −→PQ and

−→PQ
k

.
k

a. P(1,

1, 3), Q(3, 1, 0)

−
b. P(2, 0, 1), Q(1,

−
c. P(1, 0, 1), Q(1, 0,

d. P(1,

1, 2), Q(1,

−

1, 6)

3)

−
1, 2)

−

10There is an intuitive geometrical proof of Pythagoras’ theorem in Example B.3.

6
e. P(1, 0,

f. P(3,

−

3), Q(

1, 0, 3)

−

−
1, 6), Q(1, 1, 4)

Exercise 4.1.10 In each case, ﬁnd a point Q such that
−→PQ has (i) the same direction as v; (ii) the opposite direc-
tion to v.

a. P(

−

1, 2, 2), v =

1
3
1





b. P(3, 0,

1), v =

−



−






2
1
3





Exercise 4.1.11 Let u =

3
1
0



−

, v =





w =



1
−
1
5




. In each case, ﬁnd x such that:





, and

4
0
1





4.1. Vectors and Lines

221

b. au + bv + cz =

5
6
1









−

Exercise 4.1.14 Given P1(2, 1,
−
Find the coordinates of the point P:

2) and P2(1,

2, 0).

−

a. 1

5 the way from P1 to P2

b. 1

4 the way from P2 to P1

Exercise 4.1.15 Find the two points trisecting the seg-
ment between P(2, 3, 5) and Q(8,

6, 2).

−




a. 3(2u + x) + w = 2x

b. 2(3v

−

x) = 5w + u

v

3x

−

−

Exercise 4.1.16 Let P1(x1, y1, z1) and P2(x2, y2, z2) be
two points with vectors p1 and p2, respectively. If r and s
are positive integers, show that the point P lying r
r+s the
way from P1 to P2 has vector

, v =

1
1
2









0
1
2





, and





. In each case, ﬁnd numbers a, b, and c such

p =

s
r+s

p1 +

r
r+s

p2

(cid:0)

(cid:1)

(cid:0)

(cid:1)

Exercise 4.1.17 In each case, ﬁnd the point Q:

b.

x =

1
3
0





3
1
0



−

, v =















, and

4
0
1





a. −→PQ =

. In each case, show that there are no num-



b. −→PQ =

2
0
3

−

1
−
4
7

















and P = P(2,

3, 1)

−

and P = P(1, 3,

4)

−

Exercise 4.1.12 Let u =

w =

1
0
1
that x = au + bv + cw.

−









a.

x =

2
1
−
6









Exercise 4.1.13 Let u =

z =



1
1
1

bers a, b, and c such that:





a. au + bv + cz =

1
2
1









222

Vector Geometry

Exercise 4.1.18 Let u =

each case ﬁnd x:

2
0
4









−

and v =

2
1
2

−





. In





a. 2u

v = 3
v
− k
k
b. 3u + 7v =

2x)

2 (u
−
2(2x + v)
k

u
k

Exercise 4.1.19 Find all vectors u that are parallel to

v =



3
2
−
1



and satisfy

u
k

k

v
= 3
k

.
k





Exercise 4.1.20 Let P, Q, and R be the vertices of a par-
allelogram with adjacent sides PQ and PR. In each case,
ﬁnd the other vertex S.

a. P(3,

−
b. P(2, 0,

1,

1), Q(1,

−
1), Q(

−

2, 0), R(1,

−

2, 4, 1), R(3,

−

−

1, 2)

−
1, 0)

Exercise 4.1.21 In each case either prove the statement
or give an example showing that it is false.

b. The line passing through P(3,

Q(1, 0,

1).

−

c. The line passing through P(3,

1, 4) and

1, 4) and

−

−

Q(3,

1, 5).

−

d. The line parallel to

P(1, 1, 1).

1
1
1









and passing through

e. The line passing through P(1, 0,

to the line with parametric equations x =
y = 2

t, and z = 3 + 3t.

−

−

3) and parallel
1 + 2t,

−

f. The line passing through P(2,

lel to the line with parametric equations x = 2
y = 1, and z = t.

−

−

1, 1) and paral-
t,

g. The lines through P(1, 0, 1) that meet the line

with vector equation p =

1
2
0



+ t





−

points at distance 3 from P0(1, 2, 0).







2
1
2

at





a. The zero vector 0 is the only vector of length 0.

Exercise 4.1.23 In each case, verify that the points P
and Q lie on the line.

v
k

b. If

−
c. If v =

= 0, then v = w.

w
k
v, then v = 0.

−
=

=

d. If

e. If

v
k
v
k

k

k

w
k
w
k

, then v = w.
k
, then v =
k

±

w.

f. If v = tw for some scalar t, then v and w have the

same direction.

g. If v, w, and v + w are nonzero, and v and v + w

parallel, then v and w are parallel.

i. If

h.

j.

=

5v
k
=

k −
v
k
k
v + w
k
k

−
2v
k
=

, for all v.
k

v
5
k
, then v = 0.
k
v
k

+

k

, for all v and w.
w
k
k

Exercise 4.1.22 Find the vector and parametric equa-
tions of the following lines.

a. The line parallel to

P(1,

1, 3).

−

2
1
−
0









and passing through

a.

b.

4t P(

1, 3, 0), Q(11, 0, 3)

−

x = 3
−
y = 2 + t
t
z = 1

−

P(2, 3,

3), Q(

−

1, 3,

9)

−

−

x = 4
y = 3
z = 1

t

−

2t

−

Exercise 4.1.24 Find the point of intersection (if any)
of the following pairs of lines.

a.

b.

c.

x = 3 + t
2t
y = 1
z = 3 + 3t

−

x = 4 + 2s
y = 6 + 3s
z = 1 + s

−

x = 1
t
y = 2 + 2t
z =

1 + 3t

−

x = 2s
y = 1 + s
z = 3

x
y
z 

x
y
z













3
1
2

1
1
2

=



−

=







−

+ t

+ s

















1
1
1





−
2
0
3





d.

x
y
z

x
y
z









4
1
−
5

2
7
−
12

1
0
1

+ t





+ s



−














0
2
3





=

=

















Exercise 4.1.25 Show that if a line passes through the
origin, the vectors of points on the line are all scalar mul-
tiples of some ﬁxed nonzero vector.

Exercise 4.1.26 Show that every line parallel to the z
axis has parametric equations x = x0, y = y0, z = t for
some ﬁxed numbers x0 and y0.

4.1. Vectors and Lines

223

Exercise 4.1.32 Consider a quadrilateral with vertices
A, B, C, and D in order (as shown in the diagram).

A

B

D

C

If the diagonals AC and BD bisect each other, show
that the quadrilateral is a parallelogram. (This is the con-
verse of Example 4.1.2.) [Hint: Let E be the intersec-
tion of the diagonals. Show that −→AB = −→DC by writing
−→AB = −→AE + −→EB.]

Exercise 4.1.27 Let d =

a
b
c





be a vector where a,

Exercise 4.1.33 Consider the parallelogram ABCD (see
diagram), and let E be the midpoint of side AD.

b, and c are all nonzero. Show that the equations of the

line through P0(x0, y0, z0) with direction vector d can be
written in the form



x
−

a = y
x0

y0
b = z

−

z0
−
c

This is called the symmetric form of the equations.

Exercise 4.1.28 A parallelogram has sides AB, BC, CD,
1, 2), C(2, 1, 0), and the midpoint
and DA. Given A(1,
M(1, 0,

3) of AB, ﬁnd −→BD.

−

−

−

1, 2) and B = (2, 0, 1) such that

Exercise 4.1.29 Find all points C on the line through
A(1,
.
k
Exercise 4.1.30 Let A, B, C, D, E, and F be the ver-
tices of a regular hexagon, taken in order. Show that
−→AB + −→AC + −→AD + −→AE + −→AF = 3−→AD.

−→BC
= 2
k

−→AC
k

k

Exercise 4.1.31

a. Let P1, P2, P3, P4, P5, and P6 be six points equally

spaced on a circle with centre C. Show that

−→CP1 + −→CP2 + −→CP3 + −→CP4 + −→CP5 + −→CP6 = 0

b. Show that the conclusion in part (a) holds for any
even set of points evenly spaced on the circle.

c. Show that the conclusion in part (a) holds for three

points.

d. Do you think it works for any ﬁnite set of points

evenly spaced around the circle?

C

D

B

A

F

E

Show that BE and AC trisect each other; that is, show
that the intersection point is one-third of the way from E
to B and from A to C.
[Hint: If F is one-third of the
way from A to C, show that 2−→EF = −→FB and argue as in
Example 4.1.2.]

Exercise 4.1.34 The line from a vertex of a triangle to
the midpoint of the opposite side is called a median of
the triangle.
If the vertices of a triangle have vectors
u, v, and w, show that the point on each median that
is 1
3 the way from the midpoint to the vertex has vec-
tor 1
3 (u + v + w). Conclude that the point C with vector
1
3 (u + v + w) lies on all three medians. This point C is
called the centroid of the triangle.

Exercise 4.1.35 Given four noncoplanar points in space,
the ﬁgure with these points as vertices is called a tetra-
hedron. The line from a vertex through the centroid (see
previous exercise) of the triangle formed by the remain-
ing vertices is called a median of the tetrahedron. If u, v,
w, and x are the vectors of the four vertices, show that the
point on a median one-fourth the way from the centroid
to the vertex has vector 1
4 (u + v + w + x). Conclude that
the four medians are concurrent.

224

Vector Geometry

4.2 Projections and Planes

P

Q

Figure 4.2.1

Any student of geometry soon realizes that the notion of perpendicular
lines is fundamental. As an illustration, suppose a point P and a plane
are given and it is desired to ﬁnd the point Q that lies in the plane and is
closest to P, as shown in Figure 4.2.1. Clearly, what is required is to ﬁnd
the line through P that is perpendicular to the plane and then to obtain Q
as the point of intersection of this line with the plane. Finding the line
perpendicular to the plane requires a way to determine when two vectors
are perpendicular. This can be done using the idea of the dot product of
two vectors.

The Dot Product and Angles

Deﬁnition 4.4 Dot Product in R3

Givenvectorsv=

x1
y1
z1









andw=

x2
y2
z2



,theirdotproduct v
·



wisanumberdeﬁned




w= x1x2 + y1y2 + z1z2 = vT w

v
·

Because v

·

w is a number, it is sometimes called the scalar product of v and w.11

Example 4.2.1

If v =

2
1
−
3 






and w =





1
4
1 


−

, then v

w = 2

1 + (

1)

−

·

·

4 + 3

(

·

1) =

−

5.

−

·

The next theorem lists several basic properties of the dot product.

Theorem 4.2.1
Letu,v,andwdenotevectorsin R3 (or R2).

1. v

2. v

3. v

4. v

·

·

·

·

wisarealnumber.

w= w
·

v.

0= 0 = 0

v.

·

v=

2.

v
k
k
w= k(w
·
w) = u

5. (kv)

·
(v

±

6. u

·

v) = v
·
w

u

v

·

±

·

(kw) forallscalars k.

11Similarly, if v =

x1
y1

(cid:20)

(cid:21)

and w =

x2
y2

(cid:20)

(cid:21)

in R2, then v

·

w = x1x2 + y1y2.

Proof. (1), (2), and (3) are easily veriﬁed, and (4) comes from Theorem 4.1.1. The rest are properties of
matrix arithmetic (because w

v = vT w), and are left to the reader.

·

4.2. Projections and Planes

225

The properties in Theorem 4.2.1 enable us to do calculations like

(2v

3u

·

−

3w + 4z) = 6(u

v)

·

−

9(u

·

w) + 12(u

z)

·

and such computations will be used without comment below. Here is an example.

Example 4.2.2

Verify that

3w

v

k

−

k

2 = 1 when

= 2,

v

k

k

w

k

k

= 1, and v

w = 2.

·

Solution. We apply Theorem 4.2.1 several times:

3w

v

k

−

k

2 = (v
= v
= v

=
k
= 4

3w)
3w

(v
·
3(w

−
w) + 9

−
−
w)

3w)

(v
−
·
(v
3w)
−
3(v
v
−
2

·
6(v
−
12 + 9 = 1

·

·
·
v

k
−

3w)
−
v) + 9(w
·
2
w
k

k

w)

·

There is an intrinsic description of the dot product of two nonzero vectors in R3. To understand it we

require the following result from trigonometry.

Law of Cosines

Ifatrianglehassides a, b,and c,andifθ istheinteriorangleopposite c then

c2 = a2 + b2

2ab cosθ

−

a
θ q

c

p

q

−

b
b

Figure 4.2.2

Proof. We prove it when is θ acute, that is 0
2 ; the obtuse case
is similar. In Figure 4.2.2 we have p = a sinθ and q = a cosθ. Hence
Pythagoras’ theorem gives

≤

θ < π

c2 = p2 + (b

−

q)2 = a2 sin2 θ + (b

a cosθ)2
= a2(sin2 θ + cos2 θ) + b2

−

−

2ab cosθ

The law of cosines follows because sin2 θ + cos2 θ = 1 for any angle θ.

226

Vector Geometry

θ obtuse

v

θ

w

v

θ acute

θ

w

Figure 4.2.3

Theorem 4.2.2

Note that the law of cosines reduces to Pythagoras’ theorem if θ is a right
angle (because cos π

2 = 0).

Now let v and w be nonzero vectors positioned with a common tail as

in Figure 4.2.3. Then they determine a unique angle θ in the range

θ

0

≤

≤

π

This angle θ will be called the angle between v and w. Figure 4.2.3 il-
lustrates when θ is acute (less than π
2 ). Clearly
v and w are parallel if θ is either 0 or π. Note that we do not deﬁne the
angle between v and w if one of these vectors is 0.

2 ) and obtuse (greater than π

The next result gives an easy way to compute the angle between two

nonzero vectors using the dot product.

Letvandwbenonzerovectors. Ifθ istheanglebetweenvandw,then

w=

v

·

v
k

kk

w
k

cosθ

w

v

−

v

θ

w

Figure 4.2.4

Proof. We calculate
to the triangle in Figure 4.2.4 to obtain:

−

w

k

k

v

2 in two ways. First apply the law of cosines

w

v

k

−

k

2 =

2 +

v

k

k

w

k

2

k

−

v

2

k

w
k

kk

cosθ

On the other hand, we use Theorem 4.2.1:

w

v

k

−

k

2 = (v
= v

w)
v

−
v
−
2

(v
w

·
·
2(v

w)
w

−
−
·
w) +

·
v

=

w

v + w
·
2

w
k

k

k
cosθ =

−
2(v

k

·
w), and the result follows.

Comparing these we see that

v

2
−

k

kk

w

k

−

·

If v and w are nonzero vectors, Theorem 4.2.2 gives an intrinsic description of v

,
k
and the angle θ between v and w do not depend on the choice of coordinate system. Moreover, since
k
and

are nonzero (v and w are nonzero vectors), it gives a formula for the cosine of the angle θ:

w because

v
k

w
k
v
k

w

k

·

,

k

k

cosθ = v
·
v
kk
k

w
w

k

(4.1)

Since 0

θ

≤

≤

π, this can be used to ﬁnd θ.

Example 4.2.3

Compute the angle between u =

1
−
1
2 






and v =

.

2
1
1 


−





y

1
−

2 , √3

2

(cid:16)

(cid:17)

2π
3

1
−
2

O

x

4.2. Projections and Planes

227

=

= −

2
2+1
−
√6√6

Solution. Compute cosθ = v
w
·
w
v
k
kk
k
that cosθ and sinθ are deﬁned so that (cosθ, sinθ) is the point on
the unit circle determined by the angle θ (drawn counterclockwise,
starting from the positive x axis). In the present case, we know
1
2 and that 0
that cosθ =
θ
≤
that θ = 2π
3 (see the diagram).

π. Because cos π

1
2. Now recall

2, it follows

3 = 1

−

≤

−

If v and w are nonzero, equation (4.1) shows that cosθ has the same sign as v

w, so

·

w > 0 if and only if θ is acute (0
≤
w < 0 if and only if θ is obtuse ( π
2 < θ
w = 0 if and only if θ = π
2

θ < π
2 )
0)

≤

v
v
v

·
·
·

In this last case, the (nonzero) vectors are perpendicular. The following terminology is used in linear
algebra:

Deﬁnition 4.5 Orthogonal Vectors in R3

Twovectorsvandwaresaidtobeorthogonalifv= 0orw= 0ortheanglebetweenthemis π
2 .

Since v

·

w = 0 if either v = 0 or w = 0, we have the following theorem:

Theorem 4.2.3

Twovectorsvandwareorthogonalifandonlyifv
·

w= 0.

Example 4.2.4

Show that the points P(3,

1, 1), Q(4, 1, 4), and R(6, 0, 4) are the vertices of a right triangle.

−
Solution. The vectors along the sides of the triangle are

−→PQ =

1
2
3 






, −→PR =

3
1
3 






, and −→QR =

2
1
−
0 






Evidently −→PQ
and QR are perpendicular—that is, the angle at Q is a right angle.

2 + 0 = 0, so −→PQ and −→QR are orthogonal vectors. This means sides PQ

−→QR = 2

−

·

Example 4.2.5 demonstrates how the dot product can be used to verify geometrical theorems involving

perpendicular lines.

228

Vector Geometry

Example 4.2.5

A parallelogram with sides of equal length is called a rhombus. Show that the diagonals of a
rhombus are perpendicular.

u

−

v
u + v

u

v

Solution. Let u and v denote vectors along two adjacent sides
of a rhombus, as shown in the diagram. Then the diagonals are
u

v and u + v, and we compute

−

(u

v)

·

−

(u + v) = u
= u

v
·
v

(u + v)
v
u

·

−

·

v

(u + v)
−
v
u + u
−
2
2

·
v

− k

k

·
·
u
k

=
k
= 0

because

=

u
k

k

v

k

k

(it is a rhombus). Hence u

−

v and u + v are orthogonal.

Projections

In applications of vectors, it is frequently useful to write a vector as the sum of two orthogonal vectors.
Here is an example.

Example 4.2.6

Suppose a ten-kilogram block is placed on a ﬂat surface inclined 30◦ to the horizontal as in the
diagram. Neglecting friction, how much force is required to keep the block from sliding down the
surface?

w

30◦

w

w1

w2

Solution. Let w denote the weight (force due to gravity) exerted
= 10 kilograms and the direction of w is
on the block. Then
vertically down as in the diagram. The idea is to write w as a sum
w = w1 + w2 where w1 is parallel to the inclined surface and w2
is perpendicular to the surface. Since there is no friction, the force
w1 because the force w2 has no effect parallel to the
required is
w1
surface. As the angle between w and w2 is 30◦ in the diagram, we have k
2. Hence
w
k
= 1
w1
210 = 5. Thus the required force has a magnitude of 5 kilograms weight directed
2k
k
k
up the surface.

= sin 30◦ = 1

= 1

k
k

−

w

30◦

k

k

k

P

u1

u

−

d

u

u1

Q

P1

u

u1

P1

Q

Figure 4.2.5

(a)

P

u1

u

−

(b)

4.2. Projections and Planes

229

If a nonzero vector d is speciﬁed, the key idea in Example 4.2.6 is to

be able to write an arbitrary vector u as a sum of two vectors,

u = u1 + u2

d

u1 is orthogonal to d. Suppose that
where u1 is parallel to d and u2 = u
u and d
= 0 emanate from a common tail Q (see Figure 4.2.5). Let P be
the tip of u, and let P1 denote the foot of the perpendicular from P to the
line through Q parallel to d.

−

Then u1 = −→QP1 has the required properties:

1. u1 is parallel to d.

2. u2 = u

−

u1 is orthogonal to d.

3. u = u1 + u2.

Deﬁnition 4.6 Projection in R3

Thevectoru1 = −→QP1 inFigure4.2.5iscalledtheprojectionofuond. Itisdenoted

u1 = projd u

In Figure 4.2.5(a) the vector u1 = projd u has the same direction as d; however, u1 and d have opposite
directions if the angle between u and d is greater than π
2 (Figure 4.2.5(b)). Note that the projection
u1 = projd u is zero if and only if u and d are orthogonal.

Calculating the projection of u on d

= 0 is remarkably easy.

Theorem 4.2.4

Letuandd

= 0bevectors.

d
1. Theprojectionofuondisgivenby projd u= u
2 d.
·
d
k
k

2. Thevectoru

−

projd uisorthogonaltod.

Proof. The vector u1 = projd u is parallel to d and so has the form u1 = td for some scalar t. The
d = 0 by
u1 and d are orthogonal determines t.
requirement that u
Theorem 4.2.3. If u1 = td is substituted here, the condition is

In fact, it means that (u

u1)

−

−

·

−
d
It follows that t = u
2 , where the assumption that d
·
d
k
k

·

·

0 = (u

td)

d = u

d

t(d

d) = u

d

·

k
−
= 0 guarantees that

−

·

t

d
k
d

k

2

k

2

= 0.

6
6
6
6
6
230

Vector Geometry

Example 4.2.7

Find the projection of u =



to d and u2 is orthogonal to d.



2
3
1 
−


on d =

1
1
3 
−






and express u = u1 + u2 where u1 is parallel

Solution. The projection u1 of u on d is

d
u1 = projd u = u
2 d = 2+3+3
·
12+(
d
k
k

1)2+32 
−

1
1
−
3 


= 8

11 



1
1
−
3 




Hence u2 = u

u1 = 1

11 

−

, and this is orthogonal to d by Theorem 4.2.4 (alternatively,

14
25
13 


−
−

u2 = 0). Since u = u1 + u2, we are done.



observe that d

·

Example 4.2.8

u

2)

P(1, 3,
u

−

u1

−
u1
d

Q

P0(2, 0,

1)

−

Solution. Let u =



Find the shortest distance (see diagram) from the point P(1, 3,

to the line through P0(2, 0,

1) with direction vector d =

−



Also ﬁnd the point Q that lies on the line and is closest to P.


2)

−
1
1
.
−
0 


1
3
2 


−

− 

2
0
1 


−

=





1
−
3
1 


−

denote the vector from P0 to P, and let u1 denote

the projection of u on d. Thus




u1 = u
d
1
3+0
1)2+02 d =
2 d = −
·
−
12+(
d
−
k
k

2d =

−



2
−
2
0 




−→QP
k

k

=

u

k

−

u1

k

=

= √3

1
1
1 


−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

by Theorem 4.2.4. We see geometrically that the point Q on the line is closest to P, so the distance
is

To ﬁnd the coordinates of Q, let p0 and q denote the vectors of P0 and Q, respectively. Then

p0 =



and q = p0 + u1 =



. Hence Q(0, 2,

1) is the required point. It can be

−

2
0
1 


−

0
2
1 


−

checked that the distance from Q to P is √3, as expected.





4.2. Projections and Planes

231

Planes

It is evident geometrically that among all planes that are perpendicular to a given straight line there is
exactly one containing any given point. This fact can be used to give a very simple description of a plane.
To do this, it is necessary to introduce the following notion:

Deﬁnition 4.7 Normal Vector in a Plane

Anonzerovectorniscalledanormalforaplaneifitisorthogonaltoeveryvectorintheplane.

n

P0

For example, the coordinate vector k is a normal for the x-y plane.

P

Given a point P0 = P0(x0, y0, z0) and a nonzero vector n, there is a
unique plane through P0 with normal n, shaded in Figure 4.2.6. A point
P = P(x, y, z) lies on this plane if and only if the vector −→P0P is orthogonal

to n—that is, if and only if n

−→P0P = 0. Because −→P0P =

·

Figure 4.2.6

gives the following result:

Scalar Equation of a Plane

x0
y0
z0

x
y
z

−
−
−









this

Theplanethrough P0(x0, y0, z0) withnormaln=

= 0asanormalvectorisgivenby

−
Inotherwords,apoint P(x, y, z) isonthisplaneifandonlyif x, y,and z satisfythisequation.

−

−

a(x

x0) + b(y

z0) = 0



a
b
c 

y0) + c(z



Example 4.2.9

Find an equation of the plane through P0(1,

1, 3) with n =

−

Solution. Here the general scalar equation becomes

as normal.

3
1
−
2 






3(x

1)

−

−

(y + 1) + 2(z

3) = 0

−

This simpliﬁes to 3x

y + 2z = 10.

−

If we write d = ax0 + by0 + cz0, the scalar equation shows that every plane with normal n =

a linear equation of the form

ax + by + cz = d

has

a
b
c 

(4.2)





6
232

Vector Geometry

for some constant d. Conversely, the graph of this equation is a plane with n =

(assuming that a, b, and c are not all zero).

Example 4.2.10

as a normal vector

a
b
c 






Find an equation of the plane through P0(3,
2x

3y = 6.

−

1, 2) that is parallel to the plane with equation

−

Solution. The plane with equation 2x

3y = 6 has normal n =



−

. Because the two planes

are parallel, n serves as a normal for the plane we seek, so the equation is 2x
by Equation 4.2. Insisting that P0(3,
d = 2

1) = 9. Hence, the equation is 2x

1, 2) lies on the plane determines d; that is,

3y = 9.

3(

−

−



3

3y = d for some d

·

−

−

−

2
3
−
0 


Consider points P0(x0, y0, z0) and P(x, y, z) with vectors p0 =

x0
y0
z0



and p =






vector n, the scalar equation of the plane through P0(x0, y0, z0) with normal n =





. Given a nonzero

takes the vector

x
y
z 
a

b
c 






form:

Vector Equation of a Plane

Theplanewithnormaln

= 0throughthepointwithvectorp0 isgivenby

n

·

(p

−

p0) = 0

Inotherwords,thepointwithvectorpisontheplaneifandonlyifpsatisﬁesthiscondition.

Moreover, Equation 4.2 translates as follows:

Every plane with normal n has vector equation n

p = d for some number d.

·

This is useful in the second solution of Example 4.2.11.

Example 4.2.11

Find the shortest distance from the point P(2, 1,
Also ﬁnd the point Q on this plane closest to P.

3) to the plane with equation 3x

−

y + 4z = 1.

−

6
4.2. Projections and Planes

233

n
u1

P(2, 1,

3)

−

Solution 1. The plane in question has normal n =



Choose any point P0 on the plane—say P0(0,
Q(x, y, z) be the point on the plane closest to P (see the diagram).

1, 0)—and let

−



3
1
.
−
4 


u

P0(0,

1, 0)

−

Q(x, y, z)

The vector from P0 to P is u =



2
2
3 


−



tail at P0. Then −→QP = u1 and u1 is the projection of u on n:

. Now erect n with its

u
u1 = n
8
2 n = −
·
26 
n
k
k



3
1
−
4 


4
= −
13 



3
1
−
4 


Hence the distance is

−→QP
k

k

=

u1

k

k

= 4√26

13 . To calculate the point Q, let q =

p0 =

0
1
−
0 






be the vectors of Q and P0. Then

and

x
y
z 






0
1
−
0 


+





2
2
3 


−

+ 4

13 



3
1
−
4 


38
13
9
13
23
−
13








= 





This gives the coordinates of Q( 38

q = p0 + u

u1 =

−




13, 9

13 , −

23
13 ).

2
1
3 


−

x
y
z 


Solution 2. Let q =



and p =



be the vectors of Q and P. Then Q is on the line

through P with direction vector n, so q = p + tn for some scalar t. In addition, Q lies on the plane,
so n

q = 1. This determines t:





·

1 = n

·

q = n

·

(p + tn) = n

p + t

·

2 =

n
k

k

7 + t(26)

−

This gives t = 8

26 = 4

13 , so

2
1
3 

as before. This determines Q (in the diagram), and the reader can verify that the required distance
is

3
1
−
4 


38
9
23 


x
y
z 


√26, as before.

= q = p + tn =

= 1

13 

13 

+ 4

−

−













−→QP

k

k

= 4
13

234

Vector Geometry

The Cross Product

If P, Q, and R are three distinct points in R3 that are not all on some line, it is clear geometrically that
there is a unique plane containing all three. The vectors −→PQ and −→PR both lie in this plane, so ﬁnding a
normal amounts to ﬁnding a nonzero vector orthogonal to both −→PQ and −→PR. The cross product provides a
systematic way to do this.

Deﬁnition 4.8 Cross Product

Givenvectorsv1 =

x1
y1
z1









and v2 =

x2
y2
z2







,deﬁnethecrossproductv1



v2 by

×

v1

×

v2 =





y1z2
(x1z2
x1y2

z1y2
−
z1x2)
−
y1x2
−

−





z

k

j

i

O

y

x

Figure 4.2.7

v2 is often called the vector product.) There
(Because it is a vector, v1
is an easy way to remember this deﬁnition using the coordinate vectors:

×

1
0
0 


i =





0
1
0 




, j =



, and k =



0
0
1 




They are vectors of length 1 pointing along the positive x, y, and z axes,
respectively, as in Figure 4.2.7. The reason for the name is that any vector
can be written as

= xi + yj + zk

x
y
z 






With this, the cross product can be described as follows:

Determinant Form of the Cross Product

Ifv1 =

x1
y1
z1









andv2 =





x2
y2
z2



aretwovectors,then

v1

×

v2 = det




i x1 x2
j y1 y2
k z1
z2

=




wherethedeterminantisexpandedalongtheﬁrstcolumn.



i

−

y1 y2
z2 (cid:12)
z1
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

j+

x1 x2
z2 (cid:12)
z1
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k

x1 x2
y1 y2 (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Example 4.2.12

If v =

2
1
4 
−






and w =





1
3
7 


, then

i
j
k

v1

×

v2 = det





−

2 1
1 3
4 7 


=

=

=

1 3
4 7

−

2 1
4 7

i

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

19i

10j + 7k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
−





−
−

−
19
10
7 


4.2. Projections and Planes

235

j +

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

2 1
1 3

−

k

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Observe that v

w is orthogonal to both v and w in Example 4.2.12. This holds in general as can be
veriﬁed directly by computing v
w), and is recorded as the ﬁrst part of the following
×
theorem. It will follow from a more general result which, together with the second part, will be proved in
Section 4.3 where a more detailed study of the cross product will be undertaken.

w) and w

(v

(v

×

×

·

·

Theorem 4.2.5
Letvandwbevectorsin R3.

1. v

×

wisavectororthogonaltobothvandw.

2. Ifvandwarenonzero,thenv

×

w= 0ifandonlyifvandwareparallel.

It is interesting to contrast Theorem 4.2.5(2) with the assertion (in Theorem 4.2.3) that

w = 0

if and only if v and w are orthogonal.

v

·

Example 4.2.13

Find the equation of the plane through P(1, 3,

2), Q(1, 1, 5), and R(2,

−

2, 3).

−

and −→PR =



lie in the plane, so

Solution. The vectors −→PQ =

0
2
−
7 






−→PQ

×

−→PR = det





i
j
k

0
2
−
7


1
5
−
5 


1
5
−
5 


= 25i + 7j + 2k =

25
7
2 






is a normal for the plane (being orthogonal to both −→PQ and −→PR). Hence the plane has equation

25x + 7y + 2z = d

for some number d.

236

Vector Geometry

2) lies in the plane we have 25

Since P(1, 3,
equation is 25x + 7y + 2z = 42. Incidentally, the same equation is obtained (verify) if −→QP and −→QR,
or −→RP and −→RQ, are used as the vectors in the plane.

2) = d. Hence d = 42 and the

3 + 2(

1 + 7

−

−

·

·

Example 4.2.14

Find the shortest distance between the nonparallel lines

1
0
1 


Then ﬁnd the points A and B on the lines that are closest together.

2
0
1 


x
y
z 


x
y
z 


and

+ t

=

=

−



















3
1
0 


+ s

1
1
1 


−





Solution. Direction vectors for the two lines are d1 =

2
0
1 


1
1
1 


, so

1
1
1 


−

and d2 =





1
−
3
2 


=









−

n = d1

×

d2 = det

i 2
j 0
k 1





n

P2

B

A

u

P1

is perpendicular to both lines. Consider the plane shaded in
the diagram containing the ﬁrst line with n as normal. This plane
contains P1(1, 0,
1) and is parallel to the second line. Because
P2(3, 1, 0) is on the second line, the distance in question is just the
shortest distance between P2(3, 1, 0) and this plane. The vector

−

u from P1 to P2 is u = −→P1P2 =

the distance is the length of the projection of u on n.

and so, as in Example 4.2.11,

2
1
1 






distance =

u
n
= |
|
·
n
k
k

= 3
√14

= 3√14
14

n
u
2 n
·
n
k
k
(cid:13)
(cid:13)
(cid:13)

Note that it is necessary that n = d1
shown later (Theorem 4.3.4), this is guaranteed by the fact that d1 and d2 are not parallel.
The points A and B have coordinates A(1 + 2t, 0, t
2t

(cid:13)
(cid:13)
d2 be nonzero for this calculation to be possible. As is
(cid:13)

1) and B(3 + s, 1 + s,

s) for some s

2 + s

−

−

×

and t, so −→AB =

. This vector is orthogonal to both d1 and d2, and the conditions

−
1 + s
s



t 
1


d1 = 0 and −→AB
14, so the points are A( 40

−
d2 = 0 give equations 5t
−
14, 9
14 ) and B( 37
1
14, 0, −

−

·

−→AB
·
t = 13

s = 5 and t
14, 5

−
14 ). We have

5
3s = 2. The solution is s = −
14 and
= 3√14
14 , as before.

−→AB

k

k

Exercises for 4.2

Exercise 4.2.1 Compute u

v where:

·













a. u =

b. u =

c. u =

d. u =

e. u =

f. u =









































2
1
−
3

1
2
1

−

1
1
3

−

3
1
−
5

x
y
z

, v =

1
−
1
1





, v = u

, v =

, v =

2
1
−
1

6
7
5

−
−












a
b
c

, v =







, v = 0



a
b
c 


Exercise 4.2.2 Find the angle between the following
pairs of vectors.

a. u =

b. u =

c. u =

d. u =

e. u =





















1
0
3

, v =



2
0
1






3
1
−
0

7
1
−
3

2
1
1

−

1
1
−
0

















, v =

, v =

, v =

, v =




























6
−
2
0

1
4
1

−
3
6
3

0
1
1









4.2. Projections and Planes

237

f. u =

, v =

0
3
4









5√2
7
1

−
−









Exercise 4.2.3 Find all real numbers x such that:

a.

b.

2
1
3

2
1
1



−





−











and



−

and







1
x
2

are orthogonal.

x
2
1





are at an angle of π
3 .





Exercise 4.2.4 Find all vectors v =

to both:

orthogonal

x
y
z 






a. u1 =

−
−





b. u1 =



−

c. u1 =

d. u1 =







−



−



1
3
2

3
1
2

2
0
1

2
1
3

, u2 =

, u2 =

, u2 =

, u2 =

































0
1
1

2
0
1

−

0
0
0








4
0
2









Exercise 4.2.5 Find two orthogonal vectors that are both

orthogonal to v =

1
2
0

.





Exercise 4.2.6
P(2, 0,

3), Q(5,

−


Consider the triangle with vertices



2, 1), and R(7, 5, 3).

−

a. Show that it is a right-angled triangle.

b. Find the lengths of the three sides and verify the

Pythagorean theorem.

238

Vector Geometry

7, 9), B(6, 4, 4), and C(7, 10,

Show that the triangle with vertices
6) is not a right-

Exercise 4.2.7
A(4,
angled triangle.

−

−

Exercise 4.2.12 Calculate the distance from the point P
to the line in each case and ﬁnd the point Q on the line
closest to P.

Exercise 4.2.8 Find the three internal angles of the tri-
angle with vertices:

a. P(3, 2

a. A(3, 1,

b. A(3, 1,

2), B(3, 0,

2), B(5, 2,

−

−

−

−

1), and C(5, 2,

1), and C(4, 3,

1)

3)

−

−

Exercise 4.2.9 Show that the line through P0(3, 1, 4)
and P1(2, 1, 3) is perpendicular to the line through
P2(1,

1, 2) and P3(0, 5, 3).

−

line:

b. P(1,

line:

1)

=



−
x
y
z





−






1, 3)
x
y
z





=

2
1
3

−





1
0
1









3
1
2

−
−

+ t





+ t



3
1
4







v where:







Exercise 4.2.10 In each case, compute the projection of
u on v.

Exercise 4.2.13 Compute u

×

a. u =

b. u =

c. u =

d. u =

5
7
1

, v =





−



, v =

, v =

, v =


3
2
−
1

1
1
−
2

3
2
1

−
−

























2
1
3

4
1
1








3
1
−
1

6
−
4
2

























Exercise 4.2.11 In each case, write u = u1 + u2, where
u1 is parallel to v and u2 is orthogonal to v.

a. u =

b. u =

c. u =

d. u =

1
2
3







−





−







−




3
1
0

3
2
1

2
0
1

, v =

1
1
2





, v =





, v =





, v =



















−


6
2
0

1
1
1





−
1
4
7









Exercise 4.2.14 Find an equation of each of the follow-
ing planes.

a. u =

b. u =

c. u =

d. u =

















2
1
−
1

3
1
0

, v =









−

, v =








2
1
−
0

3
2
−
1









, v =

, v =









1
1
−
3

2
1
4




3
1
1

−

6
−
4
1

−













a. Passing through A(2, 1, 3), B(3,

1, 5), and

−

C(1, 2,

3).

−
b. Passing through A(1,

C(4, 7,

11).

−
c. Passing through P(2,
plane with equation 3x

−
−
d. Passing through P(3, 0,
plane with equation 2x

1, 6), B(0, 0, 1), and

−

3, 5) and parallel to the
2y

z = 0.

−
1) and parallel to the

−
y + z = 3.

−

e. Containing P(3, 0,

1) and the line

x
y
z





=









0
0
2

−

+ t









1
0
1

.





4.2. Projections and Planes

239

e. Passing through P(2, 1,

x
y
z



=





to that line.




1
2
1

−

+ t









1), intersecting the line
−
3
0
1

, and perpendicular





f. Passing through P(1, 1, 2), intersecting the line
1
1
1

, and perpendicular to

2
1
0

+ t

=











x
y
z
that line.














f. Containing P(2, 1, 0) and the line

3
1
−
2

=



+ t



x
y
z 






g. Containing the lines





1
0
1

−

.













1
1
1
1
1
0



−









x
y
z 


+ t



and

.









=




2
1
1

−

x
y
z
x
y
z









1
1
−
2

0
0
2





=

=

















+ t




+ t

h. Containing the lines

and



0
2
−
5

=



x
y
z 


1
1
3





3
1
0

+ t





−




.


Exercise 4.2.16 In each case, ﬁnd the shortest distance
from the point P to the plane and ﬁnd the point Q on the
plane closest to P.

a. P(2, 3, 0); plane with equation 5x + y + z = 1.

b. P(3, 1,

−

1); plane with equation 2x + y

z = 6.

−




i. Each point of which is
1).

1, 3) and Q(1, 1,

P(2,







equidistant

from

Exercise 4.2.17

a. Does the line through P(1, 2,

−

−

j. Each point of which is

equidistant

from

P(0, 1,

−

1) and Q(2,

1,

−

−

3).

Exercise 4.2.15 In each case, ﬁnd a vector equation of
the line.

a. Passing through P(3,

1, 4) and perpendicular to

the plane 3x

2y

−

−

−
z = 0.

b. Passing through P(2,
the plane 2x + y = 1.

−

1, 3) and perpendicular to

c. Passing through P(0, 0, 0) and perpendicular

x
y
z







+ t

2
0
1









−

1
1
0
1
1
5




.



+ t



−

to the lines



=





=



x
y
z 



2
1
3

−





d. Passing through P(1, 1,





1), and perpendicular to

−





the lines
x
y
z
x
y
z













=

=

2
0
1

+ t






+ t


5
5
2

−





1
1
2

−





−




1
2
3









and

.




3) with direction

−

lie in the plane 2x

y

−

−

z = 3?

vector d =

Explain.

1
2
3









−

b. Does the plane through P(4, 0, 5), Q(2, 2, 1), and

R(1,

−

1, 2) pass through the origin? Explain.

Show that every plane contain-
1) and Q(2, 0, 1) must also contain

Exercise 4.2.18
ing P(1, 2,
R(
1, 6,

−
5).

−

−

Exercise 4.2.19 Find the equations of the line of inter-
section of the following planes.

and

a. 2x

−

3y + 2z = 5 and x + 2y

z = 4.

−

b. 3x + y

−

2z = 1 and x + y + z = 5.

Exercise 4.2.20 In each case, ﬁnd all points of intersec-
tion of the given plane and the line





x
y
z

a.

c.

=





−

1
2
3

+ t




x




3y + 2z = 4

−
3x

−

y + z = 8

2
5
1

−





.





b.

d.

2x

y

−
x

−
4y

−

−

z = 5

3z = 6

−

Exercise 4.2.21 Find the equation of all planes:

240

Vector Geometry

a. Perpendicular to the line

2
1
3

3
0
2

.





.





2
1
−
3

=



+ t









x
y
z 


x
y
z 






b. Perpendicular to the line



=



+ t



1
0
1

−








c. Containing the origin.



d. Containing P(3, 2,

4).

−

e. Containing P(1, 1,

f. Containing P(2,

−
g. Containing the line

1) and Q(0, 1, 1).

−
1, 1) and Q(1, 0, 0).

2
1
0

=



+ t





−

x
y
z 






h. Containing the line







x
y
z



=





3
0
2

+ t





1
1
0

1
2
1

.





.



−
−










Exercise 4.2.22 If a plane contains two distinct points
P1 and P2, show that it contains every point on the line
through P1 and P2.



Exercise 4.2.23 Find the shortest distance between the
following pairs of parallel lines.

;





a.

b.

x
y
z 

x
y
z 


x
y
z 

x
y
z 


=

=

=

=

















2
1
−
3

+ t





−

1
1
4

1
0
1

3
0
2


+ t

+ t





−





3
1
0








+ t


1
−
2
2









1
1
4




3
1
0





;





















Exercise 4.2.24 Find the shortest distance between the
following pairs of nonparallel lines and ﬁnd the points on
the lines that are closest together.

;

;















2
1
3
−
1
0
1



1
1
1
3
1
0














1
0
2

x
y
z
x
y
z

=

=







3
0
1

−




1
1
1











x
y
z 

x
y
z 


x
y
z 

x
y
z 


x
y
z 

x
y
z 


1
1
0
2
1
3

3
1
1








3
1
0

=

=

=

=

=

=



−





−















−
1
2
0

1
2
3



−



+ s




+ t

+ s

+ t

+ s
















+ t

+ s








+ t





a.

b.

c.

d.


































1
1
1

−



;






2
0
1
−
1
1
0





;









Exercise 4.2.25 Show that two lines in the plane with
slopes m1 and m2 are perpendicular if and only if
1. [Hint: Example 4.1.11.]
m1m2 =

−

Exercise 4.2.26

a. Show that, of the four diagonals of a cube, no pair

is perpendicular.

b. Show that each diagonal is perpendicular to the

face diagonals it does not meet.

Exercise 4.2.27 Given a rectangular solid with sides of
lengths 1, 1, and √2, ﬁnd the angle between a diagonal
and one of the longest sides.

Exercise 4.2.28 Consider a rectangular solid with sides
of lengths a, b, and c. Show that it has two orthogonal
diagonals if and only if the sum of two of a2, b2, and c2
equals the third.

Exercise 4.2.29 Let A, B, and C(2,

−

tices of a triangle where −→AB is parallel to

1, 1) be the ver-

, −→AC is

1
1
−
1









parallel to

2
0
1
tion of the line through B and C.

−









, and angle C = 90◦ . Find the equa-

Exercise 4.2.30 If the diagonals of a parallelogram have
equal length, show that the parallelogram is a rectangle.

4.2. Projections and Planes

241

Exercise 4.2.38 Let u, v, and w be pairwise orthogonal
vectors.

a. Show that

2 =
u + v + w
k
k
b. If u, v, and w are all the same length, show that
they all make the same angle with u + v + w.

2 +
v
k
k

2.
w
k
k

2 +
k

u
k

Exercise 4.2.31 Given v =

x
y
z





in component form,

Exercise 4.2.39

show that the projections of v on i, j, and k are xi, yj, and
zk, respectively.





a. Show that n =

is orthogonal to every vector

(cid:20)
along the line ax + by + c = 0.

(cid:21)

a
b

Exercise 4.2.32

a. Can u

v =
·
your answer.

−

7 if

u
k

k

= 3 and

v
k

k

= 2? Defend

b. Show that the shortest distance from P0(x0, y0) to

the line is |

ax0+by0+c
|
√a2+b2

.

b. Find u

v if u =

2
1
2
between u and v is 2π
3 .

−





·

[Hint: If P1 is on the line, project u = −→P1P0 on n.]

= 6, and the angle

,


v
k

k



Exercise 4.2.40 Assume u and v are nonzero vectors
u is a
that are not parallel. Show that w =
k
nonzero vector that bisects the angle between u and v.

v +
k

u
k

v
k

Exercise 4.2.33 Show (u + v)
for any vectors u and v.

·

(u

−

v) =

u
k

2
k

v
− k

2
k

Exercise 4.2.34

Exercise 4.2.41 Let α, β, and γ be the angles a vector
v
= 0 makes with the positive x, y, and z axes, respec-
tively. Then cosα, cosβ, and cosγ are called the direc-
tion cosines of the vector v.

a. Show

2 +
u
u + v
k
k
k
any vectors u and v.

v

2 = 2(
u
k
k

2 +
k

v
k

2) for
k

−

b. What does this say about parallelograms?

Exercise 4.2.35 Show that if the diagonals of a paral-
lelogram are perpendicular, it is necessarily a rhombus.
[Hint: Example 4.2.5.]

Exercise 4.2.36 Let A and B be the end points of a di-
ameter of a circle (see the diagram). If C is any point on
the circle, show that AC and BC are perpendicular. [Hint:
−→AC) = 0 and −→BC in terms of u = −→OA
Express −→AB
×
and v = −→OC, where O is the centre.]

(−→AB

·

C

A

O

B

Exercise 4.2.37 Show that u and v are orthogonal, if
2 +
and only if
k

2 =
u + v
k
k

2.
v
k
k

u
k

a. If v =

a
b

c 
and cosγ = c


v
k
k

.

, show that cosα = a
v
k
k

, cosβ = b
v
k
k

,

b. Show that cos2 α+ cos2 β + cos2 γ = 1.

Exercise 4.2.42 Let v
= 0 be any nonzero vector and
suppose that a vector u can be written as u = p+q, where
p is parallel to v and q is orthogonal to v. Show that p
must equal the projection of u on v. [Hint: Argue as in
the proof of Theorem 4.2.4.]

= 0 be a nonzero vector and let
= 0 be a scalar. If u is any vector, show that the projec-

Exercise 4.2.43 Let v
a
tion of u on v equals the projection of u on av.

Exercise 4.2.44

a. Show that the Cauchy-Schwarz inequality

u
·
|
holds for all vectors u and v. [Hint:

u

v
| ≤ k
cosθ

v
kk
1 for all angles θ.]
| ≤
b. Show that

k

v

=

|

u
|
are parallel.

·

|

u
k

v
kk

k

[Hint: When is cosθ =

1?]

±

if and only if u and v

6
6
6
6
242

Vector Geometry

c. Show that

x1x2 + y1y2 + z1z2
|
|
2 + z2
2 + y2
x2
1 + z2
1 + y2
x2
2
1
≤
holds for all numbers x1, x2, y1, y2, z1, and z2.

q

q

d. Show that
and z.

xy + yz + zx
|

| ≤

x2 + y2 + z2 for all x, y,

4.3 More on the Cross Product

e. Show that (x + y + z)2

all x, y, and z.

≤

3(x2 + y2 + z2) holds for

Exercise 4.2.45
u + v
+
k
Consider the triangle with u and v as two sides.]

Prove that the triangle inequality
holds for all vectors u and v. [Hint:

k ≤ k

v
k

u

k

k

The cross product v

×

w of two R3-vectors v =

x1
y1
z1





and w =

x2
y2
z2



we observed that it can be best remembered using a determinant:






was deﬁned in Section 4.2 where





w = det

v

×





1
0
0 


0
1
0 


i x1 x2
j y1 y2
k z1
z2



=



1
0
0 


Here i =



, j =



, and k =



j +

i

−

y1 y2
z2 (cid:12)
z1
(cid:12)
(cid:12)
(cid:12)

x1 x2
z2 (cid:12)
z1
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
are the coordinate vectors, and the determinant is expanded

(cid:12)
(cid:12)
(cid:12)
(cid:12)

x1 x2
y1 y2 (cid:12)
(cid:12)
(cid:12)
(cid:12)

k

(4.3)

along the ﬁrst column. We observed (but did not prove) in Theorem 4.2.5 that v

v and w. This follows easily from the next result.





w is orthogonal to both

×

Theorem 4.3.1

Ifu=

x0
y0
z0





,v=









x1
y1
z1

,andw=





x2
y2
z2





,thenu



(v

×

·

w) = det



x0 x1 x2
y0 y1 y2
z2
z1
z0





.




Proof. Recall that u
(v
then adding. Using equation (4.3), the result is:

×

·

w) is computed by multiplying corresponding components of u and v

w and

×

u

(v

w) = x0

x1 x2
z2 (cid:12)
z1
(cid:19)
(cid:12)
(cid:12)
(cid:12)
where the last determinant is expanded along column 1.

y1 y2
z2 (cid:12)
z1
(cid:19)
(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ y0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

×

(cid:18)

·

+ z0

x1 x2
y1 y2 (cid:12)
(cid:19)
(cid:12)
(cid:12)
(cid:12)

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

= det





x0 x1 x2
y0 y1 y2
z2
z1
z0





The result in Theorem 4.3.1 can be succinctly stated as follows: If u, v, and w are three vectors in R3,

then

u

(v

·

×

w) = det

u v w

u v w

where
×
orthogonal to both v and w because the determinant of a matrix is zero if two columns are identical.

denotes the matrix with u, v, and w as its columns. Now it is clear that v

(cid:2)

(cid:3)

w is

Because of (4.3) and Theorem 4.3.1, several of the following properties of the cross product follow

(cid:2)

(cid:3)

from properties of determinants (they can also be veriﬁed directly).

Theorem 4.3.2
Letu,v,andwdenotearbitraryvectorsin R3.

4.3. More on the Cross Product

243

visavector.

6. (ku)

v= k(u

×

scalar k.

v) = u

×

(kv) forany

×

1. u

2. u

3. u

4. u

5. u

×

×

×

×

×

visorthogonaltobothuandv.

0= 0= 0

u.

×

u= 0.

v=

(v

−

×

u).

7. u

×

(v+ w) = (u

v) + (u

×

×

w).

8. (v+ w)

u= (v

×

×

u) + (w

×

u).

Proof. (1) is clear; (2) follows from Theorem 4.3.1; and (3) and (4) follow because the determinant of a
matrix is zero if one column is zero or if two columns are identical. If two columns are interchanged, the
determinant changes sign, and this proves (5). The proofs of (6), (7), and (8) are left as Exercise 4.3.15.

We now come to a fundamental relationship between the dot and cross products.

Theorem 4.3.3: Lagrange Identity12

Ifuandvareanytwovectorsin R3,then

u
k

×

v
k

2 =

u
k
k

2

2

v
k
k

(u

·

−

v)2

Proof. Given u and v, introduce a coordinate system and write u =

x1
y1
z1



and v =





x2
y2
z2



in component

form. Then all the terms in the identity can be computed in terms of the components. The detailed proof
is left as Exercise 4.3.14.









An expression for the magnitude of the vector u

If θ is the angle between u and v, substituting u

v can be easily obtained from the Lagrange identity.
u
cosθ into the Lagrange identity gives

v

×
v =

·
u
k

2

k
v
k

kk
k
2 cos2 θ =

u

v

2 =

u

2

2

v

k

×

k
k
cos2 θ = sin2 θ. But sinθ is nonnegative on the range 0

− k

k

k

k

k

k

k

k

v

2 sin2 θ

2

u
k

using the fact that 1
positive square root of both sides gives

−

θ

≤

≤

π, so taking the

u

k

v

k

×

=

k

u

kk

v
k

sinθ

12Joseph Louis Lagrange (1736–1813) was born in Italy and spent his early years in Turin. At the age of 19 he solved a
famous problem by inventing an entirely new method, known today as the calculus of variations, and went on to become one
of the greatest mathematicians of all time. His work brought a new level of rigour to analysis and his Mécanique Analytique
is a masterpiece in which he introduced methods still in use. In 1766 he was appointed to the Berlin Academy by Frederik the
Great who asserted that the “greatest mathematician in Europe” should be at the court of the “greatest king in Europe.” After
the death of Frederick, Lagrange went to Paris at the invitation of Louis XVI. He remained there throughout the revolution and
was made a count by Napoleon.

244

Vector Geometry

u

θ

sinθ

u
k

k

v

Figure 4.3.1

v

u

k

This expression for

makes no reference to a coordinate
system and, moreover, it has a nice geometrical interpretation. The
parallelogram determined by the vectors u and v has base length
v
sinθ (see Figure 4.3.1). Hence the area of the
k
k
parallelogram formed by u and v is

and altitude

u
k

×

k

k

(

u
k

k

sinθ)

=

v
k

k

u

k

v

k

×

This proves the ﬁrst part of Theorem 4.3.4.

Theorem 4.3.4

Ifuandvaretwononzerovectorsandθ istheanglebetweenuandv,then

1.

u
k

×

v
k

=

u
k

kk

v
k

sinθ = theareaoftheparallelogramdeterminedbyuandv.

2. uandvareparallelifandonlyifu

v = 0.

×

v = 0 if and only if the area of the parallelogram is zero. By Figure 4.3.1 the area
Proof of (2). By (1), u
vanishes if and only if u and v have the same or opposite direction—that is, if and only if they are parallel.

×

Example 4.3.1

P

R
and so equals 1
2k

Find the area of the triangle with vertices P(2, 1, 0), Q(3,
and R(1, 0, 1).

1, 1),

−

Q

Solution. We have −→RP =

1
1
1 
the triangle is half the area of the parallelogram (see the diagram),


2
1
0 
−


and −→RQ =

. The area of

−









−→RP

−→RQ
k

×

. We have

−→RP

×

−→RQ = det



i
j
k

1
1
1
−

2
1
−
0 


2 √1 + 4 + 9 = 1
= 1
2

=




√14.

1
−
2
−
3 
−


so the area of the triangle is 1
2k

−→RP

×

−→RQ

k

v

u

×
w

h

u

v

Figure 4.3.2

If three vectors u, v, and w are given, they determine a “squashed”
rectangular solid called a parallelepiped (Figure 4.3.2), and it is often
useful to be able to ﬁnd the volume of such a solid. The base of the solid
is the parallelogram determined by u and v, so it has area A =
by
Theorem 4.3.4. The height of the solid is the length h of the projection of
w on u

v. Hence

×

u

k

k

v

×

h =

(cid:12)
(cid:12)
(cid:12)

(u
w
·
u
k

v)
×
2
v
k
×

(cid:12)
(cid:12)
(cid:12)

u

k

v

k

×

= |

w

(u
·
u
k

v)
×
v
k
×

= |

|

w

·

v)

(u
×
A

|

Thus the volume of the parallelepiped is hA =

w

|

·

(u

v)

|

×

. This proves

Theorem 4.3.5

4.3. More on the Cross Product

245

Thevolumeoftheparallelepipeddeterminedbythreevectorsw,u,andv(Figure4.3.2)isgiven
by

(u

v)

w
·
|

.
|

×

Example 4.3.2

Find the volume of the parallelepiped determined by the vectors

w =

1
2
1 


−





, u =

, v =

1
1
0 






2
−
0
1 






Solution. By Theorem 4.3.1, w

(u

·

×

v) = det

w

|

·

(u

v)

|

×

=

3

|

| −

= 3 by Theorem 4.3.5.

1 1
2 1
1 0

−





=

2
−
0
1 


3. Hence the volume is

−

y

O

z

x

Left-hand system

z

v

k

u

u

=

We can now give an intrinsic description of the cross product u

v.
×
= 0, its
Its magnitude
direction is very nearly determined by the fact that it is orthogonal to both
u and v and so points along the line normal to the plane determined by u
and v. It remains only to decide which of the two possible directions is
correct.

sinθ is coordinate-free. If u

v
k

kk

×

×

v

k

k

x

y

O

Before this can be done, the basic issue of how coordinates are as-
signed must be clariﬁed. When coordinate axes are chosen in space, the
procedure is as follows: An origin is selected, two perpendicular lines (the
x and y axes) are chosen through the origin, and a positive direction on
each of these axes is selected quite arbitrarily. Then the line through the
origin normal to this x-y plane is called the z axis, but there is a choice of
which direction on this axis is the positive one. The two possibilities are
shown in Figure 4.3.3, and it is a standard convention that cartesian coor-
dinates are always right-hand coordinate systems. The reason for this
terminology is that, in such a system, if the z axis is grasped in the right hand with the thumb pointing in
the positive z direction, then the ﬁngers curl around from the positive x axis to the positive y axis (through
a right angle).

Right-hand system

Figure 4.3.3

Suppose now that u and v are given and that θ is the angle between them (so 0

direction of

u

v

is given by the right-hand rule.

k
Right-hand Rule

×

k

θ

≤

≤

π). Then the

Ifthevectoru
angleθ,thethumbpointsinthedirectionforu

×

visgraspedintherighthandandtheﬁngerscurlaroundfromutovthroughthe

v.

×

6
246

Vector Geometry

z

O

a

u

θ

v

b
x

c

y

Figure 4.3.4

Exercises for 4.3

To indicate why this is true, introduce coordinates in R3 as follows: Let
u and v have a common tail O, choose the origin at O, choose the x axis
so that u points in the positive x direction, and then choose the y axis
so that v is in the x-y plane and the positive y axis is on the same side
of the x axis as v. Then, in this system, u and v have component form

where a > 0 and c > 0. The situation is depicted

v should point in the



u =

and v =

a
0
0 


b
c
0 
in Figure 4.3.4. The right-hand rule asserts that u


positive z direction. But our deﬁnition of u





×
v gives

v = det

u

×



i a b
j 0 c
k 0 0 


×

=



0
0
ac 


= (ac)k


and (ac)k has the positive z direction because ac > 0.



Exercise 4.3.1 If i, j, and k are the coordinate vectors,
verify that i

k = i, and k

j = k, j

i = j.

Exercise 4.3.5 Find the volume of the parallelepiped
determined by w, u, and v when:

×

×

Exercise 4.3.2 Show that u
(u

×
w by calculating both when

(v

v)

×

×

×

×
w) need not equal

u =

1
1
1









, v =

1
1
0









, and w =

0
0
1









Exercise 4.3.3 Find two unit vectors orthogonal to both
u and v if:

a. u =

b. u =

1
2
2

, v =





−



, v =


1
2
1

−









2
1
2

3
1
2

















Exercise 4.3.4 Find the area of the triangle with the fol-
lowing vertices.

a. A(3,

−

1, 2), B(1, 1, 0), and C(1, 2,

b. A(3, 0, 1), B(5, 1, 0), and C(7, 2,

c. A(1, 1,

d. A(3,

−

1), B(2, 0, 1), and C(1,

−
1, 1), B(4, 1, 0), and C(2,

−
1, 3)

−

3, 0)

−

1)

−
1)

a. w =

b. w =







1
0
2







, v =

, v =







2
1
1

1
0
3


2
1
3

, and u =



, and u =









−

2
1
1

1
1
1









−
Exercise 4.3.6 Let P0 be a point with vector p0, and let
ax + by + cz = d be the equation of a plane with normal











n =



.

a
b
c 



a. Show that the point on the plane closest to P0 has

vector p given by

p = p0 + d

n)
(p0·
2 n.
−
n
k
k

[Hint: p = p0 + tn for some t, and p

n = d.]

·

b. Show that the shortest distance from P0 to the

d
plane is |

n)

.

|

(p0·
−
n
k
k

c. Let P′0 denote the reﬂection of P0 in the plane—
that is, the point on the opposite side of the plane
such that the line through P0 and P′0 is perpendicu-
lar to the plane.
Show that p0 + 2 d

(p0·
n)
2 n is the vector of P′0.
−
n
k
k

Exercise 4.3.7 Simplify (au + bv)

(cu + dv).

×

.

k

d

Exercise 4.3.8 Show that the shortest distance from a
point P to the line through P0 with direction vector d is
−→P0P
×
k
d
k
k

Exercise 4.3.9 Let u and v be nonzero, nonorthogo-
nal vectors. If θ is the angle between them, show that
u
v
v .
tanθ = k
ku
×
·

Exercise 4.3.10 Show that points A, B, and C are all on
one line if and only if −→AB

−→AC = 0

×

Exercise 4.3.11 Show that points A, B, C, and D are all
on one plane if and only if −→AB

−→AC) = 0

(−→AB

·

×

Exercise 4.3.12 Use Theorem 4.3.5 to conﬁrm that, if
u, v, and w are mutually perpendicular, the (rectangular)
parallelepiped they determine has volume

w

u
k

v
kk

kk

.
k

Exercise 4.3.13
lelepiped determined by u, v, and u

Show that the volume of the paral-

v is

u
k

×

2.
v
k

×

Exercise 4.3.14 Complete the proof of Theorem 4.3.3.

Exercise 4.3.15 Prove the following properties in The-
orem 4.3.2.

b.

Property 7

a.

c.

Property 6

Property 8

Exercise 4.3.16

4.3. More on the Cross Product

247

Exercise 4.3.20 Let P, Q, R, and S be four points, not
all on one plane, as in the diagram. Show that the volume
of the pyramid they determine is

−→PQ

1
6 |

·

(−→PR

−→PS)
.
|

×

[Hint: The volume of a cone with base area A and height
h as in the diagram below right is 1

3 Ah.]

Q

h

P

S

R

Exercise 4.3.21 Consider a triangle with vertices A, B,
and C, as in the diagram below. Let α, β, and γ denote
the angles at A, B, and C, respectively, and let a, b, and
c denote the lengths of the sides opposite A, B, and C,
respectively. Write u = −→AB, v = −→BC, and w = −→CA.

B

β

a. Show that w

(u
×
holds for all vectors w, u, and v.

v) = u

(v

×

·

·

w) = v

(w

u)

×

×

c

α

A

a

b

γ

C

b. Show that v

−
are orthogonal.

w and (u

v) + (v

w) + (w

u)

×

×

×

Exercise 4.3.17 Show u
v)w.
(v
[Hint: First do it for u = i, j, and k; then write u =
xi + yj + zk and use Theorem 4.3.2.]

w) = (u

w)v

(u

×

×

×

−

·

Exercise 4.3.18 Prove the Jacobi identity:

(v

u

×

×

w) + v

(w

×

×

u) + w

(u

×

×

v) = 0

[Hint: The preceding exercise.]

Exercise 4.3.19 Show that

(u

v)

·

×

(w

×

z) = det

u
v

·
·

w u
w v

z
z

·
·

(cid:21)

(cid:20)

[Hint: Exercises 4.3.16 and 4.3.17.]

a. Deduce that u + v + w = 0.

b. Show that u

v = w

u = v

w. [Hint: Compute

×
×
(u + v + w) and v

×

(u + v + w).]

u

×

×

c. Deduce the law of sines:

sinα

a = sinβ

b = sinγ

c

Exercise 4.3.22 Show that the (shortest) distance be-
tween two planes n
p = d2 with n as nor-
p = d1 and n
mal is |

d2

d1

·

·

.

|

−
n
k
k

248

Vector Geometry

Exercise 4.3.23 Let A and B be points other than the
origin, and let a and b be their vectors. If a and b are not
parallel, show that the plane through A, B, and the origin
is given by

x
y
z 




P(x, y, z)
{

| 

= sa + tb for some s and t

}

Exercise 4.3.24 Let A be a 2
rows r1 and r2. Show that

3 matrix of rank 2 with

×

is the plane through the origin with normal r1

r2.

×

Exercise 4.3.25 Given the cube with vertices P(x, y, z),
where each of x, y, and z is either 0 or 2, consider the
plane perpendicular to the diagonal through P(0, 0, 0)
and P(2, 2, 2) and bisecting it.

a. Show that the plane meets six of the edges of the

cube and bisects them.

b. Show that the six points in (a) are the vertices of a

P =

X A
{

|

X = [xy]; x, y arbitrary

}

regular hexagon.

4.4 Linear Operators on R3

Recall that a transformation T : Rn
Rm is called linear if T (x + y) = T (x) + T (y) and T (ax) = aT (x)
holds for all x and y in Rn and all scalars a. In this case we showed (in Theorem 2.6.2) that there exists
n matrix A such that T (x) = Ax for all x in Rn, and we say that T is the matrix transformation
an m
×
induced by A.

→

Deﬁnition 4.9 Linear Operator on Rn

Alineartransformation

iscalledalinearoperatoron Rn.

T : Rn

Rn

→

In Section 2.6 we investigated three important linear operators on R2: rotations about the origin, reﬂections
in a line through the origin, and projections on this line.

In this section we investigate the analogous operators on R3: Rotations about a line through the origin,
reﬂections in a plane through the origin, and projections onto a plane or line through the origin in R3. In
every case we show that the operator is linear, and we ﬁnd the matrices of all the reﬂections and projections.
To do this we must prove that these reﬂections, projections, and rotations are actually linear operators
on R3. In the case of reﬂections and rotations, it is convenient to examine a more general situation. A
transformation T : R3
R3 is said to be distance preserving if the distance between T (v) and T (w) is
the same as the distance between v and w for all v and w in R3; that is,

→

T (v)

T (w)

=

w

v

k

−

k

k

−

k

for all v and w in R3

(4.4)

Clearly reﬂections and rotations are distance preserving, and both carry 0 to 0, so the following theorem
shows that they are both linear.

Theorem 4.4.1
If T : R3

→

R3 isdistancepreserving,andif T (0) = 0,then T islinear.

4.4. Linear Operators on R3

249

z

T (v + w)

T (w)

x

T (v)

w

v

y

v + w

Figure 4.4.1

Proof. Since T (0) = 0, taking w = 0 in (4.4) shows that
for
=
k
k
k
2 =
2
all v in R3, that is T preserves length. Also,
w
v
k
k
−
2 always holds, it follows
2 =
w +
w
by (4.4). Since
k
k
w for all v and w. Hence (by Theorem 4.2.2) the
that T (v)
angle between T (v) and T (w) is the same as the angle between v and w
for all (nonzero) vectors v and w in R3.

T (v)
k
w
k

k
−
T (w) = v

k
T (w)

T (v)

2v

−

−

v

k

v

k

k

v

2

·

·

·

With this we can show that T is linear. Given nonzero vectors v and w
in R3, the vector v + w is the diagonal of the parallelogram determined by
v and w. By the preceding paragraph, the effect of T is to carry this entire
parallelogram to the parallelogram determined by T (v) and T (w), with
diagonal T (v + w). But this diagonal is T (v) + T (w) by the parallelogram
law (see Figure 4.4.1).

In other words, T (v + w) = T (v) + T (w). A similar argument shows that T (av) = aT (v) for all scalars

a, proving that T is indeed linear.

Distance-preserving linear operators are called isometries, and we return to them in Section 10.4.

Reﬂections and Projections

In Section 2.6 we studied the reﬂection Qm : R2
the same line. We found (in Theorems 2.6.5 and 2.6.6) that they are both linear and

R2 in the line y = mx and projection Pm : R2

→

R2 on

→

Qm has matrix

1

m2

−
2m

1
1+m2

(cid:20)

2m

m2

1

−

(cid:21)

and

Pm has matrix

1
1+m2

(cid:20)

1 m
m m2

.

(cid:21)

v

L
PL(v)

0

QL(v)

Figure 4.4.2

We now look at the analogues in R3.
Let L denote a line through the origin in R3. Given a vector v in R3,
the reﬂection QL(v) of v in L and the projection PL(v) of v on L are deﬁned
in Figure 4.4.2. In the same ﬁgure, we see that
PL(v) = v + 1

v] = 1

(4.5)

2[QL(v) + v]

2[QL(v)

−

so the fact that QL is linear (by Theorem 4.4.1) shows that PL is also lin-
ear.13

However, Theorem 4.2.4 gives us the matrix of PL directly. In fact, if d =

= 0 is a direction

a
b
c 






vector for L, and we write v =



, then

x
y
z 


2 d = ax+by+cz
d
PL(v) = v
·
d
k
k

a2+b2+c2 

a
b
c 




=

1
a2+b2+c2 

a2 ab ac
ab b2 bc
ac bc

x
y
z 


c2 








as the reader can verify. Note that this shows directly that PL is a matrix transformation and so gives
another proof that it is linear.

13Note that Theorem 4.4.1 does not apply to PL since it does not preserve distance.

6
250

Vector Geometry

Theorem 4.4.2

Let L denotethelinethroughtheoriginin R3 withdirectionvectord=

QL arebothlinearand

PL hasmatrix

1
a2+b2+c2 

QL hasmatrix

1
a2+b2+c2 

a2



−

−

b2
2ab
2ac


c2

b2

= 0. Then PL and

a
b
c 






2ac
2bc
a2

−

−

b2 


a2 ab ac
ab b2 bc
ac bc

c2 


2ab
a2
2bc

−

−

c2

c2

Proof. It remains to ﬁnd the matrix of QL. But (4.5) implies that QL(v) = 2PL(v)

v for each v in R3, so

−

we obtain (with some matrix arithmetic):

if v =

x
y
z 






QL(v) =




2
a2+b2+c2 

=


1
a2+b2+c2 




a2

a2 ab ac
ab b2 bc
ac bc
b2
2ab
2ac

c2 

c2
b2

−

−

1 0 0
0 1 0
0 0 1 


c2

c2

− 


2ab
a2
2bc

−

−






2ac

2bc
a2

−

−

x
y
z 


b2 






x
y
z 


as required.

v

O PM(v)

M

QM(v)

Figure 4.4.3

In R3 we can reﬂect in planes as well as lines. Let M denote a plane
through the origin in R3. Given a vector v in R3, the reﬂection QM(v) of
v in M and the projection PM(v) of v on M are deﬁned in Figure 4.4.3. As
above, we have

PM(v) = v + 1

2[QM(v)

v] = 1

2[QM(v) + v]

−

so the fact that QM is linear (again by Theorem 4.4.1) shows that PM is
also linear.

Again we can obtain the matrix directly. If n is a normal for the plane M, then Figure 4.4.3 shows that

PM(v) = v

−

projn v = v

n
v
2 n for all vectors v.
·
n
k
k

−

If n =

a
b
c 






= 0 and v =

x
y
z 






, a computation like the above gives

PM(v) =

1 0 0
0 1 0
0 0 1 






x
y
z 






ax+by+cz
a2+b2+c2 

−



a
b
c 


6
6
4.4. Linear Operators on R3

251

=

1
a2+b2+c2 



b2 + c2
ab
ac

−
−

ab
−
a2 + c2
bc

−

ac
−
bc
b2 + c2 
−






x
y
z 


This proves the ﬁrst part of

Theorem 4.4.3

Let M denotetheplanethroughtheoriginin R3 withnormaln=

= 0. Then PM and QM are

bothlinearand

PM hasmatrix

1
a2+b2+c2 

QM hasmatrix

1
a2+b2+c2 







a
b
c 

ac
−
bc
a2 + b2 
−

2ac
2bc

−
−
a2 + b2

c2 


−

b2 + c2
ab
ac

−
−
a2

ab
−
a2 + c2
bc

−

2ab

−
a2 + c2

b2

−
2bc

−


b2 + c2

−
2ab
2ac

−
−

Proof. It remains to compute the matrix of QM. Since QM(v) = 2PM(v)
tation is similar to the above and is left as an exercise for the reader.

−

v for each v in R3, the compu-

Rotations

In Section 2.6 we studied the rotation Rθ : R2
θ. Moreover, we showed in Theorem 2.6.4 that Rθ is linear and has matrix

→

R2 counterclockwise about the origin through the angle

sinθ
cosθ
sinθ cosθ

−

(cid:20)

. One

(cid:21)

extension of this is given in the following example.

Example 4.4.1

Let Rz, θ : R3
axis toward the positive y axis. Show that Rz, θ is linear and ﬁnd its matrix.

R3 denote rotation of R3 about the z axis through an angle θ from the positive x

→

z

k

Rz(j)

θ

i

θ

x

Rz(i)

y

j

Figure 4.4.4

Solution. First R is distance preserving and so is linear by
Theorem 4.4.1. Hence we apply Theorem 2.6.2 to obtain the
matrix of Rz, θ.

1
0
0 


0
1
0 


Let i =



, j =



, and k =



denote the standard



basis of R3; we must ﬁnd Rz, θ(i), Rz, θ(j), and Rz, θ(k). Clearly
Rz, θ(k) = k. The effect of Rz, θ on the x-y plane is to rotate
it counterclockwise through the angle θ. Hence Figure 4.4.4 gives





0
0
1 


Rz, θ(i) =

cosθ
sinθ
0 






, Rz, θ(j) =

sinθ
−
cosθ
0









6
252

Vector Geometry

so, by Theorem 2.6.2, Rz, θ has matrix

Rz, θ(i) Rz, θ(j) Rz, θ(k)

=

(cid:2)

(cid:3)





cosθ
sinθ 0
−
sinθ cosθ 0

0

0

1 


Example 4.4.1 begs to be generalized. Given a line L through the origin in R3, every rotation about L
through a ﬁxed angle is clearly distance preserving, and so is a linear operator by Theorem 4.4.1. However,
giving a precise description of the matrix of this rotation is not easy and will have to wait until more
techniques are available.

Transformations of Areas and Volumes

v

sv

Origin

Figure 4.4.5

v
sv

s v

+ t w

O

tw

w

Figure 4.4.6

u

w

v

O

T (w)

T (u)

O

T (v)

Figure 4.4.7

Let v be a nonzero vector in R3. Each vector in the same direction as v
whose length is a fraction s of the length of v has the form sv (see Fig-
ure 4.4.5).

With this, scrutiny of Figure 4.4.6 shows that a vector u is in the paral-
lelogram determined by v and w if and only if it has the form u = sv + tw
R3 is a linear trans-
where 0
≤
formation, we have

1. But then, if T : R3

1 and 0

→

≤

≤

≤

s

t

T (sv + tw) = T (sv) + T (tw) = sT (v) + tT (w)

Hence T (sv + tw) is in the parallelogram determined by T (v) and T (w).
Conversely, every vector in this parallelogram has the form T (sv + tw)
where sv +tw is in the parallelogram determined by v and w. For this rea-
son, the parallelogram determined by T (v) and T (w) is called the image
of the parallelogram determined by v and w. We record this discussion as:

Theorem 4.4.4
If T : R3
R2)isalinearoperator,theimageofthe
parallelogramdeterminedbyvectorsvandwistheparallelogram
determinedby T (v) and T (w).

R3 (or R2

→

→

This result is illustrated in Figure 4.4.7, and was used in Examples 2.2.15
and 2.2.16 to reveal the effect of expansion and shear transformations.
We now describe the effect of a linear transformation T : R3

R3 on
the parallelepiped determined by three vectors u, v, and w in R3 (see the
discussion preceding Theorem 4.3.5). If T has matrix A, Theorem 4.4.4
shows that this parallelepiped is carried to the parallelepiped determined
by T (u) = Au, T (v) = Av, and T (w) = Aw. In particular, we want to
discover how the volume changes, and it turns out to be closely related to
the determinant of the matrix A.

→

4.4. Linear Operators on R3

253

Theorem 4.4.5

Let vol (u, v, w) denotethevolumeoftheparallelepipeddeterminedbythreevectorsu,v,andw
in R3,andletarea (p, q) denotetheareaoftheparallelogramdeterminedbytwovectorspandq
in R2. Then:

1. If A isa 3

2. If A isa 2

×

×

Proof.

3 matrix,then vol (Au, Av, Aw) =

det (A)

vol (u, v, w).

| ·

2 matrix,then area (Ap, Aq) =

|

area (p, q).

| ·

|
det (A)

1. Let

u v w

denote the 3

×

3 matrix with columns u, v, and w. Then

(cid:2)

(cid:3)

·
by Theorem 4.3.5. Now apply Theorem 4.3.1 twice to get

|

vol (Au, Av, Aw) =

Au

(Av

Aw)

|

×

(Av

Au

·

×

Aw) = det

Au Av Aw

(cid:2)

(cid:3)

= det (A
= det (A) det
= det (A)(u

(cid:2)

u v w

)

u v w
(cid:3)
w))

×

(v
(cid:2)

(cid:3)

·

where we used Deﬁnition 2.9 and the product theorem for determinants. Finally (1) follows from
Theorem 4.3.5 by taking absolute values.

k

q1

p1

as required.

x
y
0 


2. Given p =

x
y

(cid:20)

(cid:21)

in R2, p1 =



in R3. By the diagram,

area (p, q) = vol (p1, q1, k) where k is the (length 1) coordinate

vector along the z axis. If A is a 2

2 matrix, write A1 =

(cid:21)
in block form, and observe that (Av)1 = (A1v1) for all v in R2 and
A1k = k. Hence part (1) of this theorem shows

(cid:20)

A 0
0 1

×

area (Ap, Aq) = vol (A1p1, A1q1, A1k)

=
=

|
|

det (A1)
det (A)

|

vol (p1, q1, k)
|
area (p, q)

Deﬁne the unit square and unit cube to be the square and cube corresponding to the coordinate
vectors in R2 and R3, respectively. Then Theorem 4.4.5 gives a geometrical meaning to the determinant
of a matrix A:

• If A is a 2

by A;

×

2 matrix, then

det (A)

|

|

is the area of the image of the unit square under multiplication

254

Vector Geometry

• If A is a 3
by A.

×

3 matrix, then

det (A)

|

|

is the volume of the image of the unit cube under multiplication

These results, together with the importance of areas and volumes in geometry, were among the reasons for
the initial development of determinants.

Exercises for 4.4

Exercise 4.4.1
In each case show that that T is ei-
ther projection on a line, reﬂection in a line, or rotation
through an angle, and ﬁnd the line or angle.

x
y

x
y

x
y

x
y

x
y

x
y

a. T

b. T

c. T

d. T

e. T

f. T

(cid:20)

(cid:20)

(cid:20)

(cid:20)

(cid:20)

(cid:20)

(cid:21)

x + 2y
2x + 4y

y
x

x
y

−
−

(cid:21)

(cid:20)

x
−
x

y
−
y
−
3x + 4y
−
4x + 3y

= 1
5

= 1
2

(cid:20)

(cid:20)

= 1
√2

= 1
5

(cid:20)

(cid:21)

(cid:21)

(cid:21)

(cid:21)

=

(cid:21)

(cid:20)

y
x

−
−

= 1
2

(cid:21)

(cid:20)

(cid:21)
√3y
x
√3x + y

−

(cid:21)

(cid:21)

(cid:21)

a. Rotation through π

2 , followed by projection on the

y axis, followed by reﬂection in the line y = x.

b. Projection on the line y = x followed by projection

on the line y =

x.

−

c. Projection on the x axis followed by reﬂection in

the line y = x.

Exercise 4.4.3 In each case solve the problem by ﬁnd-
ing the matrix of the operator.

a. Find the projection of v =



−

with equation 3x

5y + 2z = 0.



−

1
2
3





on the plane

b. Find the projection of v =



with equation 2x

y + 4z = 0.


−

−

0
1
3

c. Find the reﬂection of v =



with equation x

y + 3z = 0.


−

d. Find the reﬂection of v =



with equation 2x + y

5z = 0.


−

1
2
−
3

0
1
3

−













on the plane

in the plane

in the plane

in the line with

2
5
1

−





on the line

on the line









in the line with

1
1
−
7

1
1
3

−

.












2
0
3

−

2
5
−
0

e. Find the reﬂection of v =



equation

x
y
z 






= t





−


1
1
2

.




with equation

x
y
z 






= t


3
0
4

.








g. Find the projection of v =

with equation

= t

x
y
z 






h. Find the reﬂection of v =







equation

x
y
z









= t


1
1
3

.








−

Exercise 4.4.2 Determine the effect of the following
transformations.

f. Find the projection of v =



Exercise 4.4.4

a. Find the rotation of v =

through θ = π
4 .

2
3
1

−









b. Find the rotation of v =

through θ = π
6 .

1
0
3









Exercise 4.4.5 Find the matrix of the rotation in R3
about the x axis through the angle θ (from the positive
y axis to the positive z axis).

Exercise 4.4.6 Find the matrix of the rotation about the
y axis through the angle θ (from the positive x axis to the
positive z axis).

If A is 3

Exercise 4.4.7
3, show that the image of
×
the line in R3 through p0 with direction vector d is the
line through Ap0 with direction vector Ad, assuming that
Ad

= 0. What happens if Ad = 0?

×

Exercise 4.4.8 If A is 3
3 and invertible, show that the
image of the plane through the origin with normal n is
the plane through the origin with normal n1 = Bn where
w = vT w to show
B = (A−
that n1

1)T . [Hint: Use the fact that v
·
p for each p in R3.]
(Ap) = n

·

·

4.5. An Application to Computer Graphics

255

Exercise 4.4.9 Let L be the line through the origin in R2
a
b

with direction vector d =

= 0.

(cid:20)

(cid:21)

about the z axis

a. If PL denotes projection on L, show that PL has

about the z axis

matrix

1
a2+b2

(cid:20)

a2
ab
ab b2

.

(cid:21)

b. If QL denotes reﬂection in L, show that QL has ma-

trix

1
a2+b2

(cid:20)

a2

b2

−
2ab

2ab

b2

a2

−

.
(cid:21)

Exercise 4.4.10 Let n be a nonzero vector in R3, let L be
the line through the origin with direction vector n, and let
M be the plane through the origin with normal n. Show
that PL(v) = QL(v) + PM(v) for all v in R3. [In this case,
we say that PL = QL + PM.]

Exercise 4.4.11 If M is the plane through the origin in

R3 with normal n =

a
b
c 






, show that QM has matrix

b2 + c2

a2

−
2ab
2ac

−
−

1
a2+b2+c2 



−
a2 + c2

2ab

−
2bc

−

b2

2ac
2bc

−
−
a2 + b2

−

c2 


4.5 An Application to Computer Graphics

Computer graphics deals with images displayed on a computer screen, and so arises in a variety of appli-
cations, ranging from word processors, to Star Wars animations, to video games, to wire-frame images of
an airplane. These images consist of a number of points on the screen, together with instructions on how
to ﬁll in areas bounded by lines and curves. Often curves are approximated by a set of short straight-line
segments, so that the curve is speciﬁed by a series of points on the screen at the end of these segments.
Matrix transformations are important here because matrix images of straight line segments are again line
segments.14 Note that a colour image requires that three images are sent, one to each of the red, green,
and blue phosphorus dots on the screen, in varying intensities.

14If v0 and v1 are vectors, the vector from v0 to v1 is d = v1
v1 if and only if v = v0 + td for some number t in the range 0
Av = Av0 + tAd with 0

t

1, that is the image is the segment between Av0 and Av1.

v0. So a vector v lies on the line segment between v0 and
−
t
1. Thus the image of this segment is the set of vectors
≤

≤

≤

≤

6
6
256

Vector Geometry

Figure 4.5.1

5

4

1
Origin

3

2

Figure 4.5.2

Figure 4.5.3

Consider displaying the letter A. In reality, it is depicted on the screen, as in
Figure 4.5.1, by specifying the coordinates of the 11 corners and ﬁlling in the
interior.

For simplicity, we will disregard the thickness of the letter, so we require

only ﬁve coordinates as in Figure 4.5.2.

This simpliﬁed letter can then be stored as a data matrix

Vertex

1 2 3 4 5

D =

(cid:20)

0 6 5 1 3
0 0 3 3 9

(cid:21)

where the columns are the coordinates of the vertices in order. Then if we want
2 matrix A, we left-multiply this data matrix by
to transform the letter by a 2
A (the effect is to multiply each column by A and so transform each vertex).

×

For example, we can slant the letter to the right by multiplying by an x-shear

matrix A =

(cid:20)

1 0.2
0 1

(cid:21)
1 0.2
0 1

A =

(cid:20)

—see Section 2.2. The result is the letter with data matrix

0 6 5 1 3
0 0 3 3 9

=

(cid:21)

(cid:20)

(cid:21) (cid:20)

0 6 5.6 1.6 4.8
0 0 3

3

9

(cid:21)

which is shown in Figure 4.5.3.

If we want to make this slanted matrix narrower, we can now apply an x-

scale matrix B =

that shrinks the x-coordinate by 0.8. The result is

0.8 0
1
0

(cid:21)
the composite transformation

(cid:20)

BAD =

=

(cid:20)

(cid:20)

0.8 0
1
0

1 0.2
0 1

0 6 5 1 3
0 0 3 3 9

(cid:21) (cid:20)

(cid:21) (cid:20)

0 4.8 4.48 1.28 3.84
0 0

3

3

9

(cid:21)

(cid:21)

which is drawn in Figure 4.5.4.

On the other hand, we can rotate the letter about the origin through π

6 (or 30◦)
0.5
−
0.866

.
(cid:21)

by multiplying by the matrix Rπ
2

=

Figure 4.5.4

This gives

cos( π
6 )
sin( π
6 )





−

sin( π
6 )
6 ) 
cos( π


0.866
0.5

=

(cid:20)

=

Rπ
2

=

(cid:20)

(cid:20)

0.866
0.5

0.5
−
0.866

0 6 5 1 3
0 0 3 3 9

(cid:21) (cid:20)

0 5.196 2.83
0 3

5.098

0.634
3.098

−

(cid:21)
1.902
9.294

−

(cid:21)

and is plotted in Figure 4.5.5.

Figure 4.5.5

This poses a problem: How do we rotate at a point other than the origin? It
turns out that we can do this when we have solved another more basic problem.

4.5. An Application to Computer Graphics

257

It is clearly important to be able to translate a screen image by a ﬁxed vector w, that is apply the transfor-
R2 given by Tw(v) = v + w for all v in R2. The problem is that these translations are
mation Tw : R2
not matrix transformations R2
R2 because they do not carry 0 to 0 (unless w = 0). However, there is a
clever way around this.

→

→

The idea is to represent a point v =

nates of v. Then translation by w =

(cid:20)
p
q

(cid:20)

(cid:21)

x
y

(cid:21)

as a 3

×

1 column



x
y
1 




can be achieved by multiplying by a 3

3 matrix:

×

, called the homogeneous coordi-



1 0 p
0 1 q
0 0 1 




x
y
1 


=



x + p
y + q

1 


Tw(v)
1

=

(cid:20)

(cid:21)


Thus, by using homogeneous coordinates we can implement the translation Tw in the top two coordinates.





On the other hand, the matrix transformation induced by A =

a b
c d

(cid:21)

is also given by a 3

3 matrix:

×

=



(cid:20)

Av
1

(cid:21)

(cid:20)
ax + by
cx + dy
1

=





a b 0
c d 0
0 0 1 




x
y
1 



So everything can be accomplished at the expense of using 3






3 matrices and homogeneous coordinates.

×

Example 4.5.1

Rotate the letter A in Figure 4.5.2 through π

6 about the point

4
5

.

(cid:21)

(cid:20)

Solution. Using homogeneous coordinates for the vertices of the letter results in a data matrix with
three rows:

Kd =



0 6 5 1 3
0 0 3 3 9
1 1 1 1 1 




4
5

Origin

Figure 4.5.6

If we write w =

, the idea is to use a composite of

(cid:20)

(cid:21)
w so that the point
transformations: First translate the letter by
w moves to the origin, then rotate this translated letter, and then
translate it by w back to its original position. The matrix arithmetic
is as follows (remember the order of composition!):

−

0.866
0.5
0

1 0 4
0 1 5
0 0 1 

3.036
1.33 1.67






=



−
1

0.5 0
−
0.866 0

0 1 


1 0
0 1
0 0




8.232 5.866 2.402 1.134
3.768 1.768 7.964
1



1

1

1

This is plotted in Figure 4.5.6.



4
−
5
−
1 






0 6 5 1 3
0 0 3 3 9
1 1 1 1 1 






258

Vector Geometry

This discussion merely touches the surface of computer graphics, and the reader is referred to special-
ized books on the subject. Realistic graphic rendering requires an enormous number of matrix calcula-
tions. In fact, matrix multiplication algorithms are now embedded in microchip circuits, and can perform
over 100 million matrix multiplications per second. This is particularly important in the ﬁeld of three-
dimensional graphics where the homogeneous coordinates have four components and 4
4 matrices are
required.

×

Exercises for 4.5

Exercise 4.5.1 Consider the letter A described in Fig-
ure 4.5.2. Find the data matrix for the letter obtained by:

a. Rotating the letter through π

4 about the origin.

b. Rotating the letter through π

4 about

the point

1
2

.

(cid:21)

(cid:20)

Exercise 4.5.2 Find the matrix for turning the letter A
in Figure 4.5.2 upside-down in place.

Exercise 4.5.4 Find the 3
the angle θ about the point P(a, b).

×

3 matrix for rotating through

Exercise 4.5.5 Find the reﬂection of the point P in the
line y = 1 + 2x in R2 if:

a. P = P(1, 1)

b. P = P(1, 4)

Exercise 4.5.3 Find the 3

the line y = mx + b. Use

the line.

(cid:20)

×
1
m

3 matrix for reﬂecting in

c. What about P = P(1, 3)? Explain. [Hint: Exam-

as direction vector for

ple 4.5.1 and Section 4.4.]

(cid:21)

Supplementary Exercises for Chapter 4

Exercise 4.1 Suppose that u and v are nonzero vectors.
If u and v are not parallel, and au + bv = a1u + b1v, show
that a = a1 and b = b1.

Exercise 4.2 Consider a triangle with vertices A, B,
and C. Let E and F be the midpoints of sides AB and
AC, respectively, and let the medians EC and FB meet at
O. Write −→EO = s−→EC and −→FO = t−→FB, where s and t are
scalars. Show that s = t = 1
3 by expressing −→AO two ways
in the form a−→EO + b−→AC, and applying Exercise 4.1. Con-
clude that the medians of a triangle meet at the point on
each that is one-third of the way from the midpoint to the
vertex (and so are concurrent).

Exercise 4.3 A river ﬂows at 1 km/h and a swimmer
moves at 2 km/h (relative to the water). At what angle
must he swim to go straight across? What is his resulting
speed?

Exercise 4.4 A wind is blowing from the south at 75
knots, and an airplane ﬂies heading east at 100 knots.
Find the resulting velocity of the airplane.

Exercise 4.5 An airplane pilot ﬂies at 300 km/h in a di-

rection 30◦ south of east. The wind is blowing from the
south at 150 km/h.

a. Find the resulting direction and speed of the air-

plane.

b. Find the speed of the airplane if the wind is from

the west (at 150 km/h).

Exercise 4.6 A rescue boat has a top speed of 13 knots.
The captain wants to go due east as fast as possible in wa-
ter with a current of 5 knots due south. Find the velocity
vector v = (x, y) that she must achieve, assuming the x
and y axes point east and north, respectively, and ﬁnd her
resulting speed.

Exercise 4.7 A boat goes 12 knots heading north. The
current is 5 knots from the west. In what direction does
the boat actually move and at what speed?

d

a

n

Exercise 4.8 Show that the distance from a point A (with
p = d is
vector a) to the plane with vector equation n
1
n
k |
k
Exercise 4.9 If two distinct points lie in a plane, show
that the line through these points is contained in the
plane.

.
|

−

·

·

4.5. An Application to Computer Graphics

259

Exercise 4.10 The line through a vertex of a triangle,
perpendicular to the opposite side, is called an altitude
of the triangle. Show that the three altitudes of any tri-
angle are concurrent. (The intersection of the altitudes
is called the orthocentre of the triangle.) [Hint: If P is
the intersection of two of the altitudes, show that the line
through P and the remaining vertex is perpendicular to
the remaining side.]

Chapter 5

5.1 Subspaces and Spanning

Vector Space Rn

In Section 2.2 we introduced the set Rn of all n-tuples (called vectors), and began our investigation of the
matrix transformations Rn
Rm given by matrix multiplication by an m
n matrix. Particular attention
was paid to the euclidean plane R2 where certain simple geometric transformations were seen to be ma-
trix transformations. Then in Section 2.6 we introduced linear transformations, showed that they are all
matrix transformations, and found the matrices of rotations and reﬂections in R2. We returned to this in
Section 4.4 where we showed that projections, reﬂections, and rotations of R2 and R3 were all linear, and
where we related areas and volumes to determinants.

→

×

In this chapter we investigate Rn in full generality, and introduce some of the most important concepts
and methods in linear algebra. The n-tuples in Rn will continue to be denoted x, y, and so on, and will be
written as rows or columns depending on the context.

Subspaces of Rn

Deﬁnition 5.1 Subspace of Rn

Aset1U ofvectorsin Rn iscalledasubspaceof Rn ifitsatisﬁesthefollowingproperties:

S1. Thezerovector0

U.

S2. Ifx

S3. Ifx

∈

∈

U andy

∈
U,then ax

∈
U,thenx+ y

U.

∈

U foreveryrealnumber a.

∈

We say that the subset U is closed under addition if S2 holds, and that U is closed under scalar multi-
plication if S3 holds.

Clearly Rn is a subspace of itself, and this chapter is about these subspaces and their properties. The
, consisting of only the zero vector, is also a subspace because 0 + 0 = 0 and a0 = 0 for each a
set U =
in R; it is called the zero subspace. Any subspace of Rn other than {0} or Rn is called a proper subspace.
We saw in Section 4.2 that every plane M through the origin in R3
has equation ax + by + cz = 0 where a, b, and c are not all zero. Here

0
}
z

{

n

y

n =





x

M

a
b
c 

M =

is a normal for the plane and

|
1We use the language of sets. Informally, a set X is a collection of objects, called the elements of the set. The fact that x is
X. Two sets X and Y are called equal (written X = Y ) if they have the same elements. If every
X both hold if and only if

an element of X is denoted x
element of X is in the set Y , we say that X is a subset of Y , and write X
X = Y .

Y . Hence X

Y and Y

⊆

⊆

⊆

}

{

∈

·

v in R3

n

v = 0

261

262

Vector Space Rn

where v =



and n

·

v denotes the dot product introduced in Section 2.2 (see the diagram).2 Then M

is a subspace of R3. Indeed we show that M satisﬁes S1, S2, and S3 as follows:



x
y
z 


S1. 0

∈
S2. If v

S3. If v

M because n

0 = 0;

·

M and v1

∈

M , then n

M , then n

·

(av) = a(n

·

·

∈

∈

This proves the ﬁrst part of

Example 5.1.1

(v + v1) = n

v + n

·

·

v1 = 0 + 0 = 0 , so v + v1

M;

∈

v) = a(0) = 0 , so av

M.

∈

Planes and lines through the origin in R3 are all subspaces of R3.

z

L

d

y

x

Solution. We dealt with planes above. If L is a line through
the origin with direction vector d, then L =
(see
the diagram). We leave it as an exercise to verify that L satisﬁes
S1, S2, and S3.

td
{

R

∈

}

t

|

Example 5.1.1 shows that lines through the origin in R2 are subspaces; in fact, they are the only proper
subspaces of R2 (Exercise 5.1.24). Indeed, we shall see in Example 5.2.14 that lines and planes through
the origin in R3 are the only proper subspaces of R3. Thus the geometry of lines and planes through the
origin is captured by the subspace concept. (Note that every line or plane is just a translation of one of
these.)

Subspaces can also be used to describe important features of an m
×
denoted null A, and the image space of A, denoted im A, are deﬁned by

n matrix A. The null space of A,

null A =

x

{

∈

Rn

|

Ax = 0

}

and

im A =

Ax

{

x

|

∈

Rn

}

In the language of Chapter 2, null A consists of all solutions x in Rn of the homogeneous system Ax = 0,
and im A is the set of all vectors y in Rm such that Ax = y has a solution x. Note that x is in null A if it
satisﬁes the condition Ax = 0, while im A consists of vectors of the form Ax for some x in Rn. These two
ways to describe subsets occur frequently.

Example 5.1.2

If A is an m

×

n matrix, then:

1. null A is a subspace of Rn.

2We are using set notation here. In general {q

|

p} means the set of all objects q with property p.

2.

im A is a subspace of Rm.

5.1. Subspaces and Spanning

263

Solution.

1. The zero vector 0

and ax are in null A because they satisfy the required condition:

Rn lies in null A because A0 = 0.3 If x and x1 are in null A, then x + x1

A(x + x1) = Ax + Ax1 = 0 + 0 = 0 and A(ax) = a(Ax) = a0 = 0

Hence null A satisﬁes S1, S2, and S3, and so is a subspace of Rn.

2. The zero vector 0

Rm lies in im A because 0 = A0. Suppose that y and y1 are in im A, say

y = Ax and y1 = Ax1 where x and x1 are in Rn. Then

∈

∈

y + y1 = Ax + Ax1 = A(x + x1)

and ay = a(Ax) = A(ax)

show that y + y1 and ay are both in im A (they have the required form). Hence im A is a
subspace of Rm.

There are other important subspaces associated with a matrix A that clarify basic properties of A. If A

is an n

×

n matrix and λ is any number, let

Eλ(A) =

x

{

∈

Rn

|

Ax = λx

}

A vector x is in Eλ(A) if and only if (λI

−

A)x = 0, so Example 5.1.2 gives:

Example 5.1.3

Eλ(A) = null (λI

−

A) is a subspace of Rn for each n

n matrix A and number λ.

×

Eλ(A) is called the eigenspace of A corresponding to λ. The reason for the name is that, in the terminology
0
of Section 3.3, λ is an eigenvalue of A if Eλ(A)
. In this case the nonzero vectors in Eλ(A) are called
}
the eigenvectors of A corresponding to λ.

=

{

The reader should not get the impression that every subset of Rn is a subspace. For example:

U1 =

x
y

x

≥

0

satisﬁes S1 and S2, but not S3;

(cid:26)(cid:20)

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21) (cid:12)
(cid:12)
(cid:12)
Hence neither U1 nor U2 is a subspace of R2. (However, see Exercise 5.1.20.)
(cid:12)

(cid:27)
x2 = y2

satisﬁes S1 and S3, but not S2;

U2 =

(cid:26)(cid:20)

x
y

(cid:27)

3We are using 0 to represent the zero vector in both Rm and Rn. This abuse of notation is common and causes no confusion

once everybody knows what is going on.

6
264

Vector Space Rn

Spanning Sets

Let v and w be two nonzero, nonparallel vectors in R3 with their tails at the origin. The plane M through
the origin containing these vectors is described in Section 4.2 by saying that n = v
w is a normal for M,
p = 0.4 While this is a very useful way to look at planes,
and that M consists of all vectors p such that n
there is another approach that is at least as useful in R3 and, more importantly, works for all subspaces of
Rn for any n

×

1.

·

≥

av

v

0

w

bw

p

M

The idea is as follows: Observe that, by the diagram, a vector p is in

M if and only if it has the form

p = av + bw

for certain real numbers a and b (we say that p is a linear combination of
v and w). Hence we can describe M as

and we say that
describe all subspaces of Rn.

v, w
}

{

M =

ax + bw

a, b

R

.5

}
is a spanning set for M. It is this notion of a spanning set that provides a way to

∈

{

|

As in Section 1.3, given vectors x1, x2, . . . , xk in Rn, a vector of the form
+ tkxk where the ti are scalars

t1x1 + t2x2 +

· · ·

is called a linear combination of the xi, and ti is called the coefﬁcient of xi in the linear combination.

Deﬁnition 5.2 Linear Combinations and Span in Rn

Thesetofallsuchlinearcombinationsiscalledthespanofthexi andisdenoted

span

x1, x2, . . . , xk}
{
,wesaythatV isspannedbythevectorsx1, x2, . . . , xk,andthatthe
x1, x2, . . . , xk}
IfV = span
{
vectorsx1, x2, . . . , xk spanthespaceV.

t1x1 + t2x2 +
{

+ tkxk |

ti in R

· · ·

=

}

Here are two examples:

which we write as span

x
}

{

x
{
= Rx for simplicity.

span

}

=

t

tx
{

|

R

}

∈

span

x, y

=

rx + sy

r, s

{

}

{

|

R

∈

}

In particular, the above discussion shows that, if v and w are two nonzero, nonparallel vectors in R3, then

is the plane in R3 containing v and w. Moreover, if d is any nonzero vector in R3 (or R2), then

M = span

v, w

{

}

td
{
is the line with direction vector d. Hence lines and planes can both be described in terms of spanning sets.

L = span

v
}

= Rd

=

R

∈

}

{

t

|

4The vector n = v
5In particular, this implies that any vector p orthogonal to v

w is nonzero because v and w are not parallel.

×

some a and b. Can you prove this directly?

w must be a linear combination p = av + bw of v and w for

×

Example 5.1.4

5.1. Subspaces and Spanning

265

Let x = (2,
q = (2, 3, 1, 2) are in U = span

1, 2, 1) and y = (3, 4,

−

1, 1) in R4. Determine whether p = (0,
.

11, 8, 1) or

−

−
x, y
}

{

Solution. The vector p is in U if and only if p = sx + ty for scalars s and t. Equating components
gives equations

2s + 3t = 0,

s + 4t =

11,

−
This linear system has solution s = 3 and t =
q = sx + ty leads to equations

−

−

t = 8,

and

s + t = 1

2s

−

2, so p is in U . On the other hand, asking that

2s + 3t = 2,

s + 4t = 3,

2s

t = 1,

and s + t = 2

−
and this system has no solution. So q does not lie in U .

−

Theorem 5.1.1: Span Theorem

LetU = span

x1, x2, . . . , xk
{

}

in Rn. Then:

1. U isasubspaceof Rn containingeachxi.

2. IfW isasubspaceof Rn andeachxi

W,thenU

W.

⊆

∈

Proof.

1. The zero vector 0 is in U because 0 = 0x1 + 0x2 +

x = t1x1 + t2x2 +
because

· · ·

+ tkxk and y = s1x1 + s2x2 +

· · ·

+ 0xk is a linear combination of the xi. If
· · ·
+ skxk are in U , then x + y and ax are in U

x + y = (t1 + s1)x1 + (t2 + s2)x2 +

+ (tk + sk)xk, and

ax = (at1)x1 + (at2)x2 +

· · ·
Finally each xi is in U (for example, x2 = 0x1 + 1x2 +
U , proving (1).

· · ·
+ (atk)xk

+ 0xk) so S1, S2, and S3 are satisﬁed for

· · ·

2. Let x = t1x1 + t2x2 +

+ tkxk where the ti are scalars and each xi

W . Then each tixi

W satisﬁes S3. But then x

W because W satisﬁes S2 (verify). This proves (2).

∈

· · ·

∈

W because

∈

Condition (2) in Theorem 5.1.1 can be expressed by saying that span

is the smallest
subspace of Rn that contains each xi. This is useful for showing that two subspaces U and W are equal,
since this amounts to showing that both U

U . Here is an example of how it is used.

x1, x2, . . . , xk

W and W

}

{

⊆

⊆

Example 5.1.5

If x and y are in Rn, show that span

x, y

}

{

= span

x + y, x

y

.

}

−

{

266

Vector Space Rn

But x = 1

2 (x + y) + 1

2(x

−

y) are both in span

x + y, x

, so

y

}

−

{

Solution. Since both x + y and x

y are in span

x, y

−
span

x + y, x

{
y) and y = 1
2 (x + y)

{
y

−
1
2(x

−
span

, Theorem 5.1.1 gives

}

span

x, y

}

{

} ⊆

−
x + y, x

{
x + y, x

{

−

}

y

−
y

}
, as desired.

span

again by Theorem 5.1.1. Thus span

{

x, y

{
x, y
}

} ⊆
= span

It turns out that many important subspaces are best described by giving a spanning set. Here are three
n identity matrix
is called the

examples, beginning with an important spanning set for Rn itself. Column j of the n
In is denoted e j and called the jth coordinate vector in Rn, and the set

e1, e2, . . . , en

×

{

}

is any vector in Rn, then x = x1e1 + x2e2 +

+ xnen, as the reader

· · ·

standard basis of Rn. If x = 





can verify. This proves:

x1
x2
...
xn








Example 5.1.6

Rn = span

{

e1, e2, . . . , en

}

where e1, e2, . . . , en are the columns of In.

If A is an m
for null A and im A.

×

n matrix A, the next two examples show that it is a routine matter to ﬁnd spanning sets

Example 5.1.7

Given an m
by the gaussian algorithm. Then

×

n matrix A, let x1, x2, . . . , xk denote the basic solutions to the system Ax = 0 given

null A = span

x1, x2, . . . , xk

{

}

null A, then Ax = 0 so Theorem 1.3.2 shows that x is a linear combination of the

Solution. If x
∈
basic solutions; that is, null A
span

x1, x2, . . . , xk

{

⊆

span

, then x = t1x1 + t2x2 +

}
Ax = t1Ax1 + t2Ax2 +

x1, x2, . . . , xk}
· · ·
+ tkAxk = t10 + t20 +

+ tkxk for scalars ti, so

{

. On the other hand, if x is in

+ tk0 = 0

· · ·

· · ·

This shows that x

∈

null A, and hence that span

x1, x2, . . . , xk

{

} ⊆

null A. Thus we have equality.

5.1. Subspaces and Spanning

267

Example 5.1.8

Let c1, c2, . . . , cn denote the columns of the m

n matrix A. Then

×

im A = span

c1, c2, . . . , cn

{

}

Solution. If

{

e1, e2, . . ., en

is the standard basis of Rn, observe that

Ae1 Ae2

· · ·

= A

e1 e2

· · ·

en

= AIn = A =

c1 c2

Hence ci = Aei is in im A for each i, so span

(cid:3)

(cid:2)

(cid:2)

c1, c2, . . . , cn

(cid:3)

{

} ⊆

cn

.

· · ·

(cid:3)

}
Aen

Conversely, let y be in im A, say y = Ax for some x in Rn. If x = 




c1, c2,

y = Ax = x1c1 + x2c2 +

+ xncn is in span



, then Deﬁnition 2.5 gives





. . . , cn

}

· · ·
c1, c2, . . . , cn

}

{

, and the result follows.

(cid:2)

im A.
x1
x2
...
xn

This shows that im A

span

{

⊆

Exercises for 5.1

We often write vectors in Rn as rows.
Exercise 5.1.1 In each case determine whether U is a
subspace of R3. Support your answer.

s and t in R

s and t in R

.
}
.
}

r, s, and t in R,

a. U =

b. U =

c. U =

−
d. U =

|

|

(1, s, t)
{
(0, s, t)
{
(r, s, t)
{
r + 3s + 2t = 0
.
}
2)

|

(r, 3s, r
{
(r, 0, s)
{
(2r,
{

−

|
s2, t)

e. U =

f. U =

r and s in R

|

−
r2 + s2 = 0, r and s in R

.
}

r, s, and t in R

.
}

|

b. x = (1, 2, 15, 11), y = (2,
1,

z = (1,

3, 1).

1, 0, 2), and

−

−
c. x = (8, 3,

−

−
1, 0, 2,

z = (

−

3).

−

13, 20), y = (2, 1,

3, 5), and

−

d. x = (2, 5, 8, 3), y = (2,
1, 2, 2,

z = (

3).

−

−

1, 0, 5), and

−

Exercise 5.1.3 In each case determine if the given vec-
tors span R4. Support your answer.

.
}

a.

b.

.
(1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), (0, 0, 0, 1)
{
}
(1, 3,
{
(1,

2, 1, 0, 0), (0, 2, 1,

5, 0), (

1),

−

−

−
.
4, 5, 0)
}

−

Exercise 5.1.2 In each case determine if x lies in U =
. If x is in U , write it as a linear combination
span
}
of y and z; if x is not in U , show why not.

y, z
{

a. x = (2,

1, 0, 1), y = (1, 0, 0, 1), and

−
z = (0, 1, 0, 1).

Exercise 5.1.4 Is it possible that
can span the subspace U =
fend your answer.

(1, 2, 0), (2, 0, 3)
}
{
(r, s, 0)
? De-
}
{

r and s in R

|

Exercise 5.1.5 Give a spanning set for the zero subspace
0
}
{

of Rn.

268

Vector Space Rn

Exercise 5.1.6 Is R2 a subspace of R3? Defend your
answer.

Exercise 5.1.7 If U = span
U = span

x + tz, y, z
}
{

x, y, z
}
{

for every t in R.

Exercise 5.1.8 If U = span
x, y, z
{
}
x + y, y + z, z + x
U = span
.
}
{

in Rn, show that

in Rn, show that

If a
x
{

= span

Exercise 5.1.9
ax
span
{

}
Exercise 5.1.10
scalars,
span

show that
x1, x2, . . . , xk
{

}
Exercise 5.1.11 If x
x
of span
{

.
}

= 0 is a scalar,
for every vector x in Rn.

show that

. . . , ak are nonzero
=

}
If a1, a2,
a1x1, a2x2,
span
{
for any vectors xi in Rn.
= 0 in Rn, determine all subspaces

. . . , akxk

}

Exercise 5.1.12 Suppose that U = span
where each xi is in Rn. If A is an m
for each i, show that Ay = 0 for every vector y in U .

x1, x2, . . . , xk
}
{
n matrix and Axi = 0

×

Exercise 5.1.13 If A is an m
each invertible m

×

m matrix U , null (A) = null (UA).

n matrix, show that, for

Exercise 5.1.14 If A is an m
each invertible n

×

n matrix V , im (A) = im (AV ).

n matrix, show that, for

Exercise 5.1.15 Let U be a subspace of Rn, and let x be
a vector in Rn.

×

×

a. If ax is in U where a

= 0 is a number, show that x

is in U .

b. If y and x + y are in U where y is a vector in Rn,

show that x is in U .

Exercise 5.1.16 In each case either show that the state-
ment is true or give an example showing that it is false.

a. If U

= Rn is a subspace of Rn and x + y is in U ,

then x and y are both in U .

e. The empty set of vectors in Rn is a subspace of

Rn.

0
1

f.

(cid:20)

(cid:21)

is in span

1
0

,

(cid:21)

(cid:20)

2
0

.
(cid:21)(cid:27)

(cid:26)(cid:20)

Exercise 5.1.17

a. If A and B are m

n matrices, show that

×

U =

x in Rn | Ax = Bx
}
{
n, B is k
b. What if A is m

×

is a subspace of Rn.

n, and m

= k?

×

· · ·
= span

+ akxk where a1

Exercise 5.1.18 Suppose that x1, x2, . . . , xk are vectors
in Rn. If y = a1x1 + a2x2 +
= 0, show
.
that span
}

x1 x2, . . . , xk
{
Exercise 5.1.19 If U
that U = R.
Exercise 5.1.20 Let U be a nonempty subset of Rn.
Show that U is a subspace if and only if S2 and S3 hold.

y1, x2, . . . , xk
{

is a subspace of R, show

0
{

}
=

}

⊆

Exercise 5.1.21 If S and T are nonempty sets of vectors
in Rn, and if S

T , show that span

span

S
{

} ⊆

T
{

.
}

Exercise 5.1.22 Let U and W be subspaces of Rn. De-
ﬁne their intersection U
W and their sum U + W as
follows:
Rn
W =
U
∈
Rn
U +W =
∈
and a vector in W
.
}

x belongs to both U and W
x is a sum of a vector in U

x
{
x
{

.
}

|
|

∩

∩

a. Show that U

∩

W is a subspace of Rn.

b. Show that U +W is a subspace of Rn.

Exercise 5.1.23 Let P denote an invertible n
If λ is a number, show that

×

n matrix.

Eλ(PAP−

1) =

Px
{

x is in Eλ(A)
}

|

b. If U is a subspace of Rn and rx is in U for all r in

for each n

n matrix A.

R, then x is in U .

c. If U is a subspace of Rn and x is in U , then

also in U .

x is

−

d. If x is in U and U = span
x, y, z
.
}
{

span

y, z
,
}
{

then U =

×

Exercise 5.1.24 Show that every proper subspace U of
R2 is a line through the origin. [Hint: If d is a nonzero
vector in U , let L = Rd =
rd
denote the line
{
with direction vector d. If u is in U but not in L, argue
geometrically that every vector v in R2 is a linear combi-
nation of u and d.]

r in R

}

|

6
6
6
6
6
6
6
5.2 Independence and Dimension

5.2. Independence and Dimension

269

is a subspace of Rn, then every
Some spanning sets are better than others. If U = span
vector in U can be written as a linear combination of the xi in at least one way. Our interest here is
in spanning sets where each vector in U has exactly one representation as a linear combination of these
vectors.

x1, x2, . . . , xk}

{

Linear Independence

Given x1, x2, . . . , xk in Rn, suppose that two linear combinations are equal:

We are looking for a condition on the set
is unique; that is, ri = si for each i. Taking all terms to the left side gives

{

of vectors that guarantees that this representation

r1x1 + r2x2 +

+ rkxk = s1x1 + s2x2 +

+ skxk

· · ·

· · ·
x1, x2, . . . , xk}

(r1

−

s1)x1 + (r2

s2)x2 +

+ (rk

−

· · ·

−

sk)xk = 0

so the required condition is that this equation forces all the coefﬁcients ri

si to be zero.

−

Deﬁnition 5.3 Linear Independence in Rn

Withthisinmind,wecallaset
independent) ifitsatisﬁesthefollowingcondition:

x1, x2, . . . , xk}
{

ofvectorslinearlyindependent (orsimply

If t1x1 + t2x2 +

· · ·

+ tkxk = 0thent1 = t2 =

= tk = 0

· · ·

We record the result of the above discussion for reference.

Theorem 5.2.1

x1, x2, . . . , xk
If
{
span

}
x1, x2, . . . , xk
{

isanindependentsetofvectorsin Rn,theneveryvectorin

hasauniquerepresentationasalinearcombinationofthexi.

}

It is useful to state the deﬁnition of independence in different language. Let us say that a linear
combination vanishes if it equals the zero vector, and call a linear combination trivial if every coefﬁcient
is zero. Then the deﬁnition of independence can be compactly stated as follows:

A set of vectors is independent if and only if the only linear combination that vanishes is the
trivial one.

Hence we have a procedure for checking that a set of vectors is independent:

Independence Test

Toverifythataset

x1, x2, . . . , xk
{

}

ofvectorsin Rn isindependent,proceedasfollows:

1. Setalinearcombinationequaltozero: t1x1 + t2x2 +

+ tkxk = 0.

· · ·

270

Vector Space Rn

2. Showthat ti = 0 foreach i (thatis,thelinearcombinationistrivial).

Ofcourse,ifsomenontriviallinearcombinationvanishes,thevectorsarenotindependent.

Example 5.2.1

Determine whether

(1, 0,

{

2, 5), (2, 1, 0,

−

1), (1, 1, 2, 1)

−

}

is independent in R4.

Solution. Suppose a linear combination vanishes:

r(1, 0,

2, 5) + s(2, 1, 0,

−

1) + t(1, 1, 2, 1) = (0, 0, 0, 0)

−

Equating corresponding entries gives a system of four equations:

r + 2s + t = 0, s + t = 0,

2r + 2t = 0, and 5r

−

s + t = 0

−

The only solution is the trivial one r = s = t = 0 (verify), so these vectors are independent by the
independence test.

Example 5.2.2

Show that the standard basis

e1, e2, . . . , en

{

}

of Rn is independent.

Solution. The components of t1e1 + t2e2 +
· · ·
Example 5.1.6) So the linear combination vanishes if and only if each ti = 0. Hence the
independence test applies.

+ tnen are t1, t2, . . . , tn (see the discussion preceding

Example 5.2.3

x, y

If

{

}

is independent, show that

2x + 3y, x

5y

is also independent.

Solution. If s(2x + 3y) + t(x
x, y
{
}
equations have only the trivial solution s = t = 0, as required.

is independent this combination must be trivial; that is, 2s + t = 0 and 3s

−

−

{

}
5y) = 0, collect terms to get (2s + t)x + (3s

−

5t)y = 0. Since

5t = 0. These

−

Example 5.2.4
Show that the zero vector in Rn does not belong to any independent set.

Solution. No set
nontrivial linear combination 1

0, x1, x2, . . . , xk

{

}

0 + 0x1 + 0x2 +

+ 0xk = 0.

· · ·

·

of vectors is independent because we have a vanishing,

5.2. Independence and Dimension

271

Example 5.2.5

Given x in Rn, show that

x

}

{

is independent if and only if x

= 0.

Solution. A vanishing linear combination from
t = 0 because x

= 0.

x

}

{

takes the form tx = 0, t in R. This implies that

The next example will be needed later.

Example 5.2.6

Show that the nonzero rows of a row-echelon matrix R are independent.

Solution. We illustrate the case with 3 leading 1s; the general case is analogous. Suppose R has the

0 1
∗ ∗ ∗ ∗
0 0 0 1
∗ ∗
0 0 0 0 1
∗
0 0 0 0 0 0





form R = 







where

∗

indicates a nonspeciﬁed number. Let R1, R2, and R3

denote the nonzero rows of R. If t1R1 + t2R2 + t3R3 = 0 we show that t1 = 0, then t2 = 0, and
ﬁnally t3 = 0. The condition t1R1 + t2R2 + t3R3 = 0 becomes

(0, t1,

,

∗

,

∗

,

∗

∗

) + (0, 0, 0, t2,

,

∗

∗

) + (0, 0, 0, 0, t3,

∗

) = (0, 0, 0, 0, 0, 0)

Equating second entries show that t1 = 0, so the condition becomes t2R2 + t3R3 = 0. Now the same
argument shows that t2 = 0. Finally, this gives t3R3 = 0 and we obtain t3 = 0.

A set of vectors in Rn is called linearly dependent (or simply dependent) if it is not linearly indepen-

dent, equivalently if some nontrivial linear combination vanishes.

Example 5.2.7

If v and w are nonzero vectors in R3, show that
parallel.

{

v, w

}

is dependent if and only if v and w are

Solution. If v and w are parallel, then one is a scalar multiple of the other (Theorem 4.1.5), say
v, w
v = aw for some scalar a. Then the nontrivial linear combination v
is dependent.
Conversely, if
and w are parallel (by Theorem 4.1.5). A similar argument works if t

is dependent, let sv + tw = 0 be nontrivial, say s

aw = 0 vanishes, so

= 0. Then v =

t
s w so v

v, w

−

−

= 0.

}

{

{

}

With this we can give a geometric description of what it means for a set

dependent. Note that this requirement means that
0u + av + bw = 0), so M = span
{
Example 5.1.4). So we assume that

v, w

}
v, w
}

{

in R3 to be in-
is also independent (av + bw = 0 means that
is the plane containing v, w, and 0 (see the discussion preceding

u, v, w

v, w

}

{

{

}

is independent in the following example.

6
6
6
6
272

Vector Space Rn

Example 5.2.8

u

v

w

M
independent

u, v, w
{

}

v
u
w

M
not independent

u, v, w
{

}

Let u, v, and w be nonzero vectors in R3 where
u, v, w
independent. Show that
if u is not in the plane M = span
the diagrams.

}
{

{

}

is independent if and only
v, w
. This is illustrated in

v, w

}

{

}

}

−

−

av

u, v, w

, say u = av + bw, where a and b are in R. Then

is independent, suppose u is in the plane

Solution. If
{
v, w
M = span
{
1u
u, v, w
bw = 0, contradicting the independence of
On the other hand, suppose that u is not in M; we must show
is independent. If ru + sv + tw = 0 where r, s,
that
t
s
and t are in R3, then r = 0 since otherwise u =
r w is
r v + −
in M. But then sv + tw = 0, so s = t = 0 by our assumption.
This shows that

is independent, as required.

u, v, w

u, v, w

.

}

−

{

}

{

{

}

By the inverse theorem, the following conditions are equivalent for an n

n matrix A:

×

1. A is invertible.

2. If Ax = 0 where x is in Rn, then x = 0.

3. Ax = b has a solution x for every vector b in Rn.

While condition 1 makes no sense if A is not square, conditions 2 and 3 are meaningful for any matrix A
and, in fact, are related to independence and spanning. Indeed, if c1, c2, . . . , cn are the columns of A, and

if we write x = 







, then

x1
x2
...
xn






Ax = x1c1 + x2c2 +

+ xncn

· · ·

by Deﬁnition 2.5. Hence the deﬁnitions of independence and spanning show, respectively, that condition
2 is equivalent to the independence of
and condition 3 is equivalent to the requirement
that span

= Rm. This discussion is summarized in the following theorem:

c1, c2, . . . , cn

c1, c2, . . . , cn

}

{

{

}

Theorem 5.2.2

If A isan m

n matrix,let

c1, c2, . . . , cn
{

}

×

denotethecolumnsof A.

1.

c1, c2, . . . , cn
{

}

isindependentin Rm ifandonlyif Ax= 0,xin Rn,impliesx= 0.

2. Rm = span

c1, c2, . . . , cn
{

}

ifandonlyif Ax= bhasasolutionxforeveryvectorbin Rm.

For a square matrix A, Theorem 5.2.2 characterizes the invertibility of A in terms of the spanning and
independence of its columns (see the discussion preceding Theorem 5.2.2). It is important to be able to
discuss these notions for rows. If x1, x2, . . . , xk are 1
to be

n rows, we deﬁne span

x1, x2, . . . , xk

×

{

}

the set of all linear combinations of the xi (as matrices), and we say that
independent if the only vanishing linear combination is the trivial one (that is, if
independent in Rn, as the reader can verify).6

x1, x2, . . . , xk
1 , xT
xT

{

{

}

is linearly
is

2 , . . . , xT
k }

5.2. Independence and Dimension

273

Theorem 5.2.3

Thefollowingareequivalentforan n

n matrix A:

×

1. A isinvertible.

2. Thecolumnsof A arelinearlyindependent.

3. Thecolumnsof A span Rn.

4. Therowsof A arelinearlyindependent.

5. Therowsof A spanthesetofall 1

n rows.

×

Proof. Let c1, c2, . . . , cn denote the columns of A.

(2). By Theorem 2.4.5, A is invertible if and only if Ax = 0 implies x = 0; this holds if and only

is independent by Theorem 5.2.2.

(3). Again by Theorem 2.4.5, A is invertible if and only if Ax = b has a solution for every

column B in Rn; this holds if and only if span

c1, c2, . . . , cn

= Rn by Theorem 5.2.2.

⇔

(1)

(4). The matrix A is invertible if and only if AT is invertible (by Corollary 2.4.1 to Theorem
2.4.4); this in turn holds if and only if AT has independent columns (by (1)
(2)); ﬁnally, this last
statement holds if and only if A has independent rows (because the rows of A are the transposes of the
columns of AT ).

⇔

{

}

⇔

(1)
c1, c2, . . . , cn
(1)

}

if

{

⇔

(1)

⇔

(5). The proof is similar to (1)

(4).

⇔

Example 5.2.9

Show that S =

(2,

{

2, 5), (

−

3, 1, 1), (2, 7,

−

Solution. Consider the matrix A =



is independent in R3.

4)

−

}
5
1
4 


2
−
1
7

2
3
−
2
= 0, so A is invertible. Hence S is independent by

with the vectors in S as its rows. A routine

−

computation shows that det A =
117

−
Theorem 5.2.3. Note that Theorem 5.2.3 also shows that R3 = span S.

6It is best to view columns and rows as just two different notations for ordered n-tuples. This discussion will become

redundant in Chapter 6 where we deﬁne the general notion of a vector space.

6
274

Vector Space Rn

Dimension

It is common geometrical language to say that R3 is 3-dimensional, that planes are 2-dimensional and
that lines are 1-dimensional. The next theorem is a basic tool for clarifying this idea of “dimension”. Its
importance is difﬁcult to exaggerate.

Theorem 5.2.4: Fundamental Theorem
LetU beasubspaceof Rn. IfU isspannedby m vectors,andifU contains k linearlyindependent
vectors,then k

m.

≤

This proof is given in Theorem 6.3.2 in much greater generality.

Deﬁnition 5.4 Basis of Rn
IfU isasubspaceof Rn,aset
thefollowingtwoconditions:

x1, x2, . . . , xm
{

}

ofvectorsinU iscalledabasisofU ifitsatisﬁes

1.

x1, x2, . . . , xm
{

}

islinearlyindependent.

2. U = span

x1, x2, . . . , xm
{

.
}

The most remarkable result about bases7 is:

Theorem 5.2.5: Invariance Theorem

If

x1, x2, . . . , xm
{

}

and

y1, y2, . . . , yk}
{

arebasesofasubspaceU of Rn,then m = k.

Proof. We have k
is independent. Similarly, by interchanging x’s and y’s we get m

m by the fundamental theorem because

≤

{

x1, x2, . . . , xm

spans U , and

}
k. Hence m = k.

≤

y1, y2, . . . , yk}

{

The invariance theorem guarantees that there is no ambiguity in the following deﬁnition:

Deﬁnition 5.5 Dimension of a Subspace of Rn

IfU isasubspaceof Rn and
x1, x2, . . . , xm
{
basisiscalledthedimensionofU,denoted

}

isanybasisofU,thenumber, m,ofvectorsinthe

dim U = m

The importance of the invariance theorem is that the dimension of U can be determined by counting the
number of vectors in any basis.8
e1, e2, . . . , en

denote the standard basis of Rn, that is the set of columns of the identity matrix.
{
Then Rn = span
e1, e2, . . . , en
is independent by Example 5.2.2.
}
Hence it is indeed a basis of Rn in the present terminology, and we have

by Example 5.1.6, and

}
e1, e2, . . . , en

Let

{

}

{

7The plural of “basis” is “bases”.
8We will show in Theorem 5.2.6 that every subspace of Rn does indeed have a basis.

5.2. Independence and Dimension

275

Example 5.2.10

dim (Rn) = n and

e1, e2, . . . , en

is a basis.

}

{

This agrees with our geometric sense that R2 is two-dimensional and R3 is three-dimensional. It also

says that R1 = R is one-dimensional, and

1

{

}

is a basis. Returning to subspaces of Rn, we deﬁne

dim

0

}

{

= 0

This amounts to saying
to any independent set (Example 5.2.4).

0
}

{

has a basis containing no vectors. This makes sense because 0 cannot belong

Example 5.2.11

r
s
r 


Let U =






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
Solution. Clearly,



{

U = span

=

r
s
r 
dim U = 2.








u, v
}
0
0
0 


. Show that U is a subspace of R3, ﬁnd a basis, and calculate dim U .

r, s in R






= ru + sv where u =

1
0
1 
, and hence that U is a subspace of R3. Moreover, if ru + sv = 0, then


0
1
0 


r
s
r 


. It follows that

and v =













so r = s = 0. Hence

u, v
}

{

is independent, and so a basis of U . This means

Example 5.2.12

Let B =
D =

{

x1, x2, . . . , xn
}
Ax1, Ax2, . . . , Axn
}

{

be a basis of Rn. If A is an invertible n
is also a basis of Rn.

n matrix, then

×

1x is in Rn so, since B is a basis, we have

+ tnxn for ti in R. Left multiplication by A gives

Solution. Let x be a vector in Rn. Then A−
1x = t1x1 + t2x2 +
A−
· · ·
x = t1(Ax1) + t2(Ax2) +
s1(Ax1) + s2(Ax2) +
so left multiplication by A−
that each si = 0, and so proves the independence of D. Hence D is a basis of Rn.

· · ·
+ sn(Axn) = 0, where the si are in R. Then A(s1x1 + s2x2 +

1 gives s1x1 + s2x2 +

· · ·

· · ·

+ snxn) = 0
+ snxn = 0. Now the independence of B shows

· · ·

+ tn(Axn), and it follows that D spans Rn. To show independence, let

While we have found bases in many subspaces of Rn, we have not yet shown that every subspace has
a basis. This is part of the next theorem, the proof of which is deferred to Section 6.4 (Theorem 6.4.1)
where it will be proved in more generality.

276

Vector Space Rn

Theorem 5.2.6

LetU

=

0
}

{

beasubspaceof Rn. Then:

1. U hasabasisand dim U

n.

≤

2. AnyindependentsetinU canbeenlarged(byaddingvectorsfromanyﬁxedbasisofU)toa

basisofU.

3. AnyspanningsetforU canbecutdown(bydeletingvectors)toabasisofU.

Example 5.2.13

Find a basis of R4 containing S =

u, v

}

{

where u = (0, 1, 2, 3) and v = (2,

1, 0, 1).

−

Solution. By Theorem 5.2.6 we can ﬁnd such a basis by adding vectors from the standard basis of
R4 to S. If we try e1 = (1, 0, 0, 0), we ﬁnd easily that
is independent. Now add another
vector from the standard basis, say e2.
is independent. Since B has 4 = dim R4 vectors, then B
Again we ﬁnd that B =
must span R4 by Theorem 5.2.7 below (or simply verify it directly). Hence B is a basis of R4.

e1, e2, u, v

e1, u, v

}

{

}

{

Theorem 5.2.6 has a number of useful consequences. Here is the ﬁrst.

Theorem 5.2.7
LetU beasubspaceof Rn where dim U = m andlet B =
U. Then B isindependentifandonlyif B spansU.

x1, x2, . . . , xm
{

}

beasetof m vectorsin

Proof. Suppose B is independent. If B does not span U then, by Theorem 5.2.6, B can be enlarged to a
basis of U containing more than m vectors. This contradicts the invariance theorem because dim U = m,
so B spans U . Conversely, if B spans U but is not independent, then B can be cut down to a basis of U
containing fewer than m vectors, again a contradiction. So B is independent, as required.

As we saw in Example 5.2.13, Theorem 5.2.7 is a “labour-saving” result.

It asserts that, given a
subspace U of dimension m and a set B of exactly m vectors in U , to prove that B is a basis of U it sufﬁces
to show either that B spans U or that B is independent. It is not necessary to verify both properties.

Theorem 5.2.8

LetU

⊆

W besubspacesof Rn. Then:

1. dim U

dim W.

≤

2. If dim U = dim W,thenU = W.

6
5.2. Independence and Dimension

277

Proof. Write dim W = k, and let B be a basis of U .

1. If dim U > k, then B is an independent set in W containing more than k vectors, contradicting the

fundamental theorem. So dim U

k = dim W .

≤

2. If dim U = k, then B is an independent set in W containing k = dim W vectors, so B spans W by

Theorem 5.2.7. Hence W = span B = U , proving (2).

It follows from Theorem 5.2.8 that if U is a subspace of Rn, then dim U is one of the integers 0, 1, 2, . . . , n,
and that:

dim U = 0
dim U = n

0
if and only if U =
}
{
if and only if U = Rn

,

The other subspaces of Rn are called proper. The following example uses Theorem 5.2.8 to show that the
proper subspaces of R2 are the lines through the origin, while the proper subspaces of R3 are the lines and
planes through the origin.

Example 5.2.14

1. If U is a subspace of R2 or R3, then dim U = 1 if and only if U is a line through the origin.

2. If U is a subspace of R3, then dim U = 2 if and only if U is a plane through the origin.

Proof.

1. Since dim U = 1, let

be a basis of U . Then U = span
through the origin with direction vector u. Conversely each line L with direction vector d
the form L =

is a basis of U , so U has dimension 1.

, so U is the line
= 0 has

t in R

tu
{

u
}

t in R

=

u

}

{

}

{

|

td
{

|

}

. Hence

d
{
}
v, w
= 0. Let P =

2. If U

⊆

R3 has dimension 2, let

be a basis of U . Then v and w are not parallel (by Exam-
}
x in R3
ple 5.2.7) so n = v
n
denote the plane through the origin with
{
·
normal n. Then P is a subspace of R3 (Example 5.1.1) and both v and w lie in P (they are orthogonal
to n), so U = span

P by Theorem 5.1.1. Hence

x = 0

v, w

×

w

}

{

|

{

} ⊆

U

P

⊆

⊆

R3

Since dim U = 2 and dim (R3) = 3, it follows from Theorem 5.2.8 that dim P = 2 or 3, whence
P = U or R3. But P
Conversely, if U is a plane through the origin, then dim U = 0, 1, 2, or 3 by Theorem 5.2.8. But
= R3, and dim U
dim U

= R3 (for example, n is not in P) and so U = P is a plane through the origin.

= 1 by (1). So dim U = 2.

= 0 or 3 because U

and U

=

0

{

}

Note that this proof shows that if v and w are nonzero, nonparallel vectors in R3, then span
w. We gave a geometrical veriﬁcation of this fact in Section 5.1.
plane with normal n = v

×

v, w

}

{

is the

6
6
6
6
6
6
6
278

Vector Space Rn

Exercises for 5.2

In Exercises 5.2.1-5.2.6 we write vectors Rn as
rows.
Exercise 5.2.1 Which of the following subsets are inde-
pendent? Support your answer.

a.

b.

c.

d.

1, 0), (3, 2,

1), (3, 5,

−

−

1, 1), (0, 0, 1)
}
−
1), (2, 0, 1, 0), (0,

−

(1,
{
(1, 1, 1), (1,
{
(1,
{
R4

1, 1,

−

−

in R3

2)
}
in R3

2, 1,

−

2)
}

−

in

(1, 1, 0, 0), (1, 0, 1, 0), (0, 0, 1, 1),
{
(0, 1, 0, 1)
}

in R4

Exercise 5.2.2 Let
be an independent set in
Rn. Which of the following sets is independent? Support
your answer.

x, y, z, w
{

}

a.

b.

c.

d.

−

−

z, z

y, y

x
x
}
−
{
x + y, y + z, z + x
}
{
w, w
x
x
}
{
x + y, y + z, z + w, w + x
}
{

y, y

z, z

−

−

−

−

Exercise 5.2.3 Find a basis and calculate the dimension
of the following subspaces of R4.

a. span

b. span

(1,
−
{
(2, 1, 0,
{
(
{
−
2, 2,
−
d. span
(
−

(
−
{
1, 2, 2, 1)
}

c. span
(3,

−

1)
}

1, 2, 0), (2, 3, 0, 3), (1, 9,

1), (

−

−

1, 2, 1, 0), (2, 0, 3,

−

6, 6)
}
1, 1, 1, 1), (2, 7, 4, 1)
}
3),

1), (4, 4, 11,

−

−

2, 0, 3, 1), (1, 2,

1, 0), (

−

−

2, 8, 5, 3),

Exercise 5.2.4 Find a basis and calculate the dimension
of the following subspaces of R4.

a
a + b
b
a

−
b













(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a. U = 




a and b in R




b. U = 



c. U = 


d. U = 


e. U = 



f. U = 




























a + b
b
a

−
b
a

a
b
c + a
c

a
b
−
b + c
a
b + c

a
b
c
d

a
b
c
d









(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)













(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

a and b in R




a, b, and c in R


a, b, and c in R


c + d = 0 in R


−







(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
a + b



a + b = c + d in R






Exercise 5.2.5 Suppose that
R4. Show that:






x, y, z, w
}
{

is a basis of

x + aw, y, z, w
{
choice of the scalar a.

}

is also a basis of R4 for any

x + w, y + w, z + w, w
{
x, x + y, x + y + z, x + y + z + w
{
of R4.

}

is also a basis of R4.

is also a basis

}

Exercise 5.2.6 Use Theorem 5.2.3 to determine if the
following sets of vectors are a basis of the indicated
space.

1), (2, 2)
}

in R2

(3,
{
−
(1, 1,
{
(
{
−
(5, 2,
{
(2, 1,
{
(
−
(1, 0,
{
(1, 3, 3,

−

1), (1,

−
1, 1,

−
1), (1,

in R3

1, 1), (0, 0, 1)
}
1, 2), (0, 0, 1)
}
in R3

1, 0)
}

−

in R3

−
1), (1, 0, 1), (3,

−

1, 3), (1, 1, 0, 2), (0, 1, 0,

3),

−

−
1, 2, 3, 1)
}

in R4

−

2, 5), (4, 4,
in R4

10)
}

−

3, 2), (0, 1, 0,

−

3),

−

a.

b.

c.

a.

b.

c.

d.

e.

f.

Exercise 5.2.7 In each case show that the statement is
true or give an example showing that it is false.

a. If

x, y
{
dependent.

}

is independent, then

x, y, x + y
}
{

is in-

b. If

x, y, z
{
dent.

}

is independent, then

y, z
{

}

is indepen-

c. If

y, z
}
{
for any x.

is dependent, then

x, y, z
}
{

is dependent

d. If all of x1, x2,
x1, x2, . . . , xk
{

}

is independent.

. . . , xk are nonzero,

then

e. If one of x1, x2,
x1, x2, . . . , xk
{

}

is dependent.

. . . , xk

is zero,

then

5.2. Independence and Dimension

279

Exercise 5.2.12 If
show
is also independent.

x1, x2, x3, . . . , xk
{

}

x1, x1 + x2, x1 + x2 + x3, . . . , x1 + x2 +
{

· · ·

is independent,
+ xk

}

Exercise 5.2.13 If
y + x1, y + x2, y + x3, . . . , y + xk
dent, show that
{
also independent.

y, x1, x2, x3, . . . , xk
{

is indepen-
is

}

}

x1, x2, . . . , xk
{

}
x1, x2, . . . , xk
{
is independent.

Exercise 5.2.14 If
Rn, and if y is not in span
x1, x2, . . . , xk, y
{
}
Exercise 5.2.15 If A and B are matrices and the columns
of AB are independent, show that the columns of B are in-
dependent.

is independent in
, show that
}

Exercise 5.2.16 Suppose that
a b
c d

and let A =

.

(cid:20)

(cid:21)

x, y
}
{

is a basis of R2,

f. If ax + by + cz = 0, then

x, y, z
{

}

is independent.

a. If A is invertible, show that

a basis of R2.

ax + by, cx + dy
}
{

is

g. If

x, y, z
{

}

for some a, b, and c in R.

is independent, then ax + by + cz = 0

h. If

is dependent, then t1x1 +t2x2 +
x1, x2, . . . , xk
{
+tkxk = 0 for some numbers ti in R not all zero.
· · ·

}

i. If

x1, x2, . . . , xk
{
t2x2 +

}

+ tkxk = 0 for some ti in R.

is independent, then t1x1 +

· · ·

j. Every non-empty subset of a linearly independent

set is again linearly independent.

k. Every set containing a spanning set is again a

spanning set.

Exercise 5.2.8 If A is an n
n matrix, show that det A =
×
0 if and only if some column of A is a linear combination
of the other columns.

Exercise 5.2.9 Let
x, y, z
{
set in R4. Show that
some ek in the standard basis

}
x, y, z, ek
{

be a linearly independent
is a basis of R4 for
.
}

e1, e2, e3, e4
{

}

x1, x2, x3, x4, x5, x6
Exercise 5.2.10 If
{
pendent set of vectors, show that the subset
is also independent.

is an inde-
}
x2, x3, x5
{

}

Let A be any m

Exercise 5.2.11
let b1, b2, b3,
the system Ax = bi has a solution xi for each i.
b1, b2, b3, . . . , bk
{
x1, x2, x3, . . . , xk
{

n matrix, and
. . . , bk be columns in Rm such that
If
is independent in Rm, show that

}
is independent in Rn.

×

}

b. If

ax + by, cx + dy
}
{

is invertible.

is a basis of R2, show that A

Exercise 5.2.17 Let A denote an m

n matrix.

×

a. Show that null A = null (UA) for every invertible

m matrix U .

m

×

b. Show that dim ( null A) = dim ( null (AV )) for
If
×
is a basis of null A, show
is a basis of

every invertible n
x1, x2,
{
V −
that
{
null (AV ).]

}
1x2, . . . , V −

. . . , xk
1x1, V −

n matrix V .

[Hint:

1xk

}

Exercise 5.2.18 Let A denote an m

n matrix.

a. Show that

×
im A = im (AV ) for every invertible

n matrix V .

n

×

b. Show that dim ( im A) = dim ( im (UA)) for ev-
If
×
is a basis of im (UA), show that
1y2, . . . , U −
is a basis of im A.]

ery invertible m
y1, y2, . . . , yk}
{
1y1, U −
U −
{

m matrix U .

1yk}

[Hint:

W . If dim U = n

Exercise 5.2.19 Let U and W denote subspaces of Rn,
and assume that U
1, show that
either W = U or W = Rn.
Exercise 5.2.20 Let U and W denote subspaces of Rn,
and assume that U
W . If dim W = 1, show that either
U =

⊆
or U = W .

⊆

−

0
{

}

280

Vector Space Rn

5.3 Orthogonality

Length and orthogonality are basic concepts in geometry and, in R2 and R3, they both can be deﬁned
using the dot product. In this section we extend the dot product to vectors in Rn, and so endow Rn with
euclidean geometry. We then introduce the idea of an orthogonal basis—one of the most useful concepts
in linear algebra, and begin exploring some of its applications.

Dot Product, Length, and Distance

If x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) are two n-tuples in Rn, recall that their dot product was
deﬁned in Section 2.2 as follows:

x

·

y = x1y1 + x2y2 +

+ xnyn

· · ·

Observe that if x and y are written as columns then x
are written as rows). Here x

y is a 1

·

1 matrix, which we take to be a number.

y = xT y is a matrix product (and x

·

×

y = xyT if they

·

Deﬁnition 5.6 Length in Rn

Asin R3,thelength

x
k
k

ofthevectorisdeﬁnedby

x
k
k

= √x

x=

·

q

x2
1 + x2

2 +

+ x2
n

· · ·

Where

(

) indicatesthepositivesquareroot.

p

A vector x of length 1 is called a unit vector. If x
unit vector (see Theorem 5.3.6 below), a fact that we shall use later.

= 0, then

x
k 6

k

= 0 and it follows easily that

x is a

1
x
k

k

Example 5.3.1

If x = (1,
x

k

k

1,

−

−

3, 1) and y = (2, 1, 1, 0) in R4, then x

y = 2

1

3 + 0 =

= √1 + 1 + 9 + 1 = √12 = 2√3. Hence 1
2√3

−
−
x is a unit vector; similarly 1
√6

−

·

2 and
y is a unit vector.

These deﬁnitions agree with those in R2 and R3, and many properties carry over to Rn:

Theorem 5.3.1
Letx,y,andzdenotevectorsin Rn. Then:

1. x

2. x

·

·

y= y

x.

·
(y+ z) = x

3. (ax)

4.

5.

x
k
k
x
k

k ≥

y= a(x
·

·
2 = x
·
0,and

x.

z.

y+ x

·
·
y) = x

(ay) forallscalars a.

·

= 0 ifandonlyifx= 0.

x
k
k

6
6.

ax
k

k

=

a

x
k

|k

|

forallscalars a.

5.3. Orthogonality

281

Proof. (1), (2), and (3) follow from matrix arithmetic because x

and (6) is a routine veriﬁcation since
= 0 if and only if x2

1 + x2
so
xi = 0 for each i; that is, if and only if x = 0. This proves (5).

a
|
|
+ x2

2 +

· · ·

k

k

x

y = xT y; (4) is clear from the deﬁnition;
= √a2. If x = (x1, x2, . . . , xn), then
+ x2
=
n
n = 0. Since each xi is a real number this happens if and only if

1 + x2
x2

2 +

· · ·

q

k

x

k

·

Because of Theorem 5.3.1, computations with dot products in Rn are similar to those in R3. In partic-

ular, the dot product

(x1 + x2 +

+ xm)

(y1 + y2 +

+ yk)

· · ·

·

· · ·

equals the sum of mk terms, xi

y j, one for each choice of i and j. For example:

·
4y)

·

(3x

−

(7x + 2y) = 21(x

·

x) + 6(x
2

22(x

y)

y)

·

·

−

−

x)
·
2

−

8(y

y)

·

28(y

y

8

k

k

= 21

k

x
k

−

holds for all vectors x and y.

Example 5.3.2

Show that

x + y

2 =

x

2 + 2(x

y) +

k

k
Solution. Using Theorem 5.3.1 several times:

k

k

k

·

y
k

2 for any x and y in Rn.

x + y

k

k

2 = (x + y)
x

·
2 + 2(x

=

k

k

(x + y) = x

y) +

y

k

·

k

x + x
2

·

·

y + y

x + y

y

·

·

Example 5.3.3

Suppose that Rn = span
show that x = 0.

f1, f2, . . . , fk}

{

for some vectors fi. If x

·

fi = 0 for each i where x is in Rn,

Solution. We show x = 0 by showing that
span Rn, write x = t1f1 + t2f2 +

x

k

k

+ tkfk where the ti are in R. Then

= 0 and using (5) of Theorem 5.3.1. Since the fi

· · ·
2 = x

x

k

k

x = x

·
·
= t1(x
f1) + t2(x
= t1(0) + t2(0) +
= 0

·

(t1f1 + t2f2 +
f2) +

· · ·

+ tkfk)

+ tk(x

fk)

·

· · ·
+ tk(0)

·
· · ·

We saw in Section 4.2 that if u and v are nonzero vectors in R3, then
between u and v. Since
u
holds in Rn.

1 for any angle θ, this shows that

cosθ

| ≤

|

|

u
u
k
v

v
·
v
kk
k
| ≤ k

·

= cosθ where θ is the angle
u
. In this form the result

v
k

kk

282

Vector Space Rn

Theorem 5.3.2: Cauchy Inequality9
Ifxandyarevectorsin Rn,then

Moreover

x
·
|

y
|

=

x
k

kk

y
k

kk
ifandonlyifoneofxandyisamultipleoftheother.

| ≤ k

y

x
·
|

x

y
k

Proof. The inequality holds if x = 0 or y = 0 (in fact it is equality). Otherwise, write
y
= b > 0 for convenience. A computation like that preceding Example 5.3.2 gives

k

k

= a > 0 and

x

k

k

bx

ay

2 = 2ab(ab

k
y

k
−
0 and ab+x

x

y) and

bx + ay

·

−
0, and hence that

k

k
ab

y

·

≥

2 = 2ab(ab + x

y)

·

(5.1)

y

x

·

≤

ab. Hence

y

x

|

·

| ≤

ab =

x

k

,

y
k

kk

≤

bx

ay = 0 or bx + ay = 0, so one of x and y is a multiple of the other (even if a = 0 or b = 0).

= ab, so x

y = ab or x

y =

ab. Hence Equation 5.1 shows that

−

·

−

x

It follows that ab
proving the Cauchy inequality.
x
If equality holds, then

≥

−

·

y

|

·

|

−
The Cauchy inequality is equivalent to (x

(x1y1 + x2y2 + x3y3 + x4y4 + x5y5)2

≤

for all xi and yi in R.

·

y)2

·

x

2

y

2. In R5 this becomes

k
k
k
≤ k
3 + x2
2 + x2
1 + x2

(x2

4 + x2

5)(y2

1 + y2

2 + y2

3 + y2

4 + y2
5)

There is an important consequence of the Cauchy inequality. Given x and y in Rn, use Example 5.3.2

and the fact that x

y

to compute

x
kk

≤ k

y

k
x

·
x + y

2 =

2 + 2(x

k
Taking positive square roots gives:

k

k

k

y) +

y

k

·

2

k

≤ k

2 + 2

x
k

y

x
kk

k

k

+

y
k

k

2 = (

k

x + y

)2

k

Corollary 5.3.1: Triangle Inequality

Ifxandyarevectorsin Rn,then

x+ y
k

k ≤ k

x
k

+

y
.
k
k

v

w

v + w

The reason for the name comes from the observation that in R3 the
inequality asserts that the sum of the lengths of two sides of a triangle is
not less than the length of the third side. This is illustrated in the diagram.

9Augustin Louis Cauchy (1789–1857) was born in Paris and became a professor at the École Polytechnique at the age of
26. He was one of the great mathematicians, producing more than 700 papers, and is best remembered for his work in analysis
in which he established new standards of rigour and founded the theory of functions of a complex variable. He was a devout
Catholic with a long-term interest in charitable work, and he was a royalist, following King Charles X into exile in Prague after
he was deposed in 1830. Theorem 5.3.2 ﬁrst appeared in his 1812 memoir on determinants.

5.3. Orthogonality

283

Deﬁnition 5.7 Distance in Rn
Ifxandyaretwovectorsin Rn,wedeﬁnethedistance d(x, y) betweenxandyby

d(x, y) =

x
k

−

y
k

w

w

v

−

v

The motivation again comes from R3 as is clear in the diagram. This
distance function has all the intuitive properties of distance in R3, includ-
ing another version of the triangle inequality.

Theorem 5.3.3
Ifx,y,andzarethreevectorsin Rn wehave:

1. d(x, y)

≥

0 forallxandy.

2. d(x, y) = 0 ifandonlyifx= y.

3. d(x, y) = d(y, x) forallxandy.

4. d(x, z)

≤

d(x, y) + d(y, z)forallx,y,andz. Triangleinequality.

Proof. (1) and (2) restate part (5) of Theorem 5.3.1 because d(x, y) =
u
k

for every vector u in Rn. To prove (4) use the Corollary to Theorem 5.3.2:

u
k

k −

−

=

k

x

k

y

k

, and (3) follows because

d(x, z) =

x

k

z
k

−

(x
(x

=

k
≤ k

−
−

y) + (y
y)
+

k

k

−
(y

z)

k
z)
−

k

= d(x, y) + d(y, z)

Orthogonal Sets and the Expansion Theorem

Deﬁnition 5.8 Orthogonal and Orthonormal Sets

Wesaythattwovectorsxandyin Rn areorthogonalifx
·
x1, x2, . . . , xk
(SeeTheorem4.2.3). Moregenerally,aset
{
orthogonalsetif

xj = 0 forall i

= j

and xi

xi

·

y= 0,extendingtheterminologyin R3

ofvectorsin Rn iscalledan

}
= 0forall i10

Notethat
orthonormalifitisorthogonaland,inaddition,eachxi isaunitvector:

x1, x2, . . . , xk
{

isanorthogonalsetifx

= 0. Aset

x
}
{

}

ofvectorsin Rn iscalled

= 1 foreach i.

xi
k

k

10The reason for insisting that orthogonal sets consist of nonzero vectors is that we will be primarily concerned with orthog-

onal bases.

6
6
6
284

Vector Space Rn

Example 5.3.4

The standard basis

e1, e2, . . . , en

{

}

is an orthonormal set in Rn.

The routine veriﬁcation is left to the reader, as is the proof of:

Example 5.3.5

x1, x2, . . . , xk

If

{

}

is orthogonal, so also is

a1x1, a2x2, . . . , akxk

{

}

for any nonzero scalars ai.

If x

= 0, it follows from item (6) of Theorem 5.3.1 that 1
x
k

k

x is a unit vector, that is it has length 1.

Deﬁnition 5.9 Normalizing an Orthogonal Set

x1, x2, . . . , xk
{

xk
isan
Henceif
isanorthogonalset,then
}
x1, x2,
orthonormalset,andwesaythatitistheresultofnormalizingtheorthogonalset
{

x1,

x2,

· · ·

}

{

k

k

,

1
xkk
k

1
x1
k

1
x2
k

, xk

.
}

· · ·

Example 5.3.6

1
1
1
1
−

1
−
0
1
0

1
−
3
1
−
1

If f1 = 



, f2 = 



, f3 = 



, and f4 = 



then

{

f1, f2, f3, f4

}

is an orthogonal



set in R4 as is easily veriﬁed. After normalizing, the corresponding orthonormal set is

1
2f1,




f4




1
f3,
√2




1
√6

1
2√3

















f2,

{

1
0
1
2

}

v + w

w

The most important result about orthogonality is Pythagoras’ theorem.

Given orthogonal vectors v and w in R3, it asserts that

v

k
as in the diagram. In this form the result holds for any orthogonal set in Rn.

k

k

k

k

k

v + w

2 =

v

2 +

2

w

Theorem 5.3.4: Pythagoras’ Theorem

If

x1, x2, . . . , xk
{

}

isanorthogonalsetin Rn,then

x1 + x2 +
k

· · ·

+ xk

k

2 =

x1
k

k

2 +

x2
k

k

2 +

+

xk
k

k

· · ·

2.

Proof. The fact that xi

·

x j = 0 whenever i

= j gives

6
6
5.3. Orthogonality

285

x1 + x2 +

+ xk

k

· · ·

k

2 = (x1 + x2 +
x1 + x2
= (x1

· · ·

+ xk)
x2 +

·

·

+ xk)
x j
xi

·

·

(x1 + x2 +
+ xk ·
xk
+

2 + 0

· · ·
xk) + ∑
= j

i

· · ·

k

k

This is what we wanted.

=

x1

k

k

2 +

2 +

x2

k

k

· · ·

If v and w are orthogonal, nonzero vectors in R3, then they are certainly not parallel, and so are linearly

independent by Example 5.2.7. The next theorem gives a far-reaching extension of this observation.

Theorem 5.3.5
Everyorthogonalsetin Rn islinearlyindependent.

Proof. Let
{
t1x1 + t2x2 +

x1, x2, . . . , xk}
· · ·

+ tkxk = 0. Then

be an orthogonal set in Rn and suppose a linear combination vanishes, say:

0 = x1

·

0 = x1

(t1x1 + t2x2 +
x1) + t2(x1
2 + t2(0) +
2

k

·

+ tkxk)

· · ·
x2) +
·

· · ·
+ tk(0)

· · ·

·

= t1(x1
x1
= t1
x1
= t1

k

k

k

+ tk(x1

xk)

·

Since

x1

2

k

k

= 0, this implies that t1 = 0. Similarly ti = 0 for each i.

Theorem 5.3.5 suggests considering orthogonal bases for Rn, that is orthogonal sets that span Rn.
These turn out to be the best bases in the sense that, when expanding a vector as a linear combination of
the basis vectors, there are explicit formulas for the coefﬁcients.

Theorem 5.3.6: Expansion Theorem

Let

f1, f2, . . . , fm
{

}

beanorthogonalbasisofasubspaceUof Rn. IfxisanyvectorinU,wehave

x=

f1
x
·
2
f1
k
k

(cid:16)

f1 +

(cid:17)

(cid:16)

f2
x
·
2
f2
k
k

(cid:17)

f2 +

+

· · ·

fm
x
·
2
fm
k
k

fm

(cid:17)

(cid:16)

Proof. Since
t1 we take the dot product of both sides with f1:

f1, f2, . . . , fm

{

}

spans U , we have x = t1f1 +t2f2 +

+tmfm where the ti are scalars. To ﬁnd

· · ·

f1 = (t1f1 + t2f2 +

x

·

· · ·
f1) + t2(f2
·
2 + t2(0) +
2

f1
+ tm(fm

+ tmfm)
f1) +

·
· · ·
+ tm(0)

· · ·

f1)

·

= t1(f1
f1
= t1
f1
= t1

k

k

·

k

k

6
6
286

Vector Space Rn

Since f1

= 0, this gives t1 = x
·
f1
k

2 . Similarly, ti = x
f1
fi
2 for each i.
·
fi
k
k
k

The expansion in Theorem 5.3.6 of x as a linear combination of the orthogonal basis
is
fi
called the Fourier expansion of x, and the coefﬁcients t1 = x
2 are called the Fourier coefﬁcients. Note
·
fi
k
k
fi for each i. We will have a great deal more to
f1, f2, . . . , fm
that if
say about this in Section 10.5.

is actually orthonormal, then ti = x

f1, f2, . . . , fm

}

{

}

{

·

Example 5.3.7

Expand x = (a, b, c, d) as a linear combination of the orthogonal basis
in Example 5.3.6.

{

f1, f2, f3, f4

}

of R4 given

Solution. We have f1 = (1, 1, 1,
f4 = (

1, 3,

−

1, 1) so the Fourier coefﬁcients are

1), f2 = (1, 0, 1, 2), f3 = (

−

−

1, 0, 1, 0), and

−

t1 = x
·
f1
k
t2 = x
·
f2
k

f1
2 = 1
k
f2
2 = 1
k

4(a + b + c + d)

6(a + c + 2d)

a + c)

−

t3 = x
·
f3
k
t4 = x
·
f4
k

f3
2 = 1
2(
k
f4
2 = 1
k

12(

a + 3b

−

−

c + d)

The reader can verify that indeed x = t1f1 + t2f2 + t3f3 + t4f4.

A natural question arises here: Does every subspace U of Rn have an orthogonal basis? The answer is
“yes”; in fact, there is a systematic procedure, called the Gram-Schmidt algorithm, for turning any basis
of U into an orthogonal one. This leads to a deﬁnition of the projection onto a subspace U that generalizes
the projection along a vector used in R2 and R3. All this is discussed in Section 8.1.

Exercises for 5.3

We often write vectors in Rn as row n-tuples.
Exercise 5.3.1 Obtain orthonormal bases of R3 by nor-
malizing the following.

1, 2), (0, 2, 1), (5, 1,

a.

b.

−

(1,
{
(1, 1, 1), (4, 1,
{

5), (2,

−

−

2)
−
}
3, 1)
}

Exercise 5.3.2 In each case, show that the set of vectors
is orthogonal in R4.

a.

b.

(1,
{
(2,
{

−

−

1, 2, 5), (4, 1, 1,

1, 4, 5), (0,

−

−
1, 1,

1), (

−

7, 28, 5, 5)
}
1)
}

−

1), (0, 3, 2,

−

Exercise 5.3.3
In each case, show that B is an or-
thogonal basis of R3 and use Theorem 5.3.6 to expand
x = (a, b, c) as a linear combination of the basis vectors.

a. B =

b. B =

c. B =

d. B =

1), (1, 4, 1), (2,

−

1, 3), (

(1,
{
−
(1, 0,
{
(1, 2, 3), (
{
−
(1, 1, 1), (1,
{

−

1,

2, 1, 1), (4, 7, 1)
}
1, 2)
}
4, 1)
}

−
1, 1), (5,

−

−
1, 0), (1, 1,

2)
}

−

−

Exercise 5.3.4 In each case, write x as a linear combi-
nation of the orthogonal basis of the subspace U .

a. x = (13,

−
b. x = (14, 1,
U = span

20, 15); U = span

(1,
{

−

2, 3), (

−

1, 1, 1)
}

8, 5);

−
(2,
{

1, 0, 3), (2, 1,

−

2,

−

1)
}

−

Exercise 5.3.5 In each case, ﬁnd all (a, b, c, d) in R4
such that the given set is orthogonal.

a.

(1, 2, 1, 0), (1,
{
(a, b, c, d)
}

−

1, 1, 3), (2,

1, 0,

−

1),

−

6
b.

1, 1), (2, 1, 1,

(1, 0,
{
(a, b, c, d)
}

−

1), (1,

−

−

3, 1, 0),

Exercise 5.3.6 If
pute:

x
k
k

= 3,

y
k

k

= 1, and x

y =

·

−

2, com-

a.

c.

3x
k
(3x

−

−

5y
k
(2y

y)

·

b.

d.

x)

−

2x + 7y
k
k
(3x + 5y)
(x

2y)

−

·

5.3. Orthogonality

287

n(r2

1 + r2

2 +

+ r2

n) for all ri in

· · ·

≤

a. r1 + r2 +

· · ·
R and all n

+ rn
1.

≥

b. r1r2 + r1r3 + r2r3

1 + r2
r2
r3 in R. [Hint: See part (a).]

≤

2 + r2

3 for all r1, r2, and

Exercise 5.3.12

Exercise 5.3.7 In each case either show that the state-
ment is true or give an example showing that it is false.

a. Show that x and y are orthogonal in Rn if and only

if

x + y
k
k

=

x
k

y
.
k

−

a. Every independent set in Rn is orthogonal.

b. If

x, y
}
{
is also orthogonal.

is an orthogonal set in Rn, then

x, x+y
}
{

are both orthogonal in Rn,

}

d. If

c. If

x, y
{
then

is also orthogonal.

z, w
and
}
{
x, y, z, w
{
}
x1, x2
y1, y2, y3}
and
{
}
{
thogonal and xi
y j = 0 for all i and j,
·
x1, x2, y1, y2, y3}
is orthogonal.
{
. . . , xn
e. If
x1, x2, . . . , xn
{
= 0 in Rn, then

x1, x2,
{
Rn = span

.
}

}

f. If x

x
{
Exercise 5.3.8 Let v denote a nonzero vector in Rn.

is an orthogonal set.

}

is orthogonal in Rn, then

are both or-
then

b. Show that x + y and x
.
k

and only if

y
k

x
k

=

k

y are orthogonal in Rn if

−

Exercise 5.3.13

a. Show that

2 =
x + y
k
k
is orthogonal to y.

x
k

2 +
k

y
k

2 if and only if x
k

b. If x =

, y =

1
1
1
0
(cid:20)
(cid:21)
2 +
2 =
x
x + y + z
k
k
k
k
z
= 0, and y
= 0, x

(cid:21)

(cid:20)

z

and z =

2 +
y
k
k
= 0.

, show

−

2
3
(cid:20)
(cid:21)
2 but
z
k
k

·

·

that
y
x

·

Exercise 5.3.14

a. Show that P =

of Rn.

x in Rn
{

x

v = 0
}

·

|

is a subspace

a. Show that x
y in Rn.

y = 1
2
x + y
4 [
k
k

·

x
− k

2] for all x,
y
k

−

b. Show that Rv =

t in R

is a subspace of Rn.

|
c. Describe P and Rv geometrically when n = 3.

}

tv
{

Exercise 5.3.9 If A is an m
n matrix with orthonormal
×
columns, show that AT A = In. [Hint: If c1, c2, . . . , cn are
the columns of A, show that column j of AT A has entries
c1

c j, . . . , cn

c j, c2

c j].

·

·

·

1
2 (x + y) for all x

Exercise 5.3.10 Use the Cauchy inequality to show that
√xy
0. Here √xy and
1
2 (x + y) are called, respectively, the geometric mean and
arithmetic mean of x and y.

0 and y

≥

≤

≥

[Hint: Use x =

and y =

√x
√y

√y
√x

.]

(cid:21)
Exercise 5.3.11 Use the Cauchy inequality to prove
that:

(cid:21)

(cid:20)

(cid:20)

b. Show that

2 +
x
k
k
for all x, y in Rn.

y
k

2 = 1
2
k

2 +
x + y
k
k

x
k

−

2
y
k

(cid:2)

(cid:3)

Exercise 5.3.15 If A is n
of AT A is nonnegative. [Hint: Compute
is an eigenvector.]

n, show that every eigenvalue
2 where x
k

Ax
k

×

xi = 0 for all i, show that x = 0. [Hint: Show

= 0.]

Exercise 5.3.16 If Rn = span
x

·
Exercise 5.3.17 If Rn = span
y

·

xi for all i, show that x = y. [Hint: Exercise 5.3.16]

x1, . . . , xm
{

}

xi =

·

x1, . . . , xm
{

}

and
x
k
k
and x

Exercise 5.3.18 Let
of Rn. Given x and y in Rn, show that

e1, . . . , en
{

}

be an orthogonal basis

x

e1)(y
e1)
y = (x
2 +
·
·
e1
k
k

·

en)(y
en)
+ (x
·
·
2
en
k
k

· · ·

6
6
6
6
288

Vector Space Rn

5.4 Rank of a Matrix

In this section we use the concept of dimension to clarify the deﬁnition of the rank of a matrix given in
Section 1.2, and to study its properties. This requires that we deal with rows and columns in the same way.
While it has been our custom to write the n-tuples in Rn as columns, in this section we will frequently
write them as rows. Subspaces, independence, spanning, and dimension are deﬁned for rows using matrix
operations, just as for columns. If A is an m

n matrix, we deﬁne:

×

Deﬁnition 5.10 Column and Row Space of a Matrix
Thecolumnspace, col A,of A isthesubspaceof Rm spannedbythecolumnsof A.
Therowspace, row A,of A isthesubspaceof Rn spannedbytherowsof A.

Much of what we do in this section involves these subspaces. We begin with:

Lemma 5.4.1

Let A and B denote m

n matrices.

×

1. If A

2. If A

→

→

B byelementaryrowoperations,then row A = row B.

B byelementarycolumnoperations,then col A = col B.

Proof. We prove (1); the proof of (2) is analogous. It is enough to do it in the case when A
B by a single
row operation. Let R1, R2, . . . , Rm denote the rows of A. The row operation A
B either interchanges
two rows, multiplies a row by a nonzero constant, or adds a multiple of a row to a different row. We leave
the ﬁrst two cases to the reader. In the last case, suppose that a times row p is added to row q where p < q.
Then the rows of B are R1, . . . , Rp, . . . , Rq + aRp, . . . , Rm, and Theorem 5.1.1 shows that

→

→

span

{

R1, . . . , Rp, . . . , Rq, . . . , Rm

= span

{

}

R1, . . . , Rp, . . . , Rq + aRp, . . . , Rm

}

That is, row A = row B.

If A is any matrix, we can carry A

→

Hence row A = row R by Lemma 5.4.1; so the ﬁrst part of the following result is of interest.

R by elementary row operations where R is a row-echelon matrix.

Lemma 5.4.2

If R isarow-echelonmatrix,then

1. Thenonzerorowsof R areabasisof row R.

2. Thecolumnsof R containingleadingonesareabasisof col R.

Proof. The rows of R are independent by Example 5.2.6, and they span row R by deﬁnition. This proves
(1).

Let c j1, c j2, . . . , c jr denote the columns of R containing leading 1s. Then

is
independent because the leading 1s are in different rows (and have zeros below and to the left of them).

c j1, c j2, . . . , c jr}

{

5.4. Rank of a Matrix

289

Let U denote the subspace of all columns in Rm in which the last m
is just Rr with extra zeros). Hence the independent set
Since each c ji is in col R, it follows that col R = U , proving (2).

c j1, c j2, . . . , c jr}

−

{

r entries are zero. Then dim U = r (it
is a basis of U by Theorem 5.2.7.

With Lemma 5.4.2 we can ﬁll a gap in the deﬁnition of the rank of a matrix given in Chapter 1. Let A
be any matrix and suppose A is carried to some row-echelon matrix R by row operations. Note that R is
not unique. In Section 1.2 we deﬁned the rank of A, denoted rank A, to be the number of leading 1s in R,
that is the number of nonzero rows of R. The fact that this number does not depend on the choice of R was
not proved in Section 1.2. However part 1 of Lemma 5.4.2 shows that

rank A = dim ( row A)

and hence that rank A is independent of R.

Lemma 5.4.2 can be used to ﬁnd bases of subspaces of Rn (written as rows). Here is an example.

Example 5.4.1

Find a basis of U = span

{

(1, 1, 2, 3), (2, 4, 1, 0), (1, 5,

4,

−

9)

−

.

}

Solution. U is the row space of

1 1
0 1
0 0
Note that




, so

2
3
3
3
2 −
0 
0
(1, 1, 2, 3), (0, 2,


{

−

{

1 1
2 4
1 5





2
1
4
−

3
0
9 


−

. This matrix has row-echelon form

(1, 1, 2, 3), (0, 1,

3
2,

−

3)

−

}

is basis of U by Lemma 5.4.2.

3,

−

6)

−

}

is another basis that avoids fractions.

Lemmas 5.4.1 and 5.4.2 are enough to prove the following fundamental theorem.

Theorem 5.4.1: Rank Theorem

Let A denoteany m

×

n matrixofrank r. Then

dim ( col A) = dim ( row A) = r

Moreover,if A iscarriedtoarow-echelonmatrix R byrowoperations,then

1. The r nonzerorowsof R areabasisof row A.

2. Iftheleading 1slieincolumns j1, j2, . . . , jr of R,thencolumns j1, j2, . . . , jr of A area

basisof col A.

Proof. We have row A = row R by Lemma 5.4.1, so (1) follows from Lemma 5.4.2. Moreover, R = UA
where c1, c2, . . . , cn
for some invertible matrix U by Theorem 2.5.1. Now write A =
are the columns of A. Then

. . . cn

c1 c2

R = UA = U

c1 c2

(cid:2)

cn

=

· · ·

(cid:3)

(cid:2)

(cid:2)
U c1 U c2

(cid:3)

U cn

· · ·

(cid:3)

290

Vector Space Rn

U c j1, U c j2, . . . , U c jr}
is a basis of col R by Lemma 5.4.2. So, to
Thus, in the notation of (2), the set B =
{
c j1, c j2, . . . , c jr }
prove (2) and the fact that dim ( col A) = r, it is enough to show that D =
is a basis of
col A. First, D is linearly independent because U is invertible (verify), so we show that, for each j, column
c j is a linear combination of the c ji. But U c j is column j of R, and so is a linear combination of the U c ji,
+ arU c jr where each ai is a real number.
say U c j = a1U c j1 + a2U c j2 +

{

· · ·
Since U is invertible, it follows that c j = a1c j1 + a2c j2 +

+ arc jr and the proof is complete.

· · ·

Example 5.4.2

Compute the rank of A =

1 2 2
3 6 5
1 2 1





1
−
0
2 


Solution. The reduction of A to row-echelon form is as follows:

and ﬁnd bases for row A and col A.

1 2 2
3 6 5
1 2 1

1
−
0
2 

1 2 2





→ 


1
−

(cid:2)

1 2
0 0
0 0

2
1
−
1
−

1
−
3
3 

3
−

→ 

2
1
−
0

1 2
0 0
0 0

1
−
3
0 


is a basis of row A by Lemma 5.4.2.

0 0 1
Hence rank A = 2, and
Since the leading 1s are in columns 1 and 3 of the row-echelon matrix, Theorem 5.4.1 shows that

{

}

,

columns 1 and 3 of A are a basis




(cid:3)

1
3
1 


(cid:2)

,





2
5
1 






(cid:3)

of col A.





Theorem 5.4.1 has several important consequences. The ﬁrst, Corollary 5.4.1 below, follows because
the rows of A are independent (respectively span row A) if and only if their transposes are independent
(respectively span col A).



Corollary 5.4.1

If A isanymatrix,then rank A = rank (AT ).

If A is an m

dim ( col A)

≤

n matrix, we have col A
⊆
dim (Rm) = m and dim ( row A)

×

Rm and row A

Rn. Hence Theorem 5.2.8 shows that

dim (Rn) = n. Thus Theorem 5.4.1 gives:

⊆

≤

Corollary 5.4.2

If A isan m

×

n matrix,then rank A

m and rank A

n.

≤

≤

Corollary 5.4.3

rank A = rank (UA) = rank (AV ) wheneverU andV areinvertible.

5.4. Rank of a Matrix

291

Proof. Lemma 5.4.1 gives rank A = rank (UA). Using this and Corollary 5.4.1 we get

rank (AV ) = rank (AV )T = rank (V T AT ) = rank (AT ) = rank A

The next corollary requires a preliminary lemma.

Lemma 5.4.3

Let A,U,andV bematricesofsizes m

n, p

m,and n

q respectively.

×
col A,withequalityifVV ′ = In forsomeV ′.

×

×

1. col (AV )

⊆

2. row (UA)

⊆

row A,withequalityifU ′U = Im forsomeU ′.

Proof. For (1), write V =
Av1, Av2, . . . , Avq
AV =
(cid:2)
If VV ′ = In, we obtain col A = col [(AV )V ′]
(cid:3)
As to (2), we have col

(UA)T

(cid:3)

(cid:2)

⊆

= col (ATU T )

U ′U = Im, this is equality as in the proof of (1).

(cid:2)

(cid:3)

v1, v2, . . . , vq
, and each Av j is in col A by Deﬁnition 2.4. It follows that col (AV )

where v j is column j of V . Then we have

col (AV ) in the same way. This proves (1).

col (AT ) by (1), from which row (UA)

⊆

⊆

col A.

⊆

row A. If

Corollary 5.4.4

If A is m

×

n and B is n

×

m,then rank AB

≤

rank A and rank AB

rank B.

≤

Proof. By Lemma 5.4.3, col (AB)

col A and row (BA)

⊆

⊆

row A, so Theorem 5.4.1 applies.

In Section 5.1 we discussed two other subspaces associated with an m

null (A) and the image space im (A)

n matrix A: the null space

×

null (A) =

Ax = 0
}
Using rank, there are simple ways to ﬁnd bases of these spaces. If A has rank r, we have im (A) = col (A)
by Example 5.1.8, so dim [ im (A)] = dim [ col (A)] = r. Hence Theorem 5.4.1 provides a method of ﬁnding
a basis of im (A). This is recorded as part (2) of the following theorem.

and im (A) =

Ax

{

{

}

|

|

x in Rn

x in Rn

Theorem 5.4.2

Let A denotean m

×

n matrixofrank r. Then

1. The n

r basicsolutionstothesystem Ax= 0providedbythegaussianalgorithmarea

−

basisof null (A),so dim [ null (A)] = n

r.

−

2. Theorem5.4.1providesabasisof im (A) = col (A),and dim [ im (A)] = r.

Proof. It remains to prove (1). We already know (Theorem 2.2.1) that null (A) is spanned by the n
basic solutions of Ax = 0. Hence using Theorem 5.2.7, it sufﬁces to show that dim [ null (A)] = n
let

be a basis of null (A), and extend it to a basis

r
−
r. So
of Rn (by

x1, . . . , xk, xk+1, . . . , xn

x1, . . . , xk

−

{

}

{

}

Axk+1, . . . , Axn

{

}

is a basis of im (A); then n

k = r by the

−

Spanning. Choose Ax in im (A), x in Rn, and write x = a1x1 +

+akxk +ak+1xk+1 +

+ anAxn because

· · ·
x1, . . . , xk

null (A).

· · ·
+ tnAxn = 0, ti in R. Then tk+1xk+1 +

+ tnxn is in null A, so
+ tkxk for some t1, . . . , tk in R. But then the independence of the xi

} ⊆

· · ·

· · ·

{

+anxn where

· · ·

292

Vector Space Rn

Theorem 5.2.6). It is enough to show that
above and so k = n

r as required.

−

the ai are in R. Then Ax = ak+1Axk+1 +
Independence. Let tk+1Axk+1 +

tk+1xk+1 +
shows that ti = 0 for every i.

+ tnxn = t1x1 +

· · ·

· · ·

Example 5.4.3

If A =



1
1
−
2



−

−

2 1 1
2 0 1
4 1 0 


, ﬁnd bases of null (A) and im (A), and so ﬁnd their dimensions.

Solution. If x is in null (A), then Ax = 0, so x is given by solving the system Ax = 0. The
reduction of the augmented matrix to reduced form is

1
1
−
2





−

−

2 1 1 0
2 0 1 0
4 1 0 0 


→ 



1
0
0

−

2 0
0 1
0 0

−

1 0
2 0
0 0 

1
0
1 






,

1
1
2 
−










Hence r = rank (A) = 2. Here, im (A) = col (A) has basis

by Theorem 5.4.1

because the leading 1s are in columns 1 and 3. In particular, dim [ im (A)] = 2 = r as in
Theorem 5.4.2.
Turning to null (A), we use gaussian elimination. The leading variables are x1 and x3, so the
nonleading variables become parameters: x2 = s and x4 = t. It follows from the reduced matrix
that x1 = 2s + t and x3 =

2t, so the general solution is







x = 



= 



= sx1 + tx2 where x1 = 

−

2s + t
s
2t
−
t

x1
x2
x3
x4

















2
1
0
0



, and x2 = 









1
0
2
−
1

.











Hence null (A). But x1 and x2 are solutions (basic), so

null (A) = span

x1, x2

{

}

However Theorem 5.4.2 asserts that
x1, x2
directly that
Theorem 5.4.2 asserts.

{

}

is independent in this case.) In particular, dim [ null (A)] = 2 = n

is a basis of null (A). (In fact it is easy to verify
r, as

x1, x2

}

{

−

Let A be an m

n, and
it is natural to ask when these extreme cases arise. If c1, c2, . . . , cn are the columns of A, Theorem 5.2.2
spans Rm if and only if the system Ax = b is consistent for every b in Rm, and
shows that

n matrix. Corollary 5.4.2 of Theorem 5.4.1 asserts that rank A

m and rank A

c1, c2, . . . , cn

≤

≤

×

{

}

c1, c2, . . . , cn

that
theorems improve on both these results, and relate them to when the rank of A is n or m.

is independent if and only if Ax = 0, x in Rn, implies x = 0. The next two useful

{

}

5.4. Rank of a Matrix

293

Theorem 5.4.3

Thefollowingareequivalentforan m

n matrix A:

×

1. rank A = n.

2. Therowsof A span Rn.

3. Thecolumnsof A arelinearlyindependentin Rm.

4. The n

n matrix AT A isinvertible.

×
5. CA = In forsome n

m matrixC.

×

6. If Ax= 0,xin Rn,thenx= 0.

Proof. (1)
This is (2).
(2)

⇒

(2). We have row A

⊆

Rn, and dim ( row A) = n by (1), so row A = Rn by Theorem 5.2.8.

⇒

(3). By (2), row A = Rn, so rank A = n. This means dim ( col A) = n. Since the n columns of

A span col A, they are independent by Theorem 5.2.7.

(3)

⇒

(4). If (AT A)x = 0, x in Rn, we show that x = 0 (Theorem 2.4.5). We have

2 = (Ax)T Ax = xT AT Ax = xT 0 = 0

Ax
k

k

Hence Ax = 0, so x = 0 by (3) and Theorem 5.2.2.
(5). Given (4), take C = (AT A)−
1AT .
(6). If Ax = 0, then left multiplication by C (from (5)) gives x = 0.
(1). Given (6), the columns of A are independent by Theorem 5.2.2. Hence dim ( col A) = n,

(4)
(5)
(6)

⇒
⇒
⇒
and (1) follows.

Theorem 5.4.4

Thefollowingareequivalentforan m

n matrix A:

×

1. rank A = m.

2. Thecolumnsof A span Rm.

3. Therowsof A arelinearlyindependentin Rn.

4. The m

m matrix AAT isinvertible.

×
5. AC = Im forsome n

m matrixC.

×

6. Thesystem Ax= bisconsistentforeverybin Rm.

294

Vector Space Rn

Proof. (1)
(2)

(2). By (1), dim ( col A = m, so col A = Rm by Theorem 5.2.8.

⇒
(3). By (2), col A = Rm, so rank A = m. This means dim ( row A) = m. Since the m rows of A

span row A, they are independent by Theorem 5.2.7.
(4). We have rank A = m by (3), so the n

(3)

rem 5.4.3 to AT in place of A shows that (AT )T AT is invertible, proving (4).

×

m matrix AT has rank m. Hence applying Theo-

(4)
(5)

(5). Given (4), take C = AT (AAT )−
(6). Comparing columns in AC = Im gives Ac j = e j for each j, where c j and e j denote column j
j=1 r jc j

j=1 r je j, r j in R. Then Ax = b holds with x = ∑m

1 in (5).

of C and Im respectively. Given b in Rm, write b = ∑m
as the reader can verify.

⇒

⇒

⇒
⇒

(6)

⇒

(1). Given (6), the columns of A span Rm by Theorem 5.2.2. Thus col A = Rm and (1) follows.

Example 5.4.4

Show that

(cid:20)

3

x + y + z
x + y + z x2 + y2 + z2

(cid:21)

is invertible if x, y, and z are not all equal.

Solution. The given matrix has the form AT A where A =



has independent columns

because x, y, and z are not all equal (verify). Hence Theorem 5.4.3 applies.

1 x
1 y
1 z 




Theorem 5.4.3 and Theorem 5.4.4 relate several important properties of an m

n matrix A to the
invertibility of the square, symmetric matrices AT A and AAT . In fact, even if the columns of A are not
independent or do not span Rm, the matrices AT A and AAT are both symmetric and, as such, have real
eigenvalues as we shall see. We return to this in Chapter 7.

×

Exercises for 5.4

Exercise 5.4.1 In each case ﬁnd bases for the row and
column spaces of A and determine the rank of A.

2
2
4
0

1
2
0
1

−
1
3

−

a.

c.

d.













(cid:20)

8
4 6
2
1 3
5 9 10
2
1 1

−
−
−
−

5
2
−
12
−
7

−

1
3

−
−

1
2
0
1

2
6

−

−

−

1 1
1 1
2 3
3 0

b.

−






2
1
3
1

2
2
−
4
6

−













−

−
3
2

−

2
5
9
7

(cid:21)

a. U = span

(1,
{

−

1, 0, 3), (2, 1, 5, 1), (4,

−
1, 2, 5, 1), (3, 1, 4, 2, 7),

2, 5, 7)
}

b. U = span

(1,
{

−
(1, 1, 0, 0, 0), (5, 1, 6, 7, 8)
}







c. U = span 


d.

U = span







Exercise 5.4.3




1
5
6

−

1
1
0
0









, 









0
0
1
1



, 









1
0
1
0



, 









0
1
0
1

2
6
8

−

,









3
7
10

−

,









,









4
8
12


















Exercise 5.4.2 In each case ﬁnd a basis of the subspace
U .

a. Can a 3

4 matrix have independent columns?

Independent rows? Explain.

×

b. If A is 4

3 and rank A = 2, can A have indepen-

dent columns? Independent rows? Explain.

×

5.4. Rank of a Matrix

295

Exercise 5.4.8 Let A = cr where c
= 0 is a row in Rn.
Rm and r

= 0 is a column in

c. If A is an m

n.

m

≤

×

n matrix and rank A = m, show that

a. Show that col A = span

row A = span

.
}
b. Find dim ( null A).

r
{

and

c
{

}

d. Can a nonsquare matrix have its rows independent

c. Show that null A = null r.

and its columns independent? Explain.

e. Can the null space of a 3

sion 2? Explain.

6 matrix have dimen-

×

f. Suppose that A is 5

4 and null (A) = Rx for some

column x

= 0. Can dim ( im A) = 2?

×

Exercise 5.4.9
c1, c2, . . . , cn.

Let A be m

×

n with columns

a. If

c1, . . . , cn
{

b. If null A =
pendent.

}
0
{

is independent, show null A =

, show that
}

c1, . . . , cn
{

}

0
.
{
}
is inde-

Exercise 5.4.4 If A is m

n show that

×

col (A) =

Ax
{

|

x in Rn

}

Exercise 5.4.10 Let A be an n

n matrix.

×

a. Show that A2 = 0 if and only if col A

null A.

⊆

b. Conclude that if A2 = 0, then rank A

n
2 .

≤

c. Find a matrix A for which col A = null A.

Exercise 5.4.5 If A is m
×
AB = 0 if and only if col B

n and B is n
null A.

m, show that

×

⊆
Exercise 5.4.6 Show that the rank does not change when
an elementary row or column operation is performed on
a matrix.

Exercise 5.4.7
In each case ﬁnd a basis of the null
space of A. Then compute rank A and verify (1) of The-
orem 5.4.2.

3
2
4
1

1 1
0 1
2 1
1 1

−

3 5
1 0
1 1
2 0

−

−







5
2
1
4

a. A = 





b. A = 





2
2
2
4

−
−

0
1
2
2

−
−







Exercise 5.4.11 Let B be m
n. If
rank B = rank (AB), show that null B = null (AB). [Hint:
Theorem 5.4.1.]

n and let AB be k

×

×

Exercise 5.4.12
rank (AT ) = rank A.

Give a careful argument why

n matrix with
If rank A = n, show that

×

. . . , cn.

Let A be an m

Exercise 5.4.13
columns c1, c2,
AT c1, AT c2, . . . , AT cn
{
Exercise 5.4.14 If A is m
×
b lies in the column space of A if and only if
rank [A b] = rank A.

is a basis of Rn.

n and b is m

×

}

1, show that

Exercise 5.4.15

a. Show that Ax = b has a solution if and only if
rank A = rank [A b]. [Hint: Exercises 5.4.12 and
5.4.14.]

b. If Ax = b has no solution, show that

rank [A b] = 1 + rank A.

Exercise 5.4.16 Let X be a k
m

m identity matrix, show that I + X T X is invertible.

m matrix.

If I is the

×

×

6
6
6
296

Vector Space Rn

[Hint: I + X T X = AT A where A =

form.]

I
X

(cid:20)

(cid:21)

in block

a. Show that if A and B have independent columns,

so does AB.

×
Ir 0
0 0

Exercise 5.4.17
can be factored as A = PQ where P is m
dependent columns, and Q is r

n of rank r, show that A
r with r in-
n with r independent

If A is m

×

×

rows. [Hint: Let UAV =

by Theorem 2.5.3,

and write U −

1 =

(cid:20)
block form, where U1 and V1 are r

r.]

×

(cid:20)
U1 U2
U3 U4 (cid:21)

(cid:21)
and V −

1 =

V1 V2
V3 V4 (cid:21)

(cid:20)

in

Exercise 5.4.18

5.5 Similarity and Diagonalization

b. Show that if A and B have independent rows, so

does AB.

Exercise 5.4.19 A matrix obtained from A by deleting
rows and columns is called a submatrix of A. If A has an
k. [Hint:
k submatrix, show that rank A
invertible k
Show that row and column operations carry

×

≥

in block form.] Remark: It can be shown

that rank A is the largest integer r such that A has an in-
vertible r

r submatrix.

A

→

(cid:20)

Ik P
0 Q

(cid:21)

×

In Section 3.3 we studied diagonalization of a square matrix A, and found important applications (for
example to linear dynamical systems). We can now utilize the concepts of subspace, basis, and dimension
to clarify the diagonalization process, reveal some new results, and prove some theorems which could not
be demonstrated in Section 3.3.

Before proceeding, we introduce a notion that simpliﬁes the discussion of diagonalization, and is used

throughout the book.

Similar Matrices

Deﬁnition 5.11 Similar Matrices

If A and B are n
someinvertiblematrix P.

×

n matrices,wesaythat A and B aresimilar,andwrite A

B,if B = P−

1AP for

∼

B if and only if B = QAQ−

1 = Q). The language of
Note that A
similarity is used throughout linear algebra. For example, a matrix A is diagonalizable if and only if it is
similar to a diagonal matrix.

1 where Q is invertible (write P−

∼

If A

B, then necessarily B

1BQ
1 is invertible. This proves the second of the following properties of similarity (the others

A. To see why, suppose that B = P−

1AP. Then A = PBP−

1 = Q−

∼

∼

where Q = P−
are left as an exercise):

1. A
∼
2. If A
3. If A

A for all square matrices A.

B, then B
B and B

∼
∼

A.

∼
C, then A
∼

C.

∼

(5.2)

is an equivalence relation on

These properties are often expressed by saying that the similarity relation
the set of n

n matrices. Here is an example showing how these properties are used.

∼

×

Example 5.5.1

If A is similar to B and either A or B is diagonalizable, show that the other is also diagonalizable.

5.5. Similarity and Diagonalization

297

Solution. We have A
B
diagonalizable too. An analogous argument works if we assume instead that B is diagonalizable.

B. Suppose that A is diagonalizable, say A

A by (2) of (5.2), we have B

D by (3) of (5.2), so B is

D where D is diagonal. Since

D. Hence B

A and A

∼

∼

∼

∼

∼

∼

Similarity is compatible with inverses, transposes, and powers:

If A

∼

B then

1
A−

∼

B−

1,

AT

∼

BT ,

and

Ak

∼

Bk for all integers k

1.

≥

The proofs are routine matrix computations using Theorem 3.3.1. Thus, for example, if A is diagonaliz-
able, so also are AT , A−
D where D is a diagonal
1, and Dk is
matrix, we obtain AT
diagonal.

1 (if it exists), and Ak (for each k
DT , A−
1

Dk, and each of the matrices DT , D−

1). Indeed, if A

1, and Ak

D−

∼

∼

∼

≥

∼

We pause to introduce a simple matrix function that will be referred to later.

Deﬁnition 5.12 Trace of a Matrix

Thetrace tr A ofan n

×

n matrix A isdeﬁnedtobethesumofthemaindiagonalelementsof A.

In other words:

If A =

ai j

,

then tr A = a11 + a22 +

+ ann.

· · ·

It is evident that tr (A + B) = tr A + tr B and that tr (cA) = c tr A holds for all n
(cid:3)
all scalars c. The following fact is more surprising.

(cid:2)

n matrices A and B and

×

Lemma 5.5.1

Let A and B be n

×

n matrices. Then tr (AB) = tr (BA).

Proof. Write A =
ai j
di = ai1b1i + ai2b2i +

bi j

and B =
+ ainbni = ∑ j ai jb ji. Hence
(cid:3)

(cid:2)

(cid:2)

(cid:3)
· · ·

. For each i, the (i, i)-entry di of the matrix AB is given as follows:

tr (AB) = d1 + d2 +

+ dn = ∑
i

· · ·

di = ∑

∑
j

ai jb ji

!

i  

Similarly we have tr (BA) = ∑i(∑ j bi ja ji). Since these two double sums are the same, Lemma 5.5.1 is
proved.

As the name indicates, similar matrices share many properties, some of which are collected in the next

theorem for reference.

Theorem 5.5.1

If A and B aresimilar n
characteristicpolynomial,andeigenvalues.

×

n matrices,then A and B havethesamedeterminant,rank,trace,

298

Vector Space Rn

Proof. Let B = P−

1AP for some invertible matrix P. Then we have

det B = det (P−

1) det A det P = det A because det (P−

1) = 1/ det P

Similarly, rank B = rank (P−

1AP) = rank A by Corollary 5.4.3. Next Lemma 5.5.1 gives

tr (P−

1AP) = tr

P−

1(AP)

= tr

1
(AP)P−

= tr A

As to the characteristic polynomial,

cB(x) = det (xI

(cid:2)

−

(cid:3)

(cid:2)

(cid:3)

1IP)

P−

1AP

}

−
A)P

−

}

B) = det

{

x(P−
P−
= det
= det (xI
= cA(x)

−

{

1(xI
A)

Finally, this shows that A and B have the same eigenvalues because the eigenvalues of a matrix are the
roots of its characteristic polynomial.

Example 5.5.2

Sharing the ﬁve properties in Theorem 5.5.1 does not guarantee that two matrices are similar. The

1 1
0 1

1 0
0 1

matrices A =

and I =

have the same determinant, rank, trace, characteristic

(cid:20)
polynomial, and eigenvalues, but they are not similar because P−
P.

(cid:21)

(cid:21)

(cid:20)

1IP = I for any invertible matrix

Diagonalization Revisited

1AP = D
Recall that a square matrix A is diagonalizable if there exists an invertible matrix P such that P−
is a diagonal matrix, that is if A is similar to a diagonal matrix D. Unfortunately, not all matrices are

diagonalizable, for example

(see Example 3.3.10). Determining whether A is diagonalizable is

closely related to the eigenvalues and eigenvectors of A. Recall that a number λ is called an eigenvalue of
A if Ax = λx for some nonzero column x in Rn, and any such nonzero vector x is called an eigenvector of
A corresponding to λ (or simply a λ-eigenvector of A). The eigenvalues and eigenvectors of A are closely
related to the characteristic polynomial cA(x) of A, deﬁned by

1 1
0 1

(cid:20)

(cid:21)

If A is n
theorem (a repeat of Theorem 3.3.2).

×

n this is a polynomial of degree n, and its relationship to the eigenvalues is given in the following

cA(x) = det (xI

A)

−

Theorem 5.5.2

Let A bean n

n matrix.

×

1. Theeigenvaluesλ of A aretherootsofthecharacteristicpolynomial cA(x) of A.

5.5. Similarity and Diagonalization

299

2. Theλ-eigenvectorsxarethenonzerosolutionstothehomogeneoussystem

oflinearequationswithλI

A ascoefﬁcientmatrix.

−

(λI

−

A)x= 0

Example 5.5.3

Show that the eigenvalues of a triangular matrix are the main diagonal entries.

Solution. Assume that A is triangular. Then the matrix xI
ai j
a22), . . . , (x
entries (x

ann) where A =

a11), (x

A is also triangular and has diagonal

−
. Hence Theorem 3.1.4 gives

−

−

−
cA(x) = (x

a11)(x

−

(cid:2)
a22)

(x

(cid:3)
· · ·

−

−

ann)

and the result follows because the eigenvalues are the roots of cA(x).

Theorem 3.3.4 asserts (in part) that an n
vectors x1, . . . , xn such that the matrix P =
equivalent to requiring that
restate Theorem 3.3.4 as follows:

x1, . . . , xn

}

{

(cid:2)

×

n matrix A is diagonalizable if and only if it has n eigen-
x1
with the xi as columns is invertible. This is
is a basis of Rn consisting of eigenvectors of A. Hence we can

· · ·

xn

(cid:3)

Theorem 5.5.3

Let A bean n

n matrix.

×

1. A isdiagonalizableifandonlyif Rn hasabasis

of A.

x1, x2, . . . , xn
{

}

consistingofeigenvectors

2. Whenthisisthecase,thematrix P =

x1 x2

xn

isinvertibleand

1AP = diag (λ1, λ2, . . . , λn) where,foreach i,λi istheeigenvalueof A correspondingto

(cid:2)

· · ·

(cid:3)

P−
xi.

The next result is a basic tool for determining when a matrix is diagonalizable. It reveals an important
connection between eigenvalues and linear independence: Eigenvectors corresponding to distinct eigen-
values are necessarily linearly independent.

Theorem 5.5.4

Letx1, x2, . . . , xk beeigenvectorscorrespondingtodistincteigenvaluesλ1, λ2, . . . , λk ofan n
matrix A. Then
isalinearlyindependentset.

x1, x2, . . . , xk
{

}

n

×

Proof. We use induction on k. If k = 1, then
the theorem is true for some k
vanishes:

≥

1. Given eigenvectors

{

x1

{

}

is independent because x1
x1, x2, . . . , xk+1

= 0. In general, suppose
, suppose a linear combination

}

t1x1 + t2x2 +

· · ·

+ tk+1xk+1 = 0

(5.3)

6
300

Vector Space Rn

We must show that each ti = 0. Left multiply (5.3) by A and use the fact that Axi = λixi to get

If we multiply (5.3) by λ1 and subtract the result from (5.4), the ﬁrst terms cancel and we obtain

t1λ1x1 + t2λ2x2 +

· · ·

+ tk+1λk+1xk+1 = 0

(5.4)

t2(λ2

−

λ1)x2 + t3(λ3

λ1)x3 +

−

· · ·

+ tk+1(λk+1

−

λ1)xk+1 = 0

Since x2, x3, . . . , xk+1 correspond to distinct eigenvalues λ2, λ3, . . . , λk+1, the set
independent by the induction hypothesis. Hence,

{

x2, x3, . . . , xk+1

is

}

t2(λ2

−

λ1) = 0,

t3(λ3

−

λ1) = 0,

. . . ,

tk+1(λk+1

λ1) = 0

−

and so t2 = t3 =
that t1 = 0 because x1

· · ·

= 0. This is what we wanted.

= tk+1 = 0 because the λi are distinct. Hence (5.3) becomes t1x1 = 0, which implies

Theorem 5.5.4 will be applied several times; we begin by using it to give a useful condition for when

a matrix is diagonalizable.

Theorem 5.5.5

If A isan n

×

n matrixwithndistincteigenvalues,then A isdiagonalizable.

Proof. Choose one eigenvector for each of the n distinct eigenvalues. Then these eigenvectors are inde-
pendent by Theorem 5.5.4, and so are a basis of Rn by Theorem 5.2.7. Now use Theorem 5.5.3.

Example 5.5.4

Show that A =



1 0 0
1 2 3
1 1 0 


−



−

is diagonalizable.

Solution. A routine computation shows that cA(x) = (x
1. Hence Theorem 5.5.5 applies.
eigenvalues 1, 3, and

−

1)(x

−

3)(x + 1) and so has distinct

However, a matrix can have multiple eigenvalues as we saw in Section 3.3. To deal with this situation,
we prove an important lemma which formalizes a technique that is basic to diagonalization, and which
will be used three times below.

6
Lemma 5.5.2

5.5. Similarity and Diagonalization

301

x1, x2, . . . , xk
Let
{
abasis

}
x1, x2, . . . , xk, . . . , xn
{

bealinearlyindependentsetofeigenvectorsofan n

n matrix A,extenditto

×

of Rn,andlet

}

P =

x1 x2

xn

· · ·

bethe(invertible)n
distinct)eigenvaluesof A correspondingtox1, x2, . . ., xk respectively,then P−

(cid:3)
n matrixwiththexi asitscolumns. Ifλ1, λ2, . . . , λk arethe(notnecessarily
1AP hasblockform

×

(cid:2)

P−

1AP =

(cid:20)

diag (λ1, λ2, . . . , λk)
0

B
A1

(cid:21)

where B hassize k

(n

×

−

k) and A1 hassize (n

k)

−

×

(n

−

k).

Proof. If

{

e1, e2, . . . , en

is the standard basis of Rn, then

}
e1 e2

. . . en

= In = P−

(cid:2)

(cid:3)

1
1P = P−
=

x1 x2
1x1 P−
P−
(cid:2)

· · ·
1x2

xn

· · ·

P−
(cid:3)

1xn

Comparing columns, we have P−

1xi = ei for each 1

n. On the other hand, observe that

(cid:3)

P−

1AP = P−

1A

x1 x2

Hence, if 1

i

≤

≤

k, column i of P−

(cid:2)

xn

=

(cid:3)

(cid:2)

· · ·
1AP is

(P−

1A)x2

(P−

1A)xn

· · ·

(cid:3)

(P−

1A)xi = P−

1(λixi) = λi(P−

1xi) = λiei

This describes the ﬁrst k columns of P−

1AP, and Lemma 5.5.2 follows.

Note that Lemma 5.5.2 (with k = n) shows that an n
eigenvectors of A, as in (1) of Theorem 5.5.3.

×

n matrix A is diagonalizable if Rn has a basis of

Deﬁnition 5.13 Eigenspace of a Matrix

Ifλ isaneigenvalueofan n

×

n matrix A,deﬁnetheeigenspaceof A correspondingtoλ by

Eλ(A) =

xin Rn
{

Ax= λx
}

|

(cid:2)
i

≤
≤
1A)x1
(P−

This is a subspace of Rn and the eigenvectors corresponding to λ are just the nonzero vectors in Eλ(A). In
fact Eλ(A) is the null space of the matrix (λI

A):

Eλ(A) =

x

A)x = 0

= null (λI

A)

{
Hence, by Theorem 5.4.2, the basic solutions of the homogeneous system (λI
gaussian algorithm form a basis for Eλ(A). In particular

−

−

}

|

A)x = 0 given by the

−

−
(λI

dim Eλ(A) is the number of basic solutions x of (λI

A)x = 0

−

(5.5)

302

Vector Space Rn

Now recall (Deﬁnition 3.7) that the multiplicity11 of an eigenvalue λ of A is the number of times λ occurs
as a root of the characteristic polynomial cA(x) of A. In other words, the multiplicity of λ is the largest
integer m

1 such that

≥

cA(x) = (x

λ)mg(x)

−
for some polynomial g(x). Because of (5.5), the assertion (without proof) in Theorem 3.3.5 can be stated
as follows: A square matrix is diagonalizable if and only if the multiplicity of each eigenvalue λ equals
dim [Eλ(A)]. We are going to prove this, and the proof requires the following result which is valid for any
square matrix, diagonalizable or not.

Lemma 5.5.3

Letλ beaneigenvalueofmultiplicitym ofasquarematrix A. Then dim [Eλ(A)]

m.

≤

Proof. Write dim [Eλ(A)] = d. It sufﬁces to show that cA(x) = (x
because m is the highest power of (x
of Eλ(A). Then Lemma 5.5.2 shows that an invertible n

λ) that divides cA(x). To this end, let

−

−

n matrix P exists such that

{

λ)dg(x) for some polynomial g(x),
x1, x2, . . . , xd
be a basis

}

P−

1AP =

×
λId B
A1
0

in block form, where Id denotes the d
cA′(x) = cA(x) by Theorem 5.5.1. But Theorem 3.1.5 gives

×

(cid:20)

(cid:21)
d identity matrix. Now write A′ = P−

1AP and observe that

cA(x) = cA′(x) = det (xIn

A′) = det

−

(cid:20)

= det [(x

(x

B

λ)Id
−
−
0
d
−
λ)Id] det [(xIn

xIn

A1

−

−
d
−

(cid:21)
A1)]

−
λ)dg(x)

where g(x) = cA1(x). This is what we wanted.

= (x

−

It is impossible to ignore the question when equality holds in Lemma 5.5.3 for each eigenvalue λ. It
n matrices A for which cA(x) factors completely
turns out that this characterizes the diagonalizable n
over R. By this we mean that cA(x) = (x
λn), where the λi are real numbers (not
−
necessarily distinct); in other words, every eigenvalue of A is real. This need not happen (consider A =

λ1)(x

×
λ2)

· · ·

(x

−

−

0
1

1
−
0

(cid:20)

), and we investigate the general case below.

(cid:21)

Theorem 5.5.6

Thefollowingareequivalentforasquarematrix A forwhich cA(x) factorscompletely.

1. A isdiagonalizable.

2. dim [Eλ(A)] equalsthemultiplicityofλ foreveryeigenvalueλ ofthematrix A.

11This is often called the algebraic multiplicity of λ.

Proof. Let A be n
×
multiplicity of λi and write di = dim

Eλi(A)

. Then

n and let λ1, λ2, . . . , λk be the distinct eigenvalues of A. For each i, let mi denote the

5.5. Similarity and Diagonalization

303

(cid:2)
cA(x) = (x

(cid:3)
λ1)m1(x

−

−

λ2)m2 . . . (x

λk)mk

−
mi for each i by Lemma 5.5.3.

so m1 +
(1)

+ mk = n because cA(x) has degree n. Moreover, di
(2). By (1), Rn has a basis of n eigenvectors of A, so let ti of them lie in Eλi(A) for each i. Since
di for each i by Theorem 5.2.4.

the subspace spanned by these ti eigenvectors has dimension ti, we have ti
Hence

· · ·
⇒

≤

≤

n = t1 +

· · ·

d1 +

+ tk ≤
+ mk so, since di

· · ·

+ dk ≤
≤

m1 +

+ mk = n

· · ·

⇒

· · ·

· · ·

(2)

+ dk = m1 +

It follows that d1 +

(1). Let Bi denote a basis of Eλi(A) for each i, and let B = B1

mi for each i, we must have di = mi. This is (2).
Bk. Since each Bi contains
mi vectors by (2), and since the Bi are pairwise disjoint (the λi are distinct), it follows that B contains n
vectors. So it sufﬁces to show that B is linearly independent (then B is a basis of Rn). Suppose a linear
combination of the vectors in B vanishes, and let yi denote the sum of all terms that come from Bi. Then yi
lies in Eλi(A), so the nonzero yi are independent by Theorem 5.5.4 (as the λi are distinct). Since the sum
of the yi is zero, it follows that yi = 0 for each i. Hence all coefﬁcients of terms in yi are zero (because Bi
is independent). Since this holds for each i, it shows that B is independent.

∪ · · · ∪

Example 5.5.5

If A =



5
4
4
−

8
1
4
−

16
8
11 


−

and B =






Solution. We have cA(x) = (x + 3)2(x
corresponding eigenspaces are Eλ1(A) = span

−

2 1
2 1
1 0

−

1
2
2 
−
−


1) so the eigenvalues are λ1 =

x1, x2

{

}

−
and Eλ2(A) = span

3 and λ2 = 1. The
where

x3

{

}

show that A is diagonalizable but B is not.

x1 =



, x2 =

1
−
1
0 
−

is independent, we have dim (Eλ1(A)) = 2 which is the

2
−
0
1 


2
1
1 


, x3 =










x1, x2

{

as the reader can verify. Since
multiplicity of λ1. Similarly, dim (Eλ2(A)) = 1 equals the multiplicity of λ2. Hence A is
diagonalizable by Theorem 5.5.6, and a diagonalizing matrix is P =
Turning to B, cB(x) = (x + 1)2(x
corresponding eigenspaces are Eλ1(B) = span

x1 x2 x3
1 and λ2 = 3. The
(cid:2)
and Eλ2(B) = span

3) so the eigenvalues are λ1 =
y1}

(cid:3)
where

y2}

−

−

}

{

{

.

1
−
2
1 


y1 =





, y2 =



5
6
1 


−



Here dim (Eλ1(B)) = 1 is smaller than the multiplicity of λ1, so the matrix B is not diagonalizable,
again by Theorem 5.5.6. The fact that dim (Eλ1(B)) = 1 means that there is no possibility of
ﬁnding three linearly independent eigenvectors.

304

Vector Space Rn

Complex Eigenvalues

0
1

A =

All the matrices we have considered have had real eigenvalues. But this need not be the case: The matrix
has characteristic polynomial cA(x) = x2 + 1 which has no real roots. Nonetheless, this
matrix is diagonalizable; the only difference is that we must use a larger set of scalars, the complex
numbers. The basic properties of these numbers are outlined in Appendix A.

1
−
0

(cid:20)

(cid:21)

Indeed, nearly everything we have done for real matrices can be done for complex matrices. The
methods are the same; the only difference is that the arithmetic is carried out with complex numbers rather
than real ones. For example, the gaussian algorithm works in exactly the same way to solve systems of
linear equations with complex coefﬁcients, matrix multiplication is deﬁned the same way, and the matrix
inversion algorithm works in the same way.

But the complex numbers are better than the real numbers in one respect: While there are polynomials
like x2 + 1 with real coefﬁcients that have no real root, this problem does not arise with the complex
numbers: Every nonconstant polynomial with complex coefﬁcients has a complex root, and hence factors
completely as a product of linear factors. This fact is known as the fundamental theorem of algebra.12

Example 5.5.6

Diagonalize the matrix A =

0
1

1
−
0

.

(cid:21)

(cid:20)

Solution. The characteristic polynomial of A is

cA(x) = det (xI

−

A) = x2 + 1 = (x

i)(x + i)

1. Hence the eigenvalues are λ1 = i and λ2 =

−
i, with corresponding eigenvectors

−

and x2 =

. Hence A is diagonalizable by the complex version of Theorem 5.5.5,

where i2 =
1
i
−

x1 =

−

1
i

(cid:20)

(cid:21)
and the complex version of Theorem 5.5.3 shows that P =

(cid:20)

(cid:21)

x1 x2

=

1 1
i
i
−

(cid:20)

is invertible

(cid:21)

and P−

1AP =

0
λ1
0 λ2

=

(cid:21)

(cid:20)

i
0

0

i
−

(cid:21)

(cid:20)

(cid:2)
. Of course, this can be checked directly.

(cid:3)

We shall return to complex linear algebra in Section 8.7.

12This was a famous open problem in 1799 when Gauss solved it at the age of 22 in his Ph.D. dissertation.

5.5. Similarity and Diagonalization

305

Symmetric Matrices13

On the other hand, many of the applications of linear algebra involve a real matrix A and, while A will
have complex eigenvalues by the fundamental theorem of algebra, it is always of interest to know when
the eigenvalues are, in fact, real. While this can happen in a variety of ways, it turns out to hold whenever
A is symmetric. This important theorem will be used extensively later. Surprisingly, the theory of complex
eigenvalues can be used to prove this useful result about real eigenvalues.

Let z denote the conjugate of a complex number z. If A is a complex matrix, the conjugate matrix A
.

is deﬁned to be the matrix obtained from A by conjugating every entry. Thus, if A =
For example,

, then A =

zi j

zi j

(cid:2)

(cid:3)

(cid:2)

(cid:3)

If A =

(cid:20)

i + 2
i

−

5
3 + 4i

then A =

(cid:21)

i + 2
i
−

(cid:20)

5

−

3

4i

(cid:21)

Recall that z + w = z + w and zw = z w hold for all complex numbers z and w. It follows that if A and B
are two complex matrices, then

A + B = A + B,

AB = A B

and λA = λ A

hold for all complex scalars λ. These facts are used in the proof of the following theorem.

Theorem 5.5.7
Let A beasymmetricrealmatrix. Ifλ isanycomplexeigenvalueof A,thenλ isreal.14

Proof. Observe that A = A because A is real. If λ is an eigenvalue of A, we show that λ is real by showing
= 0 and Ax = λx.
that λ = λ. Let x be a (possibly complex) eigenvector corresponding to λ, so that x
Deﬁne c = xT x.

If we write x = 





z1
z2
...
zn








where the zi are complex numbers, we have

c = xT x = z1z1 + z2z2 +

+ znzn =

z1

|

|

· · ·

2 +

z2

2 +

· · ·

|
|
= 0 (as x

+

2

zn

|

|

= 0). We show that λ = λ by

Thus c is a real number, and c > 0 because at least one of the zi
verifying that λc = λc. We have

λc = λ(xT x) = (λx)T x = (Ax)T x = xT AT x

At this point we use the hypothesis that A is symmetric and real. This means AT = A = A so we continue
the calculation:

13This discussion uses complex conjugation and absolute value. These topics are discussed in Appendix A.
14This theorem was ﬁrst proved in 1829 by the great French mathematician Augustin Louis Cauchy (1789–1857).

6
6
6
306

Vector Space Rn

λc = xT AT x = xT (A x) = xT (Ax) = xT (λx)
= xT (λ x)
= λxT x
= λc

as required.

The technique in the proof of Theorem 5.5.7 will be used again when we return to complex linear algebra
in Section 8.7.

Example 5.5.7

Verify Theorem 5.5.7 for every real, symmetric 2

2 matrix A.

×

we have cA(x) = x2

(a + c)x + (ac

b2), so the eigenvalues are given

−

Solution. If A =

by λ = 1

2[(a + c)

(cid:20)
±

a b
b c

p

(cid:21)
(a + c)2

4(ac

−

−
(a + c)2

−
b2)]. But here

4(ac

−

−

b2) = (a

−

c)2 + 4b2

0

≥

for any choice of a, b, and c. Hence, the eigenvalues are real numbers.

Exercises for 5.5

Exercise 5.5.1 By computing the trace, determinant,
and rank, show that A and B are not similar in each case.

Exercise 5.5.2

Show that 

1 2
2 0
1 1
4 3

−

1
1
0
0

0
1
1
0

−

and













1
1
−
0
5

1
−
0
1
1

−


−
−

Exercise 5.5.3 If A

−

3
1
4
1

0
1
1
4

∼



are not similar.




B, show that:

1 2
2 1

, B =

1 1
1 1

−

(cid:21)

(cid:20)

1 1
2 1

(cid:21)
0
1

(cid:21)

(cid:21)

, B =

, B =

, B =

(cid:20)

(cid:20)

(cid:20)

3
1

2
3

3
2

2
1

(cid:21)
1
1

1
1

−

−
3 1
1 2

(cid:21)
−
2 1 1
1 0 1
1 1 0

a. A =

b. A =

c. A =

d. A =

e. A =

f. A =

(cid:20)

(cid:20)

(cid:20)

(cid:20)









−

1
−
2

(cid:21)

(cid:21)

a.

c.

AT

rA

∼

∼

BT

rB for r in R

b.

d.

A−
An

1

∼

1

B−
∼
Bn for n

1

≥

, B =



−
−





, B =









1
2
3

−

2
4
6

−

2
6
0

1
2
3

−
−
1
3
0

−




3
9
−
0





Exercise 5.5.4 In each case, decide whether the matrix
1AP is di-
A is diagonalizable. If so, ﬁnd P such that P−
agonal.

1 0 0
1 2 1
0 0 1





a.





3
0
5

0 6
3 0
0 2

−





b.





1
1
0

2
1
3

−

−


3
2
5

−

5.5. Similarity and Diagonalization

307

3 1
2 1
1 0

c.





−

6
0
3

−





4 0 0
0 2 2
2 3 1





d.





Exercise 5.5.10 Let A be a diagonalizable n
n matrix
with eigenvalues λ1, λ2, . . . , λn (including multiplici-
ties). Show that:

×

Exercise 5.5.5 If A is invertible, show that AB is similar
to BA for all B.

b.

a. det A = λ1λ2

λn

· · ·
tr A = λ1 + λ2 +

+ λn

· · ·

Exercise 5.5.6 Show that the only matrix similar to a
scalar matrix A = rI, r in R, is A itself.

Exercise 5.5.7 Let λ be an eigenvalue of A with cor-
1AP is similar to A,
responding eigenvector x. If B = P−
1x is an eigenvector of B corresponding to
show that P−
λ.

Exercise 5.5.8 If A
∼
properties, show that B has the same property.

B and A has any of the following

a. Idempotent, that is A2 = A.

b. Nilpotent, that is Ak = 0 for some k

1.

≥

c. Invertible.

Exercise 5.5.11 Given a polynomial p(x) = r0 + r1x +
+ rnxn and a square matrix A, the matrix p(A) =
+ rnAn is called the evaluation of p(x)
1 p(A)P for

1AP. Show that p(B) = P−

· · ·
r0I + r1A +
at A. Let B = P−
all polynomials p(x).

· · ·

Exercise 5.5.12 Let P be an invertible n
A is any n

n matrix, write TP(A) = P−

n matrix. If
1AP. Verify that:

×

×

a.

c.

e.

f.

g.

b.

d.

TP(AB) = TP(A)TP(B)

TP(rA) = rTP(A)

TP(I) = I

TP(A + B) = TP(A) +
TP(B)
TP(Ak) = [TP(A)]k for k
If A is invertible, TP(A−

≥
1) = [TP(A)]−
If Q is invertible, TQ[TP(A)] = TPQ(A).

1.

1

Exercise 5.5.9 Let A denote an n
matrix.

×

n upper triangular

a. Show that two diagonalizable matrices are similar
if and only if they have the same eigenvalues with
the same multiplicities.

Exercise 5.5.13

a. If all the main diagonal entries of A are distinct,

show that A is diagonalizable.

b. If all the main diagonal entries of A are equal,
show that A is diagonalizable only if it is already
diagonal.

c. Show that

1 0 1
0 1 0
0 0 2




1 1 0
0 1 0
0 0 2









is diagonalizable but that





is not diagonalizable.

b. If A is diagonalizable, show that A

AT .

∼

c. Show that A

AT if A =

∼

1 1
0 1

(cid:21)

(cid:20)

2 and diagonalizable, show
has dimension 2 or 4. [Hint:
1AP = D, show that X is in C(A) if and only if

Exercise 5.5.14 If A is 2
that C(A) =
X A = AX
X
{
If P−
P−

1X P is in C(D).]

×
}

|

Exercise 5.5.15
If A is diagonalizable and p(x) is a
polynomial such that p(λ) = 0 for all eigenvalues λ of
A, show that p(A) = 0 (see Example 3.3.9). In particular,
show cA(A) = 0. [Remark: cA(A) = 0 for all square ma-
trices A—this is the Cayley-Hamilton theorem, see The-
orem 11.1.2.]

Exercise 5.5.16 Let A be n
values. If AC = CA, show that C is diagonalizable.

n with n distinct real eigen-

×

308

Vector Space Rn

Exercise 5.5.17 Let A =

c a b
a b c
b c a

.




B =





a. Show that x3

0 a b
a 0 c
b c 0





and





for all n

≥

0. Deﬁne

0
0
...
0
r0

1
0
...
0
r1

0
1
...
0
r2

0
0
...
1
rk

−

1

···
···

···
···

xn
xn+1
...
xn+k

−

1

.








, Vn = 








A = 





Then show that:

roots by considering A.

−

(a2 + b2 + c2)x

2abc has real

−

a. Vn = AnV0 for all n.

b. cA(x) = xk

1xk

−

1

rk

−

−

− · · · −

r1x

r0

−

b. Show that a2 + b2 + c2

ering B.

ab + ac + bc by consid-

≥

c. If λ is an eigenvalue of A, the eigenspace Eλ has
1)T is an
[Hint: Use cA(λ) = 0 to show that

dimension 1, and x = (1, λ, λ2, . . . , λk
eigenvector.
Eλ = Rx.]

−

Exercise 5.5.18 Assume the 2
2 matrix A is similar to
an upper triangular matrix. If tr A = 0 = tr A2, show that
A2 = 0.

×

d. A is diagonalizable if and only if the eigenvalues
[Hint: See part (c) and Theo-

of A are distinct.
rem 5.5.4.]

Exercise 5.5.19 Show that A is similar to AT for all 2

matrices A. [Hint: Let A =

. If c = 0 treat the

(cid:21)
(cid:20)
= 0 separately. If c
cases b = 0 and b
case c = 1 using Exercise 5.5.12(d).]

= 0, reduce to the

a b
c d

2

×

Exercise 5.5.20 Refer to Section 3.4 on linear recur-
rences. Assume that the sequence x0, x1, x2, . . . satisﬁes

xn+k = r0xn + r1xn+1 +

+ rk

−

1xn+k

1

−

· · ·

e. If λ1, λ2,

1 +

+ tkλn

. . . , λk are distinct real eigenvalues,
there exist constants t1, t2, . . . , tk such that xn =
t1λn
k holds for all n. [Hint: If D is di-
agonal with λ1, λ2, . . . , λk as the main diagonal
entries, show that An = PDnP−
1 has entries that are
2 , . . . , λn
1 , λn
linear combinations of λn
k .]

· · ·

Exercise 5.5.21 Suppose A is 2
tr A

= 0 show that A = 0.

×

2 and A2 = 0.

If

5.6 Best Approximation and Least Squares

Often an exact solution to a problem in applied mathematics is difﬁcult to obtain. However, it is usually
just as useful to ﬁnd arbitrarily close approximations to a solution. In particular, ﬁnding “linear approx-
imations” is a potent technique in applied mathematics. One basic case is the situation where a system
of linear equations has no solution, and it is desirable to ﬁnd a “best approximation” to a solution to the
system. In this section best approximations are deﬁned and a method for ﬁnding them is described. The
result is then applied to “least squares” approximation of data.

Suppose A is an m

×

n matrix and b is a column in Rm, and consider the system

Ax = b

of m linear equations in n variables. This need not have a solution. However, given any column z
the distance
column z in Rn that is as close as possible to a solution in the sense that

Rn,
is a measure of how far Az is from b. Hence it is natural to ask whether there is a

Az
k

−

∈

b

k

is the minimum value of

Ax

b

k

−

k

b

Az

−
as x ranges over all columns in Rn.

k

k

6
6
6
5.6. Best Approximation and Least Squares

309

The answer is “yes”, and to describe it deﬁne

U =

Ax

{

|

x lies in Rn

}

b

Az

b

−

0

Az

U

This is a subspace of Rn (verify) and we want a vector Az in U as close as
possible to b. That there is such a vector is clear geometrically if n = 3 by
the diagram. In general such a vector Az exists by a general result called
the projection theorem that will be proved in Chapter 8 (Theorem 8.1.3).
Moreover, the projection theorem gives a simple way to compute z because
it also shows that the vector b
Az is orthogonal to every vector Ax in U .
Thus, for all x in Rn,

−

0 = (Ax)

(b

·

−

Az) = (Ax)T (b

−

Az) = xT AT (b
[AT (b
= x

−

Az)

Az)]

·

−

In other words, the vector AT (b
orthogonal to itself). Hence z satisﬁes

−

Az) in Rn is orthogonal to every vector in Rn and so must be zero (being

(AT A)z = AT b

Deﬁnition 5.14 Normal Equations

Thisisasystemoflinearequationscalledthenormalequationsforz.

n matrix AT A
Note that this system can have more than one solution (see Exercise 5.6.5). However, the n
is invertible if (and only if) the columns of A are linearly independent (Theorem 5.4.3); so, in this case,
z is uniquely determined and is given explicitly by z = (AT A)−
1AT b. However, the most efﬁcient way to
ﬁnd z is to apply gaussian elimination to the normal equations.
This discussion is summarized in the following theorem.

×

Theorem 5.6.1: Best Approximation Theorem

Let A bean m

×

n matrix,letbbeanycolumnin Rm,andconsiderthesystem

Ax= b

of m equationsin n variables.

1. Anysolutionztothenormalequations

(AT A)z= AT b

isabestapproximationtoasolutionto Ax= binthesensethat
valueof

asxrangesoverallcolumnsin Rn.

b
k

−

Ax
k

b
k

−

Az
k

istheminimum

2. Ifthecolumnsof A arelinearlyindependent,then AT A isinvertibleandzisgivenuniquely

byz= (AT A)−

1AT b.

310

Vector Space Rn

We note in passing that if A is n

n and invertible, then

×

z = (AT A)−

1AT b = A−

1b

= 0. Hence if A has independent columns, then
is the solution to the system of equations, and
−
(AT A)−
1AT is playing the role of the inverse of the nonsquare matrix A. The matrix AT (AAT )−
1 plays a
similar role when the rows of A are linearly independent. These are both special cases of the generalized
inverse of a matrix A (see Exercise 5.6.14). However, we shall not pursue this topic here.

Az

b

k

k

Example 5.6.1

The system of linear equations

−

3x
y = 4
x + 2y = 0
2x + y = 1

has no solution. Find the vector z =

x0
y0

(cid:20)

(cid:21)

that best approximates a solution.

Solution. In this case,

A =



3
1
2

1
−
2
1 




, so AT A =

3 1 2
1 2 1

−

(cid:20)

3
1
2

1
−
2
1 


(cid:21)





14 1
1 6

=

(cid:20)

(cid:21)

is invertible. The normal equations (AT A)z = AT b are

14 1
1 6

z =

(cid:21)

(cid:20)

14
3
−

(cid:21)

(cid:20)

, so z = 1
83

87
56

−

(cid:21)

(cid:20)

Thus x0 = 87
approximately,

83 and y0 = −

56
83 . With these values of x and y, the left sides of the equations are,

3x0

−

y0 = 317
83 = 3.82
25
0.30
83 =
−
83 = 1.42

x0 + 2y0 = −
2x0 + y0 = 118

This is as close as possible to a solution.

Example 5.6.2

The average number g of goals per game scored by a hockey player seems to be related linearly to
two factors: the number x1 of years of experience and the number x2 of goals in the preceding 10
games. The data on the following page were collected on four players. Find the linear function

5.6. Best Approximation and Least Squares

311

g = a0 + a1x1 + a2x2 that best ﬁts these data.

g
0.8
0.8
0.6
0.4

x1
5
3
1
2

x2
3
4
5
1

Solution. If the relationship is given by g = r0 + r1x1 + r2x2, then the data can be described as
follows:

1 5 3
1 3 4
1 1 5
1 2 1

r0
r1
r2















= 





0.8
0.8
0.6
0.4







Using the notation in Theorem 5.6.1, we get

z = (AT A)−

1AT b

= 1

42 



119
17
19

−
−

−

17
5
1

−

19
1
5 






1 1 1 1
5 3 1 2
3 4 5 1 


0.14
0.09
0.08 








=





Hence the best-ﬁtting function is g = 0.14 + 0.09x1 + 0.08x2. The amount of computation would
have been reduced if the normal equations had been constructed and then solved by gaussian
elimination.







0.8
0.8
0.6
0.4







Least Squares Approximation

In many scientiﬁc investigations, data are collected that relate two variables. For example, if x is the
number of dollars spent on advertising by a manufacturer and y is the value of sales in the region in
question, the manufacturer could generate data by spending x1, x2, . . . , xn dollars at different times and
measuring the corresponding sales values y1, y2, . . . , yn.

y

Line 1

Line 2

(x4, y4)

(x3, y3)

(x5, y5)

Suppose it is known that a linear relationship exists between the vari-
ables x and y—in other words, that y = a + bx for some constants a and
b. If the data are plotted, the points (x1, y1), (x2, y2), . . . , (xn, yn) may
appear to lie on a straight line and estimating a and b requires ﬁnding
the “best-ﬁtting” line through these data points. For example, if ﬁve data
points occur as shown in the diagram, line 1 is clearly a better ﬁt than line
2. In general, the problem is to ﬁnd the values of the constants a and b
such that the line y = a + bx best approximates the data in question. Note
that an exact ﬁt would be obtained if a and b were such that yi = a + bxi
were true for each data point (xi, yi). But this is too much to expect. Ex-
perimental errors in measurement are bound to occur, so the choice of a and b should be made in such a
way that the errors between the observed values yi and the corresponding ﬁtted values a + bxi are in some

(x1, y1)

(x2, y2)

x

0

312

Vector Space Rn

sense minimized. Least squares approximation is a way to do this.

The ﬁrst thing we must do is explain exactly what we mean by the best ﬁt of a line y = a + bx to an
observed set of data points (x1, y1), (x2, y2), . . . , (xn, yn). For convenience, write the linear function
r0 + r1x as

f (x) = r0 + r1x

so that the ﬁtted points (on the line) have coordinates (x1, f (x1)), . . . , (xn, f (xn)).

y

d1

(xn, f (xn))
(xn, yn)

(xi, f (xi))

dn

(xi, yi)
di
f(x)

y =
(x1, f (x1))

(x1, y1)

0

x1

xi

xn

x

The second diagram is a sketch of what the line y = f (x) might look
like. For each i the observed data point (xi, yi) and the ﬁtted point
(xi,
f (xi)) need not be the same, and the distance di between them mea-
sures how far the line misses the observed point. For this reason di is often
called the error at xi, and a natural measure of how close the line y = f (x)
is to the observed data points is the sum d1 + d2 +
+ dn of all these
errors. However, it turns out to be better to use the sum of squares

· · ·

S = d2

1 + d2

2 +

+ d2
n

· · ·

as the measure of error, and the line y = f (x) is to be chosen so as to make this sum as small
This line is said to be the least squares approximating line for the data points
as possible.
(x1, y1), (x2, y2), . . . , (xn, yn).

The square of the error di is given by d2

i = [yi

f (xi)]2 for each i, so the quantity S to be minimized is

−

the sum:

S = [y1

f (x1)]2 + [y2

f (x2)]2 +

+ [yn

f (xn)]2

−

· · ·

−

−

Note that all the numbers xi and yi are given here; what is required is that the function f be chosen in such
a way as to minimize S. Because f (x) = r0 + r1x, this amounts to choosing r0 and r1 to minimize S. This
problem can be solved using Theorem 5.6.1. The following notation is convenient.

x1
x2
...
xn

y1
y2
...
yn



y = 








Then the problem takes the following form: Choose r0 and r1 such that

f (x) = 





x = 















and





f (x1)
f (x2)
...
f (xn)

r0 + r1x1
r0 + r1x2
...
r0 + r1xn








= 





S = [y1

−

f (x1)]2 + [y2

f (x2)]2 +

+ [yn

−

· · ·

f (xn)]2 =

y

k

−

f (x)

2

k

−

is as small as possible. Now write

1 x1
1 x2
...
...
1 xn








M = 





and

r =

r0
r1

(cid:20)

(cid:21)

Then Mr = f (x), so we are looking for a column r =

such that

y

Mr

2 is as small as possible.

In other words, we are looking for a best approximation z to the system Mr = y. Hence Theorem 5.6.1
applies directly, and we have

r0
r1

(cid:20)

(cid:21)

k

−

k

5.6. Best Approximation and Least Squares

313

Theorem 5.6.2

Supposethatndatapoints (x1, y1), (x2, y2), . . . , (xn, yn) aregiven,whereatleasttwoof
x1, x2, . . . , xn aredistinct. Put

y1
y2
...
yn

y= 












M = 





1 x1
1 x2
...
...
1 xn








Thentheleastsquaresapproximatinglineforthesedatapointshasequation

y = z0 + z1x

wherez=

z0
z1

(cid:20)

(cid:21)

isfoundbygaussianeliminationfromthenormalequations

(MT M)z= MT y

Theconditionthatatleasttwoof x1, x2, . . . , xn aredistinctensuresthat MT M isaninvertible
matrix,sozisunique:

z= (MT M)−

1MT y

Example 5.6.3

Let data points (x1, y1), (x2, y2), . . . , (x5, y5) be given as in the accompanying table. Find the
least squares approximating line for these data.

y
x
1 1
3 2
4 3
6 4
7 5

Solution. In this case we have

MT M =

(cid:20)

(cid:20)

=

1
1
x1 x2

· · ·
· · ·

1 x1
1 x2
...
...
1 x5
+ x5
+ x2



1
x5 (cid:21)





x1 +
· · ·
1 +
· · ·








x1 +

5

· · ·

+ x5 x2

5

21
21 111

=

(cid:20)

(cid:21)

5 (cid:21)

314

Vector Space Rn

and MT y =

(cid:20)

(cid:20)

=

1
1
x1 x2

· · ·
· · ·



1
x5 (cid:21)

y1 + y2 +
x1y1 + x2y2 +

· · ·
· · ·

15
78

=

(cid:20)

(cid:21)



y1
y2
...
y5









+ y5
+ x5y5 (cid:21)
z0
z1

(cid:20)

(cid:21)

so the normal equations (MT M)z = MT y for z =

become

5

21
21 111

(cid:20)

=

(cid:21)

(cid:20)

The solution (using gaussian elimination) is z =

z0
z1

z0
z1

=

(cid:21)

(cid:20)

=

(cid:21)

(cid:20)

15
78

(cid:21)

0.24
0.66

(cid:21)

(cid:20)

least squares approximating line for these data is y = 0.24 + 0.66x. Note that MT M is indeed
invertible here (the determinant is 114), and the exact solution is

to two decimal places, so the

z = (MT M)−

1MT y = 1
114

111
21

−

(cid:20)

21
5

−

15
78

= 1
114

(cid:21)

(cid:20)

27
75

= 1
38

(cid:21)

(cid:20)

9
25

(cid:21)

(cid:21) (cid:20)

Least Squares Approximating Polynomials

Suppose now that, rather than a straight line, we want to ﬁnd a polynomial

y = f (x) = r0 + r1x + r2x2 +

+ rmxm

· · ·

of degree m that best approximates the data pairs (x1, y1), (x2, y2), . . . , (xn, yn). As before, write

x1
x2
...
xn

x = 












y = 





y1
y2
...
yn








and

f (x1)
f (x2)
...
f (xn)








f (x) = 





For each xi we have two values of the variable y, the observed value yi, and the computed value f (xi). The
problem is to choose f (x)—that is, choose r0, r1, . . . , rm —such that the f (xi) are as close as possible to
the yi. Again we deﬁne “as close as possible” by the least squares condition: We choose the ri such that

y

k
is as small as possible.

f (x)

k

−

2 = [y1

−

f (x1)]2 + [y2

f (x2)]2 +

+ [yn

−

· · ·

f (xn)]2

−










y1
y2
...
yn

5.6. Best Approximation and Least Squares

315

Deﬁnition 5.15 Least Squares Approximation

Apolynomial f (x) satisfyingthisconditioniscalledaleastsquaresapproximatingpolynomial
ofdegree m forthegivendatapairs.

If we write

M =

1 x1 x2
1
1 x2 x2
2
...
...
...
1 xn x2
n

xm
1
xm
2
...
xm
n

· · ·

· · ·

· · ·














and

r =

r0
r1
...
rm







2 is as small as possible; that is, we








we see that f (x) = Mr. Hence we want to ﬁnd r such that
want a best approximation z to the system Mr = y. Theorem 5.6.1 gives the ﬁrst part of Theorem 5.6.3.

Mr

−

k

k

y

Theorem 5.6.3

Let n datapairs (x1, y1), (x2, y2), . . . , (xn, yn) begiven,andwrite



M =

y= 




1. Ifzisanysolutiontothenormalequations















1 x1 x2
1
1 x2 x2
2
...
...
...
1 xn x2
n

· · ·

· · ·

· · ·

xm
1
xm
2
...
xm
n










z=

z0
z1
...
zm



















(MT M)z= MT y

thenthepolynomial

z0 + z1x + z2x2 +
isaleastsquaresapproximatingpolynomialofdegreemforthegivendatapairs.

+ zmxm

· · ·

2. Ifatleast m + 1 ofthenumbers x1, x2, . . . , xn aredistinct(so n

invertibleandzisuniquelydeterminedby

m + 1),thematrix MT M is

≥

z= (MT M)−

1MT y

Proof. It remains to prove (2), and for that we show that the columns of M are linearly independent
(Theorem 5.4.3). Suppose a linear combination of the columns vanishes:

If we write q(x) = r0 + r1x +

r0 




· · ·



x1
x2
...
xn

1
1
= 
...

1



+ rmxm, equating coefﬁcients shows that

+ rm 





+ r1 





xm
1
xm
2
...
xm
n
















· · ·

+





0
0
...
0








q(x1) = q(x2) =

= q(xn) = 0

· · ·

316

Vector Space Rn

Hence q(x) is a polynomial of degree m with at least m + 1 distinct roots, so q(x) must be the zero poly-
nomial (see Appendix D or Theorem 6.5.4). Thus r0 = r1 =

= rm = 0 as required.

· · ·

Example 5.6.4

Find the least squares approximating quadratic y = z0 + z1x + z2x2 for the following data points.

(

3, 3), (

−

1, 1), (0, 1), (1, 2), (3, 4)

−

Solution. This is an instance of Theorem 5.6.3 with m = 2. Here

1
1
1
1
1

−
−

3 9
1 1
0 0
1 1
3 9









Hence,

y =

3
1
1
2
4

















M =

MT M =

1
3
−
9





−

1 1 1 1
1 0 1 3
1 0 1 9 


















1
1
1
1
1

MT y =

1
3
−
9





−

1 1 1 1
1 0 1 3
1 0 1 9 


The normal equations for z are

−
−

3 9
1 1
0 0
1 1
3 9

5
0
0 20
20

20
0
0 164 


=





11
4
66 


=





















3
1
1
2
4









5
0
0 20

20
0
0 164 


z =





11
4
66 


20





whence z =



1.15
0.20
0.26 




This means that the least squares approximating quadratic for these data is
y = 1.15 + 0.20x + 0.26x2.

5.6. Best Approximation and Least Squares

317

Other Functions

There is an extension of Theorem 5.6.3 that should be mentioned. Given data pairs (x1, y1), (x2, y2),
. . . , (xn, yn), that theorem shows how to ﬁnd a polynomial

k

y

f (x)

f (x) = r0 + r1x +

+ rmxm
2 is as small as possible, where x and f (x) are as before. Choosing the appropriate
such that
polynomial f (x) amounts to choosing the coefﬁcients r0, r1, . . . , rm, and Theorem 5.6.3 gives a formula
for the optimal choices. Here f (x) is a linear combination of the functions 1, x, x2, . . . , xm where the ri
are the coefﬁcients, and this suggests applying the method to other functions. If f0(x), f1(x), . . . , fm(x)
are given functions, write

· · ·

−

k

where the ri are real numbers. Then the more general question is whether r0, r1, . . . , rm can be found such
that

2 is as small as possible where

y

f (x)

k

−

k

f (x) = r0 f0(x) + r1 f1(x) +

+ rm fm(x)

· · ·

f (x1)
f (x2)
...
f (xm)

f (x) = 




Such a function f (x) is called a least squares best approximation for these data pairs of the form
r0 f0(x) + r1 f1(x) +

+ rm fm(x), ri in R. The proof of Theorem 5.6.3 goes through to prove








· · ·

Theorem 5.6.4

Let n datapairs (x1, y1), (x2, y2), . . . , (xn, yn) begiven,andsupposethat m + 1 functions
f0(x), f1(x), . . . , fm(x) arespeciﬁed. Write

y1
y2
...
yn



y= 





M = 





f0(x1)
f0(x2)
...
f0(xn)

f1(x1)
f1(x2)
...
f1(xn)





1. Ifzisanysolutiontothenormalequations

fm(x1)
fm(x2)
...
fm(xn)

· · ·
· · ·

· · ·

z1
z2
...
zm















z= 





thenthefunction

(MT M)z= MT y

z0 f0(x) + z1 f1(x) +

+ zm fm(x)

· · ·

isthebestapproximationforthesedataamongallfunctionsoftheform
r0 f0(x) + r1 f1(x) +

+ rm fm(x) wherethe ri arein R.

· · ·

2. If MT M isinvertible(thatis,if rank (M) = m + 1),thenzisuniquelydetermined;infact,

z= (MT M)−

1(MT y).

Clearly Theorem 5.6.4 contains Theorem 5.6.3 as a special case, but there is no simple test in gen-
eral for whether MT M is invertible. Conditions for this to hold depend on the choice of the functions
f0(x), f1(x), . . . , fm(x).

318

Vector Space Rn

Example 5.6.5

Given the data pairs (
the form r0x + r12x.

1, 0), (0, 1), and (1, 4), ﬁnd the least squares approximating function of

−

Solution. The functions are f0(x) = x and f1(x) = 2x, so the matrix M is

In this case MT M = 1
4


6
8
6 21

(cid:21)

(cid:20)

M =



f0(x1)
f0(x2)
f0(x3)

f1(x1)
f1(x2)
f1(x3)

−

1
1 2−
0 20
1 21 


= 1

2 



−

2 1
0 2
2 4 


=









is invertible, so the normal equations

1
4

(cid:20)

8
6
6 21

z =

(cid:21)

(cid:20)

4
9

(cid:21)

have a unique solution z = 1
11

10
16

(cid:20)

(cid:21)

f (x) = 10

11x + 16

11 2x. Note that f (x) =

. Hence the best-ﬁtting function of the form r0x + r12x is

f (
1)
−
f (0)
f (1)









= 





2
−
11
16
11
42
11



, compared with y =






0
1
4 






Exercises for 5.6

Exercise 5.6.1 Find the best approximation to a solution
of each of the following systems of equations.

b.

a.

−

z = 5
x + y
y + 6z = 1
2x
−
3x + 2y
z = 6
−
x + 4y + z = 0

−

3x + y + z = 6
z = 1
2x + 3y
y + z = 0
2x
3y + 3z = 8
3x

−

−
−

a. (0, 1), (2, 2), (3, 3), (4, 5)

b. (

−

2, 1), (0, 0), (3, 2), (4, 3)

Exercise 5.6.4 Find a least squares approximating func-
tion of the form r0x + r1x2 + r22x for each of the follow-
ing sets of data pairs.

Exercise 5.6.2 Find the least squares approximating line
y = z0 + z1x for each of the following sets of data points.

a. (1, 1), (3, 2), (4, 3), (6, 4)

b. (2, 4), (4, 3), (7, 2), (8, 1)

a. (

−

1, 1), (0, 3), (1, 1), (2, 0)

b. (0, 1), (1, 1), (2, 5), (3, 10)

c. (

−

d. (

−

−
2, 3), (

1,

1), (0, 1), (1, 2), (2, 4), (3, 6)

1, 1), (0, 0), (1,

2), (2,

−

4)

−

−

Exercise 5.6.5
function of the form r0 + r1x2 + r2 sin πx
following sets of data pairs.

Find the least squares approximating
2 for each of the

Exercise 5.6.3
Find the least squares approximating
quadratic y = z0 + z1x + z2x2 for each of the following
sets of data points.

a. (0, 3), (1, 0), (1,

1), (

1, 2)

−
2 ), (0, 1), (2, 5), (3, 9)

−

1, 1

b. (

−

1y (in the notation of Theorem 5.6.3).

Exercise 5.6.6 If M is a square invertible matrix, show
that z = M−
Exercise 5.6.7 Newton’s laws of motion imply that an
object dropped from rest at a height of 100 metres will
1
2 gt2 metres t seconds later,
be at a height s = 100
where g is a constant called the acceleration due to grav-
ity. The values of s and t given in the table are observed.
Write x = t2, ﬁnd the least squares approximating line
s = a + bx for these data, and use b to estimate g.

−

Then ﬁnd the least squares approximating quadratic
s = a0 + a1t + a2t2 and use the value of a2 to estimate g.

1

2

t
3
s 95 80 56

Exercise 5.6.8 A naturalist measured the heights yi (in
metres) of several spruce trees with trunk diameters xi (in
centimetres). The data are as given in the table. Find the
least squares approximating line for these data and use
it to estimate the height of a spruce tree with a trunk of
diameter 10 cm.

xi
yi

8

7

5
16
12
2 3.3 4 7.3 7.9 10.1

13

Exercise 5.6.9 The yield y of wheat in bushels per acre
appears to be a linear function of the number of days x1 of
sunshine, the number of inches x2 of rain, and the num-
ber of pounds x3 of fertilizer applied per acre. Find the
best ﬁt to the data in the table by an equation of the form
y = r0 + r1x1 + r2x2 + r3x3. [Hint: If a calculator for in-
verting AT A is not available, the inverse is given in the
answer.]

x2

x1

x3
y
28 50 18 10
30 40 20 16
21 35 14 10
23 40 12 12
23 30 16 14

Exercise 5.6.10

a. Use m = 0 in Theorem 5.6.3 to show that the
best-ﬁtting horizontal line y = a0 through the data
points (x1, y1), . . . , (xn, yn) is
y = 1

+ yn)

n (y1 + y2 +

· · ·
the average of the y coordinates.

5.6. Best Approximation and Least Squares

319

b. Deduce the conclusion in (a) without using Theo-

rem 5.6.3.

Exercise 5.6.11 Assume n = m + 1 in Theorem 5.6.3 (so
M is square). If the xi are distinct, use Theorem 3.2.6 to
1y and that
show that M is invertible. Deduce that z = M−
the least squares polynomial is the interpolating polyno-
mial (Theorem 3.2.6) and actually passes through all the
data points.

|

×

x
{

Exercise 5.6.12 Let A be any m
n matrix and write
AT Ax = 0
. Let b be an m-column. Show that,
K =
}
if z is an n-column such that
is minimal, then all
such vectors have the form z + x for some x
K. [Hint:
b
k
Exercise 5.6.13 Given the situation in Theorem 5.6.4,
write

b
k
is minimal if and only if AT Ay = AT b.]

Ay
k

Az
k

−

−

∈

f (x) = r0 p0(x) + r1 p1(x) +

+ rm pm(x)

· · ·

Suppose that f (x) has at most k roots for any choice of
the coefﬁcients r0, r1, . . . , rm, not all zero.

a. Show that MT M is invertible if at least k + 1 of the

xi are distinct.

b. If at least two of the xi are distinct, show that
there is always a best approximation of the form
r0 + r1ex.

c. If at least three of the xi are distinct, show that
there is always a best approximation of the form
r0 + r1x + r2ex. [Calculus is needed.]

Exercise 5.6.14 If A is an m
n matrix, it can be proved
×
m matrix A# satisfying the
that there exists a unique n
×
following four conditions: AA#A = A; A#AA# = A#; AA#
and A#A are symmetric. The matrix A# is called the gen-
eralized inverse of A, or the Moore-Penrose inverse.

a. If A is square and invertible, show that A# = A−

1.

b. If rank A = m, show that A# = AT (AAT )−

1.

c. If rank A = n, show that A# = (AT A)−

1AT .

320

Vector Space Rn

5.7 An Application to Correlation and Variance

Suppose the heights h1, h2, . . . , hn of n men are measured. Such a data set is called a sample of the heights
of all the men in the population under study, and various questions are often asked about such a sample:
What is the average height in the sample? How much variation is there in the sample heights, and how can
it be measured? What can be inferred from the sample about the heights of all men in the population? How
do these heights compare to heights of men in neighbouring countries? Does the prevalence of smoking
affect the height of a man?

The analysis of samples, and of inferences that can be drawn from them, is a subject called mathemat-
ical statistics, and an extensive body of information has been developed to answer many such questions.
In this section we will describe a few ways that linear algebra can be used.

It is convenient to represent a sample

x1, x2, . . . , xn

as a sample vector15 x =

x1 x2

in Rn. This being done, the dot product in Rn provides a convenient tool to study the sample and describe
(cid:3)
some of the statistical concepts related to it. The most widely known statistic for describing a data set is
the sample mean x deﬁned by16

(cid:2)

xn

· · ·

{

}

x = 1

n(x1 + x2 +

+ xn) = 1
n

· · ·

n
∑
i=1

xi

The mean x is “typical” of the sample values xi, but may not itself be one of them. The number xi
x is
called the deviation of xi from the mean x. The deviation is positive if xi > x and it is negative if xi < x.
Moreover, the sum of these deviations is zero:

−

Sample x

1

−

0 1

x

Centred
Sample xc

3

−

2

−

1

−

xc

n
∑
i=1

x) =

(xi

−

n
∑
i=1

xi

! −

nx = nx

nx = 0

−

(5.6)

This is described by saying that the sample mean x is central to the

sample values xi.

If the mean x is subtracted from each data value xi, the resulting data
x are said to be centred. The corresponding data vector is

−

xi

xc =

x1

x x2

x

xn

x

−

−
(cid:3)
and (5.6) shows that the mean xc = 0. For example, we have plotted the
sample x =
in the ﬁrst diagram. The mean is x = 2,
2
3
is also plotted. Thus, the effect of centring is to shift
−
−

(cid:2)
1 0 1 4 6

−
1 2 4
(cid:2)

· · ·

−

(cid:3)

and the centred sample xc =
the data by an amount x (to the left if x is positive) so that the mean moves to 0.

−

(cid:2)

(cid:3)

Another question that arises about samples is how much variability there is in the sample

x =

x1 x2

xn

· · ·

that is, how widely are the data “spread out” around the sample mean x. A natural measure of variability
would be the sum of the deviations of the xi about the mean, but this sum is zero by (5.6); these deviations

(cid:2)

(cid:3)

15We write vectors in Rn as row matrices, for convenience.
16The mean is often called the “average” of the sample values xi, but statisticians use the term “mean”.

 
5.7. An Application to Correlation and Variance

321

cancel out. To avoid this cancellation, statisticians use the squares (xi
of variability. More precisely, they compute a statistic called the sample variance s2

−

x)2 of the deviations as a measure
x deﬁned17 as follows:

s2
x = 1
1 [(x1
n
−

−

x)2 + (x2

x)2 +

+ (xn

x)2] = 1
n
1
−

−

· · ·

−

n
∑
i=1

(xi

x)2

−

The sample variance will be large if there are many xi at a large distance from the mean x, and it will
be small if all the xi are tightly clustered about the mean. The variance is clearly nonnegative (hence the
notation s2

x), and the square root sx of the variance is called the sample standard deviation.
The sample mean and variance can be conveniently described using the dot product. Let

denote the row with every entry equal to 1. If x =
the sample mean is given by the formula

(cid:2)
x = 1
n (x
·
Moreover, remembering that x is a scalar, we have x1 =
is given by

(cid:2)

−
Thus we obtain a formula for the sample variance:

−

(cid:2)

xc = x

x1 =

x1

(cid:2)
x x2
−

x

· · ·

xn

(cid:3)
x
−

(cid:3)

1 =

1 1

1

· · ·
x1 x2

(cid:3)
· · ·

xn

, then x

·

1 = x1 + x2 +

+ xn, so

· · ·

1)

x x

· · ·

(cid:3)

x

, so the centred sample vector xc

s2
x = 1
n
1 k
−

xc

2 = 1
n
1k
−

k

x

x1

2

k

−

Linear algebra is also useful for comparing two different samples. To illustrate how, consider two exam-
ples.

The following table represents the number of sick days at work per

year and the yearly number of visits to a physician for 10 individuals.

Sick
Days

Sick
Days

Individual
1 2 3 4 5
Doctor visits 2 6 8 1 5 10 3 9 7
4 7 7
2 4 8 3 5

7 8 9 10
4
2

Sick days

9

6

Doctor Visits

The data are plotted in the scatter diagram where it is evident that,
roughly speaking, the more visits to the doctor the more sick days. This is
an example of a positive correlation between sick days and doctor visits.
Now consider the following table representing the daily doses of vita-

min C and the number of sick days.

Individual 1 2 3 4 5 6 7 8 9 10
Vitamin C 1 5 7 0 4 9 2 8 6
3
Sick days
5
5 2 2 6 2 1 4 3 2

The scatter diagram is plotted as shown and it appears that the more vita-
min C taken, the fewer sick days. In this case there is a negative correla-
tion between daily vitamin C and sick days.

Vitamin C Doses

17Since there are n sample values, it seems more natural to divide by n here, rather than by n

1
is that then the sample variance s2x provides a better estimate of the variance of the entire population from which the sample
was drawn.

1. The reason for using n

−

−

322

Vector Space Rn

In both these situations, we have paired samples, that is observations of two variables are made for ten
individuals: doctor visits and sick days in the ﬁrst case; daily vitamin C and sick days in the second case.
The scatter diagrams point to a relationship between these variables, and there is a way to use the sample
to compute a number, called the correlation coefﬁcient, that measures the degree to which the variables
are associated.

To motivate the deﬁnition of the correlation coefﬁcient, suppose two paired samples

x =

x1 x2

xn

, and y =

y1 y2

yn

· · ·

are given and consider the centred samples

(cid:3)
x1

x x2

(cid:2)
x
−

−
(cid:3)
x will be negative if xk
If xk is large among the xi’s, then the deviation xk
−
is small among the xi’s. The situation is similar for y, and the following table displays the sign of the
quantity (xi

−
x will be positive; and xk

y) in all four cases:

x)(yk

· · ·

· · ·

−

−

−

−

(cid:2)

(cid:3)

(cid:2)

xn

x

(cid:3)
and yc =

y1

y y2

y

yn

y

(cid:2)

· · ·
xc =

−

−

y) :

Sign of (xi

x)(yk

−
−
xi large
xi small
yi large
positive negative
yi small negative positive

Intuitively, if x and y are positively correlated, then two things happen:

1. Large values of the xi tend to be associated with large values of the yi, and

2. Small values of the xi tend to be associated with small values of the yi.

It follows from the table that, if x and y are positively correlated, then the dot product

yc =

xc

·

n
∑
i=1

(xi

x)(yi

y)

−

−

is positive. Similarly xc
correlation coefﬁcient18 r is deﬁned by

·

yc is negative if x and y are negatively correlated. With this in mind, the sample

r = r(x, y) = xc
·
xc
k k
k

yc
yck

Bearing the situation in R3 in mind, r is the cosine of the “angle” between the vectors xc and yc, and so
1 and 1. Moreover, we would expect r to be near 1 (or
1) if these
we would expect it to lie between
vectors were pointing in the same (opposite) direction, that is the “angle” is near zero (or π).

−

−

This is conﬁrmed by Theorem 5.7.1 below, and it is also borne out in the examples above. If we
compute the correlation between sick days and visits to the physician (in the ﬁrst scatter diagram above)
the result is r = 0.90 as expected. On the other hand, the correlation between daily vitamin C doses and
sick days (second scatter diagram) is r =

0.84.

However, a word of caution is in order here. We cannot conclude from the second example that taking
more vitamin C will reduce the number of sick days at work. The (negative) correlation may arise because

−

18The idea of using a single number to measure the degree of relationship between different variables was pioneered by
Francis Galton (1822–1911). He was studying the degree to which characteristics of an offspring relate to those of its parents.
The idea was reﬁned by Karl Pearson (1857–1936) and r is often referred to as the Pearson correlation coefﬁcient.

5.7. An Application to Correlation and Variance

323

of some third factor that is related to both variables. For example, case it may be that less healthy people
are inclined to take more vitamin C. Correlation does not imply causation. Similarly, the correlation
between sick days and visits to the doctor does not mean that having many sick days causes more visits to
the doctor. A correlation between two variables may point to the existence of other underlying factors, but
it does not necessarily mean that there is a causality relationship between the variables.

Our discussion of the dot product in Rn provides the basic properties of the correlation coefﬁcient:

Theorem 5.7.1

x1 x2

Letx=
r = r(x, y) denotethecorrelationcoefﬁcient. Then:
(cid:3)

andy=

y1 y2

· · ·

· · ·

xn

(cid:2)

(cid:2)

yn

be(nonzero)pairedsamples,andlet

(cid:3)

1.

1
−

≤

r

≤

1.

2. r = 1 ifandonlyifthereexist a and b > 0 suchthat yi = a + bxi foreach i.

3. r =

1 ifandonlyifthereexist a and b < 0 suchthat yi = a + bxi foreach i.

−

Proof. The Cauchy inequality (Theorem 5.3.2) proves (1), and also shows that r =
of xc and yc is a scalar multiple of the other. This in turn holds if and only if yc = bxc for some b
it is easy to verify that r = 1 when b > 0 and r =

1 when b < 0.

1 if and only if one
= 0, and

±

Finally, yc = bxc means yi

y = b(xi

x) for each i; that is, yi = a + bxi where a = y

if yi = a + bxi, then y = a + bx (verify), so y1
words, yc = bxc. This completes the proof.

−

−

y = (a + bxi)

−

−

(a + bx) = b(x1

−

bx. Conversely,
x) for each i. In other

−

−

Properties (2) and (3) in Theorem 5.7.1 show that r(x, y) = 1 means that there is a linear relation
with positive slope between the paired data (so large x values are paired with large y values). Similarly,
r(x, y) =
1 means that there is a linear relation with negative slope between the paired data (so small x
values are paired with small y values). This is borne out in the two scatter diagrams above.

−

We conclude by using the dot product to derive some useful formulas for computing variances and
, the key ob-

and y =

x1 x2

y1 y2

xn

yn

correlation coefﬁcients. Given samples x =
servation is the following formula:

· · ·

(cid:2)
xc

yc = x

y

·

−

·

(cid:3)

nx y

(cid:2)

(cid:3)

(5.7)

Indeed, remembering that x and y are scalars:

· · ·

y1)

xc

·

yc = (x
= x
= x
= x
= x

·
·
·
·

−
y
y
y
y

(y
x1)
·
−
(y1)
x
·
y(x
1)
·
y(nx)
nx y

−

−
−
−
−

(x1)
x(1

y + (x1)(y1)
·
1)
y) + xy(1

−
−
x(ny) + x y(n)

·

·

Taking y = x in (5.7) gives a formula for the variance s2

x = 1
n
1 k
−

2 of x.

xc

k

6
324

Vector Space Rn

Variance Formula

If x isasamplevector,then s2

xc
k
(cid:0)
We also get a convenient formula for the correlation coefﬁcient, r = r(x, y) = xc
·
xc
k k
k
and the fact that s2

x = 1
n
1
−

2 give:

nx2

−

k

(cid:1)

.

2

yc
yck

x = 1
n
1 k
−

xc

k

. Moreover, (5.7)

Correlation Formula

Ifxandyaresamplevectors,then

r = r(x, y) =

x
·
(n

nx y
−
1)sxsy

y

−

Finally, we give a method that simpliﬁes the computations of variances and correlations.

Data Scaling

x1 x2

andy=
Letx=
c,and d,considernewsamplesz=
andw=
(cid:3)
(cid:3)
zi = a + bxi,foreach i and wi = c + dyi foreach i. Then:

y1 y2
z2
z1
(cid:2)

yn
zn

· · ·

· · ·

· · ·

xn

(cid:2)

(cid:2)

(cid:3)

w1 w2

· · ·

wn

where

(cid:2)

(cid:3)

besamplevectors. Givenconstants a, b,

a. z = a + bx

b. s2

z = b2s2

x,so sz =

sx

b

|

|

c. If b and d havethesamesign,then r(x, y) = r(z, w).

The veriﬁcation is left as an exercise. For example, if x =
100 yields z =
x = 100

1
2 3
1
3 = 99.67, and s2

1 0
3
−
3 = 4.67.

−
z = 14

−

(cid:2)

−

(cid:2)

(cid:3)

101 98 103 99 100 97

. A routine calculation shows that z =

, subtracting
z = 14
3 , so

1
3 and s2
(cid:3)

−

Exercises for 5.7

Exercise 5.7.1 The following table gives IQ scores for 10 fathers and their eldest sons. Calculate the means, the
variances, and the correlation coefﬁcient r. (The data scaling formula is useful.)

1

8
3
Father’s IQ 140 131 120 115 110 106 100 95
Son’s IQ

10
86
109 120 105 99 100 94

130 138 110

9
91

99

7

2

6

5

4

Exercise 5.7.2 The following table gives the number of years of education and the annual income (in thousands)
of 10 individuals. Find the means, the variances, and the correlation coefﬁcient. (Again the data scaling formula is
useful.)

2

1

Individual
10
3
Years of education 12 16 13 18 19 12 18 19 12 14
Yearly income
31 48 35 28 55 40 39 60 32 35
(1000’s)

4

8

5

7

6

9

5.7. An Application to Correlation and Variance

325

Exercise 5.7.3 If x is a sample vector, and xc is the centred sample, show that xc = 0 and the standard deviation of
xc is sx.

Exercise 5.7.4 Prove the data scaling formulas found on page 324: (a), (b), and (c).

Supplementary Exercises for Chapter 5
Exercise 5.1
In each case either show that the state-
ment is true or give an example showing that it is false.
Throughout, x, y, z, x1, x2, . . . , xn denote vectors in Rn.

j. If ax + by + cz = 0 where a, b, and c are in R, then

x, y, z
}
{

is independent.

a. If U is a subspace of Rn and x + y is in U , then x

and y are both in U .

b. If U is a subspace of Rn and rx is in U , then x is

in U .

c. If U is a nonempty set and sx + ty is in U for any
s and t whenever x and y are in U , then U is a
subspace.

d. If U is a subspace of Rn and x is in U , then

in U .

x is

−

e. If

x, y
{
dependent.

}

is independent, then

x, y, x + y
}
{

is in-

f. If

x, y, z
}
{
pendent.

is independent, then

x, y
}
{

is inde-

g. If

x, y
}
{
independent.

is not independent, then

x, y, z
}
{

is not

k. If

x, y, z
}
{

for some a, b, and c in R.

is independent, then ax + by + cz = 0

l. If

x1, x2, . . . , xn
{

t1x1 + t2x2 +

is not independent, then
+ tnxn = 0 for ti in R not all zero.

}

· · ·

m. If

x1, x2, . . . , xn
{

t1x1 + t2x2 +

is independent, then
+ tnxn = 0 for some ti in R.

}

· · ·

n. Every set of four non-zero vectors in R4 is a basis.

o. No basis of R3 can contain a vector with a compo-

nent 0.

p. R3 has a basis of the form

and y are vectors.

x, x + y, y
{

}

where x

q. Every basis of R5 contains one column of I5.

r. Every nonempty subset of a basis of R3 is again a

. . . , xn are nonzero,

then

basis of R3.

h. If all of x1, x2,
x1, x2, . . . , xn
{

}

is independent.

i. If one of x1, x2,
x1, x2, . . . , xn
{

}

. . . , xn
is not independent.

is zero,

then

s. If

x1, x2, x3, x4
{

of R4, then
also a basis of R4.

}

and

y1, y2, y3, y4}
{

x1 + y1, x2 + y2, x3 + y3, x4 + y4}
{

are bases
is

Chapter 6

Vector Spaces

In this chapter we introduce vector spaces in full generality. The reader will notice some similarity with
the discussion of the space Rn in Chapter 5. In fact much of the present material has been developed in
that context, and there is some repetition. However, Chapter 6 deals with the notion of an abstract vector
space, a concept that will be new to most readers. It turns out that there are many systems in which a
natural addition and scalar multiplication are deﬁned and satisfy the usual rules familiar from Rn. The
study of abstract vector spaces is a way to deal with all these examples simultaneously. The new aspect is
that we are dealing with an abstract system in which all we know about the vectors is that they are objects
that can be added and multiplied by a scalar and satisfy rules familiar from Rn.

The novel thing is the abstraction. Getting used to this new conceptual level is facilitated by the work
done in Chapter 5: First, the vector manipulations are familiar, giving the reader more time to become
accustomed to the abstract setting; and, second, the mental images developed in the concrete setting of Rn
serve as an aid to doing many of the exercises in Chapter 6.

The concept of a vector space was ﬁrst introduced in 1844 by the German mathematician Hermann
Grassmann (1809-1877), but his work did not receive the attention it deserved. It was not until 1888 that
the Italian mathematician Guiseppe Peano (1858-1932) clariﬁed Grassmann’s work in his book Calcolo
Geometrico and gave the vector space axioms in their present form. Vector spaces became established with
the work of the Polish mathematician Stephan Banach (1892-1945), and the idea was ﬁnally accepted in
1918 when Hermann Weyl (1885-1955) used it in his widely read book Raum-Zeit-Materie (“Space-Time-
Matter”), an introduction to the general theory of relativity.

6.1

Examples and Basic Properties

Many mathematical entities have the property that they can be added and multiplied by a number. Numbers
n as is any
themselves have this property, as do m
scalar multiple of such a matrix. Polynomials are another familiar example, as are the geometric vectors
in Chapter 4. It turns out that there are many other types of mathematical objects that can be added and
multiplied by a scalar, and the general study of such systems is introduced in this chapter. Remarkably,
much of what we could say in Chapter 5 about the dimension of subspaces in Rn can be formulated in this
generality.

n matrices: The sum of two such matrices is again m

×

×

327

328

Vector Spaces

Deﬁnition 6.1 Vector Spaces

AvectorspaceconsistsofanonemptysetV ofobjects(calledvectors)thatcanbeadded,thatcan
bemultipliedbyarealnumber(calledascalarinthiscontext),andforwhichcertainaxioms
hold.1IfvandwaretwovectorsinV,theirsumisexpressedasv+ w,andthescalarproductofv
byarealnumber a isdenotedas av. Theseoperationsarecalledvectoradditionandscalar
multiplication,respectively,andthefollowingaxiomsareassumedtohold.

Axioms for vector addition

A1. If u and v are in V , then u + v is in V .

A2. u + v = v + u for all u and v in V .

A3. u + (v + w) = (u + v) + w for all u, v, and w in V .

A4. An element 0 in V exists such that v + 0 = v = 0 + v for every v in V .

A5. For each v in V , an element

v in V exists such that

−

v + v = 0 and v + (

−

v) = 0.

−

Axioms for scalar multiplication

S1. If v is in V , then av is in V for all a in R.

S2. a(v + w) = av + aw for all v and w in V and all a in R.

S3. (a + b)v = av + bv for all v in V and all a and b in R.

S4. a(bv) = (ab)v for all v in V and all a and b in R.

S5. 1v = v for all v in V .

The content of axioms A1 and S1 is described by saying that V is closed under vector addition and scalar
v in axiom A5 is
multiplication. The element 0 in axiom A4 is called the zero vector, and the vector
called the negative of v.

−

The rules of matrix arithmetic, when applied to Rn, give

Example 6.1.1

Rn is a vector space using matrix addition and scalar multiplication.2

It is important to realize that, in a general vector space, the vectors need not be n-tuples as in Rn. They
can be any kind of objects at all as long as the addition and scalar multiplication are deﬁned and the axioms
are satisﬁed. The following examples illustrate the diversity of the concept.

The space Rn consists of special types of matrices. More generally, let Mmn denote the set of all m

matrices with real entries. Then Theorem 2.1.1 gives:

n

×

1The scalars will usually be real numbers, but they could be complex numbers, or elements of an algebraic system called a

ﬁeld. Another example is the ﬁeld Q of rational numbers. We will look brieﬂy at ﬁnite ﬁelds in Section 8.8.

2We will usually write the vectors in Rn as n-tuples. However, if it is convenient, we will sometimes denote them as rows

or columns.

Example 6.1.2

6.1. Examples and Basic Properties

329

The set Mmn of all m
n matrices is a vector space using matrix addition and scalar multiplication.
The zero element in this vector space is the zero matrix of size m
n, and the vector space negative
of a matrix (required by axiom A5) is the usual matrix negative discussed in Section 2.1. Note that
Mmn is just Rmn in different notation.

×

×

In Chapter 5 we identiﬁed many important subspaces of Rn such as im A and null A for a matrix A. These
are all vector spaces.

Example 6.1.3
Show that every subspace of Rn is a vector space in its own right using the addition and scalar
multiplication of Rn.

Solution. Axioms A1 and S1 are two of the deﬁning conditions for a subspace U of Rn (see
Section 5.1). The other eight axioms for a vector space are inherited from Rn. For example, if x
and y are in U and a is a scalar, then a(x + y) = ax + ay because x and y are in Rn. This shows that
axiom S2 holds for U ; similarly, the other axioms also hold for U .

Example 6.1.4

Let V denote the set of all ordered pairs (x, y) and deﬁne addition in V as in R2. However, deﬁne a
new scalar multiplication in V by

Determine if V is a vector space with these operations.

a(x, y) = (ay, ax)

Solution. Axioms A1 to A5 are valid for V because they hold for matrices. Also a(x, y) = (ay, ax)
is again in V , so axiom S1 holds. To verify axiom S2, let v = (x, y) and w = (x1, y1) be typical
elements in V and compute

a(v + w) = a(x + x1, y + y1) = (a(y + y1), a(x + x1))
av + aw = (ay, ax) + (ay1, ax1) = (ay + ay1, ax + ax1)

Because these are equal, axiom S2 holds. Similarly, the reader can verify that axiom S3 holds.
However, axiom S4 fails because

a(b(x, y)) = a(by, bx) = (abx, aby)

need not equal ab(x, y) = (aby, abx). Hence, V is not a vector space. (In fact, axiom S5 also fails.)

Sets of polynomials provide another important source of examples of vector spaces, so we review some

basic facts. A polynomial in an indeterminate x is an expression

p(x) = a0 + a1x + a2x2 +

+ anxn

· · ·

330

Vector Spaces

where a0, a1, a2, . . . , an are real numbers called the coefﬁcients of the polynomial. If all the coefﬁcients
are zero, the polynomial is called the zero polynomial and is denoted simply as 0.
= 0, the
highest power of x with a nonzero coefﬁcient is called the degree of p(x) denoted as deg p(x). The
coefﬁcient itself is called the leading coefﬁcient of p(x). Hence deg (3 + 5x) = 1, deg (1 + x + x2) = 2,
and deg (4) = 0. (The degree of the zero polynomial is not deﬁned.)

If p(x)

Let P denote the set of all polynomials and suppose that

p(x) = a0 + a1x + a2x2 +
q(x) = b0 + b1x + b2x2 +

· · ·

· · ·

are two polynomials in P (possibly of different degrees). Then p(x) and q(x) are called equal [written
p(x) = q(x)] if and only if all the corresponding coefﬁcients are equal—that is, a0 = b0, a1 = b1, a2 = b2,
and so on. In particular, a0 + a1x + a2x2 +
= 0 means that a0 = 0, a1 = 0, a2 = 0, . . . , and this is the
reason for calling x an indeterminate. The set P has an addition and scalar multiplication deﬁned on it as
follows: if p(x) and q(x) are as before and a is a real number,

· · ·

p(x) + q(x) = (a0 + b0) + (a1 + b1)x + (a2 + b2)x2 +

ap(x) = aa0 + (aa1)x + (aa2)x2 +

· · ·

· · ·

Evidently, these are again polynomials, so P is closed under these operations, called pointwise addition
and scalar multiplication. The other vector space axioms are easily veriﬁed, and we have

Example 6.1.5

The set P of all polynomials is a vector space with the foregoing addition and scalar multiplication.
The zero vector is the zero polynomial, and the negative of a polynomial
p(x) = a0 + a1x + a2x2 + . . . is the polynomial
a2x2
negating all the coefﬁcients.

. . . obtained by

p(x) =

a1x

a0

−

−

−

−

−

There is another vector space of polynomials that will be referred to later.

Example 6.1.6

Given n
≥
polynomial. That is

1, let Pn denote the set of all polynomials of degree at most n, together with the zero

Pn =

{

a0 + a1x + a2x2 +

+ anxn

|

· · ·

a0, a1, a2, . . . , an in R

.

}

Then Pn is a vector space. Indeed, sums and scalar multiples of polynomials in Pn are again in Pn,
and the other vector space axioms are inherited from P. In particular, the zero vector and the
negative of a polynomial in Pn are the same as those in P.

x

If a and b are real numbers and a < b, the interval [a, b] is deﬁned to be the set of all real numbers
b. A (real-valued) function f on [a, b] is a rule that associates to every number x in
x such that a
[a, b] a real number denoted f (x). The rule is frequently speciﬁed by giving a formula for f (x) in terms of
x. For example, f (x) = 2x, f (x) = sin x, and f (x) = x2 + 1 are familiar functions. In fact, every polynomial
p(x) can be regarded as the formula for a function p.

≤

≤

6
y

1

O

y = x2 = f (x)

y = f (x) + g(x)

= x2

x

−

x

1

y =

−

x = g(x)

6.1. Examples and Basic Properties

331

The set of all functions on [a, b] is denoted F[a, b]. Two functions
f and g in F[a, b] are equal if f (x) = g(x) for every x in [a, b], and we
describe this by saying that f and g have the same action. Note that two
polynomials are equal in P (deﬁned prior to Example 6.1.5) if and only if
they are equal as functions.

If f and g are two functions in F[a, b], and if r is a real number, deﬁne

the sum f + g and the scalar product r f by

( f + g)(x) = f (x) + g(x)

(r f )(x) = r f (x)

for each x in [a, b]
for each x in [a, b]

In other words, the action of f + g upon x is to associate x with the number f (x) + g(x), and r f
associates x with r f (x). The sum of f (x) = x2 and g(x) =
x is shown in the diagram. These operations
on F[a, b] are called pointwise addition and scalar multiplication of functions and they are the usual
operations familiar from elementary algebra and calculus.

−

Example 6.1.7

The set F[a, b] of all functions on the interval [a, b] is a vector space using pointwise addition and
scalar multiplication. The zero function (in axiom A4), denoted 0, is the constant function deﬁned
by

0(x) = 0

for each x in [a, b]

The negative of a function f is denoted

f and has action deﬁned by

−
f )(x) =

−

(

−

f (x)

for each x in [a, b]

Axioms A1 and S1 are clearly satisﬁed because, if f and g are functions on [a, b], then f + g and
r f are again such functions. The veriﬁcation of the remaining axioms is left as Exercise 6.1.14.

Other examples of vector spaces will appear later, but these are sufﬁciently varied to indicate the scope
of the concept and to illustrate the properties of vector spaces to be discussed. With such a variety of
examples, it may come as a surprise that a well-developed theory of vector spaces exists. That is, many
properties can be shown to hold for all vector spaces and hence hold in every example. Such properties
are called theorems and can be deduced from the axioms. Here is an important example.

Theorem 6.1.1: Cancellation
Letu,v,andwbevectorsinavectorspaceV. Ifv+ u= v+ w,thenu= w.

Proof. We are given v + u = v + w. If these were numbers instead of vectors, we would simply subtract v
from both sides of the equation to obtain u = w. This can be accomplished with vectors by adding
v to
both sides of the equation. The steps (using only the axioms) are as follows:

−

v + u = v + w

v + (v + u) =
−
v + v) + u = (
−

−
(
−

v + (v + w)
v + v) + w

(axiom A5)
(axiom A3)

332

Vector Spaces

This is the desired conclusion.3

0 + u = 0 + w
u = w

(axiom A5)
(axiom A4)

As with many good mathematical theorems, the technique of the proof of Theorem 6.1.1 is at least as
important as the theorem itself. The idea was to mimic the well-known process of numerical subtraction
v
in a vector space V as follows: To subtract a vector v from both sides of a vector equation, we added
−
to both sides. With this in mind, we deﬁne difference u

v of two vectors in V as

−
v = u + (

v)

−

u

−

We shall say that this vector is the result of having subtracted v from u and, as in arithmetic, this operation
has the property given in Theorem 6.1.2.

Theorem 6.1.2

IfuandvarevectorsinavectorspaceV,theequation

x+ v= u

hasoneandonlyonesolutionxinV givenby

x= u

v

−

Proof. The difference x = u

−
x + v = (u

v is indeed a solution to the equation because (using several axioms)

v) + v = [u + (

v)] + v = u + (

−

−

v + v) = u + 0 = u

−

To see that this is the only solution, suppose x1 is another solution so that x1 + v = u. Then x + v = x1 + v
(they both equal u), so x = x1 by cancellation.

Similarly, cancellation shows that there is only one zero vector in any vector space and only one
negative of each vector (Exercises 6.1.10 and 6.1.11). Hence we speak of the zero vector and the negative
of a vector.

The next theorem derives some basic properties of scalar multiplication that hold in every vector space,

and will be used extensively.

Theorem 6.1.3

LetvdenoteavectorinavectorspaceV andlet a denotearealnumber.

1. 0v= 0.

2. a0= 0.

3. If av= 0,theneither a = 0 orv= 0.

4. (

1)v=

−

v.

−

3Observe that none of the scalar multiplication axioms are needed here.

6.1. Examples and Basic Properties

333

5. (

a)v=

−

(av) = a(

−

v).

−

Proof.

1. Observe that 0v + 0v = (0 + 0)v = 0v = 0v + 0 where the ﬁrst equality is by axiom S3. It follows

that 0v = 0 by cancellation.

2. The proof is similar to that of (1), and is left as Exercise 6.1.12(a).

3. Assume that av = 0. If a = 0, there is nothing to prove; if a

= 0 means we can scalar-multiply the equation av = 0 by the scalar 1

= 0, we must show that v = 0. But
a. The result (using (2) and

a
Axioms S5 and S4) is

v = 1v =

1
a a

v = 1

a(av) = 1

a 0 = 0

4. We have

v + v = 0 by axiom A5. On the other hand,

(cid:0)

(cid:1)

−

(

1)v + v = (

−

−

1)v + 1v = (

1 + 1)v = 0v = 0

−
1)v + v =

v + v (because both are equal to 0), so

−

using (1) and axioms S5 and S3. Hence (
(

v by cancellation.

1)v =

−

−

−

5. The proof is left as Exercise 6.1.12.4

The properties in Theorem 6.1.3 are familiar for matrices; the point here is that they hold in every vector
space. It is hard to exaggerate the importance of this observation.

Axiom A3 ensures that the sum u + (v + w) = (u + v) + w is the same however it is formed, and we
write it simply as u + v + w. Similarly, there are different ways to form any sum v1 + v2 +
+ vn, and
Axiom A3 guarantees that they are all equal. Moreover, Axiom A2 shows that the order in which the
vectors are written does not matter (for example: u + v + w + z = z + u + w + v).

· · ·

Similarly, Axioms S2 and S3 extend. For example

a(u + v + w) = a [u + (v + w)] = au + a(v + w) = au + av + aw

for all a, u, v, and w. Similarly (a + b + c)v = av + bv + cv hold for all values of a, b, c, and v (verify).
More generally,

a(v1 + v2 +
(a1 + a2 +

· · ·

+ vn) = av1 + av2 +
+ an)v = a1v + a2v +

· · ·

+ avn
+ anv

· · ·
· · ·

≥

1, all numbers a, a1, . . . , an, and all vectors, v, v1, . . . , vn. The veriﬁcations are by induc-
hold for all n
tion and are left to the reader (Exercise 6.1.13). These facts—together with the axioms, Theorem 6.1.3,
and the deﬁnition of subtraction—enable us to simplify expressions involving sums of scalar multiples of
vectors by collecting like terms, expanding, and taking out common factors. This has been discussed for
the vector space of matrices in Section 2.1 (and for geometric vectors in Section 4.1); the manipulations
in an arbitrary vector space are carried out in the same way. Here is an illustration.

6
6
334

Vector Spaces

Example 6.1.8

If u, v, and w are vectors in a vector space V , simplify the expression

2(u + 3w)

3(2w

v)

−

−

−

3[2(2u + v

4w)

4(u

−

−

−

2w)]

Solution. The reduction proceeds as though u, v, and w were matrices or variables.

3[2(2u + v
3[4u + 2v

−
8w

4w)

4(u
−
−
4u + 8w]

2w)]

−

−

−
−

−

3(2w
v)
6w + 3v
3[2v]
6v

2(u + 3w)
−
= 2u + 6w
−
= 2u + 3v
−
= 2u + 3v
−
3v
= 2u

−

Condition (2) in Theorem 6.1.3 points to another example of a vector space.

Example 6.1.9

A set

0

}

{

with one element becomes a vector space if we deﬁne

0 + 0 = 0

and

a0 = 0 for all scalars a.

The resulting space is called the zero vector space and is denoted

0

.

}

{

The vector space axioms are easily veriﬁed for
zero subspace (consisting of the zero vector of V alone) is a copy of the zero vector space.

. In any vector space V , Theorem 6.1.3 shows that the

0
}

{

Exercises for 6.1

Exercise 6.1.1 Let V denote the set of ordered triples
(x, y, z) and deﬁne addition in V as in R3. For each of
the following deﬁnitions of scalar multiplication, decide
whether V is a vector space.

a. a(x, y, z) = (ax, y, az)

b. a(x, y, z) = (ax, 0, az)

c. a(x, y, z) = (0, 0, 0)

d. a(x, y, z) = (2ax, 2ay, 2az)

Exercise 6.1.2 Are the following sets vector spaces with
the indicated operations? If not, why not?

a. The set V of nonnegative real numbers; ordinary

addition and scalar multiplication.

b. The set V of all polynomials of degree

together with 0; operations of P.

3,

≥

c. The set of all polynomials of degree

tions of P.

3; opera-

≤

d. The set

1, x, x2, . . .
; operations of P.
}
{
e. The set V of all 2

2 matrices of the form

×

; operations of M22.

a b
0 c

(cid:21)

(cid:20)

f. The set V of 2

2 matrices with equal column

sums; operations of M22.

×

g. The set V of 2

2 matrices with zero determinant;

usual matrix operations.

×

h. The set V of real numbers; usual operations.

6.1. Examples and Basic Properties

335

Exercise 6.1.6
au + bv + cw = 0 in V implies that a = b = c = 0.

In each case show that the condition

a. V = R4; u = (2, 1, 0, 2), v = (1, 1,

w = (0, 1, 2, 1)

1, 0),

−

i. The set V of complex numbers; usual addition and

b. V = M22; u =

multiplication by a real number.

w =

1
1

1
1

1 0
0 1

, v =

(cid:21)

(cid:20)

0 1
1 0

,
(cid:21)

(cid:20)

j. The set V of all ordered pairs (x, y) with the
addition of R2, but using scalar multiplication
a(x, y) = (ax,

ay).

−

k. The set V of all ordered pairs (x, y) with the
addition of R2, but using scalar multiplication
a(x, y) = (x, y) for all a in R.

l. The set V of all functions f : R

R with point-
wise addition, but scalar multiplication deﬁned by
(a f )(x) = f (ax).

→

m. The set V of all 2

2 matrices whose entries sum

to 0; operations of M22.

×

n. The set V of all 2

tion of M22 but scalar multiplication
a

X = aX T .

∗

×

2 matrices with the addi-
deﬁned by

∗

Exercise 6.1.3 Let V be the set of positive real numbers
with vector addition being ordinary multiplication, and
v = va. Show that V is a
scalar multiplication being a
vector space.

·

Exercise 6.1.4 If V is the set of ordered pairs (x, y) of
real numbers, show that it is a vector space with addition
(x, y) + (x1, y1) = (x + x1, y + y1 + 1) and scalar mul-
tiplication a(x, y) = (ax, ay + a
1). What is the zero
vector in V ?

−

Exercise 6.1.5 Find x and y (in terms of u and v) such
that:

a.

2x + y = u
5x + 3y = v

b.

3x
4x

−
−

2y = u
5y = v

(cid:20)

(cid:21)
c. V = P; u = x3 + x, v = x2 + 1, w = x3

−

x2 + x + 1

−

d. V = F[0, π]; u = sin x, v = cos x, w = 1—the con-

stant function

Exercise 6.1.7 Simplify each of the following.

a. 3[2(u

2v

w) + 3(w

b. 4(3u

−
+ 6(w

−

−
v + w)
−
v)
u

−

−

v)]

−

7(u

3(v

−

−

−

−

w)

3v

−

w)]

2[(3u

2v)

−

Exercise 6.1.8 Show that x = v is the only solution to
the equation x + x = 2v in a vector space V . Cite all ax-
ioms used.

Exercise 6.1.9 Show that
Cite all axioms used.

−

0 = 0 in any vector space.

Exercise 6.1.10 Show that the zero vector 0 is uniquely
determined by the property in axiom A4.

Exercise 6.1.11 Given a vector v, show that its negative
v is uniquely determined by the property in axiom A5.

−
Exercise 6.1.12

a. Prove (2) of Theorem 6.1.3. [Hint: Axiom S2.]

b. Prove that (

a)v =

−

(av) in Theorem 6.1.3 by
a)v + av. Then do it using (4)

−

ﬁrst computing (
of Theorem 6.1.3 and axiom S4.

−

c. Prove that a(

v) =

−
two ways, as in part (b).

−

(av) in Theorem 6.1.3 in

Exercise 6.1.13 Let v, v1, . . . , vn denote vectors in a
vector space V and let a, a1, . . . , an denote numbers.
Use induction on n to prove each of the following.

a. a(v1 + v2 +

· · ·

+ vn) = av1 + av2 +

b. (a1 + a2 +

· · ·

+ an)v = a1v + a2v +

+ avn

+ anv

· · ·

· · ·

336

Vector Spaces

Exercise 6.1.14 Verify axioms A2—A5 and S2—S5 for
the space F[a, b] of functions on [a, b] (Example 6.1.7).

Exercise 6.1.15 Prove each of the following for vectors
u and v and scalars a and b.

a. If av = 0, then a = 0 or v = 0.

b. If av = bv and v

= 0, then a = b.

c. If av = aw and a

= 0, then v = w.

Exercise 6.1.16 By calculating (1 + 1)(v + w) in two
ways (using axioms S2 and S3), show that axiom A2 fol-
lows from the other axioms.
Exercise 6.1.17 Let V be a vector space, and deﬁne V n
to be the set of all n-tuples (v1, v2, . . . , vn) of n vec-
tors vi, each belonging to V . Deﬁne addition and scalar
multiplication in V n as follows:

(u1, u2, . . . , un) + (v1, v2, . . . , vn)
= (u1 + v1, u2 + v2, . . . , un + vn)
a(v1, v2, . . . , vn) = (av1, av2, . . . , avn)

Show that V n is a vector space.

Exercise 6.1.18 Let V n be the vector space of n-tuples
If A
from the preceding exercise, written as columns.

6.2 Subspaces and Spanning Sets

n matrix, and X is in V n, deﬁne AX in V m by

is an m
matrix multiplication. More precisely, if

×

v1
...
vn

A = [ai j] and X = 

,



let AX = 

where ui = ai1v1 + ai2v2 +
Prove that:

· · ·






+ ainvn for each i.




u1
...
un






a. B(AX ) = (BA)X

b. (A + A1)X = AX + A1X

c. A(X + X1) = AX + AX1

d. (kA)X = k(AX ) = A(kX ) if k is any number

e. IX = X if I is the n

n identity matrix

×

f. Let E be an elementary matrix obtained by per-
forming a row operation on the rows of In (see
Section 2.5). Show that EX is the column re-
sulting from performing that same row operation
on the vectors (call them rows) of X .
[Hint:
Lemma 2.5.1.]

Chapter 5 is essentially about the subspaces of Rn. We now extend this notion.

Deﬁnition 6.2 Subspaces of a Vector Space

IfV isavectorspace,anonemptysubsetU
spaceusingtheadditionandscalarmultiplicationofV.

⊆

V iscalledasubspaceofV ifU isitselfavector

Subspaces of Rn (as deﬁned in Section 5.1) are subspaces in the present sense by Example 6.1.3. Moreover,
the deﬁning properties for a subspace of Rn actually characterize subspaces in general.

6
6
6.2. Subspaces and Spanning Sets

337

Theorem 6.2.1: Subspace Test

AsubsetU ofavectorspaceisasubspaceofV ifandonlyifitsatisﬁesthefollowingthree
conditions:

1. 0liesinU where0isthezerovectorofV.

2. Ifu1 andu2 areinU,thenu1 + u2 isalsoinU.

3. IfuisinU,then auisalsoinU foreachscalar a.

Proof. If U is a subspace of V , then (2) and (3) hold by axioms A1 and S1 respectively, applied to the
vector space U . Since U is nonempty (it is a vector space), choose u in U . Then (1) holds because 0 = 0u
is in U by (3) and Theorem 6.1.3.

Conversely, if (1), (2), and (3) hold, then axioms A1 and S1 hold because of (2) and (3), and axioms
A2, A3, S2, S3, S4, and S5 hold in U because they hold in V . Axiom A4 holds because the zero vector 0
of V is actually in U by (1), and so serves as the zero of U . Finally, given u in U , then its negative
u in V
u serves as the negative
is again in U by (3) because
of u in U .

1)u (again using Theorem 6.1.3). Hence

u = (

−

−

−

−

Note that the proof of Theorem 6.2.1 shows that if U is a subspace of V , then U and V share the same zero
vector, and that the negative of a vector in the space U is the same as its negative in V .

Example 6.2.1

If V is any vector space, show that

and V are subspaces of V .

0
}

{

Solution. U = V clearly satisﬁes the conditions of the subspace test. As to U =
conditions because 0 + 0 = 0 and a0 = 0 for all a in R.

0
}

{

, it satisﬁes the

The vector space

0
}

{

Example 6.2.2

is called the zero subspace of V .

Let v be a vector in a vector space V . Show that the set

of all scalar multiples of v is a subspace of V .

Rv =

av

{

|

a in R

}

Solution. Because 0 = 0v, it is clear that 0 lies in Rv. Given two vectors av and a1v in Rv, their
sum av + a1v = (a + a1)v is also a scalar multiple of v and so lies in Rv. Hence Rv is closed under
addition. Finally, given av, r(av) = (ra)v lies in Rv for all r
multiplication. Hence the subspace test applies.

R, so Rv is closed under scalar

∈

In particular, given d

= 0 in R3, Rd is the line through the origin with direction vector d.
The space Rv in Example 6.2.2 is described by giving the form of each vector in Rv. The next example

describes a subset U of the space Mnn by giving a condition that each matrix of U must satisfy.

6
338

Vector Spaces

Example 6.2.3

Let A be a ﬁxed matrix in Mnn. Show that U =

X in Mnn

{

|

AX = X A

}

is a subspace of Mnn.

Solution. If 0 is the n
U . Next suppose that X and X1 lie in U so that AX = X A and AX1 = X1A. Then

n zero matrix, then A0 = 0A, so 0 satisﬁes the condition for membership in

×

A(X + X1) = AX + AX1 = X A + X1A + (X + X1)A

A(aX ) = a(AX ) = a(X A) = (aX )A

for all a in R, so both X + X1 and aX lie in U . Hence U is a subspace of Mnn.

Suppose p(x) is a polynomial and a is a number. Then the number p(a) obtained by replacing x by a
6x + 2x2, then
12 + 8 = 1. If p(a) = 0, the number a is called a root of p(x).

in the expression for p(x) is called the evaluation of p(x) at a. For example, if p(x) = 5
the evaluation of p(x) at a = 2 is p(2) = 5

−

−

Example 6.2.4

Consider the set U of all polynomials in P that have 3 as a root:

Show that U is a subspace of P.

U =

p(x)

P

|

∈

{

p(3) = 0

}

Solution. Clearly, the zero polynomial lies in U . Now let p(x) and q(x) lie in U so p(3) = 0 and
q(3) = 0. We have (p + q)(x) = p(x) + q(x) for all x, so (p + q)(3) = p(3) + q(3) = 0 + 0 = 0, and
U is closed under addition. The veriﬁcation that U is closed under scalar multiplication is similar.

Recall that the space Pn consists of all polynomials of the form

a0 + a1x + a2x2 +

+ anxn

· · ·

where a0, a1, a2, . . . , an are real numbers, and so is closed under the addition and scalar multiplication in
P. Moreover, the zero polynomial is included in Pn. Thus the subspace test gives Example 6.2.5.

Example 6.2.5

Pn is a subspace of P for each n

0.

≥

The next example involves the notion of the derivative f ′ of a function f . (If the reader is not fa-
miliar with calculus, this example may be omitted.) A function f deﬁned on the interval [a, b] is called
differentiable if the derivative f ′(r) exists at every r in [a, b].

Example 6.2.6

Show that the subset D[a, b] of all differentiable functions on [a, b] is a subspace of the vector
space F[a, b] of all functions on [a, b].

6.2. Subspaces and Spanning Sets

339

Solution. The derivative of any constant function is the constant function 0; in particular, 0 itself is
differentiable and so lies in D[a, b]. If f and g both lie in D[a, b] (so that f ′ and g′ exist), then it is
a theorem of calculus that f + g and r f are both differentiable for any r
( f + g)′ = f ′ + g′ and (r f )′ = r f ′, so both lie in D[a, b]. This shows that D[a, b] is a subspace of
F[a, b].

R. In fact,

∈

Linear Combinations and Spanning Sets

Deﬁnition 6.3 Linear Combinations and Spanning

v1, v2, . . . , vn
{

Let
combinationofthevectorsv1, v2, . . . , vn ifitcanbeexpressedintheform

beasetofvectorsinavectorspaceV. Asin Rn,avectorviscalledalinear

}

where a1, a2, . . . , an arescalars,calledthecoefﬁcientsofv1, v2, . . . , vn. Thesetofalllinear
combinationsofthesevectorsiscalledtheirspan,andisdenotedby

v= a1v1 + a2v2 +

+ anvn

· · ·

span

v1, v2, . . . , vn
{

}

=

{

a1v1 + a2v2 +

+ anvn

ai in R

}

|

· · ·

If it happens that V = span
the span of two vectors v and w is the set

v1, v2, . . . , vn

{

, these vectors are called a spanning set for V . For example,

}

of all sums of scalar multiples of these vectors.

span

v, w

=

{

}

{

sv + tw

|

s and t in R

}

Example 6.2.7

Consider the vectors p1 = 1 + x + 4x2 and p2 = 1 + 5x + x2 in P2. Determine whether p1 and p2 lie
in span

x2, 3 + 5x + 2x2

1 + 2x

.

{

−

}

Solution. For p1, we want to determine if s and t exist such that

Equating coefﬁcients of powers of x (where x0 = 1) gives

p1 = s(1 + 2x

−

x2) + t(3 + 5x + 2x2)

1 = s + 3t,

1 = 2s + 5t,

and

4 =

s + 2t

−

These equations have the solution s =
span
.
}
Turning to p2 = 1 + 5x + x2, we are looking for s and t such that

x2, 3 + 5x + 2x2

2 and t = 1, so p1 is indeed in

1 + 2x

−

−

{

p2 = s(1 + 2x

x2) + t(3 + 5x + 2x2)

−
Again equating coefﬁcients of powers of x gives equations 1 = s + 3t, 5 = 2s + 5t, and 1 =
But in this case there is no solution, so p2 is not in span

x2, 3 + 5x + 2x2

1 + 2x

.

s + 2t.

−

{

−

}

340

Vector Spaces

We saw in Example 5.1.6 that Rm = span

e1, e2, . . . , em

columns of the m
an analogous spanning set for each space Mmn. For example, each 2

m identity matrix. Of course Rm = Mm1 is the set of all m

×

×
2 matrix has the form

{

where the vectors e1, e2, . . . , em are the
1 matrices, and there is

}

a b
c d

(cid:20)

= a

(cid:21)

(cid:20)

1 0
0 0

+ b

(cid:21)

(cid:20)

M22 = span

1 0
0 0

,

(cid:21)

(cid:20)

(cid:26)(cid:20)

0 1
0 0

0 1
0 0

so

Similarly, we obtain

Example 6.2.8

×

(cid:21)

,

0 0
1 0

+ c

(cid:21)

(cid:20)

+ d

(cid:20)

0 0
0 1

(cid:21)

0 0
1 0

,

(cid:21)

(cid:20)

0 0
0 1

(cid:21)(cid:27)

(cid:21)

(cid:20)

Mmn is the span of the set of all m
entries zero.

×

n matrices with exactly one entry equal to 1, and all other

The fact that every polynomial in Pn has the form a0 + a1x + a2x2 +

shows that

Example 6.2.9

Pn = span

{

1, x, x2, . . . , xn

.

}

+ anxn where each ai is in R

· · ·

= Rv is a subspace for any vector v in a vector
a in R
In Example 6.2.2 we saw that span
space V . More generally, the span of any set of vectors is a subspace. In fact, the proof of Theorem 5.1.1
goes through to prove:

av

=

}

}

{

v

{

|

Theorem 6.2.2

LetU = span

v1, v2, . . . , vn
{

}

inavectorspaceV. Then:

1. U isasubspaceofV containingeachofv1, v2, . . . , vn.

2. U isthe“smallest”subspacecontainingthesevectorsinthesensethatanysubspacethat

containseachofv1, v2, . . . , vn mustcontainU.

Here is how condition 2 in Theorem 6.2.2 is used. Given vectors v1, . . . , vk in a vector space V and a

subspace U

V , then:

⊆

{
The following examples illustrate this.

span

v1, . . . , vn

U

⇔

} ⊆

each vi

U

∈

Example 6.2.10

Show that P3 = span

{

x2 + x3, x, 2x2 + 1, 3

.

}

Solution. Write U = span

x2 + x3, x, 2x2 + 1, 3

{

. Then U

}

⊆

P3, and we use the fact that

6.2. Subspaces and Spanning Sets

341

U . In fact, x and 1 = 1
3 ·

⊆

3 clearly lie in U . But then

and

x3 = (x2 + x3)

x2

−

to show that P3

}
x2 = 1

2[(2x2 + 1)
U by Theorem 6.2.2.

1]

−

P3 = span
{
successively,

1, x, x2, x3

also lie in U . Hence P3

⊆

Example 6.2.11

Let u and v be two vectors in a vector space V . Show that

span

u, v

}

{

= span

{

u + 2v, u

v

}

−

Solution. We have span
u

v lie in span

u, v

u + 2v, u

v

{
} ⊆
. On the other hand,

−

span

u, v

}

{

by Theorem 6.2.2 because both u + 2v and

−

{

}
u = 1

so span

u, v

{

} ⊆

span

{

u + 2v, u

v

}

−

Exercises for 6.2

3(u + 2v) + 2

3 (u

v)

and

v = 1

3(u + 2v)

1
3(u

v)

−

−

−
, again by Theorem 6.2.2.

Exercise 6.2.1 Which of the following are subspaces of
P3? Support your answer.

Exercise 6.2.2 Which of the following are subspaces of
M22? Support your answer.

a. U =

f (x)

f (x)

P3, f (2) = 1
}

∈

|

{

b. U =

xg(x)
{

|

g(x)

P2

}

∈

c. U =

xg(x)
{

|

g(x)

P3

}

∈

d. U =

xg(x) + (1
{

−

x)h(x)

|

g(x) and h(x)

P2

}

∈

e. U = The set of all polynomials in P3 with constant

term 0

a. U =

a b
0 c

(cid:26)(cid:20)

b. U =

c. U =

d. U =

a b
c d

(cid:26)(cid:20)
A
{
A
{

A

|

∈

A

∈

|

e. U =

f. U =

A

A

A
{
A
{

|

|

∈

∈

a, b, and c in R

(cid:27)

a + b = c + d; a, b, c, d in R

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

2 matrix

×

(cid:21)(cid:12)
(cid:12)
(cid:12)
M22, A = AT
(cid:12)

}
M22, AB = 0
, B a ﬁxed 2
}
M22, A2 = A

}

M22, A is not invertible

}

g. U =
2

A

A
{

|
∈
2 matrices

×

M22, BAC = CAB

, B and C ﬁxed
}

f. U =

f (x)

f (x)

P3, deg f (x) = 3
}

∈

|

{

Exercise 6.2.3 Which of the following are subspaces of
F[0, 1]? Support your answer.

a. U =

f

{

|

f (0) = 0
}

342

Vector Spaces

b. U =

c. U =

d. U =

e. U =

f

f

f

f

{

{

{

{

|

|

|

|

f (0) = 1
}

f (0) = f (1)
}

f (x)

0 for all x in [0, 1]
}

≥

f (x) = f (y) for all x and y in [0, 1]
}

f (x + y) = f (x) + f (y) for all

f. U =

f
|
x and y in [0, 1]
}

{

g. U =

f

{

|

f is integrable and

Exercise 6.2.4 Let A be an m
columns b in Rm is U =
x
|
of Rn? Support your answer.

x
{

∈

1
0 f (x)dx = 0
}
R
n matrix. For which
a subspace

×
Rn, Ax = b

}

Exercise 6.2.5 Let x be a vector in Rn (written as a col-
A
umn), and deﬁne U =

Mmn

Ax
{

|

∈

.
}

a. Show that U is a subspace of Rm.

b. Show that U = Rm if x

= 0.

Exercise 6.2.6 Write each of the following as a linear
combination of x + 1, x2 + x, and x2 + 2.

a.

c.

x2 + 3x + 2
x2 + 1

3x + 1

b.

2x2

xd.

−

Exercise 6.2.7 Determine whether v lies in span
in each case.

u, w
{

}

a. v = 3x2

2x

1; u = x2 + 1, w = x + 2

−
b. v = x; u = x2 + 1, w = x + 2

−

c. v =

d. v =

(cid:20)

(cid:20)

1 3
1 1

−

4
3

−
1
5

; u =

; u =

(cid:21)

(cid:21)

1
2

1
2

(cid:20)

(cid:20)

−

−

1
1

1
1

(cid:21)

(cid:21)

, w =

, w =

2 1
1 0

2 1
1 0

(cid:20)

(cid:20)

(cid:21)

(cid:21)

Exercise 6.2.8 Which of the following functions lie in
span

cos2 x, sin2 x
? (Work in F[0, π].)
}
{

a.

c.

cos 2x
x2

Exercise 6.2.9

1b.

d.

1 + x2

a. Show that R3 is spanned by

b. Show that P2 is spanned by

.
(1, 0, 1), (1, 1, 0), (0, 1, 1)
}
{
1 + 2x2, 3x, 1 + x
.
}
{
c. Show that M22 is spanned by
0 1
1 0

1 1
0 1

1 0
0 0

1 0
0 1

,

,

,

(cid:26)(cid:20)

(cid:21)

(cid:20)

(cid:21)

(cid:20)

(cid:21)

(cid:20)

.
(cid:21)(cid:27)

Exercise 6.2.10 If X and Y are two sets of vectors in a
vector space V , and if X
span X

Y , show that

span Y .

⊆

Exercise 6.2.11 Let u, v, and w denote vectors in a vec-
tor space V . Show that:

⊆

a. span

b. span

u, v, w
{
u, v, w
{

}

}

= span

= span

Exercise 6.2.12 Show that

u + v, u + w, v + w
}
{
u
{

v, u + w, w

−

}

span

v1, v2, . . . , vn, 0
}
{
holds for any set of vectors

= span

v1, v2, . . . , vn
{
v1, v2, . . . , vn
{

.
}

}

Exercise 6.2.13
If X and Y are nonempty subsets of
a vector space V such that span X = span Y = V , must
there be a vector common to both X and Y ? Justify your
answer.

Exercise 6.2.14 Is it possible that
can span the subspace U =

(1, 2, 0), (1, 1, 1)
}
{
a and b in R
(a, b, 0)
?
}
{

|

Exercise 6.2.15 Describe span

0
{

.
}

Exercise 6.2.16 Let v denote any vector in a vector
space V . Show that span
= 0.

v
}
{
Exercise 6.2.17 Determine all subspaces of Rv where
v

= 0 in some vector space V .

for any a

= span

av
{

}

Exercise 6.2.18 Suppose V = span
u = a1v1 + a2v2 +
a1

= 0, show that V = span

. If
}
+ anvn where the ai are in R and

v1, v2, . . . , vn
{

· · ·

u, v2, . . . , vn
{

.
}

Exercise 6.2.19 If Mnn = span
that Mnn = span

1 , AT
AT
{

Exercise 6.2.20 If Pn = span
and a is in R, show that pi(a)

A1, A2, . . . , Ak
{
2 , . . . , AT
.
k }
p1(x), p2(x), . . . , pk(x)
}

, show
}

{
= 0 for some i.

Exercise 6.2.21 Let U be a subspace of a vector space
V .

a. If au is in U where a

= 0, show that u is in U .

b. If u and u + v are in U , show that v is in U .

6
6
6
6
6
6
6.3. Linear Independence and Dimension

343

Exercise 6.2.22 Let U be a nonempty subset of a vector
space V . Show that U is a subspace of V if and only if
u1 + au2 lies in U for all u1 and u2 in U and all a in R.

and let

X = 

v1
...
vn

u1
...
un






Y = 











as in Exercise 6.1.18.

p(x) in P

Exercise 6.2.23 Let U =
be the
{
set in Example 6.2.4. Use the factor theorem (see Sec-
tion 6.5) to show that U consists of multiples of x
3;
P
3)q(x)
(x
that is, show that U =
. Use this
}
{
to show that U is a subspace of P.

p(3) = 0
}

q(x)

−

−

∈

|

|

Exercise 6.2.24 Let A1, A2, . . . , Am denote n
ces. If 0
that

Rn and A1y = A2y =

cannot span Mnn.

∈
A1, A2, . . . , Am
{

= y

· · ·

}

n matri-
×
= Amy = 0, show

Exercise 6.2.25
u1, u2, . . . , un
{

}

Let

v2,
and
be sets of vectors in a vector space,

v1,
{

. . . ,

vn

}

a. Show that span

v1, . . . , vn
{
if and only if AY = X for some n

} ⊆

span

u1, . . . , un
{

n matrix A.

}

×

b. If X = AY where A is invertible, show that
= span

span

u1, . . . , un
{

.
}

v1, . . . , vn
{

}

Exercise 6.2.26 If U and W are subspaces of a vector
space V , let U
. Show
}
U .
W or W
that U

v
{
W is a subspace if and only if U

v is in U or v is in W

W =

∪

|

⊆

⊆

∪

Exercise 6.2.27 Show that P cannot be spanned by a
ﬁnite set of polynomials.

6.3 Linear Independence and Dimension

Deﬁnition 6.4 Linear Independence and Dependence

Asin Rn,asetofvectors
simplyindependent) ifitsatisﬁesthefollowingcondition:

v1, v2, . . . , vn
{

}

inavectorspaceV iscalledlinearlyindependent (or

If

s1v1 + s2v2 +

· · ·

+ snvn = 0,

then s1 = s2 =

= sn = 0.

· · ·

Asetofvectorsthatisnotlinearlyindependentissaidtobelinearlydependent (orsimply
dependent).

The trivial linear combination of the vectors v1, v2, . . . , vn is the one with every coefﬁcient zero:

This is obviously one way of expressing 0 as a linear combination of the vectors v1, v2, . . . , vn, and they
are linearly independent when it is the only way.

0v1 + 0v2 +

+ 0vn

· · ·

Example 6.3.1

Show that

{

1 + x, 3x + x2, 2 + x

x2

}

−

is independent in P2.

Solution. Suppose a linear combination of these polynomials vanishes.

s1(1 + x) + s2(3x + x2) + s3(2 + x

x2) = 0

−

6
344

Vector Spaces

Equating the coefﬁcients of 1, x, and x2 gives a set of linear equations.

s1 +
+ 2s3 = 0
s1 + 3s2 + s3 = 0
s3 = 0

s2

−

The only solution is s1 = s2 = s3 = 0.

Example 6.3.2

Show that
interval [0, 2π].

sin x, cos x
}

{

is independent in the vector space F[0, 2π] of functions deﬁned on the

Solution. Suppose that a linear combination of these functions vanishes.

s1(sin x) + s2(cos x) = 0

This must hold for all values of x in [0, 2π] (by the deﬁnition of equality in F[0, 2π]). Taking
x = 0 yields s2 = 0 (because sin 0 = 0 and cos 0 = 1). Similarly, s1 = 0 follows from taking x = π
2
(because sin π

2 = 1 and cos π

2 = 0).

Example 6.3.3

Suppose that
independent.

{

u, v

}

is an independent set in a vector space V . Show that

u + 2v, u

3v

}

−

is also

{

Solution. Suppose a linear combination of u + 2v and u

3v vanishes:

s(u + 2v) + t(u

−

−
3v) = 0

We must deduce that s = t = 0. Collecting terms involving u and v gives

(s + t)u + (2s

3t)v = 0

−

Because
{
solution is s = t = 0.

u, v

}

is independent, this yields linear equations s + t = 0 and 2s

3t = 0. The only

−

Example 6.3.4

Show that any set of polynomials of distinct degrees is independent.

Solution. Let p1, p2, . . . , pm be polynomials where deg (pi) = di. By relabelling if necessary, we
may assume that d1 > d2 >

> dm. Suppose that a linear combination vanishes:

· · ·

t1 p1 + t2 p2 +

+ tm pm = 0

· · ·

where each ti is in R. As deg (p1) = d1, let axd1 be the term in p1 of highest degree, where a

= 0.

6
6.3. Linear Independence and Dimension

345

Since d1 > d2 >
combination t1 p1 + t2 p2 +
· · ·
t1 = 0 (because a
= 0). But then t2 p2 +
t2 = 0. Continuing, we obtain ti = 0 for each i, as desired.

· · ·

· · ·

> dm, it follows that t1axd1 is the only term of degree d1 in the linear

+ tm pm = 0. This means that t1axd1 = 0, whence t1a = 0, hence

+ tm pm = 0 so we can repeat the argument to show that

Example 6.3.5

Suppose that A is an n
I, A, A2, . . . , Ak
B =

{

×
1
−

n matrix such that Ak = 0 but Ak

1
−

= 0. Show that

is independent in Mnn.

}

Solution. Suppose r0I + r1A + r2A2 +

+ rk

1Ak
−

· · ·

1 = 0. Multiply by Ak
−

1:
−

r0Ak

1 + r1Ak + r2Ak+1 +
−

+ rk

1A2k
−

· · ·

2 = 0
−

Since Ak = 0, all the higher powers are zero, so this becomes r0Ak
and we have r1A1 + r2A2 +
+ rk
· · ·
Continuing, we obtain ri = 0 for each i, so B is independent.

1 = 0. But Ak
1
−
−
1 = 0. Now multiply by Ak
2 to conclude that r1 = 0.
−
−

1Ak
−

= 0, so r0 = 0,

The next example collects several useful properties of independence for reference.

Example 6.3.6

Let V denote a vector space.

1. If v

= 0 in V , then

v
}

{

is an independent set.

2. No independent set of vectors in V can contain the zero vector.

Solution.

1. Let tv = 0, t in R. If t

= 0, then v = 1v = 1

t (tv) = 1

t 0 = 0, contrary to assumption. So t = 0.

2. If

v1, v2, . . . , vk

is independent and (say) v2 = 0, then 0v1 + 1v2 +

+ 0vk = 0 is a

nontrivial linear combination that vanishes, contrary to the independence of
v1, v2, . . . , vk

.

· · ·

{

{

}

}

A set of vectors is independent if 0 is a linear combination in a unique way. The following theorem
shows that every linear combination of these vectors has uniquely determined coefﬁcients, and so extends
Theorem 5.2.1.

Theorem 6.3.1

Let

v1, v2, . . . , vn
{

}

bealinearlyindependentsetofvectorsinavectorspaceV. Ifavectorvhas

6
6
6
6
6
346

Vector Spaces

two(ostensiblydifferent)representations

v = s1v1 + s2v2 +
v = t1v1 + t2v2 +

+ snvn
+ tnvn

· · ·
· · ·

aslinearcombinationsofthesevectors,then s1 = t1, s2 = t2, . . . , sn = tn. Inotherwords,every
vectorinV canbewritteninauniquewayasalinearcombinationofthevi.

Proof. Subtracting the equations given in the theorem gives

(s1

−

t1)v1 + (s2

t2)v2 +

+ (sn

−

· · ·

−

tn)vn = 0

The independence of

v1, v2, . . . , vn

gives si

}

−

{

ti = 0 for each i, as required.

The following theorem extends (and proves) Theorem 5.2.4, and is one of the most useful results in

linear algebra.

Theorem 6.3.2: Fundamental Theorem

SupposeavectorspaceV canbespannedby n vectors. Ifanysetof m vectorsinV islinearly
independent,then m

n.

≤

}

{

· · ·

v1, v2, . . . , vn

u1, u2, . . . , um

, and suppose that

+ anvn where each ai is in R. As u1

= 0 (after relabelling the vi). Then V = span

Proof. Let V = span
Then u1 = a1v1 + a2v2 +
zero, say a1
Hence, write u2 = b1u1 + c2v2 + c3v3 +
so, as before, V = span
}
procedure continues until all the vectors vi are replaced by the vectors u1, u2, . . . , un.
V = span
independence of the ui. Hence, the assumption m > n cannot be valid, so m

is an independent set in V .
= 0 (Example 6.3.6), not all of the ai are
as the reader can verify.
u1, u2
is independent;
, again after possible relabelling of the vi. If m > n, this
In particular,
. But then un+1 is a linear combination of u1, u2, . . . , un contrary to the
n and the theorem is proved.

u1, v2, v3, . . . , vn
}
= 0 because
{

+ cnvn. Then some ci

u1, u2, v3, . . . , vn

u1, u2, . . . , un

· · ·

}

{

{

{

}

{

}

≤

If V = span

v1, v2, . . . , vn

is an independent set in V , the above proof
n but also that m of the (spanning) vectors v1, v2, . . . , vn can be replaced by
shows not only that m
the (independent) vectors u1, u2, . . . , um and the resulting set will still span V . In this form the result is
called the Steinitz Exchange Lemma.

u1, u2, . . . , um

, and if

≤

}

{

{

}

6
6
6
6.3. Linear Independence and Dimension

347

Deﬁnition 6.5 Basis of a Vector Space

Asin Rn,aset
e1, e2, . . . , en
{
thefollowingtwoconditions:

}

ofvectorsinavectorspaceViscalledabasisofV ifitsatisﬁes

1.

e1, e2, . . . , en
{

}

islinearlyindependent

2. V = span

e1, e2, . . . , en
{

}

is a basis, then every vector in V can be written as a linear
Thus if a set of vectors
combination of these vectors in a unique way (Theorem 6.3.1). But even more is true: Any two (ﬁnite)
bases of V contain the same number of vectors.

e1, e2, . . . , en

{

}

Theorem 6.3.3: Invariance Theorem

Let

e1, e2, . . . , en
{

}

and

f1, f2, . . . , fm
{

}

betwobasesofavectorspaceV. Then n = m.

Proof. Because V = span
rem 6.3.2 that m

{
n. Similarly n

e1, e2, . . . , en

and

f1, f2, . . . , fm

}

{

m, so n = m, as asserted.

≤

≤

is independent, it follows from Theo-

}

Theorem 6.3.3 guarantees that no matter which basis of V is chosen it contains the same number of

vectors as any other basis. Hence there is no ambiguity about the following deﬁnition.

Deﬁnition 6.6 Dimension of a Vector Space

e1, e2, . . . , en
{

If
calledthedimensionofV,andwewrite

}

isabasisofthenonzerovectorspaceV,thenumber n ofvectorsinthebasisis

dim V = n

Thezerovectorspace

0
}
{

isdeﬁnedtohavedimension 0:

dim

0
}
{

= 0

In our discussion to this point we have always assumed that a basis is nonempty and hence that the di-
has no basis (by Example 6.3.6) so our
mension of the space is at least 1. However, the zero space
insistence that dim
. Thus the
statement that “the dimension of a vector space is the number of vectors in any basis” holds even for the
zero space.

= 0 amounts to saying that the empty set of vectors is a basis of

}

{

{

0

0

}

}

{

0

We saw in Example 5.2.9 that dim (Rn) = n and, if e j denotes column j of In, that

is
a basis (called the standard basis). In Example 6.3.7 below, similar considerations apply to the space Mmn
of all m

n matrices; the veriﬁcations are left to the reader.

e1, e2, . . . , en

{

}

×

Example 6.3.7

The space Mmn has dimension mn, and one basis consists of all m
entry equal to 1 and all other entries equal to 0. We call this the standard basis of Mmn.

n matrices with exactly one

×

348

Vector Spaces

Example 6.3.8

Show that dim Pn = n + 1 and that

1, x, x2, . . . , xn

{

}

is a basis, called the standard basis of Pn.

Solution. Each polynomial p(x) = a0 + a1x +
1, x, . . . , xn, so Pn = span
vanishes, a01 + a1x +
1, x, . . . , xn
{
dim (Pn) = n + 1.

1, x, . . . , xn

· · ·

}

}

{

+ anxn = 0, then a0 = a1 =

· · ·

· · ·

+ anxn in Pn is clearly a linear combination of

. However, if a linear combination of these vectors

= an = 0 because x is an indeterminate. So

is linearly independent and hence is a basis containing n + 1 vectors. Thus,

Example 6.3.9

If v

= 0 is any nonzero vector in a vector space V , show that span

= Rv has dimension 1.

v

}

{

Solution.
basis of Rv, and so dim Rv = 1.

{

v

}

clearly spans Rv, and it is linearly independent by Example 6.3.6. Hence

is a

v

}

{

Example 6.3.10

Let A =

1 1
0 0

(cid:21)

(cid:20)

and consider the subspace

|
of M22. Show that dim U = 2 and ﬁnd a basis of U .

{

U =

X in M22

AX = X A

}

Solution. It was shown in Example 6.2.3 that U is a subspace for any choice of the matrix A. In the

present case, if X =

is in U , the condition AX = X A gives z = 0 and x = y + w. Hence

x
y
z w

each matrix X in U can be written

(cid:20)

(cid:21)

X =

(cid:20)

y + w y
w

0

= y

(cid:21)

(cid:20)

1 1
0 0

+ w

(cid:21)

(cid:20)

1 0
0 1

(cid:21)

so U = span B where B =

. Moreover, the set B is linearly independent

(verify this), so it is a basis of U and dim U = 2.

1 1
0 0

,

(cid:21)

(cid:20)

1 0
0 1

(cid:26)(cid:20)

(cid:21)(cid:27)

Example 6.3.11

Show that the set V of all symmetric 2

×

2 matrices is a vector space, and ﬁnd the dimension of V .

Solution. A matrix A is symmetric if AT = A. If A and B lie in V , then

(A + B)T = AT + BT = A + B

and

(kA)T = kAT = kA

using Theorem 2.1.2. Hence A + B and kA are also symmetric. As the 2

2 zero matrix is also in

×

6
6.3. Linear Independence and Dimension

349

V , this shows that V is a vector space (being a subspace of M22). Now a matrix A is symmetric
when entries directly across the main diagonal are equal, so each 2
form

2 symmetric matrix has the

1 0
0 0

a c
c b

(cid:21)

= a

(cid:20)
0 0
0 1

,

,

(cid:20)
1 0
0 0

+ b

(cid:20)

(cid:21)
0 1
1 0

0 0
0 1

+ c

(cid:21)

(cid:20)

×
0 1
1 0

(cid:21)

Hence the set B =

spans V , and the reader can verify that B is

(cid:21)
linearly independent. Thus B is a basis of V , so dim V = 3.

(cid:21)(cid:27)

(cid:26)(cid:20)

(cid:21)

(cid:20)

(cid:20)

It is frequently convenient to alter a basis by multiplying each basis vector by a nonzero scalar. The

next example shows that this always produces another basis. The proof is left as Exercise 6.3.22.

Example 6.3.12

v1, v2, . . . , vn

Let B =
}
a1, a2, . . . , an, write D =
}
of D. In particular, if B is a basis of V , so also is D.

a1v1, a2v2, . . . , anvn

{

{

be nonzero vectors in a vector space V . Given nonzero scalars

. If B is independent or spans V , the same is true

a.

b.

c.

(cid:26)(cid:20)

d.

(cid:26)(cid:20)

Exercises for 6.3

Exercise 6.3.1 Show that each of the following sets of
vectors is independent.

in P2

(cid:26)(cid:20)

1 + x, 1
{
−
x2, x + 1, 1
{

x, x + x2

}
x2

}

x

−

−

in P2

f. V = F[0, 1];

(cid:8)

(cid:9)
x2

6 ,

1
x2+x

−

−

1
5x+6 ,

x2

1

9

−

o

n

d. V = M22;

1
−
0

0
1

,

(cid:20)
−
e. V = F[1, 2];

(cid:21)

,

(cid:21)

(cid:20)

1 1
1 1

,

(cid:21)

(cid:20)

0
1

−

1
−
0

(cid:21)(cid:27)

1
1

1
−
1

−
x , 1
1

x2 , 1
x3

,

1 1
0 0
(cid:21)
in M22

,

1 1
1 0
(cid:21)
in M22

(cid:20)

(cid:20)

1 0
1 0

0
1

,

(cid:21)

(cid:20)

0
1

−

,

(cid:21)

(cid:20)

0 1
0 1

(cid:21)(cid:27)

0 1
1 1

,

(cid:21)

(cid:20)

1 0
1 1

,

(cid:21)

(cid:20)

1 1
0 1

(cid:21)(cid:27)

Exercise 6.3.2 Which of the following subsets of V are
independent?

a. V = P2;

b. V = P2;

x2 + 1, x + 1, x
}
{
x2
{

−

x + 3, 2x2 + x + 5, x2 + 5x + 1
}

Exercise 6.3.3 Which of the following are independent
in F[0, 2π]?

a.

b.

c.

sin2 x, cos2 x
{
}
1, sin2 x, cos2 x
}
{
x, sin2 x, cos2 x
}
{

Exercise 6.3.4 Find all values of a such that the follow-
ing are independent in R3.

a.

b.

−

1, 0), (a, 1, 0), (0, 2, 3)
}

(1,
{
(2, a, 1), (1, 0, 1), (0, 1, 3)
}
{

Exercise 6.3.5 Show that the following are bases of the
space V indicated.

c. V = M22;

1 1
0 1

,

(cid:21)

(cid:20)

1 0
1 1

1 0
0 1

,

(cid:21)

(cid:20)

(cid:26)(cid:20)

(cid:21)(cid:27)

a.

; V = R3
(1, 1, 0), (1, 0, 1), (0, 1, 1)
}
{

350

Vector Spaces

b.

c.

1, 1, 1), (1,

(
−
{

−

1, 1), (1, 1,

; V = R3
1)
}

−

Exercise 6.3.10

1 0
0 1
(cid:26)(cid:20)
V = M22

,

(cid:21)

(cid:20)

0 1
1 0

,

(cid:21)

(cid:20)

1 1
0 1

,

(cid:21)

(cid:20)

1 0
0 0

;
(cid:21)(cid:27)

a. Let V denote the set of all 2

2 matrices with
equal column sums. Show that V is a subspace
of M22, and compute dim V .

×

d.

1 + x, x + x2, x2 + x3, x3
{

; V = P3
}

b. Repeat part (a) for 3

c. Repeat part (a) for n

3 matrices.

n matrices.

×

×

Exercise 6.3.6 Exhibit a basis and calculate the dimen-
sion of each of the following subspaces of P2.

Exercise 6.3.11

a.

b.

c.

d.

a(1 + x) + b(x + x2)
{

|

a and b in R

}

a + b(x + x2)
{

|

a and b in R

}

p(x)

p(x)

{

{

|

|

p(1) = 0
}

p(x) = p(

−

x)
}

Exercise 6.3.7 Exhibit a basis and calculate the dimen-
sion of each of the following subspaces of M22.

a.

A
{

|

AT =

A

}

−

1 1
1 0

=

(cid:21)

(cid:20)

1 1
1 0

−

A

(cid:21)

(cid:27)

(cid:20)

−

1 0
1 0

=

(cid:21)

(cid:20)

0 0
0 0

(cid:20)

−

(cid:21)(cid:27)

1 1
1 0

=

(cid:21)

(cid:20)

0 1
1 1

−

A

(cid:21)

(cid:27)

(cid:20)

−

b.

A

A

(cid:26)

c.

A

(cid:26)

d.

A

(cid:26)

A

A

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Exercise 6.3.8 Let A =

1 1
0 0
M22 and AX = X

(cid:20)

(cid:21)
.
}

U =

X

X
{

|

∈

and deﬁne

g.

a. Let V =

(x2 + x+ 1)p(x)
{

. Show that
}
V is a subspace of P4 and ﬁnd dim V . [Hint: If
f (x)g(x) = 0 in P, then f (x) = 0 or g(x) = 0.]

p(x) in P2

|

b. Repeat with V =
subset of P5.

(x2
{

−

x)p(x)

|

p(x) in P3

, a
}

c. Generalize.

Exercise 6.3.12 In each case, either prove the assertion
or give an example showing that it is false.

a. Every set of four nonzero polynomials in P3 is a

basis.

b. P2 has a basis of polynomials f (x) such that

f (0) = 0.

c. P2 has a basis of polynomials f (x) such that

f (0) = 1.

d. Every basis of M22 contains a noninvertible ma-

trix.

e. No independent subset of M22 contains a matrix A

with A2 = 0.

f. If

u, v, w
{
for some a, b, c.

}

is independent then, au + bv+ cw = 0

u, v, w
{
some a, b, c.

}

is independent if au + bv + cw = 0 for

a. Find a basis of U containing A.

b. Find a basis of U not containing A.

Exercise 6.3.9 Show that the set C of all complex num-
bers is a vector space with the usual operations, and ﬁnd
its dimension.

h. If

i. If

j. If

k. If

l. If

}

u, v
{
u, v
}
{
u, v, w
{
u, v, w
{

}
u, v, w
{

}

is independent, so is

is independent, so is

u, u + v
.
{
}
u, v, u + v
.
}
{

is independent, so is

}

is independent, so is

is independent, so is

u, v
.
{
}
u + w, v + w
{

.
}

u + v + w
.
}
{

m. If u

= 0 and v

= 0 then

is dependent if and

only if one is a scalar multiple of the other.

u, v
}
{

n. If dim V = n, then no set of more than n vectors

can be independent.

o. If dim V = n, then no set of fewer than n vectors

can span V .

Exercise 6.3.13 Let A
n matrices,
= 0 and B
and assume that A is symmetric and B is skew-symmetric
(that is, BT =

is independent.

B). Show that

= 0 be n

×

−

A, B
{

}

Exercise 6.3.14 Show that every set of vectors contain-
ing a dependent set is again dependent.

Exercise 6.3.15 Show that every nonempty subset of an
independent set of vectors is again independent.

{

is independent in F[a, b].

Exercise 6.3.16 Let f and g be functions on [a, b], and
assume that f (a) = 1 = g(b) and f (b) = 0 = g(a). Show
that

f , g
}
Exercise 6.3.17 Let
be independent
in Mmn, and suppose that U and V are invertible ma-
n, respectively. Show that
m and n
trices of size m
×
UA1V , UA2V , . . . , UAkV
is independent.
{
}
Exercise 6.3.18 Show that
only if neither v nor w is a scalar multiple of the other.

A1, A2, . . . , Ak
{

is independent if and

v, w
{

×

}

}

Exercise 6.3.19 Assume that
is independent in
a vector space V . Write u′ = au + bv and v′ = cu + dv,
where a, b, c, and d are numbers. Show that
is
a
c
b d

independent if and only if the matrix

u′, v′}
{
is invert-

u, v
{

}

ible. [Hint: Theorem 2.4.5.]

(cid:20)

(cid:21)

Exercise 6.3.20 If
w is not in span

v1, v2, . . . , vk
{

}

v1, v2, . . . , vk
{

, show that:
}

is independent and

a.

b.

w, v1, v2, . . . , vk
{
v1 + w, v2 + w, . . . , vk + w
}
{

}

is independent.

is independent.

v1, v2, . . . , vk
Exercise 6.3.21
}
{
v1, v1 + v2, . . . , v1 + v2 +
show that
{
independent.

If

is independent,
is also

+ vk

· · ·

}

Exercise 6.3.22 Prove Example 6.3.12.

Exercise 6.3.23
}
Which of the following are dependent?

u, v, w, z
{

Let

be independent.

6.3. Linear Independence and Dimension

351

b.

c.

d.

v, v

u + v, v + w, w + u
}
{
u
z, z
u
{
}
u + v, v + w, w + z, z + u
}
{

w, w

−

−

−

−

}

and

respectively.

w1, w2
{

u1, u2, u3
{

Exercise 6.3.24 Let U and W be subspaces of V with
If U
bases
and W have only the zero vector in common, show that
u1, u2, u3, w1, w2
{
}
Exercise 6.3.25 Let
als. Show that
deg p

p, q
be independent polynomi-
}
is independent if and only if

p, q, pq
}
1.

is independent.

1 and deg q

}

{

{

If z is a complex number, show that

is independent if and only if z is not real.

≥

≥
Exercise 6.3.26
z, z2
{
Exercise 6.3.27 Let B =
1 , AT
AT
write B′ =
{

}

A1, A2, . . . , An
{
2 , . . . , AT
n } ⊆

Mnm. Show that:

} ⊆

Mmn, and

a. B is independent if and only if B′ is independent.

b. B spans Mmn if and only if B′ spans Mnm.

Exercise 6.3.28
If V = F[a, b] as in Example 6.1.7,
show that the set of constant functions is a subspace of
dimension 1 ( f is constant if there is a number c such
that f (x) = c for all x).

Exercise 6.3.29

a. If U is

an invertible n

n matrix and
is a basis of Mmn, show that

×

A1, A2, . . . , Amn
{
A1U , A2U , . . . , AmnU
{

}

is also a basis.

}
b. Show that part (a) fails if U is not invertible. [Hint:

Theorem 2.4.5.]

Exercise 6.3.30 Show that
of R2 if and only if

(a, b), (a1, b1)
}
{
is a basis of P1.
a + bx, a1 + b1x
}
{

is a basis

Exercise 6.3.31 Find the dimension of the subspace
span

of F[0, 2π].

1, sin2 θ, cos 2θ
}
{

Exercise 6.3.32 Show that F[0, 1] is not ﬁnite dimen-
sional.

Exercise 6.3.33 If U and W are subspaces of V , deﬁne
W as follows:
their intersection U
v is in both U and W
W =

U

∩

∩

v
{

|

}

a.

v, v

u
{

−

−

w, w

u
}

−

a. Show that U

W .

∩

W is a subspace contained in U and

6
6
6
6
352

Vector Spaces

b. Show that U

W =

is
}
independent for any nonzero vectors u in U and w
in W .

if and only if

u, w
}
{

0
{

∩

c. If B and D are bases of U and W , and if U
v is in B or D

D =

, show that B
}

0
{
dependent.

∪

v
{

|

W =
is in-

∩
}

Exercise 6.3.34
(u, w)
V =
{

|

u in U and w in W

.
}

If U and W are vector spaces,

let

a. Show that V is a vector space if (u, w) +
(u1, w1) = (u + u1, w + w1) and a(u, w) =
(au, aw).

Exercise 6.3.35 Let Dn denote the set of all functions f
to R.
from the set

1, 2, . . . , n
}
{

a. Show that Dn is a vector space with pointwise ad-

dition and scalar multiplication.

b. Show that

S1, S2, . . . , Sn
{

is a basis of Dn where,
for each k = 1, 2, . . . , n, the function Sk is deﬁned
by Sk(k) = 1, whereas Sk( j) = 0 if j

= k.

}

Exercise 6.3.36 A polynomial p(x) is called even if
p(x). Let En and
p(
On denote the sets of even and odd polynomials in Pn.

x) = p(x) and odd if p(

x) =

−

−

−

a. Show that En is a subspace of Pn and ﬁnd dim En.

b. If dim U = m and dim W = n, show that

b. Show that On is a subspace of Pn and ﬁnd dim On.

dim V = m + n.

c. If V1, . . . , Vm are vector spaces, let

V = V1
=

× · · · ×

Vm
(v1, . . . , vm)
{

vi

Vi for each i
}

∈

|

denote the space of n-tuples from the Vi with com-
ponentwise operations (see Exercise 6.1.17).
If
dim Vi = ni for each i, show that dim V = n1 +

+ nm.

· · ·

6.4 Finite Dimensional Spaces

v1, . . . , vn
Exercise 6.3.37 Let
}
{
vector space V , and let A be an n
u1, . . . , un by

be independent in a
n matrix. Deﬁne

×



= A 





u1
...
un

v1
...
vn



u1, . . . , un
(See Exercise 6.1.18.) Show that
{
pendent if and only if A is invertible.










is inde-

}

Up to this point, we have had no guarantee that an arbitrary vector space has a basis—and hence no
guarantee that one can speak at all of the dimension of V . However, Theorem 6.4.1 will show that any
space that is spanned by a ﬁnite set of vectors has a (ﬁnite) basis: The proof requires the following basic
lemma, of interest in itself, that gives a way to enlarge a given independent set of vectors.

Lemma 6.4.1: Independent Lemma

v1, v2, . . . , vk
{
span

}

Let
u /
∈

beanindependentsetofvectorsinavectorspaceV. Ifu

v1, v2, . . . , vk
{

,then
}

u, v1, v2, . . . , vk
{

}

isalsoindependent.

V but5

∈

+ tkvk = 0; we must show that all the coefﬁcients are zero. First, t = 0
Proof. Let tu + t1v1 + t2v2 +
t2
t1
because, otherwise, u =
v1, v2, . . . , vk
t v2
t v1
, contrary to our assumption.
Hence t = 0. But then t1v1 + t2v2 +
+ tkvk = 0 so the rest of the ti are zero by the independence of
v1, v2, . . . , vk

. This is what we wanted.

tk
t vk is in span

− · · · −
· · ·

· · ·
−

−

}

{

{

}

5If X is a set, we write a

X to indicate that a is an element of the set X. If a is not an element of X, we write a /
∈

∈

X.

6
z

u

0

x

v1

v2

y

span

v1, v2}

{

6.4. Finite Dimensional Spaces

353

.

{

Note that the converse of Lemma 6.4.1 is also true: if
u, v1, v2, . . . , vk
is independent, then u is not in
{
}
v1, v2, . . . , vk}
span

As an illustration, suppose that

is inde-
pendent in R3. Then v1 and v2 are not parallel, so
span
is a plane through the origin (shaded in
the diagram). By Lemma 6.4.1, u is not in this plane if
and only if

is independent.

v1, v2

v1, v2

u, v1, v2

{

}

{

}

{

}

Deﬁnition 6.7 Finite Dimensional and Inﬁnite Dimensional Vector Spaces

AvectorspaceV iscalledﬁnitedimensionalifitisspannedbyaﬁnitesetofvectors. Otherwise,
V iscalledinﬁnitedimensional.

Thus the zero vector space

0

}

{

is ﬁnite dimensional because

is a spanning set.

0
}

{

Lemma 6.4.2

LetV beaﬁnitedimensionalvectorspace. IfU isanysubspaceofV,thenanyindependentsubset
ofU canbeenlargedtoaﬁnitebasisofU.

∈

U such that u1 /
∈

) = U we are done; otherwise choose u2

Proof. Suppose that I is an independent subset of U . If span I = U then I is already a basis of U . If
= U , choose u1
span I
span I. Hence the set I
is independent by Lemma 6.4.1.
u1
If span (I
). Hence
is independent, and the process continues. We claim that a basis of U will be reached
I
eventually. Indeed, if no basis of U is ever reached, the process creates arbitrarily large independent sets
in V . But this is impossible by the fundamental theorem because V is ﬁnite dimensional and so is spanned
by a ﬁnite set of vectors.

u1
}
U such that u2 /
∈

u1, u2

∪ {
}

span (I

∪ {

∪ {

∪ {

u1

∈

}

}

Theorem 6.4.1

LetV beaﬁnitedimensionalvectorspacespannedbymvectors.

1. V hasaﬁnitebasis,and dim V

m.

≤

2. EveryindependentsetofvectorsinV canbeenlargedtoabasisofV byaddingvectorsfrom

anyﬁxedbasisofV.

3. IfU isasubspaceofV,then

a. U isﬁnitedimensionaland dim U

b. If dim U = dim V thenU = V.

dim V.

≤

6
354

Vector Spaces

Proof.

1. If V =
Then

{

0

{
v
}

, then V has an empty basis and dim V = 0
}
is independent, so (1) follows from Lemma 6.4.2 with U = V .

m. Otherwise, let v

≤

= 0 be a vector in V .

2. We reﬁne the proof of Lemma 6.4.2. Fix a basis B of V and let I be an independent subset of V .
= V , then B is not contained in I (because
b1
is independent by

B such that b1 /
∈
) = V we are done; otherwise a similar argument shows that (I

If span I = V then I is already a basis of V . If span I
B spans V ). Hence choose b1
∈
b1
Lemma 6.4.1. If span (I
}
b1, b2
{
∈
a basis of V will be reached eventually.

) is independent for some b2

∪
B. Continue this process. As in the proof of Lemma 6.4.2,

span I. Hence the set I

∪ {

∪ {

}

}

3.

a. This is clear if U =

can be enlarged to a ﬁnite basis
B of U by Lemma 6.4.2, proving that U is ﬁnite dimensional. But B is independent in V , so
dim U

dim V by the fundamental theorem.

. Otherwise, let u

= 0 in U . Then

u
}

0
}

{

{

≤
b. This is clear if U =

because V has a basis; otherwise, it follows from (2).

0

}

{

Theorem 6.4.1 shows that a vector space V is ﬁnite dimensional if and only if it has a ﬁnite basis (possibly
empty), and that every subspace of a ﬁnite dimensional space is again ﬁnite dimensional.

Example 6.4.1

Enlarge the independent set D =

1 1
1 0

(cid:26)(cid:20)

Solution. The standard basis of M22 is

,

(cid:21)

(cid:20)

1 0
0 0

0 1
1 1

,

(cid:21)

(cid:20)

,

(cid:21)

(cid:20)

0 1
0 0

1 0
1 1

,

(cid:21)

(cid:20)

(cid:21)(cid:27)

0 0
1 0

(cid:26)(cid:20)

to a basis of M22.

0 0
0 1

,

(cid:21)

(cid:20)

, so

(cid:21)(cid:27)

including one of these in D will produce a basis by Theorem 6.4.1. In fact including any of these
matrices in D produces an independent set (verify), and hence a basis by Theorem 6.4.4. Of course

these vectors are not the only possibilities, for example, including

1 1
0 1

(cid:20)

(cid:21)

works as well.

Example 6.4.2

Find a basis of P3 containing the independent set

1 + x, 1 + x2

.

}

{
1, x, x2, x3
1, 1 + x, 1 + x2, x3

{

Solution. The standard basis of P3 is
we use 1 and x3, the result is
have distinct degrees (Example 6.3.4), and so is a basis by Theorem 6.4.1. Of course, including
1, x
}

, so including two of these vectors will do. If
}
. This is independent because the polynomials
}

would not work!

1, x2

or

{

{

{

}

Example 6.4.3

Show that the space P of all polynomials is inﬁnite dimensional.

Solution. For each n

≥

1, P has a subspace Pn of dimension n + 1. Suppose P is ﬁnite dimensional,

6
6
6
6.4. Finite Dimensional Spaces

355

say dim P = m. Then dim Pn
since n is arbitrary, so P must be inﬁnite dimensional.

≤

dim P by Theorem 6.4.1, that is n + 1

m. This is impossible

≤

The next example illustrates how (2) of Theorem 6.4.1 can be used.

Example 6.4.4
If c1, c2, . . . , ck are independent columns in Rn, show that they are the ﬁrst k columns in some
invertible n

n matrix.

×

Solution. By Theorem 6.4.1, expand
Rn. Then the matrix A =
n

{
. . . ck ck+1
n matrix and it is invertible by Theorem 5.2.3.
(cid:2)

c1, c2, . . . , ck}

c1 c2

×

(cid:3)

to a basis

c1, c2, . . . , ck, ck+1, . . . , cn
}
with this basis as its columns is an

{

of

. . . cn

Theorem 6.4.2

LetU andW besubspacesoftheﬁnitedimensionalspaceV.

1. IfU

2. IfU

⊆

⊆

W,then dim U

dim W.

≤

W and dim U = dim W,thenU = W.

Proof. Since W is ﬁnite dimensional, (1) follows by taking V = W in part (3) of Theorem 6.4.1. Now
assume dim U = dim W = n, and let B be a basis of U . Then B is an independent set in W . If U
= W ,
= W , so B can be extended to an independent set of n + 1 vectors in W by Lemma 6.4.1.
then span B
This contradicts the fundamental theorem (Theorem 6.3.2) because W is spanned by dim W = n vectors.
Hence U = W , proving (2).

Theorem 6.4.2 is very useful. This was illustrated in Example 5.2.13 for R2 and R3; here is another

example.

Example 6.4.5

If a is a number, let W denote the subspace of all polynomials in Pn that have a as a root:

W =

p(x)

{
a)2, . . . , (x

|
a)n

p(x)

∈

Pn and p(a) = 0

}

is a basis of W .

Show that

(x

a), (x

{

−
−
Solution. Observe ﬁrst that (x
a), (x
independent because they have distinct degrees (Example 6.3.4). Write

}
a)2, . . . , (x

−

−

−

−

a)n are members of W , and that they are

U = span

(x

a), (x

a)2, . . . , (x

a)n

}
Pn, dim U = n, and dim Pn = n + 1. Hence n
Then we have U
Theorem 6.4.2. Since dim W is an integer, we must have dim W = n or dim W = n + 1. But then
= Pn, it follows that W = U , as required.
W = U or W = Pn, again by Theorem 6.4.2. Because W

n + 1 by

dim W

W

≤

≤

−

−

−

⊆

⊆

{

6
6
6
356

Vector Spaces

A set of vectors is called dependent if it is not independent, that is if some nontrivial linear combina-

tion vanishes. The next result is a convenient test for dependence.

Lemma 6.4.3: Dependent Lemma

Aset D =
in D isalinearcombinationoftheothers.

v1, v2, . . . , vk
{

}

ofvectorsinavectorspaceVisdependentifandonlyifsomevector

Proof. Let v2 (say) be a linear combination of the rest: v2 = s1v1 + s3v3 +

+ skvk. Then

· · ·

s1v1 + (

1)v2 + s3v3 +

−

+ skvk = 0

· · ·

is a nontrivial linear combination that vanishes, so D is dependent. Conversely, if D is dependent, let
t1v1 + t2v2 +
tk
t2

+ tkvk = 0 where some coefﬁcient is nonzero. If (say) t2

vk is a linear combination of the others.

= 0, then v2 =

· · ·

v3

v1

t1
t2

t3
t2

−

−

−

· · · −

Lemma 6.4.1 gives a way to enlarge independent sets to a basis; by contrast, Lemma 6.4.3 shows that

spanning sets can be cut down to a basis.

Theorem 6.4.3

LetV beaﬁnitedimensionalvectorspace. AnyspanningsetforV canbecutdown(bydeleting
vectors)toabasisofV.

Proof. Since V is ﬁnite dimensional, it has a ﬁnite spanning set S. Among all spanning sets contained in S,
choose S0 containing the smallest number of vectors. It sufﬁces to show that S0 is independent (then S0 is a
basis, proving the theorem). Suppose, on the contrary, that S0 is not independent. Then, by Lemma 6.4.3,
some vector u
of vectors in S0 other than u. It follows
that span S0 = span S1, that is, V = span S1. But S1 has fewer elements than S0 so this contradicts the
choice of S0. Hence S0 is independent after all.

S0 is a linear combination of the set S1 = S0

u
}

\ {

∈

Note that, with Theorem 6.4.1, Theorem 6.4.3 completes the promised proof of Theorem 5.2.6 for the case
V = Rn.

Example 6.4.6

Find a basis of P3 in the spanning set S =

1, x + x2, 2x

3x2, 1 + 3x

2x2, x3

.

}

−

−

{

Solution. Since dim P3 = 4, we must eliminate one polynomial from S. It cannot be x3 because
the span of the rest of S is contained in P2. But eliminating 1 + 3x
Note that 1 + 3x

−
2x2 is the sum of the ﬁrst three polynomials in S.

2x2 does leave a basis (verify).

−

Theorems 6.4.1 and 6.4.3 have other useful consequences.

Theorem 6.4.4

LetV beavectorspacewith dim V = n,andsuppose S isasetofexactly n vectorsinV. Then S is
independentifandonlyif S spansV.

6
6.4. Finite Dimensional Spaces

357

Proof. Assume ﬁrst that S is independent. By Theorem 6.4.1, S is contained in a basis B of V . Hence
S

= n =
Conversely, assume that S spans V , so S contains a basis B by Theorem 6.4.3. Again

B, it follows that S = B. In particular S spans V .

so, since S

⊆

B

|

|

|

|

= n =

S

|

|

so,

B

|

|

since S

⊇

B, it follows that S = B. Hence S is independent.

One of independence or spanning is often easier to establish than the other when showing that a set of
vectors is a basis. For example if V = Rn it is easy to check whether a subset S of Rn is orthogonal (hence
independent) but checking spanning can be tedious. Here are three more examples.

Example 6.4.7

Consider the set S =
show that S is a basis of Pn.

{

p0(x), p1(x), . . . , pn(x)

of polynomials in Pn. If deg pk(x) = k for each k,

}

Solution. The set S is independent—the degrees are distinct—see Example 6.3.4. Hence S is a
basis of Pn by Theorem 6.4.4 because dim Pn = n + 1.

Example 6.4.8

Let V denote the space of all symmetric 2
matrices.

×

2 matrices. Find a basis of V consisting of invertible

Solution. We know that dim V = 3 (Example 6.3.11), so what is needed is a set of three invertible,
symmetric matrices that (using Theorem 6.4.4) is either independent or spans V . The set

1 0
0 1

1
0

,

(cid:21)

(cid:20)

0
1
−

,

(cid:21)

(cid:20)

0 1
1 0

(cid:26)(cid:20)

(cid:21)(cid:27)

is independent (verify) and so is a basis of the required type.

Example 6.4.9

Let A be any n
such that

×

n matrix. Show that there exist n2 + 1 scalars a0, a1, a2, . . . , an2 not all zero,

where I denotes the n

n identity matrix.

×

a0I + a1A + a2A2 +

+ an2An2

= 0

· · ·

Solution. The space Mnn of all n
n2 + 1 matrices I, A, A2, . . . , An2
combination vanishes. This is the desired conclusion.

n matrices has dimension n2 by Example 6.3.7. Hence the
×
cannot be independent by Theorem 6.4.4, so a nontrivial linear

The result in Example 6.4.9 can be written as f (A) = 0 where f (x) = a0 + a1x + a2x2 +
. In
other words, A satisﬁes a nonzero polynomial f (x) of degree at most n2. In fact we know that A satisﬁes
a nonzero polynomial of degree n (this is the Cayley-Hamilton theorem—see Theorem 8.7.10), but the
brevity of the solution in Example 6.4.6 is an indication of the power of these methods.

· · ·

+ an2xn2

If U and W are subspaces of a vector space V , there are two related subspaces that are of interest, their

sum U +W and their intersection U

W , deﬁned by

∩
U +W =

u + w

{

u

|

∈

U and w

W

}

∈

358

Vector Spaces

W =

U

∩

V

v

{

∈

|

v

∈

U and v

W

}

∈

It is routine to verify that these are indeed subspaces of V , that U
W is contained in both U and W , and
that U +W contains both U and W . We conclude this section with a useful fact about the dimensions of
these spaces. The proof is a good illustration of how the theorems in this section are used.

∩

Theorem 6.4.5

SupposethatU andW areﬁnitedimensionalsubspacesofavectorspaceV. ThenU +W isﬁnite
dimensionaland

dim (U +W ) = dim U + dim W

dim (U

W ).

∩

−

Proof. Since U
⊆
of U by Theorem 6.4.1. Similarly extend

U , it has a ﬁnite basis, say

W

∩

{
x1, . . . , xd

x1, . . . , xd

}
to a basis

. Extend it to a basis

x1, . . . , xd, w1, . . . , wp

x1, . . . , xd, u1, . . . , um
of W . Then

{

}

U +W = span

{

{
x1, . . . , xd, u1, . . . , um, w1, . . . , wp

{

}

}

}

as the reader can verify, so U +W is ﬁnite dimensional. For the rest, it sufﬁces to show that
x1, . . . , xd, u1, . . . , um, w1, . . . , wp

is independent (verify). Suppose that

{

}

r1x1 +

· · ·

+ rdxd + s1u1 +

· · ·

+ smum + t1w1 +

+ tpwp = 0

· · ·

(6.1)

where the ri, s j, and tk are scalars. Then

r1x1 +

+ rdxd + s1u1 +

+ smum =

(t1w1 +

+ tpwp)

· · ·
is in U (left side) and also in W (right side), and so is in U
= tp = 0, because
combination of
Similarly, s1 =
required.

x1, . . . , xd
= sm = 0, so (6.1) becomes r1x1 +

, so t1 =

{
· · ·

· · ·

· · ·

· · ·

}

· · ·
−
W . Hence (t1w1 +
x1, . . . , xd, w1, . . . , wp
}
+ rdxd = 0. It follows that r1 =

∩
{

· · ·

+ tpwp) is a linear
is independent.
= rd = 0, as

· · ·

Theorem 6.4.5 is particularly interesting if U
u1, . . . , um

. Then there are no vectors xi in the above
are bases of U and W respectively,
}
is a basis of U + W . In this case U +W is said to be a direct sum (written

{
w1, . . . , wp

W =
and

0
}

∩
}

{

{

proof, and the argument shows that if
u1, . . . , um, w1, . . . , wp
then
U

{
W ); we return to this in Chapter 9.

}

⊕

Exercises for 6.4

Exercise 6.4.1 In each case, ﬁnd a basis for V that in-
cludes the vector v.

Exercise 6.4.2 In each case, ﬁnd a basis for V among
the given vectors.

a. V = R3, v = (1,

1, 1)

−

b. V = R3, v = (0, 1, 1)

c. V = M22, v =

(cid:20)
d. V = P2, v = x2

1 1
1 1

(cid:21)

x + 1

−

a. V = R3,
(1, 1,
{

b. V = P2,

−

1), (2, 0, 1), (

−
x2 + 3, x + 2, x2
{

1, 1,

2), (1, 2, 1)
}

−

2x

1, x2 + x
}

−

−

Exercise 6.4.3 In each case, ﬁnd a basis of V containing
v and w.

a. V = R4, v = (1,

1, 1,

−

−

1), w = (0, 1, 0, 1)

b. V = R4, v = (0, 0, 1, 1), w = (1, 1, 1, 1)

1 0
0 1

c. V = M22, v =

, w =

(cid:20)
(cid:20)
d. V = P3, v = x2 + 1, w = x2 + x

(cid:21)

0 1
1 0

(cid:21)

Exercise 6.4.4

a. If z is not a real number, show that

is a basis
of the real vector space C of all complex numbers.

z, z2
{

}

b. If z is neither real nor pure imaginary, show that

z, z
{

}

is a basis of C.

Exercise 6.4.5 In each case use Theorem 6.4.4 to decide
if S is a basis of V .

a. V = M22;
1 1
1 1

S =

(cid:26)(cid:20)

0 1
1 1

,

(cid:20)

,

(cid:21)

(cid:20)

0 0
1 1

,

(cid:21)

(cid:20)

0 0
0 1

(cid:21)(cid:27)

(cid:21)

b. V = P3; S =

2x2, 1 + x, 3, 1 + x + x2 + x3
{

}

Exercise 6.4.6

a. Find a basis of M22 consisting of matrices with the

property that A2 = A.

b. Find a basis of P3 consisting of polynomials
whose coefﬁcients sum to 4. What if they sum
to 0?

Exercise 6.4.7 If
which of the following are bases.

u, v, w
}
{

is a basis of V , determine

a.

b.

c.

d.

u + v, u + w, v + w
}
{
2u + v + 3w, 3u + v
{
u, u + v + w
}
{
u, u + w, u
{

−

w, v + w
}

w, u

4w
}

−

−

Exercise 6.4.8

6.4. Finite Dimensional Spaces

359

Exercise 6.4.9 Show that any nonzero vector in a ﬁnite
dimensional vector space is part of a basis.

Exercise 6.4.10
If A is a square matrix, show that
det A = 0 if and only if some row is a linear combina-
tion of the others.

Exercise 6.4.11 Let D, I, and X denote ﬁnite, nonempty
sets of vectors in a vector space V . Assume that D is de-
pendent and I is independent. In each case answer yes or
no, and defend your answer.

a. If X

b. If X

c. If X

d. If X

⊇

⊆

⊇

⊆

D, must X be dependent?

D, must X be dependent?

I, must X be independent?

I, must X be independent?

Exercise 6.4.12
dim U = 2, show that either U

If U and W are subspaces of V and
1.

W or dim (U

W )

⊆

∩

≤

Exercise 6.4.13 Let A be a nonzero 2
X in M22
write U =
{
[Hint: I and A are in U .]

X A = AX

×
. Show that dim U
}

2 matrix and
2.

≥

|

Exercise 6.4.14
U =

0
{

, U = R2, or U is a line through the origin.
}

⊆

If U

R2 is a subspace, show that

Exercise 6.4.15 Given v1, v2, v3, . . . , vk, and v, let U =
and W = span
span
.
}
Show that either dim W = dim U or dim W = 1 +
dim U .

v1, v2, . . . , vk, v
{

v1, v2, . . . , vk
{

}

Exercise 6.4.16 Suppose U is a subspace of P1,
U
U = R(a + x) for some a in R.

, and U
0
}
{

= P1. Show that either U = R or

=

Exercise 6.4.17 Let U be a subspace of V and assume
dim V = 4 and dim U = 2. Does every basis of V result
from adding (two) vectors to some basis of U ? Defend
your answer.

Exercise 6.4.18 Let U and W be subspaces of a vector
space V .

a. Can two vectors span R3? Can they be linearly

independent? Explain.

b. Can four vectors span R3? Can they be linearly

independent? Explain.

a. If dim V = 3, dim U = dim W = 2, and U

= W ,

show that dim (U

W ) = 1.

∩

b. Interpret (a.) geometrically if V = R3.

6
6
6
360

Vector Spaces

Exercise 6.4.19 Let U
W be subspaces of V with
dim U = k and dim W = m, where k < m. If k < l < m,
show that a subspace X exists where U
W and
dim X = l.

⊆

⊆

⊆

X

Exercise 6.4.20 Let B =
be a maximal in-
dependent set in a vector space V . That is, no set of more
than n vectors S is independent. Show that B is a basis of
V .

v1, . . . , vn
{

}

Exercise 6.4.21 Let B =
v1, . . . , vn
be a minimal
{
spanning set for a vector space V . That is, V cannot be
spanned by fewer than n vectors. Show that B is a basis
of V .

}

Exercise 6.4.22

a. Let p(x) and q(x) lie in P1 and suppose that
= 0, and p(2) = 0 = q(1). Show
If

p(1)
that
rp(x) + sq(x) = 0, evaluate at x = 1, x = 2.]

= 0, q(2)
p(x), q(x)
}

is a basis of P1.

[Hint:

{

b. Let B =

{

p0(x), p1(x), . . . , pn(x)
}

be a set of
polynomials in Pn. Assume that there exist num-
bers a0, a1, . . . , an such that pi(ai)
= 0 for each i
but pi(a j) = 0 if i is different from j. Show that B
is a basis of Pn.

a. Show that V is a vector space.

b. Show that V is not ﬁnite dimensional.

c. [For those with some calculus.] Show that the set
an exists) is

of convergent sequences (that is, lim
n
∞
→
a subspace, also of inﬁnite dimension.

Exercise 6.4.24 Let A be an n
X in Mnn
U =
{
[Hint: Exercise 6.3.34.]

|

, show that dim U = n(n
AX = 0
}

−

×

n matrix of rank r. If
r).

Exercise 6.4.25 Let U and W be subspaces of V .

a. Show that U + W is a subspace of V containing

both U and W .

b. Show that span

u and w.

u, w
{

}

= Ru+Rw for any vectors

c. Show that

span

u1, . . . , um, w1, . . . , wn
{
= span

+ span

u1, . . . , um
{

}

}
w1, . . . , wn
{

}

Exercise 6.4.23 Let V be the set of all inﬁnite sequences
(a0, a1, a2, . . . ) of real numbers. Deﬁne addition and
scalar multiplication by

(a0, a1, . . . ) + (b0, b1, . . . ) = (a0 + b0, a1 + b1, . . . )

and

r(a0, a1, . . . ) = (ra0, ra1, . . . )

6.5 An Application to Polynomials

for any vectors ui in U and w j in W .

Exercise 6.4.26 If A and B are m
n matrices, show
×
that rank (A + B)
rank A + rank B. [Hint: If U and V
are the column spaces of A and B, respectively, show that
the column space of A + B is contained in U +V and that
dim (U +V )

dim U + dim V . (See Theorem 6.4.5.)]

≤

≤

The vector space of all polynomials of degree at most n is denoted Pn, and it was established in Section 6.3
that Pn has dimension n + 1; in fact,
is a basis. More generally, any n + 1 polynomials
of distinct degrees form a basis, by Theorem 6.4.4 (they are independent by Example 6.3.4). This proves

1, x, x2, . . . , xn

}

{

Theorem 6.5.1

Let p0(x), p1(x), p2(x), . . . , pn(x) bepolynomialsinPn ofdegrees 0, 1, 2, . . . , n,respectively.
Then

isabasisofPn.

p0(x), . . . , pn(x)

{

}

An immediate consequence is that

a. Hence we have the following:

1, (x

{

−

a), (x

−

a)2, . . . , (x

a)n

}

−

is a basis of Pn for any number

6
6
6
Corollary 6.5.1

6.5. An Application to Polynomials

361

If a isanynumber,everypolynomial f (x) ofdegreeatmost n hasanexpansioninpowersof
(x

a):

f (x) = a0 + a1(x

a) + a2(x

−

−

a)2 +

+ an(x

−

· · ·

a)n

(6.2)

−

If f (x) is evaluated at x = a, then equation (6.2) becomes

f (x) = a0 + a1(a

a) +

−

· · ·

+ an(a

−

a)n = a0

Hence a0 = f (a), and equation (6.2) can be written f (x) = f (a) + (x
of degree n
f (x) = (x

−
a)g(x). Conversely, every such polynomial certainly satisﬁes f (a) = 0, and we obtain:

a)g(x), where g(x) is a polynomial
1). If it happens that f (a) = 0, then it is clear that f (x) has the form

1 (this assumes that n

−

≥

−

Corollary 6.5.2

Let f (x) beapolynomialofdegree n
RemainderTheorem

≥

1 andlet a beanynumber. Then:

1. f (x) = f (a) + (x

−

FactorTheorem

a)g(x) forsomepolynomialg(x) ofdegree n

1.

−

2. f (a) = 0 ifandonlyif f (x) = (x

a)g(x) forsomepolynomial g(x).

−

The polynomial g(x) can be computed easily by using “long division” to divide f (x) by (x
Appendix D.

−

a)—see

All the coefﬁcients in the expansion (6.2) of f (x) in powers of (x

a) can be determined in terms of the
derivatives of f (x).6 These will be familiar to students of calculus. Let f (n)(x) denote the nth derivative
of the polynomial f (x), and write f (0)(x) = f (x). Then, if

−

−
it is clear that a0 = f (a) = f (0)(a). Differentiation gives

−

f (x) = a0 + a1(x

a) + a2(x

a)2 +

+ an(x

a)n

−

· · ·

f (1)(x) = a1 + 2a2(x

a) + 3a3(x

−

a)2 +

−

· · ·

+ nan(x

and substituting x = a yields a1 = f (1)(a). This continues to give a2 = f (2)(a)
where k! is deﬁned as k! = k(k
1. Hence we obtain the following:
2

1)

2!

−

· · ·

·

a)n
1
−

−
, a3 = f (3)(a)

3!

, . . . , ak = f (k)(a)

k!

,

Corollary 6.5.3: Taylor’s Theorem

If f (x) isapolynomialofdegree n,then

f (x) = f (a) + f (1)(a)

1!

(x

−

a) + f (2)(a)

2!

(x

a)2 +

−

· · ·

+ f (n)(a)
n!

(x

a)n

−

6The discussion of Taylor’s theorem can be omitted with no loss of continuity.

362

Vector Spaces

Example 6.5.1

Expand f (x) = 5x3 + 10x + 2 as a polynomial in powers of x

1.

−

Solution. The derivatives are f (1)(x) = 15x2 + 10, f (2)(x) = 30x, and f (3)(x) = 30. Hence the
Taylor expansion is

f (x) = f (1) + f (1)(1)
1!
= 17 + 25(x

(x

1) + f (2)(1)

(x
2!
−
1)2 + 5(x

1)2 + f (3)(1)
1)3

3!

−
1) + 15(x

1)3

(x

−

−

−

−

Taylor’s theorem is useful in that it provides a formula for the coefﬁcients in the expansion. It is dealt

with in calculus texts and will not be pursued here.

Theorem 6.5.1 produces bases of Pn consisting of polynomials of distinct degrees. A different criterion

is involved in the next theorem.

Theorem 6.5.2

Let f0(x), f1(x), . . . , fn(x) benonzeropolynomialsinPn. Assumethatnumbers a0, a1, . . . , an
existsuchthat

fi(ai)
= 0
fi(a j) = 0

foreach i
= j
if i

Then

f0(x), . . . , fn(x)

1.

{

}

isabasisofPn.

2. If f (x) isanypolynomialinPn,itsexpansionasalinearcombinationofthesebasisvectorsis

f (x) = f (a0)

f0(a0) f0(x) + f (a1)

f1(a1) f1(x) +

+ f (an)

fn(an) fn(x)

· · ·

Proof.

1. It sufﬁces (by Theorem 6.4.4) to show that

dim Pn = n + 1). Suppose that

f0(x), . . . ,

fn(x)

}

{

is linearly independent (because

r0 f0(x) + r1 f1(x) +

+ rn fn(x) = 0, ri

R

· · ·
Because fi(a0) = 0 for all i > 0, taking x = a0 gives r0 f0(a0) = 0. But then r0 = 0 because f0(a0)
The proof that ri = 0 for i > 0 is analogous.

∈

= 0.

2. By (1),

f (x) = r0 f0(x) +

+ rn fn(x) for some numbers ri. Once again, evaluating at a0 gives

f (a0) = r0 f0(a0), so r0 = f (a0)/ f0(a0). Similarly, ri = f (ai)/ fi(ai) for each i.

· · ·

6
6
6
6.5. An Application to Polynomials

363

Example 6.5.2

Show that

x2

{

−

x, x2

−

2x, x2

Solution. Write f0(x) = x2
f2(x) = x2
−
a0 = 2, a1 = 1, and a2 = 0.

3x + 2 = (x

−

−
1)(x

−

3x + 2

is a basis of P2.

−
x = x(x

}
1), f1(x) = x2

−

2x = x(x

2), and

−

−

2). Then the conditions of Theorem 6.5.2 are satisﬁed with

We investigate one natural choice of the polynomials fi(x) in Theorem 6.5.2. To illustrate, let a0, a1,

and a2 be distinct numbers and write

f0(x) = (x
−
(a0
−
Then f0(a0) = f1(a1) = f2(a2) = 1, and fi(a j) = 0 for i
fi(ai) = 1 for each i, the formula for expanding any polynomial is simpliﬁed.

f1(x) = (x
−
(a1
−

f2(x) = (x
−
(a2
−

a1)(x
−
a1)(a0

a0)(x
−
a0)(a1

a2)
a2)

a2)
a2)

−

−

a0)(x
−
a0)(a2

a1)
a1)

−

= j. Hence Theorem 6.5.2 applies, and because

In fact, this can be generalized with no extra effort. If a0, a1, . . . , an are distinct numbers, deﬁne the

Lagrange polynomials δ0(x), δ1(x), . . . , δn(x) relative to these numbers as follows:

δk(x) = ∏i
∏i

=k(x
−
=k(ak−

ai)
ai)

k = 0, 1, 2, . . . , n

Here the numerator is the product of all the terms (x
ak) omitted,
and a similar remark applies to the denominator. If n = 2, these are just the polynomials in the preceding
paragraph. For another example, if n = 3, the polynomial δ1(x) takes the form

a1), . . . , (x

an) with (x

a0), (x

−

−

−

−

δ1(x) = (x
a0)(x
−
a0)(a1
(a1
−

−
−

a2)(x
−
a2)(a1

−

a3)

a3)

In the general case, it is clear that δi(ai) = 1 for each i and that δi(a j) = 0 if i
specializes as Theorem 6.5.3.

= j. Hence Theorem 6.5.2

Theorem 6.5.3: Lagrange Interpolation Expansion

Let a0, a1, . . . , an bedistinctnumbers. Thecorrespondingset

δ0(x), δ1(x), . . . , δn(x)
{

}

ofLagrangepolynomialsisabasisofPn,andanypolynomial f (x) inPn hasthefollowingunique
expansionasalinearcombinationofthesepolynomials.

f (x) = f (a0)δ0(x) + f (a1)δ1(x) +

+ f (an)δn(x)

· · ·

Example 6.5.3

Find the Lagrange interpolation expansion for f (x) = x2
a2 = 1.

−

2x + 1 relative to a0 =

1, a1 = 0, and

−

6
6
6
6
364

Vector Spaces

Solution. The Lagrange polynomials are

x)

δ0 = (x
0)(x
1)
2(x2
1) = 1
−
−
1
0)(
1
(
−
−
−
−
−
δ1 = (x+1)(x
1)
(x2
1)
1) =
−
(0+1)(0
−
−
−
δ2 = (x+1)(x
0)
2(x2 + x)
0) = 1
−
(1+1)(1
−

Because f (

1) = 4, f (0) = 1, and f (1) = 0, the expansion is

−

f (x) = 2(x2

x)

−

−

(x2

1)

−

The Lagrange interpolation expansion gives an easy proof of the following important fact.

Theorem 6.5.4

Let f (x) beapolynomialinPn,andlet a0, a1, . . . , an denotedistinctnumbers. If f (ai) = 0 forall
i,then f (x) isthezeropolynomial(thatis,allcoefﬁcientsarezero).

Proof. All the coefﬁcients in the Lagrange expansion of f (x) are zero.

Exercises for 6.5

Exercise 6.5.1
f (a) = g(a), show that
some polynomial h(x).

If polynomials f (x) and g(x) satisfy
a)h(x) for
g(x) = (x

f (x)

−

−

Exercises 6.5.2, 6.5.3, 6.5.4, and 6.5.5 require poly-
nomial differentiation.
Exercise 6.5.2 Expand each of the following as a poly-
nomial in powers of x

1.

−

a.

b.

c.

f (x) = x3

2x2 + x

1

−

−

f (x) = x3 + x + 1

f (x) = x4

d.

f (x) = x3

3x2 + 3x

−

Exercise 6.5.3 Prove Taylor’s theorem for polynomi-
als.

Exercise 6.5.4 Use Taylor’s theorem to derive the bino-
mial theorem:

(1 + x)n =

n
0

(cid:18)

(cid:19)

+

n
1
(cid:18)

(cid:19)

x +

n
2
(cid:18)

x2 +

(cid:19)

+

· · ·

xn

n
n
(cid:19)
(cid:18)

Here the binomial coefﬁcients

n
r

are deﬁned by

(cid:0)
= n!
r!(n
−

(cid:1)
r)!

n
r

(cid:18)

(cid:19)
2

·

where n! = n(n

1)

1 if n

1 and 0! = 1.

−

· · ·

≥

Exercise 6.5.5 Let f (x) be a polynomial of degree n.
Show that, given any polynomial g(x) in Pn, there exist
numbers b0, b1, . . . , bn such that

g(x) = b0 f (x) + b1 f (1)(x) +

+ bn f (n)(x)

· · ·

where f (k)(x) denotes the kth derivative of f (x).

Exercise 6.5.6 Use Theorem 6.5.2 to show that the fol-
lowing are bases of P2.

a.

b.

x2
{
x2
{

−

−

2x, x2 + 2x, x2

3x + 2, x2

−

4
}

−
4x + 3, x2

5x + 6
}

−

Exercise 6.5.7 Find the Lagrange interpolation expan-
sion of f (x) relative to a0 = 1, a1 = 2, and a2 = 3 if:

a.

f (x) = x2 + 1

b.

f (x) = x2 + x + 1

6.6. An Application to Differential Equations

365

Exercise 6.5.8 Let a0, a1, . . . , an be distinct numbers.
If f (x) and g(x) in Pn satisfy f (ai) = g(ai) for all i, show
that f (x) = g(x). [Hint: See Theorem 6.5.4.]

by using the fact that if p(x)q(x) = 0 in P, then
either p(x) = 0 or q(x) = 0.]

∈

Exercise 6.5.9 Let a0, a1, . . . , an be distinct numbers.
Pn+1 satisﬁes f (ai) = 0 for each i = 0, 1, . . . , n,
If f (x)
show that f (x) = r(x
an) for some r
a1)
in R. [Hint: r is the coefﬁcient of xn+1 in f (x). Consider
f (x)

an) and use Theorem 6.5.4.]

a0)(x

a0)

r(x

· · ·

(x

(x

−

−

−

−

−

· · ·

−

Exercise 6.5.10 Let a and b denote distinct numbers.

a. Show that

(x
{
(x
{
basis of P2.

b. Show that

−

−

a), (x

−
a)2, (x

b)
}
a)(x

−

is a basis of P1.

b), (x

b)2

}

−

is a

−

Exercise 6.5.11 Let a and b be two distinct numbers.
Assume that n

2 and let

≥

Un =

{

f (x) in Pn

f (a) = 0 = f (b)
.
}

|

a. Show that

Un =

(x
{

−

a)(x

−

b)p(x)

|

p(x) in Pn

−

2

}

b. Show that dim Un = n

1.

−
[Hint: If p(x)q(x) = 0 in P, then either p(x) = 0,
or q(x) = 0.]

(x
{
a)(x

c. Show that
−
is a basis of Pn.
. . . , (x
[Hint: If a linear combination vanishes, evaluate
at x = a and x = b. Then reduce to the case n
2

a)n, (x
b)n
−

a)n
−
b)n

−
1, (x

−
−

1(x

b),

−

−

}

−

a)n
2(x
(x
c. Show
−
−
−
{
b)n
1
. . . , (x
a)(x
−
−
−
−
sis of Un. [Hint: Exercise 6.5.10.]

1(x
b), (x
−
b)n
2, (x
−

a)n
−
−
a)2(x

−

b)2,
is a ba-

}

6.6 An Application to Differential Equations

R differentiable if it can be differentiated as many times as we want.

Call a function f : R
If f
is a differentiable function, the nth derivative f (n) of f is the result of differentiating n times. Thus
f (0) = f , f (1) = f ′, f (2) = f (1)
0. For small values of n
these are often written as f , f ′, f ′′, f ′′′, . . . .

′, . . . and, in general, f (n+1) = f (n)

′ for each n

→

≥

If a, b, and c are numbers, the differential equations

f ′′ + a f ′ + b f = 0 or

f ′′′ + a f ′′ + b f ′ + c f = 0

are said to be of second-order and third-order, respectively. In general, an equation

2 f (n
1 f (n
f (n) + an
2) +
1) + an
−
−
−
−

· · ·

+ a2 f (2) + a1 f (1) + a0 f (0) = 0,

ai in R

(6.3)

is called a differential equation of order n. In this section we investigate the set of solutions to (6.3) and,
if n is 1 or 2, ﬁnd explicit solutions. Of course an acquaintance with calculus is required.

Let f and g be solutions to (6.3). Then f + g is also a solution because ( f + g)(k) = f (k) + g(k) for all
k, and a f is a solution for any a in R because (a f )(k) = a f (k). It follows that the set of solutions to (6.3) is
a vector space, and we ask for the dimension of this space.

We have already dealt with the simplest case (see Theorem 3.5.1):

Theorem 6.6.1

Thesetofsolutionsoftheﬁrst-orderdifferentialequation f ′ + a f = 0 isaone-dimensionalvector
spaceand

isabasis.

ax

e−

{

}

There is a far-reaching generalization of Theorem 6.6.1 that will be proved in Theorem 7.4.1.

366

Vector Spaces

Theorem 6.6.2

Thesetofsolutionstothe nthorderequation(6.3)hasdimension n.

Remark
Every differential equation of order n can be converted into a system of n linear ﬁrst-order equations (see
Exercises 3.5.6 and 3.5.7). In the case that the matrix of this system is diagonalizable, this approach
provides a proof of Theorem 6.6.2. But if the matrix is not diagonalizable, Theorem 7.4.1 is required.

Theorem 6.6.1 suggests that we look for solutions to (6.3) of the form eλx for some number λ. This is
0, so substituting

a good idea. If we write f (x) = eλx, it is easy to verify that f (k)(x) = λkeλx for each k
f in (6.3) gives

≥

2λn
1λn
(λn + an
2 +
1 + an
−
−
−
−

· · ·

+ a2λ2 + a1λ1 + a0)eλx = 0

= 0 for all x, this shows that eλx is a solution of (6.3) if and only if λ is a root of the characteristic

Since eλx
polynomial c(x), deﬁned to be

2xn
1xn
c(x) = xn + an
2 +
1 + an
−
−
−
−

· · ·

+ a2x2 + a1x + a0

This proves Theorem 6.6.3.

Theorem 6.6.3

Ifλ isreal,thefunction eλx isasolutionof(6.3)ifandonlyifλ isarootofthecharacteristic
polynomial c(x).

Example 6.6.1
Find a basis of the space U of solutions of f ′′′ −
Solution. The characteristic polynomial is x3
λ1 = 1, λ2 =
(by Lemma 6.6.1 below) so, since dim (U ) = 3 by Theorem 6.6.2,

1, and λ3 = 2. Hence ex, e−

2 f ′′ −
2x2
x

f ′ −

1 = (x

−

−

−

−

−

2 f = 0.

1)(x + 1)(x

2), with roots

x, and e2x are all in U . Moreover they are independent
ex, e−
is a basis of U .

−
x, e2x

{

}

Lemma 6.6.1

Ifλ1, λ2, . . . , λk aredistinct,then

eλ1x, eλ2x, . . . , eλkx

{

}

islinearlyindependent.

Proof. If r1eλ1x + r2eλ2x +
r2e(λ2
+ rke(λk−
r1 = 0 also. This is what we wanted.

λ1)x +
−

· · ·

+ rkeλkx = 0 for all x, then r1 + r2e(λ2

λ1)x +
−

· · ·
λ1)x is a constant. Since the λi are distinct, this forces r2 =

· · ·

+ rke(λk−

λ1)x = 0; that is,
= rk = 0, whence

· · ·

6
6.6. An Application to Differential Equations

367

Theorem 6.6.4

LetU denotethespaceofsolutionstothesecond-orderequation

f ′′ + a f ′ + b f = 0

where a and b arerealconstants. Assumethatthecharacteristicpolynomial x2 + ax + b hastwo
realrootsλ and µ. Then

1. Ifλ

= µ,then

2. Ifλ = µ,then

{

{

eλx, eµx

}
eλx, xeλx

}

isabasisofU.

isabasisofU.

Proof. Since dim (U ) = 2 by Theorem 6.6.2, (1) follows by Lemma 6.6.1, and (2) follows because the set
eλx, xeλx

is independent (Exercise 6.6.3).

{

}

Example 6.6.2

Find the solution of f ′′ + 4 f ′ + 4 f = 0 that satisﬁes the boundary conditions f (0) = 1,
f (1) =

1.

−

Solution. The characteristic polynomial is x2 + 4x + 4 = (x + 2)2, so
2x, xe−
e−
{
f (x) = ce−

is a basis for the space of solutions, and the general solution takes the form
2x. Applying the boundary conditions gives 1 = f (0) = c and

2x
}
2x + dxe−

−

2 is a double root. Hence

1 = f (1) = (c + d)e−

2. Hence c = 1 and d =

(1 + e2), so the required solution is

−

−

f (x) = e−

2x

−

(1 + e2)xe−

2x

One other question remains: What happens if the roots of the characteristic polynomial are not real?
To answer this, we must ﬁrst state precisely what eλx means when λ is not real. If q is a real number,
deﬁne

eiq = cos q + i sin q

where i2 =
λ = p + iq, where p and q are real numbers, we deﬁne

−

1. Then the relationship eiqeiq1 = ei(q+q1) holds for all real q and q1, as is easily veriﬁed. If

eλ = epeiq = ep(cos q + i sin q)

Then it is a routine exercise to show that

1. eλeµ = eλ+µ

2. eλ = 1 if and only if λ = 0

3. (eλx)′ = λeλx

These easily imply that f (x) = eλx is a solution to f ′′ + a f ′ + b f = 0 if λ is a (possibly complex) root of
the characteristic polynomial x2 + ax + b. Now write λ = p + iq so that

f (x) = eλx = epx cos(qx) + iepx sin(qx)

6
368

Vector Spaces

For convenience, denote the real and imaginary parts of f (x) as u(x) = epx cos(qx) and v(x) = epx sin(qx).
Then the fact that f (x) satisﬁes the differential equation gives

0 = f ′′ + a f ′ + b f = (u′′ + au′ + bu) + i(v′′ + av′ + bv)

Equating real and imaginary parts shows that u(x) and v(x) are both solutions to the differential equation.
This proves part of Theorem 6.6.5.

Theorem 6.6.5

LetU denotethespaceofsolutionsofthesecond-orderdifferentialequation

f ′′ + a f ′ + b f = 0

where a and b arereal. Supposeλ isanonrealrootofthecharacteristicpolynomialx2 + ax + b. If
λ = p + iq,where p and q arereal,then

isabasisofU.

epx cos(qx), epx sin(qx)

{

}

Proof. The foregoing discussion shows that these functions lie in U . Because dim U = 2 by Theo-
rem 6.6.2, it sufﬁces to show that they are linearly independent. But if

repx cos(qx) + sepx sin(qx) = 0

for all x, then r cos(qx) + s sin(qx) = 0 for all x (because epx
x = π

= 0 because λ is not real). This is what we wanted.

2q gives s = 0 (q

= 0). Taking x = 0 gives r = 0, and taking

2 f ′ + 2 f = 0 that satisﬁes f (0) = 2 and f ( π

Example 6.6.3
Find the solution f (x) to f ′′ −
Solution. The characteristic polynomial x2
(quite arbitrarily) gives p = q = 1 in the notation of Theorem 6.6.5, so
for the space of solutions. The general solution is thus f (x) = ex(r cos x + s sin x). The boundary
conditions yield 2 = f (0) = r and 0 = f ( π
solution is f (x) = 2ex cos x.

2 ) = eπ/2s. Thus r = 2 and s = 0, and the required

−
ex cos x, ex sin x
}

2x + 2 has roots 1 + i and 1

i. Taking λ = 1 + i

2 ) = 0.

−

{

is a basis

The following theorem is an important special case of Theorem 6.6.5.

Theorem 6.6.6

If q

= 0 isarealnumber,thespaceofsolutionstothedifferentialequation f ′′ + q2 f = 0 hasbasis

cos(qx), sin(qx)

.
}

{

Proof. The characteristic polynomial x2 + q2 has roots qi and

qi, so Theorem 6.6.5 applies with p = 0.

−

6
6
6
In many situations, the displacement s(t) of some object at time t turns out to have an oscillating form

s(t) = c sin(at) + d cos(at). These are called simple harmonic motions. An example follows.

6.6. An Application to Differential Equations

369

Example 6.6.4

d(t)

A weight is attached to an extension spring (see diagram). If it is pulled
from the equilibrium position and released, it is observed to oscillate up
and down. Let d(t) denote the distance of the weight below the equilibrium
position t seconds later. It is known (Hooke’s law) that the acceleration
d′′(t) of the weight is proportional to the displacement d(t) and in the opposite
direction. That is,

where k > 0 is called the spring constant. Find d(t) if the maximum extension
is 10 cm below the equilibrium position and ﬁnd the period of the oscillation

d′′(t) =

kd(t)

−

(time taken for the weight to make a full oscillation).

Solution. It follows from Theorem 6.6.6 (with q2 = k) that

d(t) = r sin(√k t) + s cos(√k t)

where r and s are constants. The condition d(0) = 0 gives s = 0, so d(t) = r sin(√k t). Now the
maximum value of the function sin x is 1 (when x = π

). Hence

2 ), so r = 10 (when t = π
2√k

d(t) = 10 sin(√k t)

Finally, the weight goes through a full oscillation as √k t increases from 0 to 2π. The time taken is
t = 2π
√k

, the period of the oscillation.

Exercises for 6.6

Exercise 6.6.1 Find a solution f to each of the follow-
ing differential equations satisfying the given boundary
conditions.

a.

b.

c.

d.

e.

f.

g.

3 f = 0; f (1) = 2

f ′ −
f ′ + f = 0; f (1) = 1
f ′′ + 2 f ′ −
f ′′ + f ′ −
f ′′ −
f ′′ −
f ′′ −
f (1) = 1

ea

3a f ′ + 2a2 f = 0; a

−

15 f = 0; f (1) = f (0) = 0

6 f = 0; f (0) = 0, f (1) = 1

2 f ′ + f = 0; f (1) = f (0) = 1

4 f ′ + 4 f = 0; f (0) = 2, f (

1) = 0

−
= 0; f (0) = 0,

h.

f ′′ −

a2 f = 0, a

= 0; f (0) = 1, f (1) = 0

i.

j.

f ′′ −

2 f ′ + 5 f = 0; f (0) = 1, f ( π

4 ) = 0

f ′′ + 4 f ′ + 5 f = 0; f (0) = 0, f ( π

2 ) = 1

Exercise 6.6.2 If the characteristic polynomial of
f ′′ + a f ′ + b f = 0 has real roots, show that f = 0 is the
only solution satisfying f (0) = 0 = f (1).

Exercise 6.6.3 Complete the proof of Theorem 6.6.2.
[Hint: If λ is a double root of x2 + ax + b, show that
2λ and b = λ2. Hence xeλx is a solution.]
a =

−

6
6
370

Vector Spaces

Exercise 6.6.4

a. Given the equation f ′ + a f = b, (a

= 0), make the
substitution f (x) = g(x) + b/a and obtain a dif-
ferential equation for g. Then derive the general
solution for f ′ + a f = b.

b. Find the general solution to f ′ + f = 2.

Exercise 6.6.5 Consider the differential equation
f ′ + a f ′ + b f = g, where g is some ﬁxed function. As-
sume that f0 is one solution of this equation.

a. Show that the general solution is c f1 + d f2 + f0,
is any

where c and d are constants and
}
basis for the solutions to f ′′ + a f ′ + b f = 0.

f1,

f2

{

b. Find a solution to f ′′ + f ′ −

[Hint: Try f (x) = −

1
3 x3.]

6 f = 2x3

x2

−

−

2x.

b. Find the half-life of the element—the time it takes

to decay to half its mass.

Exercise 6.6.7 The population N(t) of a region at time
t increases at a rate proportional to the population. If the
population doubles in 5 years and is 3 million initially,
ﬁnd N(t).

Exercise 6.6.8 Consider a spring, as in Example 6.6.4.
If the period of the oscillation is 30 seconds, ﬁnd the
spring constant k.

Exercise 6.6.9 As a pendulum swings (see the diagram),
let t measure the time since it was vertical. The angle
θ = θ(t) from the vertical can be shown to satisfy the
equation θ′′ + kθ = 0, provided that θ is small. If the
maximal angle is θ = 0.05 radians, ﬁnd θ(t) in terms of
k. If the period is 0.5 seconds, ﬁnd k. [Assume that θ = 0
when t = 0.]

Exercise 6.6.6 A radioactive element decays at a rate
proportional to the amount present. Suppose an initial
mass of 10 grams decays to 8 grams in 3 hours.

θ

a. Find the mass t hours later.

Supplementary Exercises for Chapter 6

Exercise 6.3 If A is an m
n matrix, show that A has
×
rank m if and only if col A contains every column of Im.
Exercise 6.4 Show that null A = null (AT A) for any real
matrix A.

×

Exercise 6.5 Let A be an m
that dim ( null A) = n
Choose a basis
to a basis
Az1, . . . , Azm
{

−
x1, . . . , xk
{
x1, . . . , xk, z1, . . . , zm
{
is a basis of col A.

n matrix of rank r. Show
r (Theorem 5.4.3) as follows.
of null A and extend it
of Rn. Show that

}

}

}

Exercise 6.1 (Requires calculus) Let V denote the space
R for which the derivatives f ′ and
of all functions f : R
f ′′ exist. Show that f1, f2, and f3 in V are linearly inde-
pendent provided that their wronskian w(x) is nonzero
for some x, where

→

w(x) = det 

f1(x)
f ′1(x)
f ′′1 (x)

f2(x)
f ′2(x)
f ′′2 (x)

f3(x)
f ′3(x)
f ′′3 (x)






Exercise 6.2 Let
v1, v2, . . . , vn
}
{
ten as columns), and let A be an n
×




be a basis of Rn (writ-
n matrix.

a. If A is invertible, show that

is a basis of Rn.

Av1, Av2, . . . , Avn
{

}

b. If

Av1, Av2, . . . , Avn
{

A is invertible.

is a basis of Rn, show that

}

6
Chapter 7

Linear Transformations

→

W and T : V

W is a rule that assigns to each vector v in V a uniquely
If V and W are vector spaces, a function T : V
determined vector T (v) in W . As mentioned in Section 2.2, two functions S : V
W
are equal if S(v) = T (v) for every v in V . A function T : V
W is called a linear transformation if
T (v + v1) = T (v) + T (v1) for all v, v1 in V and T (rv) = rT (v) for all v in V and all scalars r. T (v) is
Rm and shown
called the image of v under T . We have already studied linear transformation T : Rn
→
(in Section 2.6) that they are all given by multiplication by a uniquely determined m
n matrix A; that
×
is T (x) = Ax for all x in Rn. In the case of linear operators R2
R2, this yields an important way to
describe geometric functions such as rotations about the origin and reﬂections in a line through the origin.
In the present chapter we will describe linear transformations in general, introduce the kernel and
image of a linear transformation, and prove a useful result (called the dimension theorem) that relates the
dimensions of the kernel and image, and uniﬁes and extends several earlier results. Finally we study the
notion of isomorphic vector spaces, that is, spaces that are identical except for notation, and relate this to
composition of transformations that was introduced in Section 2.3.

→

→

→

→

7.1

Examples and Elementary Properties

Deﬁnition 7.1 Linear Transformations of Vector Spaces

IfV andW are twovectorspaces, a function T : V
alineartransformationifitsatisﬁesthefollowingaxioms.

→

W iscalled

T

v

V

T (v)

W

T1.
T2.

T (v+ v1) = T (v) + T (v1)
T (rv) = rT (v)

forallvandv1 inV .
forallvinV and r in R.

A linear transformation T : V
visualizedasinthediagram.

→

V is called a linear operator on V. The situationcan be

Axiom T1 is just the requirement that T preserves vector addition. It asserts that the result T (v + v1)
of adding v and v1 ﬁrst and then applying T is the same as applying T ﬁrst to get T (v) and T (v1) and
then adding. Similarly, axiom T2 means that T preserves scalar multiplication. Note that, even though the
additions in axiom T1 are both denoted by the same symbol +, the addition on the left forming v + v1 is
carried out in V , whereas the addition T (v) + T (v1) is done in W . Similarly, the scalar multiplications rv
and rT (v) in axiom T2 refer to the spaces V and W , respectively.

We have already seen many examples of linear transformations T : Rn

in Rn as columns, Theorem 2.6.2 shows that, for each such T , there is an m
T (x) = Ax for every x in Rn. Moreover, the matrix A is given by A =
where
deﬁned by

e1, e2, . . . , en

is the standard basis of Rn. We denote this transformation by TA : Rn

T (e1) T (e2)

· · ·

×

{

}

(cid:2)

T (en)

Rm,
(cid:3)

→

Rm. In fact, writing vectors
n matrix A such that

→

TA(x) = Ax for all x in Rn

Example 7.1.1 lists three important linear transformations that will be referred to later. The veriﬁcation

of axioms T1 and T2 is left to the reader.

371

372

Linear Transformations

Example 7.1.1

If V and W are vector spaces, the following are linear transformations:

Identity operator V
→
Zero transformation V
Scalar operator V
V

→

V

→

1V : V
W 0 : V
a : V

→
→
→

V where 1V (v) = v for all v in V
W where 0(v) = 0 for all v in V
V where a(v) = av for all v in V
(Here a is any real number.)

The symbol 0 will be used to denote the zero transformation from V to W for any spaces V and W . It

was also used earlier to denote the zero function [a, b]

R.

→

The next example gives two important transformations of matrices. Recall that the trace tr A of an
n matrix A is the sum of the entries on the main diagonal.

n

×

Example 7.1.2

Show that the transposition and trace are linear transformations. More precisely,

→
→
are both linear transformations.

R : Mmn
S : Mmn

Mnm
R

where R(A) = AT for all A in Mmn
where S(A) = tr A for all A in Mnn

Solution. Axioms T1 and T2 for transposition are (A + B)T = AT + BT and (rA)T = r(AT ),
respectively (using Theorem 2.1.2). The veriﬁcations for the trace are left to the reader.

Example 7.1.3

If a is a scalar, deﬁne Ea : Pn
linear transformation (called evaluation at a).

→

R by Ea(p) = p(a) for each polynomial p in Pn. Show that Ea is a

Solution. If p and q are polynomials and r is in R, we use the fact that the sum p + q and scalar
product rp are deﬁned as for functions:

(p + q)(x) = p(x) + q(x)

and

(rp)(x) = rp(x)

for all x. Hence, for all p and q in Pn and all r in R:

Ea(p + q) = (p + q)(a) = p(a) + q(a) = Ea(p) + Ea(q),

and

Ea(rp) = (rp)(a) = rp(a) = rEa(p).

Hence Ea is a linear transformation.

The next example involves some calculus.

7.1. Examples and Elementary Properties

373

Example 7.1.4

Show that the differentiation and integration operations on Pn are linear transformations. More
precisely,

D : Pn

I : Pn

→

→

are linear transformations.

Pn

1 where D [p(x)] = p′(x) for all p(x) in Pn
x

−

Pn+1 where I [p(x)] =

0

Z

p(t)dt for all p(x) in Pn

Solution. These restate the following fundamental properties of differentiation and integration.

[p(x) + q(x)]′ = p′(x) + q′(x)

and

[rp(x)]′ = (rp)′(x)

x
0 [p(t) + q(t)]dt =

x
0 p(t)dt +

R

R

x
0 q(t)dt
R

and

x
0 rp(t)dt = r

R

x
0 p(t)dt
R

The next theorem collects three useful properties of all linear transformations. They can be described
by saying that, in addition to preserving addition and scalar multiplication (these are the axioms), linear
transformations preserve the zero vector, negatives, and linear combinations.

Theorem 7.1.1

Let T : V

→

W bealineartransformation.

1. T (0) = 0.

2. T (

v) =

−

T (v) forallvinV.

−

3. T (r1v1 + r2v2 +

· · ·

+ rkvk) = r1T (v1) + r2T (v2) +

· · ·

+ rkT (vk) forallvi inV andall ri in R.

Proof.

1. T (0) = T (0v) = 0T (v) = 0 for any v in V .

2. T (

v) = T [(

−

1)v] = (

−

1)T (v) =

−

T (v) for any v in V .

−

3. The proof of Theorem 2.6.1 goes through.

The ability to use the last part of Theorem 7.1.1 effectively is vital to obtaining the beneﬁts of linear

transformations. Example 7.1.5 and Theorem 7.1.2 provide illustrations.

Example 7.1.5

Let T : V
T (v1) in terms of w and w1.

→

W be a linear transformation. If T (v

3v1) = w and T (2v

−

−

v1) = w1, ﬁnd T (v) and

Solution. The given relations imply that

T (v)

−

3T (v1) = w

374

Linear Transformations

2T (v)

−

T (v1) = w1

by Theorem 7.1.1. Subtracting twice the ﬁrst from the second gives T (v1) = 1
substitution gives T (v) = 1

w).

5(w1

2w). Then

−

5(3w1

−

The full effect of property (3) in Theorem 7.1.1 is this: If T : V

W is a linear transformation and
v1, v2, . . . , vn
T (v1), T (v2), . . . , T (vn) are known, then T (v) can be computed for every vector v in span
}
spans V , then T (v) is determined for all v in V by the choice of
In particular, if
T (v1), T (v2), . . . , T (vn). The next theorem states this somewhat differently. As for functions in gen-
W are called equal (written T = S) if they have
W and S : V
eral, two linear transformations T : V
the same action; that is, if T (v) = S(v) for all v in V .

v1, v2, . . . , vn

→

→

→

{

}

{

.

Theorem 7.1.2

Let T : V
V = span

W and S : V
→
v1, v2, . . . , vn
{

W betwolineartransformations. Supposethat

→
. IfT(vi) = S(vi) foreach i,then T = S.
}

Proof. If v is any vector in V = span
is in R. Since T (vi) = S(vi) for each i, Theorem 7.1.1 gives

v1, v2, . . . , vn

}

{

, write v = a1v1 + a2v2 +

+ anvn where each ai

· · ·

T (v) = T (a1v1 + a2v2 +

+ anvn)

· · ·

= a1T (v1) + a2T (v2) +
= a1S(v1) + a2S(v2) +
= S(a1v1 + a2v2 +
= S(v)

· · ·

· · ·
· · ·
+ anvn)

+ anT (vn)
+ anS(vn)

Since v was arbitrary in V , this shows that T = S.

Example 7.1.6

Let V = span
}
show that T = 0, the zero transformation from V to W .

v1, . . . , vn

. Let T : V

→

{

W be a linear transformation. If T (v1) =

= T (vn) = 0,

· · ·

Solution. The zero transformation 0 : V
so T (vi) = 0(vi) holds for each i. Hence T = 0 by Theorem 7.1.2.

→

W is deﬁned by 0(v) = 0 for all v in V (Example 7.1.1),

Theorem 7.1.2 can be expressed as follows: If we know what a linear transformation T : V

W does
to each vector in a spanning set for V , then we know what T does to every vector in V . If the spanning set
is a basis, we can say much more.

→

Theorem 7.1.3

b1, b2, . . . , bn
LetV andW bevectorspacesandlet
{
w1, w2, . . . , wn inW (theyneednotbedistinct),thereexistsauniquelineartransformation

beabasisofV. Givenanyvectors

}

7.1. Examples and Elementary Properties

375

W satisfying T (bi) = wi foreach i = 1, 2, . . . , n. Infact,theactionof T isasfollows:

T : V
Givenv= v1b1 + v2b2 +

→

+ vnbn inV, vi in R,then

· · ·

T (v) = T (v1b1 + v2b2 +

+ vnbn) = v1w1 + v2w2 +

+ vnwn.

· · ·

· · ·

Proof. If a transformation T does exist with T (bi) = wi for each i, and if S is any other such transformation,
then T (bi) = wi = S(bi) holds for each i, so S = T by Theorem 7.1.2. Hence T is unique if it exists, and
it remains to show that there really is such a linear transformation. Given v in V , we must specify T (v) in
+ vnbn, where v1, . . . , vn are uniquely
W . Because
determined by v (this is Theorem 6.3.1). Hence we may deﬁne T : V

is a basis of V , we have v = v1b1 +

b1, . . . , bn

W by

· · ·

{

}

→

T (v) = T (v1b1 + v2b2 +

+ vnbn) = v1w1 + v2w2 +

+ vnwn

· · ·

· · ·

for all v = v1b1 +
left to the reader.

· · ·

+ vnbn in V . This satisﬁes T (bi) = wi for each i; the veriﬁcation that T is linear is

This theorem shows that linear transformations can be deﬁned almost at will: Simply specify where
the basis vectors go, and the rest of the action is dictated by the linearity. Moreover, Theorem 7.1.2 shows
that deciding whether two linear transformations are equal comes down to determining whether they have
of a vector space V , there is a different
the same effect on the basis vectors. So, given a basis
}
W for every ordered selection w1, w2, . . . , wn of vectors in W (not necessarily
linear transformation V
distinct).

b1, . . . , bn

→

{

Example 7.1.7

Find a linear transformation T : P2

M22 such that

→

T (1 + x) =

1 0
0 0

,

(cid:21)

(cid:20)

T (x + x2) =

0 1
1 0

,

(cid:21)

(cid:20)

and

T (1 + x2) =

0 0
0 1

.

(cid:21)

(cid:20)

Solution. The set
linear combination of these vectors. In fact

1 + x, x + x2, 1 + x2

{

}

is a basis of P2, so every vector p = a + bx + cx2 in P2 is a

p(x) = 1

2(a + b

c)(1 + x) + 1
2 (

−

Hence Theorem 7.1.3 gives

−

a + b + c)(x + x2) + 1

2 (a

b + c)(1 + x2)

−

T [p(x)] = 1

2 (a + b

c)

−
(cid:20)
c
a + b
a + b + c

−

1 0
0 0

+ 1
2(

(cid:21)

−
a + b + c
b + c
a

−

(cid:21)

−

= 1
2

(cid:20)

−

a + b + c)

0 1
1 0

(cid:21)

(cid:20)

+ 1

2(a

−

b + c)

0 0
0 1

(cid:20)

(cid:21)

376

Linear Transformations

Exercises for 7.1

Exercise 7.1.1 Show that each of the following func-
tions is a linear transformation.

c. If T : R2

a. T : R2
x axis)

→

R2; T (x, y) = (x,

y) (reﬂection in the

−

1
1

T

(cid:20)

, ﬁnd T

0
1

(cid:21)

(cid:21)

(cid:20)

b. T : R3

R3; T (x, y, z) = (x, y,

→
the x-y plane)

z) (reﬂection in

−

c. T : C

→
d. T : Mmn
Q an n

×
e. T : Mnn

→

f. T : Pn

g. T : Pn

h. T : Rn

i. T : Pn

C; T (z) = z (conjugation)

Mkl; T (A) = PAQ, P a k

→
l matrix, both ﬁxed

×

m matrix,

Mnn; T (A) = AT + A

→
R; T [p(x)] = p(0)

R; T (r0 + r1x +

+ rnxn) = rn

· · ·

R; T (x) = x

·

z, z a ﬁxed vector in Rn

Pn; T [p(x)] = p(x + 1)

→

→

→

j. T : Rn
where

V ; T (r1,

→
e1, . . . , en
{

}

, rn) = r1e1 +
· · ·
is a ﬁxed basis of V

· · ·

+ rnen

k. T : V

R; T (r1e1 +

+ rnen) = r1, where

→

e1, . . . , en
{

}

is a ﬁxed basis of V

· · ·

Exercise 7.1.2 In each case, show that T is not a linear
transformation.

a. T : Mnn

→

R; T (A) = det A

b. T : Mnm

c. T : R

→

R; T (A) = rank A

→
R; T (x) = x2

d. T : V

V ; T (v) = v + u where u
vector in V (T is called the translation by u)

= 0 is a ﬁxed

→

Exercise 7.1.3 In each case, assume that T is a linear
transformation.

R2 and T

→

=

→

=

1
3

(cid:20)

(cid:20)
1
1

−

(cid:20)

=

(cid:21)
−

1
3

1
1

,

(cid:21)

(cid:20)

.

(cid:21)

0
1

,

(cid:21)

(cid:20)
.

=

(cid:21)
1
7

d. If T : R2

R2 and T

T

1
1

1
0

, ﬁnd T

(cid:21)
(cid:20)
e. If T : P2

(cid:20)

(cid:21)

(cid:20)
P2 and T (x + 1) = x, T (x

−

(cid:21)

→
T (x2) = 0, ﬁnd T (2 + 3x

x2).

1) = 1,

−

−
R and T (x + 2) = 1, T (1) = 5,

f. If T : P2

T (x2 + x) = 0, ﬁnd T (2

→

x + 3x2).

−

Exercise 7.1.4 In each case, ﬁnd a linear transformation
with the given properties and compute T (v).

a. T : R2
T (

R3; T (1, 2) = (1, 0, 1),
1, 0) = (0, 1, 1); v = (2, 1)

→

−

b. T : R2

R3; T (2,
T (1, 1) = (0, 1, 0); v = (

→

−

1) = (1,

−
1, 2)

1, 1),

−

c. T : P2
T (x

−

P3; T (x2) = x3, T (x + 1) = 0,

→
1) = x; v = x2 + x + 1

d. T : M22

→

R; T

1 0
0 0

= 3, T

0 1
1 0

=

1,

−

1 0
1 0

T

(cid:20)

(cid:21)

(cid:20)
= 0 = T

(cid:21)
0 0
0 1

(cid:20)
; v =

(cid:21)
a b
c d

(cid:20)

(cid:21)

(cid:20)

(cid:21)

Exercise 7.1.5 If T : V
ﬁnd T (v) and T (w) if:

→

V is a linear transformation,

a. T (v + w) = v

−
b. T (v + 2w) = 3v

2w and T (2v

w and T (v

−

−

−

w) = 2v

w) = 2v

4w

−

Exercise 7.1.6 If T : V
show that T (v
V .

−

→
v1) = T (v)

−

W is a linear transformation,
T (v1) for all v and v1 in

a. If T : V
T (3v1

−
b. If T : V

T (3v1 + 2v2).

→

R and T (v1) = 1, T (v2) =

→
5v2).

R and T (v1) = 2, T (v2) =

1, ﬁnd

3, ﬁnd

−

−

e1, e2
{

be the standard basis of R2.
Exercise 7.1.7 Let
}
Is it possible to have a linear transformation T such that
T (e1) lies in R while T (e2) lies in R2? Explain your an-
swer.

6
Exercise 7.1.8 Let
T : V

v1, . . . , vn
{

}

V be a linear transformation.

be a basis of V and let

→

a. If T (vi) = vi for each i, show that T = 1V .

b. If T (vi) =

vi for each i, show that T =

scalar operator (see Example 7.1.1).

−

1 is the

−

Exercise 7.1.9 If A is an m
column k of A. Show that Ck : Mmn
transformation for each k = 1, . . . , n.

×

n matrix, let Ck(A) denote
Rm is a linear

→

Exercise 7.1.10 Let
Given k, 1
≤
Pk(r1e1 +
· · ·
formation for each k.

e1, . . . , en
{
n, deﬁne Pk : Rn

}
k
→
+ rnen) = rkek. Show that Pk a linear trans-

be a basis of Rn.
Rn by

≤

W and T : V

Exercise 7.1.11 Let S : V
transformations. Given a in R, deﬁne functions
W by (S + T )(v) =
(S + T ) : V
W and (aT ) : V
S(v) + T (v) and (aT )(v) = aT (v) for all v in V . Show
that S + T and aT are linear transformations.

W be linear

→

→

→

→

Exercise 7.1.12 Describe all linear transformations
T : R

V .

→

Exercise 7.1.13 Let V and W be vector spaces, let V
be ﬁnite dimensional, and let v
= 0 in V . Given any
w in W , show that there exists a linear transformation
W with T (v) = w. [Hint: Theorem 6.4.1 and
T : V
→
Theorem 7.1.3.]
Exercise 7.1.14 Given y in Rn, deﬁne Sy : Rn
y for all x in Rn (where
Sy(x) = x
introduced in Section 5.3).

R by
is the dot product

→

·

·

7.1. Examples and Elementary Properties

377

Exercise 7.1.16 Show that differentiation is the only lin-
1
ear transformation Pn
for each k = 0, 1, 2, . . . , n.

Pn that satisﬁes T (xk) = kxk

→

−

Exercise 7.1.17 Let T : V
tion and let v1, . . . , vn denote vectors in V .

→

W be a linear transforma-

is linearly independent,

a. If

T (v1),
{
show that

. . . , T (vn)
}
v1, . . . , vn
is also independent.
}
{
b. Find T : R2
(a) is false.

R2 for which the converse of part

→

Exercise 7.1.18 Suppose T : V
V is a linear operator
with the property that T [T (v)] = v for all v in V . (For
example, transposition in Mnn or conjugation in C.) If
v
v, T (v)
is linearly independent
}
{
= v and T (v)
if and only if T (v)
=

= 0 in V , show that

→

v.

Exercise 7.1.19
Ta, b : C

−
If a and b are real numbers, deﬁne
C by Ta, b(r + si) = ra + sbi for all r + si in C.

→

a. Show that Ta, b is linear and Ta, b(z) = Ta, b(z) for
all z in C. (Here z denotes the conjugate of z.)

b. If T : C

C is linear and T (z) = T (z) for all z in

C, show that T = Ta, b for some real a and b.

→

Exercise 7.1.20 Show that the following conditions are
equivalent for a linear transformation T : M22

M22.

→

a. Show that Sy : Rn
for any y in Rn.

→

R is a linear transformation

1.

tr [T (A)] = tr A for all A in M22.

b. Show that every linear transformation T : Rn

R
arises in this way; that is, T = Sy for some y in Rn.
is the standard basis of Rn,
[Hint: If
write Sy(ei) = yi for each i. Use Theorem 7.1.1.]

e1, . . . , en
{

→

}

2. T

r11
r21

(cid:20)

r12
r22 (cid:21)

r22B22 for matrices Bi j such that
tr B11 = 1 = tr B22 and tr B12 = 0 = tr B21.

= r11B11 + r12B12 + r21B21 +

Exercise 7.1.15 Let T : V
tion.

→

W be a linear transforma-

Exercise 7.1.21 Given a in R, consider the evaluation
R deﬁned in Example 7.1.3.
map Ea : Pn

→

a. If U is a subspace of V , show that

T (u)
T (U ) =
{
the image of U under T ).

u in U

}

|

is a subspace of W (called

b. If P is a subspace of W , show that

v in V
{
preimage of P under T ).

T (v) in P

}

|

is a subspace of V (called the

a. Show that Ea is a linear transformation satisfy-
ing the additional condition that Ea(xk) = [Ea(x)]k
holds for all k = 0, 1, 2, . . . . [Note: x0 = 1.]

b. If T : Pn

R is a linear transformation satisfying
T (xk) = [T (x)]k for all k = 0, 1, 2, . . . , show that
T = Ea for some a in R.

→

6
6
6
6
378

Linear Transformations

→

R is any linear transfor-
Exercise 7.1.22 If T : Mnn
mation satisfying T (AB) = T (BA) for all A and B in Mnn,
show that there exists a number k such that T (A) = k tr A
for all A. (See Lemma 5.5.1.) [Hint: Let Ei j denote the
j) position and zeros else-
n
×
where.

n matrix with 1 in the (i,

Show that EikEl j =

show that T (Ei j) = 0 if i
T (E11) = T (E22) =
Ei j
use the fact that
{

· · ·
1
|

. Use this to

if k
= l
if k = l

0
Ei j
(cid:26)
= j and
= T (Enn). Put k = T (E11) and

i, j

n
}

≤

is a basis of Mnn.]

≤

C be a linear transforma-
Exercise 7.1.23 Let T : C
→
tion of the real vector space C and assume that T (a) = a
for every real number a. Show that the following are
equivalent:

a. T (zw) = T (z)T (w) for all z and w in C.

b. Either T = 1C or T (z) = z for each z in C (where

z denotes the conjugate).

7.2 Kernel and Image of a Linear Transformation

This section is devoted to two important subspaces associated with a linear transformation T : V

W .

→

Deﬁnition 7.2 Kernel and Image of a Linear Transformation

Thekernelof T (denoted ker T)andtheimageof T (denoted im T or T (V ))aredeﬁnedby

ker T =
im T =

vinV
{
T (v)
{

|

T (v) = 0
}
= T (V )

|
vinV

}

The kernel of T is often called the nullspace of T because it consists of all
vectors v in V satisfying the condition that T (v) = 0. The image of T is
often called the range of T and consists of all vectors w in W of the form
w = T (v) for some v in V . These subspaces are depicted in the diagrams.

Example 7.2.1
Let TA : Rn
m

→

×

Rm be the linear transformation induced by the

n matrix A, that is TA(x) = Ax for all columns x in Rn. Then

ker TA =
im TA =

x
{
|
Ax
{

Ax = 0
}
x in Rn
}

|

= null A
= im A

and

T

T

ker T

V

V

W

0

im T

W

Hence the following theorem extends Example 5.1.2.

6
6
7.2. Kernel and Image of a Linear Transformation

379

Theorem 7.2.1

Let T : V

→

W bealineartransformation.

1. ker T isasubspaceofV.

2. im T isasubspaceofW.

Proof. The fact that T (0) = 0 shows that ker T and im T contain the zero vector of V and W respectively.

1. If v and v1 lie in ker T , then T (v) = 0 = T (v1), so

T (v + v1) = T (v) + T (v1) = 0 + 0 = 0

T (rv) = rT (v) = r0 = 0

for all r in R

Hence v + v1 and rv lie in ker T (they satisfy the required condition), so ker T is a subspace of V
by the subspace test (Theorem 6.2.1).

2. If w and w1 lie in im T , write w = T (v) and w1 = T (v1) where v, v1

V . Then

∈

w + w1 = T (v) + T (v1) = T (v + v1)

rw = rT (v) = T (rv)

for all r in R

Hence w + w1 and rw both lie in im T (they have the required form), so im T is a subspace of W .

Given a linear transformation T : V

W :

→

dim ( ker T ) is called the nullity of T and denoted as nullity (T )
dim ( im T ) is called the rank of T and denoted as rank (T )

The rank of a matrix A was deﬁned earlier to be the dimension of col A, the column space of A. The two
usages of the word rank are consistent in the following sense. Recall the deﬁnition of TA in Example 7.2.1.

Example 7.2.2

Given an m

×

n matrix A, show that im TA = col A, so rank TA = rank A.

cn

in terms of its columns. Then

Solution. Write A =

(cid:2)

c1

· · ·
im TA =

}
using Deﬁnition 2.5. Hence im TA is the column space of A; the rest follows.

· · ·

{

{

}

|

=

x1c1 +

+ xncn

xi in R

x in Rn

(cid:3)
Ax
|

Often, a useful way to study a subspace of a vector space is to exhibit it as the kernel or image of a

linear transformation. Here is an example.

380

Linear Transformations

Example 7.2.3

Deﬁne a transformation P : Mnn
and that:

→

Mnn by P(A) = A

−

AT for all A in Mnn. Show that P is linear

a. ker P consists of all symmetric matrices.

b.

im P consists of all skew-symmetric matrices.

Solution. The veriﬁcation that P is linear is left to the reader. To prove part (a), note that a matrix
AT , and this occurs if and only if A = AT —that is, A is
A lies in ker P just when 0 = P(A) = A
symmetric. Turning to part (b), the space im P consists of all matrices P(A), A in Mnn. Every such
matrix is skew-symmetric because

−

P(A)T = (A

AT )T = AT

−

A =

−

P(A)

−

On the other hand, if S is skew-symmetric (that is, ST =

S), then S lies in im P. In fact,

P

1
2 S

= 1
2 S

−

T

1
2S

= 1

2(S

−

(cid:2)

(cid:3)

(cid:2)

(cid:3)

−
ST ) = 1

2(S + S) = S

One-to-One and Onto Transformations

Deﬁnition 7.3 One-to-one and Onto Linear Transformations

Let T : V

→

W bealineartransformation.

1. T issaidtobeontoif im T = W.

2. T issaidtobeone-to-oneif T (v) = T (v1) impliesv= v1.

A vector w in W is said to be hit by T if w = T (v) for some v in V . Then T is onto if every vector in W
is hit at least once, and T is one-to-one if no element of W gets hit twice. Clearly the onto transformations
T are those for which im T = W is as large a subspace of W as possible. By contrast, Theorem 7.2.2
shows that the one-to-one transformations T are the ones with ker T as small a subspace of V as possible.

Theorem 7.2.2

If T : V

→

W isalineartransformation,then T isone-to-oneifandonlyif ker T =

0
.
}
{

Proof. If T is one-to-one, let v be any vector in ker T . Then T (v) = 0, so T (v) = T (0). Hence v = 0
because T is one-to-one. Hence ker T =
0
Conversely, assume that ker T =
}
{
v1 lies in ker T =
T (v1) = 0, so v
−

0
{
and let T (v) = T (v1) with v and v1 in V . Then

v1 = 0, so v = v1,

. This means that v

T (v
proving that T is one-to-one.

v1) = T (v)

−

−

−

}

}

{

0

.

7.2. Kernel and Image of a Linear Transformation

381

Example 7.2.4

The identity transformation 1V : V

→

V is both one-to-one and onto for any vector space V .

Example 7.2.5

Consider the linear transformations

S : R3
T : R2

R2
R3

→

→

given by S(x, y, z) = (x + y, x

given by T (x, y) = (x + y, x

−

y)

−
y, x)

Show that T is one-to-one but not onto, whereas S is onto but not one-to-one.

Solution. The veriﬁcation that they are linear is omitted. T is one-to-one because

ker T =

(x, y)

{

|

x + y = x

y = x = 0

(0, 0)

=

}

{

}

−

However, it is not onto. For example (0, 0, 1) does not lie in im T because if
(0, 0, 1) = (x + y, x
Turning to S, it is not one-to-one by Theorem 7.2.2 because (0, 0, 1) lies in ker S. But every
element (s, t) in R2 lies in im S because (s, t) = (x + y, x
fact, x = 1

−
t), and z = 0). Hence S is onto.

y, x) for some x and y, then x + y = 0 = x

y and x = 1, an impossibility.

y) = S(x, y, z) for some x, y, and z (in

−

−

2 (s + t), y = 1

2 (s

−

Example 7.2.6

Let U be an invertible m

m matrix and deﬁne

×
T : Mmn

Mmn

by T (X ) = U X for all X in Mmn

→

Show that T is a linear transformation that is both one-to-one and onto.

Solution. The veriﬁcation that T is linear is left to the reader. To see that T is one-to-one, let
T (X ) = 0. Then U X = 0, so left-multiplication by U −
one-to-one. Finally, if Y is any member of Mmn, then U −
T (U −

1 gives X = 0. Hence ker T =
1Y lies in Mmn too, and

1Y ) = Y . This shows that T is onto.

1Y ) = U (U −

{

}

0

, so T is

The linear transformations Rn

n matrix A (Theorem 2.6.2).
The next theorem gives conditions under which they are onto or one-to-one. Note the connection with
Theorem 5.4.3 and Theorem 5.4.4.

Rm all have the form TA for some m

→

×

382

Linear Transformations

Theorem 7.2.3

Let A bean m
TA(x) = Axforallcolumnsxin Rn.

n matrix,andlet TA : Rn

×

Rm bethelineartransformationinducedby A,thatis

→

1. TA isontoifandonlyif rank A = m.

2. TA isone-to-oneifandonlyif rank A = n.

Proof.

1. We have that im TA is the column space of A (see Example 7.2.2), so TA is onto if and only if the
column space of A is Rm. Because the rank of A is the dimension of the column space, this holds if
and only if rank A = m.

2. ker TA =

x in Rn

Ax = 0

, so (using Theorem 7.2.2) TA is one-to-one if and only if Ax = 0 implies

x = 0. This is equivalent to rank A = n by Theorem 5.4.3.

{

|

}

The Dimension Theorem

n matrix of rank r and let TA : Rn

Rm denote the corresponding matrix transfor-
Let A denote an m
mation given by TA(x) = Ax for all columns x in Rn. It follows from Example 7.2.1 and Example 7.2.2
that im TA = col A, so dim ( im TA) = dim ( col A) = r. On the other hand Theorem 5.4.2 shows that
dim ( ker TA) = dim ( null A) = n

r. Combining these we see that

→

×

−

dim ( im TA) + dim ( ker TA) = n for every m

n matrix A

×

The main result of this section is a deep generalization of this observation.

Theorem 7.2.4: Dimension Theorem

Let T : V
dimensional. ThenV isalsoﬁnitedimensionaland

→

W beanylineartransformationandassumethat ker T and im T arebothﬁnite

dim V = dim ( ker T ) + dim ( im T )

Inotherwords, dim V = nullity (T ) + rank (T ).

Proof. Every vector in im T = T (V ) has the form T (v) for some v in V . Hence let
be a basis of im T , where the ei lie in V . Let
and dim ( ker T ) = k, so it sufﬁces to show that B =

T (e1), T (e2), . . . , T (er)
be any basis of ker T . Then dim ( im T ) = r
is a basis of V .

e1, . . . , er, f1, . . . , fk

f1, f2, . . . , fk

{

}

{

}

{

}

1. B spans V . If v lies in V , then T (v) lies in im V , so

T (v) = t1T (e1) + t2T (e2) +

+ trT (er)

ti in R

· · ·

This implies that v
−
Hence v is a linear combination of the vectors in B.

− · · · −

t2e2

t1e1

−

trer lies in ker T and so is a linear combination of f1, . . . , fk.

7.2. Kernel and Image of a Linear Transformation

383

2. B is linearly independent. Suppose that ti and s j in R satisfy

t1e1 +

· · ·

+ trer + s1f1 +

+ skfk = 0

· · ·

(7.1)

Applying T gives t1T (e1) +
T (e1), . . . , T (er)
of

· · ·
yields t1 =

{

}

= tr = 0. But then (7.1) becomes

· · ·

+trT (er) = 0 (because T (fi) = 0 for each i). Hence the independence

so s1 =

· · ·

= sk = 0 by the independence of

{

. This proves that B is linearly independent.

}

s1f1 +

+ skfk = 0

· · ·
f1, . . . , fk

Note that the vector space V is not assumed to be ﬁnite dimensional in Theorem 7.2.4. In fact, verify-
ing that ker T and im T are both ﬁnite dimensional is often an important way to prove that V is ﬁnite
dimensional.

Note further that r + k = n in the proof so, after relabelling, we end up with a basis

B =

{

e1, e2,

. . . , er, er+1,

. . . , en

}

er+1, . . . , en

of V with the property that
In fact, if V is known in advance to be ﬁnite dimensional, then any basis
extended to a basis
matter how this is done, the vectors
we record it for reference. The proof is much like that of Theorem 7.2.4 and is left as Exercise 7.2.26.

is a basis of im T .
of ker T can be
of V by Theorem 6.4.1. Moreover, it turns out that, no
will be a basis of im T . This result is useful, and

e1, e2, . . . , er, er+1, . . . , en

is a basis of ker T and

}
er+1, . . . , en

T (e1), . . . , T (er)

T (e1), . . . , T (er)

}

{

}

}

}

{

{

{

{

Theorem 7.2.5

Let T : V
→
er+1, . . . , en
that
{
r = rank T.

}

W bealineartransformation,andlet
isabasisof ker T. Then

e1, . . . , er, er+1, . . . , en
{

}

T (e1), . . . , T (er)

isabasisof im T,andhence

beabasisofV such

{

}

The dimension theorem is one of the most useful results in all of linear algebra.

It shows that if
either dim ( ker T ) or dim ( im T ) can be found, then the other is automatically known. In many cases it is
easier to compute one than the other, so the theorem is a real asset. The rest of this section is devoted to
illustrations of this fact. The next example uses the dimension theorem to give a different proof of the ﬁrst
part of Theorem 5.4.2.

Example 7.2.7

Let A be an m
Ax = 0 of m homogeneous equations in n variables has dimension n

n matrix of rank r. Show that the space null A of all solutions of the system

×

r.

−

Solution. The space in question is just ker TA, where TA : Rn
all columns x in Rn. But dim ( im TA) = rank TA = rank A = r by Example 7.2.2, so
dim ( ker TA) = n

r by the dimension theorem.

→

Rm is deﬁned by TA(x) = Ax for

−

384

Linear Transformations

Example 7.2.8

If T : V

→

W is a linear transformation where V is ﬁnite dimensional, then

dim ( ker T )

≤

dim V and

dim ( im T )

dim V

≤

Indeed, dim V = dim ( ker T ) + dim ( im T ) by Theorem 7.2.4. Of course, the ﬁrst inequality also
follows because ker T is a subspace of V .

Example 7.2.9

Let D : Pn
Pn
hence conclude that D is onto.

→

−

1 be the differentiation map deﬁned by D [p(x)] = p′(x). Compute ker D and

Solution. Because p′(x) = 0 means p(x) is constant, we have dim ( ker D) = 1. Since
dim Pn = n + 1, the dimension theorem gives

This implies that im D = Pn

dim ( im D) = (n + 1)

−
1, so D is onto.

−

dim ( ker D) = n = dim (Pn

1)

−

Of course it is not difﬁcult to verify directly that each polynomial q(x) in Pn

1 is the derivative of some
polynomial in Pn (simply integrate q(x)!), so the dimension theorem is not needed in this case. However,
in some situations it is difﬁcult to see directly that a linear transformation is onto, and the method used in
Example 7.2.9 may be by far the easiest way to prove it. Here is another illustration.

−

Example 7.2.10

Given a in R, the evaluation map Ea : Pn
and onto, and hence conclude that
subspace of all polynomials p(x) for which p(a) = 0.

→
a), (x

(x

−

−

{

R is given by Ea [p(x)] = p(a). Show that Ea is linear

a)2, . . . , (x

a)n

is a basis of ker Ea, the

}

−

Solution. Ea is linear by Example 7.1.3; the veriﬁcation that it is onto is left to the reader. Hence
dim ( im Ea) = dim (R) = 1, so dim ( ker Ea) = (n + 1)
1 = n by the dimension theorem. Now
each of the n polynomials (x
linearly independent (they have distinct degrees). Hence they are a basis because dim ( ker Ea) = n.

a)n clearly lies in ker Ea, and they are

a)2, . . . , (x

a), (x

−

−

−

−

We conclude by applying the dimension theorem to the rank of a matrix.

Example 7.2.11

If A is any m

×

n matrix, show that rank A = rank AT A = rank AAT .

Solution. It sufﬁces to show that rank A = rank AT A (the rest follows by replacing A with AT ).
Write B = AT A, and consider the associated matrix transformations

TA : Rn

→

Rm and TB : Rn

Rn

→

7.2. Kernel and Image of a Linear Transformation

385

The dimension theorem and Example 7.2.2 give

rank A = rank TA = dim ( im TA) = n
rank B = rank TB = dim ( im TB) = n

dim ( ker TA)
dim ( ker TB)

−
−

so it sufﬁces to show that ker TA = ker TB. Now Ax = 0 implies that Bx = AT Ax = 0, so ker TA is
contained in ker TB. On the other hand, if Bx = 0, then AT Ax = 0, so

k
This implies that Ax = 0, so ker TB is contained in ker TA.

k

Ax

2 = (Ax)T (Ax) = xT AT Ax = xT 0 = 0

Exercises for 7.2

Exercise 7.2.1 For each matrix A, ﬁnd a basis for the
kernel and image of TA, and ﬁnd the rank and nullity of
TA.

a.

c.

1
3
1

1
3
4
0











2
1
3

−
2
1
1
−
2





−

1 1
0 2
2 0

1
2
5
2

−

−







2 1
1 0
1 1

2
1
1
0

−

1
1
2
3

b.

d.











1 3
3 1
4 2

−

−





0
3
3
6

−
−







Exercise 7.2.2 In each case, (i) ﬁnd a basis of ker T ,
and (ii) ﬁnd a basis of im T . You may assume that T is
linear.

a. T : P2

b. T : P2

c. T : R3

d. T : R3

→

→

→

→

R2; T (a + bx + cx2) = (a, b)

R2; T (p(x)) = (p(0), p(1))

R3; T (x, y, z) = (x + y, x + y, 0)

R4; T (x, y, z) = (x, x, y, y)

e. T : M22

M22; T

→

a b
c d

(cid:20)

=

(cid:21)

(cid:20)

a + b b + c
c + d d + a

(cid:21)

f. T : M22

R; T

→

a b
c d

= a + d

(cid:21)
(cid:20)
R; T (r0 + r1x +

+ rnxn) = rn

· · ·
R; T (r1, r2, . . . , rn) = r1 + r2 +

g. T : Pn

h. T : Rn

→

→

+ rn

· · ·

i. T : M22

M22; T (X ) = X A

→
0 1
1 0

(cid:21)

A =

(cid:20)

AX , where

−

j. T : M22

→

M22; T (X ) = X A, where A =

1 1
0 0

(cid:21)

(cid:20)

R be lin-
Exercise 7.2.3 Let P : V
→
ear transformations, where V is a vector space. Deﬁne
T : V

R2 by T (v) = (P(v), Q(v)).

R and Q : V

→

→

a. Show that T is a linear transformation.

b. Show that ker T = ker P

ker Q, the set of vec-

tors in both ker P and ker Q.

∩

Exercise 7.2.4 In each case, ﬁnd a basis
. . . , er, er+1,
e1,
B =
{
er+1, . . . , en
{
rem 7.2.5.

of V such that
}
is a basis of ker T , and verify Theo-

. . . , en

}

a. T : R3

R4; T (x, y, z) = (x

→

z, 2x + z, 2y

3z)

−

y + 2z, x + y

−

−

b. T : R3
3z, z

−

R4; T (x, y, z) = (x + y + z, 2x

→
3y, 3x + 4z)

y +

−

Exercise 7.2.5 Show that every matrix X in Mnn has the
form X = AT
2A for some matrix A in Mnn. [Hint: The
dimension theorem.]

−

Exercise 7.2.6 In each case either prove the statement
or give an example in which it is false. Throughout, let
T : V
W be a linear transformation where V and W are
ﬁnite dimensional.

→

386

Linear Transformations

a. If V = W , then ker T

im T .

⊆
b. If dim V = 5, dim W = 3, and dim ( ker T ) = 2,

then T is onto.

c. If dim V = 5 and dim W = 4, then ker T

=

0
{

.
}

d. If ker T = V , then W =

.
}
0
, then ker T = V .
}
{
f. If W = V , and im T

e. If W =

0
{

ker T , then T = 0.

⊆

g. If

e1, e2, e3
{

}

is a basis of V and

T (e1) = 0 = T (e2), then dim ( im T )

1.

≤

h. If dim ( ker T )

≤

dim W , then dim W

1
2 dim V .

≥
dim W .

i. If T is one-to-one, then dim V

≤

j. If dim V

dim W , then T is one-to-one.

≤
k. If T is onto, then dim V

dim W .

≥

dim W , then T is onto.

Exercise 7.2.9 Let T : V
V be a linear transformation
where V is ﬁnite dimensional. Show that exactly one of
(i) and (ii) holds: (i) T (v) = 0 for some v
= 0 in V ; (ii)
T (x) = v has a solution x in V for every v in V .

→

Exercise 7.2.10 Let T : Mnn
T (A) = tr A for all A in Mnn. Show that
dim ( ker T ) = n2
1.

→

−

R denote the trace map:

Exercise 7.2.11 Show that the following are equivalent
for a linear transformation T : V

W .

→

1.

3.

ker T = V

T = 0

2.

im T =

0
{

}

Exercise 7.2.12 Let A and B be m
n matri-
ces, respectively. Assume that Ax = 0 implies Bx = 0 for
every n-column x. Show that rank A
[Hint: Theorem 7.2.4.]

n and k

rank B.

×

≥

×

Exercise 7.2.13 Let A be an m
×
Thinking of Rn as rows, deﬁne V =
Show that dim V = m

r.

n matrix of rank r.
x in Rm
.
}
{

xA = 0

|

Exercise 7.2.14 Consider

−

a + c = b + d

(cid:21) (cid:12)
(cid:12)
(cid:12)
(cid:12)
R with S

(cid:27)
a b
c d

a. Consider S : M22

= a + c

→

−
d. Show that S is linear and onto and that V is

b
a subspace of M22. Compute dim V .

−

(cid:20)

(cid:21)

l. If dim V

m. If

≥
T (v1),
{
v1, . . . , vk
{
n. If

}
v1, . . . , vk
{

spans W .

. . . , T (vk)
}
is independent.

is independent,

then

a b
c d

V =

(cid:26)(cid:20)

spans V , then

T (v1), . . . , T (vk)
}
{

}

Exercise 7.2.7 Show that linear independence is pre-
served by one-to-one transformations and that spanning
sets are preserved by onto transformations. More pre-
W is a linear transformation, show that:
cisely, if T : V

→

a. If T is one-to-one and

in V , then
W .

T (v1), . . . , T (vn)
}
{

v1, . . . , vn
{

is independent
}
is independent in

v1,
b. If T is onto and V = span
{
T (v1), . . . , T (vn)
.
}
{

W = span

. . . , vn

, then
}

Exercise 7.2.8 Given
deﬁne T : Rn
→
Show that T is linear, and that:

v1, . . . , vn
{

V by T (r1, . . . , rn) = r1v1 +

}

in a vector space V ,
+ rnvn.

· · ·

a. T is one-to-one if and only if

dependent.

v1, . . . , vn
{

}

is in-

b. T is onto if and only if V = span

v1, . . . , vn
{

.
}

b. Consider T : V

R with T

→

a b
c d

(cid:20)

(cid:21)

= a + c.

Show that T is linear and onto, and use this in-
formation to compute dim ( ker T ).

Exercise 7.2.15 Deﬁne T : Pn
→
sum of all the coefﬁcients of p(x).

R by T [p(x)] = the

a. Use

the dimension theorem to show that

dim ( ker T ) = n.

b. Conclude that
of ker T .

x
{

−

1, x2

−

1, . . . , xn

1
}

−

is a basis

Exercise 7.2.16 Use the dimension theorem to prove
Theorem 1.3.1: If A is an m
n matrix with m < n, the
system Ax = 0 of m homogeneous equations in n vari-
ables always has a nontrivial solution.

×

Exercise 7.2.17 Let B be an n
sider the subspaces U =
V =

n matrix, and con-
and
. Show that dim U + dim V = mn.
}

A in Mmn, AB = 0
}

A in Mmn

AB
{

A
{

×

|

|

6
6
7.2. Kernel and Image of a Linear Transformation

387

Exercise 7.2.18 Let U and V denote, respectively, the
spaces of even and odd polynomials in Pn. Show that
dim U + dim V = n + 1. [Hint: Consider T : Pn
Pn
p(
where T [p(x)] = p(x)

x).]

→

−

−

Rn be a linear operator
Exercise 7.2.25 Let T : Rn
of rank 1, where Rn is written as rows. Show that there
exist numbers a1, a2, . . . , an and b1, b2, . . . , bn such that
T (X ) = X A for all rows X in Rn, where

→

Exercise 7.2.19 Show that every polynomial
Pn
1 can be written as f (x) = p(x + 1)
polynomial p(x) in Pn. [Hint: Deﬁne T : Pn
T [p(x)] = p(x + 1)

f (x) in
p(x) for some
Pn
1 by

p(x).]

→

−

−

−

−

Exercise 7.2.20 Let U and V denote the spaces of sym-
metric and skew-symmetric n
n matrices. Show that
dim U + dim V = n2.

×

a1b1 a1b2
a2b1 a2b2
...
...
anb1 anb2

a1bn
a2bn
...
anbn

· · ·
· · ·

· · ·








A = 





[Hint: im T = Rw for w = (b1, . . . , bn) in Rn.]

Exercise 7.2.26 Prove Theorem 7.2.5.

of V so that T (r1e1 +r2e2 +

R be a nonzero linear trans-
Exercise 7.2.27 Let T : V
→
formation, where dim V = n. Show that there is a basis
+rnen) = r1.
e1, . . . , en
{
}
Exercise 7.2.28 Let f
gree m
(p
Tf (p) = p

1. If p is any polynomial, recall that

f )(x) = p [ f (x)]. Deﬁne Tf : Pn

= 0 be a ﬁxed polynomial of de-

Pn+m by

· · ·

→

≥

f .

◦

◦

Exercise 7.2.21 Assume that B in Mnn satisﬁes Bk = 0
1. Show that every matrix in Mnn has
for some k
A for some A in Mnn. [Hint: Show that
the form BA
T : Mnn
→
T (A) = BA

Mnn is linear and one-to-one where

A for each A.]

≥
−

−

a. Show that Tf is linear.

b. Show that Tf is one-to-one.

Exercise 7.2.29 Let U be a subspace of a ﬁnite dimen-
sional vector space V .

Exercise 7.2.22 Fix a column y
U =

= 0 in Rn and let
Ay = 0
. Show that dim U = n(n
}

A in Mnn
{

|

a. Show that U = ker T for some linear operator

T : V

V .

→

1).

−

b. Show that U = im S for some linear operator

S : V
V .
rem 7.1.3.]

→

[Hint: Theorem 6.4.1 and Theo-

Exercise 7.2.23 If B in Mmn has rank r, let U =
A in
{
Mnn
and W =
BA = 0
. Show that
}
}
|
dim U = n(n
r) and dim W = nr. [Hint: Show that U
−
consists of all matrices A whose columns are in the null
space of B. Use Example 7.2.7.]

A in Mnn

BA
{

|

Exercise 7.2.30 Let V and W be ﬁnite dimensional vec-
tor spaces.

a. Show that dim W

≤
exists an onto linear transformation T : V
[Hint: Theorem 6.4.1 and Theorem 7.1.3.]

dim V if and only if there
W .

→

Exercise 7.2.24 Let T : V
V be a linear transforma-
→
0
tion where dim V = n. If ker T
, show that
}
{
every vector v in V can be written v = u + w for some u
in ker T and w in im T . [Hint: Choose bases B
ker T
im T , and use Exercise 6.3.33.]
and D

im T =

⊆

∩

⊆

b. Show that dim W

≥

dim V if and only if there ex-
W .

ists a one-to-one linear transformation T : V
[Hint: Theorem 6.4.1 and Theorem 7.1.3.]

→

Exercise 7.2.31 Let A and B be n
sume that AX B = 0, X
and B are both invertible. [Hint: Dimension Theorem.]

n matrices, and as-
Mnn, implies X = 0. Show that A

×

∈

6
6
388

Linear Transformations

7.3 Isomorphisms and Composition

Often two vector spaces can consist of quite different types of vectors but, on closer examination, turn out
to be the same underlying space displayed in different symbols. For example, consider the spaces

R2 =

(a, b)

{

a, b

R

}

∈

|

and P1 =

a + bx

{

a, b

R

}

∈

|

Compare the addition and scalar multiplication in these spaces:

(a, b) + (a1, b1) = (a + a1, b + b1)
r(a, b) = (ra, rb)

(a + bx) + (a1 + b1x) = (a + a1) + (b + b1)x

r(a + bx) = (ra) + (rb)x

Clearly these are the same vector space expressed in different notation: if we change each (a, b) in R2 to
a + bx, then R2 becomes P1, complete with addition and scalar multiplication. This can be expressed by
P1 that is both one-to-one and onto.
noting that the map (a, b)
In this form, we can describe the general situation.

a + bx is a linear transformation R2

7→

→

Deﬁnition 7.4 Isomorphic Vector Spaces

Alineartransformation T : V
vectorspacesV andW aresaidtobeisomorphicifthereexistsanisomorphismT : V
wewriteV ∼= W whenthisisthecase.

W iscalledanisomorphismifitisbothontoandone-to-one. The

W,and

→

→

Example 7.3.1

The identity transformation 1V : V

→

V is an isomorphism for any vector space V .

Example 7.3.2

If T : Mmn
Hence Mmn ∼= Mnm.

→

Mnm is deﬁned by T (A) = AT for all A in Mmn, then T is an isomorphism (verify).

Example 7.3.3
Isomorphic spaces can “look” quite different. For example, M22 ∼= P3 because the map
T : M22

= a + bx + cx2 + dx3 is an isomorphism (verify).

P3 given by T

→

a b
c d

(cid:20)

(cid:21)

The word isomorphism comes from two Greek roots: iso, meaning “same,” and morphos, meaning

“form.” An isomorphism T : V

W induces a pairing

→

T (v)

v

↔

between vectors v in V and vectors T (v) in W that preserves vector addition and scalar multiplication.
Hence, as far as their vector space properties are concerned, the spaces V and W are identical except

x
y
z 


T





x
y
z 


7.3. Isomorphisms and Composition

389

for notation. Because addition and scalar multiplication in either space are completely determined by the
same operations in the other space, all vector space properties of either space are completely determined
by those of the other.

One of the most important examples of isomorphic spaces was considered in Chapter 4. Let A denote
the set of all “arrows” with tail at the origin in space, and make A into a vector space using the paral-
lelogram law and the scalar multiple law (see Section 4.1). Then deﬁne a transformation T : R3
A by
taking

→

= the arrow v from the origin to the point P(x, y, z).

In Section 4.1 matrix addition and scalar multiplication were shown to correspond to the parallelogram
law and the scalar multiplication law for these arrows, so the map T is a linear transformation. Moreover T
is an isomorphism: it is one-to-one by Theorem 4.1.2, and it is onto because, given an arrow v in A with tip

x
y
z 


P(x, y, z), we have T



= v. This justiﬁes the identiﬁcation v =



in Chapter 4 of the geometric



arrows with the algebraic matrices. This identiﬁcation is very useful. The arrows give a “picture” of the
matrices and so bring geometric intuition into R3; the matrices are useful for detailed calculations and so
bring analytic precision into geometry. This is one of the best examples of the power of an isomorphism
to shed light on both spaces being considered.



The following theorem gives a very useful characterization of isomorphisms: They are the linear

transformations that preserve bases.

Theorem 7.3.1

IfV andW areﬁnitedimensionalspaces,thefollowingconditionsareequivalentforalinear
transformation T : V

W.

→

1. T isanisomorphism.

2. If

e1, e2, . . . , en
{

}
3. Thereexistsabasis

W.

isanybasisofV,then

T (e1), T (e2), . . . , T (en)

isabasisofW.

}

{

e1, e2, . . . , en
{

}

ofV suchthat

{

T (e1), T (e2), . . . , T (en)

isabasisof

}

(2). Let

e1, . . . , en
⇒
{
+ tnen) = 0, so t1e1 +

Proof. (1)
}
T (t1e1 +
· · ·
independence of the ei, so
T (e1), . . . , T (en)
W . Because T is onto, w = T (v) for some v in V , so write v = t1e1 +
+ tnT (en), proving that
w = T (v) = t1T (e1) +

+ tnT (en) = 0 with ti in R, then
· · ·
0
). But then each ti = 0 by the
{
is independent. To show that it spans W , choose w in
+ tnen. Hence we obtain

be a basis of V . If t1T (e1) +
+ tnen = 0 (because ker T =

T (e1), . . . , T (en)

· · ·
spans W .

· · ·

}

}

{

{

}

· · ·

(3). This is because V has a basis.
(1). If T (v) = 0, write v = v1e1 +

(2)
(3)

⇒
⇒

+ vnen where each vi is in R. Then

· · ·
0 = T (v) = v1T (e1) +

+ vnT (en)

· · ·
and T is one-to-one. To show that T is onto, let

so v1 =

· · ·

= vn = 0 by (3). Hence v = 0, so ker T =

0
}

{

390

Linear Transformations

w be any vector in W . By (3) there exist w1, . . . , wn in R such that

w = w1T (e1) +

· · ·

+ wnT (en) = T (w1e1 +

+ wnen)

· · ·

Thus T is onto.

Theorem 7.3.1 dovetails nicely with Theorem 7.1.3 as follows. Let V and W be vector spaces of
are bases of V and W , respectively.
W such that

f1, f2, . . . , fn
dimension n, and suppose that
Theorem 7.1.3 asserts that there exists a linear transformation T : V

e1, e2, . . . , en

and

}

{

{

}
→
for each i = 1, 2, . . . , n

T (ei) = fi

T (e1), . . . , T (en)

Then
more, the action of T is prescribed by

{

}

is evidently a basis of W , so T is an isomorphism by Theorem 7.3.1. Further-

T (r1e1 +

· · ·

+ rnen) = r1f1 +

+ rnfn

· · ·

so isomorphisms between spaces of equal dimension can be easily deﬁned as soon as bases are known. In
particular, this shows that if two vector spaces V and W have the same dimension then they are isomorphic,
that is V ∼= W . This is half of the following theorem.

Theorem 7.3.2
IfV andW areﬁnitedimensionalvectorspaces,thenV ∼= W ifandonlyif dim V = dim W.

Proof. It remains to show that if V ∼= W then dim V = dim W . But if V ∼= W , then there exists an isomor-
T (e1), . . . , T (en)
phism T : V
}
{
is a basis of W by Theorem 7.3.1, so dim W = n = dim V .

W . Since V is ﬁnite dimensional, let

be a basis of V . Then

e1, . . . , en

→

}

{

Corollary 7.3.1

LetU,V,andW denotevectorspaces. Then:

1. V ∼= V foreveryvectorspaceV.
2. IfV ∼= W thenW ∼= V.
3. IfU ∼= V andV ∼= W,thenU ∼= W.

The proof is left to the reader. By virtue of these properties, the relation ∼= is called an equivalence relation
on the class of ﬁnite dimensional vector spaces. Since dim (Rn) = n it follows that

Corollary 7.3.2
IfV isavectorspaceand dim V = n,thenV isisomorphicto Rn.

If V is a vector space of dimension n, note that there are important explicit isomorphisms V

Rn.
for the standard basis of Rn. By

→

Fix a basis B =

b1, b2, . . . , bn

{

}

of V and write

e1, e2, . . . , en

{

}

Theorem 7.1.3 there is a unique linear transformation CB : V

Rn given by

→

7.3. Isomorphisms and Composition

391

CB(v1b1 + v2b2 +

· · ·

+ vnbn) = v1e1 + v2e2 +

· · ·

v1
v2
...
vn








+ vnen = 





where each vi is in R. Moreover, CB(bi) = ei for each i so CB is an isomorphism by Theorem 7.3.1, called
the coordinate isomorphism corresponding to the basis B. These isomorphisms will play a central role
in Chapter 9.

The conclusion in the above corollary can be phrased as follows: As far as vector space properties
are concerned, every n-dimensional vector space V is essentially the same as Rn; they are the “same”
vector space except for a change of symbols. This appears to make the process of abstraction seem less
important—just study Rn and be done with it! But consider the different “feel” of the spaces P8 and M33
even though they are both the “same” as R9: For example, vectors in P8 can have roots, while vectors in
M33 can be multiplied. So the merit in the abstraction process lies in identifying common properties of
the vector spaces in the various examples. This is important even for ﬁnite dimensional spaces. However,
the payoff from abstraction is much greater in the inﬁnite dimensional case, particularly for spaces of
functions.

Example 7.3.4

Let V denote the space of all 2
T (1) = I, where I is the 2

×

2 identity matrix.

2 symmetric matrices. Find an isomorphism T : P2

×

V such that

→

is a basis of P2, and we want a basis of V containing I. The set

Solution.
1 0
0 1

{

1, x, x2
}
0 1
1 0

,

,

0 0
0 1

(cid:26)(cid:20)
Example 6.3.11). Hence deﬁne T : P2

(cid:21)(cid:27)

(cid:21)

(cid:21)

(cid:20)

(cid:20)

T (x2) =

0 0
0 1

Theorem 7.3.1, and its action is given by

(cid:20)

(cid:21)

is independent in V , so it is a basis because dim V = 3 (by

V by taking T (1) =

→

1 0
0 1

(cid:21)

(cid:20)

, T (x) =

0 1
1 0

,

(cid:21)

(cid:20)

, and extending linearly as in Theorem 7.1.3. Then T is an isomorphism by

T (a + bx + cx2) = aT (1) + bT (x) + cT (x2) =

b

a
b a + c

(cid:21)

(cid:20)

The dimension theorem (Theorem 7.2.4) gives the following useful fact about isomorphisms.

Theorem 7.3.3

IfV andW havethesamedimension n,alineartransformation T : V
iseitherone-to-oneoronto.

→

W isanisomorphismifit

Proof. The dimension theorem asserts that dim ( ker T ) + dim ( im T ) = n, so dim ( ker T ) = 0 if and only
if dim ( im T ) = n. Thus T is one-to-one if and only if T is onto, and the result follows.

392

Linear Transformations

Composition

Suppose that T : V
so, as in Section 2.3, it is possible to deﬁne a new function V

W and S : W

→

→

U are linear transformations. They link together as in the diagram

U by ﬁrst applying T and then S.

→

Deﬁnition 7.5 Composition of Linear Transformations

T

S

V

W

U

Given linear transformationsV T
−→
ST : V

U of T and S isdeﬁnedby

W S
−→

→

U, the composite

ST (v) = S [T (v)]

forallvinV

Theoperationofformingthenewfunction ST iscalledcomposition.1

The action of ST can be described compactly as follows: ST means ﬁrst T then S.
Not all pairs of linear transformations can be composed. For example, if T : V

U
U is deﬁned, but T S cannot be formed unless U = V because

W and S : W

→

→

are linear transformations then ST : V
S : W

U and T : V

→
W do not “link” in that order.2

→

→

Moreover, even if ST and T S can both be formed, they may not be equal. In fact, if S : Rm

Rn and
T : Rn
Rm are induced by matrices A and B respectively, then ST and T S can both be formed (they are
induced by AB and BA respectively), but the matrix products AB and BA may not be equal (they may not
even be the same size). Here is another example.

→

→

M22 and T : M22

M22 by S

→

→

M22. Describe the action of ST and T S, and show that ST

a b
c d

(cid:20)

=

(cid:21)

c d
a b
(cid:20)
= T S.

(cid:21)

and T (A) = AT for

Example 7.3.5

Deﬁne: S : M22

A

∈

Solution. ST

T S

(cid:20)

a b
c d

(cid:21)

b d
a c

(cid:21)

, whereas

a c
b d

(cid:21)
c a
d b

(cid:20)
=

(cid:20)

=

(cid:20)
.

(cid:21)

a b
c d

(cid:20)
= T

= S

(cid:21)
c d
a b
a b
c d

(cid:20)

(cid:20)

(cid:21)

(cid:21)

It is clear that T S

need not equal ST

a b
c d

(cid:20)

(cid:21)

, so T S

= ST .

The next theorem collects some basic properties of the composition operation.

Theorem 7.3.4: 3
LetV T
−→

W S
−→

U R
−→

Z belineartransformations.

1. ThecompositeST isagainalineartransformation.

1In Section 2.3 we denoted the composite as S
2Actually, all that is required is U

V .

⊆

T . However, it is more convenient to use the simpler notation ST .

◦

6
6
7.3. Isomorphisms and Composition

393

2. T 1V = T and 1W T = T.

3. (RS)T = R(ST ).

Proof. The proofs of (1) and (2) are left as Exercise 7.3.25. To prove (3), observe that, for all v in V :

(RS)T

{

}

(v) = (RS) [T (v)] = R

S [T (v)]
}

{

= R

{

(ST )(v)

=

}

{

R(ST )

(v)

}

Up to this point, composition seems to have no connection with isomorphisms. In fact, the two notions

are closely related.

Theorem 7.3.5

LetV andW beﬁnitedimensionalvectorspaces. Thefollowingconditionsareequivalentfora
lineartransformation T : V

W.

→

1. T isanisomorphism.

2. Thereexistsalineartransformation S : W

V suchthat ST = 1V and T S = 1W.

→

Moreover,inthiscase S isalsoanisomorphismandisuniquelydeterminedby T:

IfwinW iswrittenasw= T (v), then S(w) = v.

Proof. (1)
Theorem 7.3.1. Hence (using Theorem 7.1.3), deﬁne a linear transformation S : W

is a basis of V , then D =

T (e1), . . . , T (en)

(2). If B =

e1, . . . , en

⇒

{

{

}

is a basis of W by

}
V by
→

S[T (ei)] = ei

for each i

(7.2)

Since ei = 1V (ei), this gives ST = 1V by Theorem 7.1.2. But applying T gives T [S [T (ei)]] = T (ei) for
each i, so T S = 1W (again by Theorem 7.1.2, using the basis D of W ).

(2)

(1). If T (v) = T (v1), then S [T (v)] = S [T (v1)]. Because ST = 1V by (2), this reads v = v1; that

is, T is one-to-one. Given w in W , the fact that T S = 1W means that w = T [S(w)], so T is onto.

⇒

Finally, S is uniquely determined by the condition ST = 1V because this condition implies (7.2). S
is an isomorphism because it carries the basis D to B. As to the last assertion, given w in W , write
w = r1T (e1) +

+ rnT (en). Then w = T (v), where v = r1e1 +

+ rnen. Then S(w) = v by (7.2).

· · ·

· · ·

Given an isomorphism T : V

W , the unique isomorphism S : W

Theorem 7.3.5 is called the inverse of T and is denoted by T −

→

1. Hence T : V

→

V satisfying condition (2) of
V are

W and T −

1 : W

→

→

3Theorem 7.3.4 can be expressed by saying that vector spaces and linear transformations are an example of a category. In
general a category consists of certain objects and, for any two objects X and Y , a set mor (X, Y ). The elements α of mor (X, Y )
are called morphisms from X to Y and are written α : X
Y . It is assumed that identity morphisms and composition are deﬁned
in such a way that Theorem 7.3.4 holds. Hence, in the category of vector spaces the objects are the vector spaces themselves and
the morphisms are the linear transformations. Another example is the category of metric spaces, in which the objects are sets
equipped with a distance function (called a metric), and the morphisms are continuous functions (with respect to the metric).
The category of sets and functions is a very basic example.

→

394

Linear Transformations

related by the fundamental identities:

T −

1 [T (v)] = v for all v in V

and

T

T −

1(w)

= w for all w in W

In other words, each of T and T −
of Theorem 7.3.5 shows how to deﬁne T −
an example.

1 reverses the action of the other. In particular, equation (7.2) in the proof
1 using the image of a basis under the isomorphism T . Here is

(cid:2)

(cid:3)

Example 7.3.6

Deﬁne T : P1
T −

1.

→

P1 by T (a + bx) = (a

−

b) + ax. Show that T has an inverse, and ﬁnd the action of

Solution. The transformation T is linear (verify). Because T (1) = 1 + x and T (x) =
the basis B =
1 + x,
back to B, that is,

−
. Hence T is an isomorphism, and T −

to the basis D =

1, x
}

{

}

1, T carries
1 carries D

{

1
−
1(1 + x) = 1

T −

and T −

1(

1) = x

−

Because a + bx = b(1 + x) + (b

a)(

1), we obtain

−

−
1(1 + x) + (b
1(a + bx) = bT −

T −

a)T −

1(

−

1) = b + (b

−

a)x

−

Sometimes the action of the inverse of a transformation is apparent.

Example 7.3.7

b1, b2, . . . , bn
If B =
is an isomorphism deﬁned by

}

{

is a basis of a vector space V , the coordinate transformation CB : V

Rn

→

CB(v1b1 + v2b2 +

· · ·

+ vnbn) = (v1, v2, . . . , vn)T

1
B : Rn
The way to reverse the action of CB is clear: C−

V is given by

→
B (v1, v2, . . . , vn) = v1b1 + v2b2 +

1
C−

+ vnbn

for all vi in V

· · ·

Condition (2) in Theorem 7.3.5 characterizes the inverse of a linear transformation T : V

W as the
V that satisﬁes ST = 1V and T S = 1W . This often determines the inverse.

→

(unique) transformation S : W

→

Example 7.3.8

Deﬁne T : R3

→

1.
R3 by T (x, y, z) = (z, x, y). Show that T 3 = 1R3, and hence ﬁnd T −

Solution. T 2(x, y, z) = T [T (x, y, z)] = T (z, x, y) = (y, z, x). Hence

T 3(x, y, z) = T

T 2(x, y, z)

= T (y, z, x) = (x, y, z)

Since this holds for all (x, y, z), it shows that T 3 = 1R3, so T (T 2) = 1R3 = (T 2)T . Thus T −
by (2) of Theorem 7.3.5.

1 = T 2

(cid:2)

(cid:3)

7.3. Isomorphisms and Composition

395

Example 7.3.9

Deﬁne T : Pn

→

Rn+1 by T (p) = (p(0), p(1), . . . , p(n)) for all p in Pn. Show that T −

1 exists.

Solution. The veriﬁcation that T is linear is left to the reader. If T (p) = 0, then p(k) = 0 for
k = 0, 1, . . . , n, so p has n + 1 distinct roots. Because p has degree at most n, this implies that
p = 0 is the zero polynomial (Theorem 6.5.4) and hence that T is one-to-one. But
dim Pn = n + 1 = dim Rn+1, so this means that T is also onto and hence is an isomorphism. Thus
1, we
T −
have merely shown that such a description exists. To give it explicitly requires some ingenuity; one
method involves the Lagrange interpolation expansion (Theorem 6.5.3).

1 exists by Theorem 7.3.5. Note that we have not given a description of the action of T −

Exercises for 7.3

Exercise 7.3.1 Verify that each of the following is an
isomorphism (Theorem 7.3.3 is useful).

Exercise 7.3.4 In each case, compute the action of ST
and T S, and show that ST

= T S.

a. T : R3

b. T : R3

→

→

R3; T (x, y, z) = (x + y, y + z, z + x)

a. S : R2

R2 with S(x, y) = (y, x); T : R2

R3; T (x, y, z) = (x, x + y, x + y + z)

→

with T (x, y) = (x, 0)

R2

→

c. T : C

→
d. T : Mmn
ible

C; T (z) = z

Mmn; T (X ) = U XV , U and V invert-

→

e. T : P1

→

R2; T [p(x)] = [p(0), p(1)]

f. T : V

V ; T (v) = kv, k

→
any vector space

= 0 a ﬁxed number, V

d. S : M22

T : M22

b)

−

g. T : M22

h. T : Mmn

→

→

R4; T

a b
c d

(cid:20)

(cid:21)
Mnm; T (A) = AT

= (a + b, d, c, a

Exercise 7.3.2 Show that

a + bx + cx2, a1 + b1x + c1x2, a2 + b2x + c2x2
{

}

is a basis of R3.

is a basis of P2 if and only if
(a, b, c), (a1, b1, c1), (a2, b2, c2)
}
{
Exercise 7.3.3 If V is any vector space, let V n denote the
space of all n-tuples (v1, v2, . . . , vn), where each vi lies
in V . (This is a vector space with component-wise oper-
ations; see Exercise 6.1.17.) If C j(A) denotes the jth col-
(Rm)n
umn of the m
is an isomorphism if
T (A) =
sists of columns.)
(cid:2)

n matrix A, show that T : Mmn

. (Here Rm con-

C1(A) C2(A)

Cn(A)

· · ·

→

×

(cid:3)

b. S : R3
T : R3

c. S : P2
T : P2

→
→

→
→

R3 with S(x, y, z) = (x, 0, z);
R3 with T (x, y, z) = (x + y, 0, y + z)

P2 with S(p) = p(0) + p(1)x + p(2)x2;
P2 with T (a + bx + cx2) = b + cx + ax2

→

→

M22 with S

(cid:20)
M22 with T

(cid:20)

a b
c d
a b
c d

=

(cid:21)

(cid:20)
=

(cid:21)

(cid:20)

a 0
0 d
c a
d b

;

(cid:21)

(cid:21)

Exercise 7.3.5 In each case, show that the linear trans-
formation T satisﬁes T 2 = T .

a. T : R4

b. T : R2

→

→

R4; T (x, y, z, w) = (x, 0, z, 0)

R2; T (x, y) = (x + y, 0)

c. T : P2

P2;
T (a + bx + cx2) = (a + b

→

c) + cx + cx2

−

d. T : M22

→

a b
c d

M22;
= 1
2

(cid:21)

(cid:20)

T

(cid:20)

a + c b + d
a + c b + d

(cid:21)

Exercise 7.3.6 Determine whether each of the following
transformations T has an inverse and, if so, determine the
action of T −

1.

6
6
a. T : R3

R3;

b. If S and T are both onto, show that ST is onto.

396

Linear Transformations

T (x, y, z) = (x + y, y + z, z + x)

T (x, y, z, t) = (x + y, y + z, z + t, t + x)

→

→

b. T : R4

R4;

c. T : M22

→

a b
c d

M22;

=

(cid:20)
(cid:21)
M22;

(cid:20)
d. T : M22

T

T

a
2a

−
−

c
b
c 2b

d
d

−
−

(cid:21)

→

a b
c d

=

a + 2c b + 2d
b
a 3d
3c

(cid:20)
e. T : P2

f. T : P2

→

→

(cid:20)

(cid:21)
(cid:21)
−
R3; T (a + bx + cx2) = (a

−

−
R3; T (p) = [p(0), p(1), p(

1)]

−

c, 2b, a + c)

Exercise 7.3.7 In each case, show that T is self-inverse,
that is: T −

1 = T .

a. T : R4

→

b. T : R2
number

→

R4; T (x, y, z, w) = (x,

y,

−

−

z, w)

R2; T (x, y) = (ky

x, y), k any ﬁxed

−

c. T : Pn

→

Pn; T (p(x)) = p(3

x)

−

d. T : M22
A = 1
4

(cid:20)

→
5
3

M22; T (X ) = AX where

3
5

−
−

(cid:21)

Exercise 7.3.8 In each case, show that T 6 = 1R4 and so
determine T −

1.

a. T : R4

b. T : R4

→

→

R4; T (x, y, z, w) = (

−

x, z, w, y)

R4; T (x, y, z, w) = (

−

y, x

y, z,

w)

−

−

Exercise 7.3.9 In each case, show that T is an isomor-
1 explicitly.
phism by deﬁning T −

a. T : Pn

→
b. T : Mnn

→
is invertible in Mnn.

Pn is given by T [p(x)] = p(x + 1).

Mnn is given by T (A) = UA where U

Exercise 7.3.10 Given linear transformations
V T
−→

W S
−→

U :

Exercise 7.3.11 Let T : V
tion.

→

W be a linear transforma-

a. If T is one-to-one and T R = T R1 for transforma-

tions R and R1 : U

→

V , show that R = R1.

b. If T is onto and ST = S1T for transformations S
U , show that S = S1.

and S1 : W

→

Exercise 7.3.12 Consider the linear transformations
V T
−→

W R
−→

U .

a. Show that ker T

ker RT .

⊆

b. Show that im RT

⊆
Exercise 7.3.13 Let V T
−→
tions.

im R.

U S
−→

W be linear transforma-

a. If ST is one-to-one, show that T is one-to-one and

that dim V

dim U .

≤

b. If ST is onto, show that S is onto and that

dim W

≤

dim U .

1.

Exercise 7.3.14 Let T : V
V be a linear transforma-
→
tion. Show that T 2 = 1V if and only if T is invertible and
T = T −
Exercise 7.3.15 Let N be a nilpotent n
is, Nk = 0 for some k). Show that T : Mnm
an isomorphism if T (X ) = X
ker T , show that X = NX = N2X =
rem 7.3.3.]

n matrix (that
Mnm is
→
[Hint: If X is in
. Then use Theo-

NX .

· · ·

×

−

→

Exercise 7.3.16 Let T : V
W be a linear transforma-
e1, . . . , er, er+1, . . . , en
be any basis of V
tion, and let
{
er+1, . . . , en
is a basis of ker T . Show that
such that
}
{
e1, . . . , er
im T ∼= span
. [Hint: See Theorem 7.2.5.]
}
{
Exercise 7.3.17 Is every isomorphism T : M22
M22
given by an invertible matrix U such that T (X ) = U X for
all X in M22? Prove your answer.

→

}

Exercise 7.3.18 Let Dn denote the space of all func-
to R (see Exercise 6.3.35). If
tions f from
T : Dn

1, 2, . . . , n
}
{
Rn is deﬁned by

→

T ( f ) = ( f (1), f (2), . . . , f (n)),

a. If S and T are both one-to-one, show that ST is

one-to-one.

show that T is an isomorphism.

Exercise 7.3.19

a. Let V be the vector space of Exercise 6.1.3. Find

Exercise 7.3.25 Prove (1) and (2) of Theorem 7.3.4.

7.3. Isomorphisms and Composition

397

an isomorphism T : V

R1.

→
b. Let V be the vector space of Exercise 6.1.4. Find

Exercise 7.3.26 Deﬁne T : Pn
T (p) = p(x) + xp′(x) for all p in Pn.

→

Pn by

an isomorphism T : V

R2.

→

a. Show that T is linear.

Exercise 7.3.20 Let V T
W S
V be linear transforma-
−→
−→
tions such that ST = 1V . If dim V = dim W = n, show
1. [Hint: Exercise 7.3.13 and
1 and T = S−
that S = T −
Theorem 7.3.3, Theorem 7.3.4, and Theorem 7.3.5.]
W S
Exercise 7.3.21 Let V T
V be functions such that
−→
−→
T S = 1W and ST = 1V . If T is linear, show that S is also
linear.

Exercise 7.3.22 Let A and B be matrices of size p
and n
by R(X ) = AX B.

q. Assume that mn = pq. Deﬁne R : Mmn

→

×

m
×
Mpq

a. Show that Mmn ∼= Mpq by comparing dimensions.
b. Show that R is a linear transformation.

[Hint: Show that T : Mmn

c. Show that if R is an isomorphism, then m = p
Mpn
and n = q.
→
given by T (X ) = AX and S : Mmn
Mmq given
by S(X ) = X B are both one-to-one, and use the
dimension theorem.]

→

Exercise 7.3.23 Let T : V
tion such that T 2 = 0 is the zero transformation.

V be a linear transforma-

→

a. If V

=

0
{

b. If R : V

, show that T cannot be invertible.
}
V is deﬁned by R(v) = v + T (v) for all

v in V , show that R is linear and invertible.

→

Exercise 7.3.24
Let V consist of all sequences
[x0, x1, x2, . . . ) of numbers, and deﬁne vector operations

[xo, x1, . . . ) + [y0, y1, . . . ) = [x0 + y0, x1 + y1, . . . )

r[x0, x1, . . . ) = [rx0, rx1, . . . )

a. Show that V is a vector space of inﬁnite dimen-

sion.

b. Deﬁne T : V

V and S : V

V by

→

→

T [x0, x1, . . . ) = [x1, x2, . . . ) and
S[x0, x1,
Show that
T S = 1V , so T S is one-to-one and onto, but that T
is not one-to-one and S is not onto.

. . . ) = [0, x0, x1,

. . . ).

b. Show that ker T =

0
}
{
isomorphism. [Hint: Write p(x) = a0 +a1x+
anxn and compare coefﬁcients if p(x) =

and conclude that T is an
+
· · ·
xp′(x).]

−
c. Conclude that each q(x) in Pn has the form

q(x) = p(x) + xp′(x) for some unique polynomial
p(x).

d. Does this remain valid if T is deﬁned by

T [p(x)] = p(x)

xp′(x)? Explain.

−

Exercise 7.3.27 Let T : V
tion, where V and W are ﬁnite dimensional.

→

W be a linear transforma-

a. Show that T is one-to-one if and only if there
V with
exists a linear transformation S : W
ST = 1V .
is a basis of
V and T is one-to-one, show that W has a basis
T (e1), . . . , T (en), fn+1, . . . , fn+k
and use The-
{
orem 7.1.2 and Theorem 7.1.3.]

e1, . . . , en
{

[Hint: If

→

}

}

b. Show that T is onto if and only if there exists a
V with T S = 1W .
linear transformation S : W
e1,
[Hint: Let
be a basis of
{
er+1, . . . , en
V such that
is a basis of ker T .
{
Use Theorem 7.2.5, Theorem 7.1.2 and Theo-
rem 7.1.3.]

. . . , er, . . . , en

→

}

}

Exercise 7.3.28 Let S and T be linear transformations
V

W , where dim V = n and dim W = m.

→

a. Show that ker S = ker T if and only if T = RS
[Hint: Let
W .
be a basis of V such that
is a basis of ker S = ker T . Use
and

for some isomorphism R : W
e1, . . . , er, . . . , en
{
er+1, . . . , en
{
Theorem 7.2.5 to extend
T (e1), . . . , T (er)
}
{

S(e1), . . . , S(er)
}
{

to bases of W .]

→

}

}

b. Show that

→

im S = im T if and only if T = SR
for some isomorphism R : V
V . [Hint: Show
that dim ( ker S) = dim ( ker T ) and choose bases
e1, . . . , er, . . . , en
of V
}
}
{
er+1, . . . , en
are bases
where
}
{
of ker S and ker T , respectively. If 1
r, show
that S(ei) = T (gi) for some gi in V , and prove that
g1, . . . , gr, fr+1, . . . , fn
{

f1, . . . , fr, . . . , fn
and
{
fr+1, . . . , fn
and
}
{
i
≤

is a basis of V .]

≤

}

6
398

Linear Transformations

Exercise 7.3.29 If T : V
V is a linear transformation
→
where dim V = n, show that T ST = T for some isomor-
e1, . . . , er, er+1, . . . , en}
phism S : V
{
T (e1), . . . , T (er)
be as in Theorem 7.2.5. Extend
to
}
{
a basis of V , and use Theorem 7.3.1, Theorem 7.1.2 and
Theorem 7.1.3.]

V . [Hint: Let

→

Exercise 7.3.30 Let A and B denote m
each case show that (1) and (2) are equivalent.

×

n matrices. In

a. (1) A and B have the same null space. (2) B = PA

for some invertible m

m matrix P.

×

b. (1) A and B have the same range. (2) B = AQ for

some invertible n

n matrix Q.

×

[Hint: Use Exercise 7.3.28.]

7.4 A Theorem about Differential Equations

Differential equations are instrumental in solving a variety of problems throughout science, social science,
and engineering. In this brief section, we will see that the set of solutions of a linear differential equation
(with constant coefﬁcients) is a vector space and we will calculate its dimension. The proof is pure linear
algebra, although the applications are primarily in analysis. However, a key result (Lemma 7.4.3 below)
can be applied much more widely.

We denote the derivative of a function f : R

R by f ′, and f will be called differentiable if it can
be differentiated any number of times. If f is a differentiable function, the nth derivative f (n) of f is the
′, . . . , and in general f (n+1) = f (n)
result of differentiating n times. Thus f (0) = f , f (1) = f ′, f (2) = f (1)
for each n

0. For small values of n these are often written as f , f ′, f ′′, f ′′′, . . . .

→

′

≥

If a, b, and c are numbers, the differential equations

are said to be of second order and third-order, respectively. In general, an equation

a f ′

f ′′

−

−

b f = 0 or

f ′′′

a f ′′

b f ′

−

−

−

c f = 0

f (n)

−

an

1 f (n
−
−

1)

an

2 f (n
−
−

2)

− · · · −

a2 f (2)

a1 f (1)

−

−

a0 f (0) = 0, ai in R

(7.3)

−

is called a differential equation of order n. We want to describe all solutions of this equation. Of course
a knowledge of calculus is required.
The set F of all functions R

R is a vector space with operations as described in Example 6.1.7. If f
and g are differentiable, we have ( f + g)′ = f ′ + g′ and (a f )′ = a f ′ for all a in R. With this it is a routine
matter to verify that the following set is a subspace of F:

→

Dn =

f : R

R

f is differentiable and is a solution to (7.3)
}

|

→

{

Our sole objective in this section is to prove

Theorem 7.4.1
ThespaceDn hasdimension n.

As will be clear later, the proof of Theorem 7.4.1 requires that we enlarge Dn somewhat and allow our
differentiable functions to take values in the set C of complex numbers. To do this, we must clarify what
it means for a function f : R
C to be differentiable. For each real number x write f (x) in terms of its
real and imaginary parts fr(x) and fi(x):

→

f (x) = fr(x) + i fi(x)

7.4. A Theorem about Differential Equations

399

R, called the real and imaginary parts of f ,
This produces new functions fr : R
respectively. We say that f is differentiable if both fr and fi are differentiable (as real functions), and we
deﬁne the derivative f ′ of f by

R and fi : R

→

→

We refer to this frequently in what follows.4

f ′ = f ′r + i f ′i

(7.4)

With this, write D∞ for the set of all differentiable complex valued functions f : R

C . This is a
complex vector space using pointwise addition (see Example 6.1.7), and the following scalar multiplica-
tion: For any w in C and f in D∞, we deﬁne w f : R
C by (w f )(x) = w f (x) for all x in R. We will be
working in D∞ for the rest of this section. In particular, consider the following complex subspace of D∞:

→

→

D∗n =

f : R

C

f is a solution to (7.3)
}

|

→

{

Clearly, Dn

⊆

D∗n, and our interest in D∗n comes from

Lemma 7.4.1
If dim C(D∗n) = n,then dim R(Dn) = n.

Proof. Observe ﬁrst that if dim C(D∗n) = n, then dim R(D∗n) = 2n. [In fact, if
D∗n then
pairs ( f , g) with f and g in Dn is a real vector space with componentwise operations. Deﬁne

is a R-basis of D∗n]. Now observe that the set Dn

g1, . . . , gn, ig1, . . . , ign

g1, . . . , gn

×

{

{

}

is a C-basis of
}
Dn of all ordered

θ : D∗n →

Dn

×

Dn

given by θ( f ) = ( fr, fi) for f in D∗n

One veriﬁes that θ is onto and one-to-one, and it is R-linear because f
Hence D∗n ∼= Dn
have

fi are both R-linear.
Dn as R-spaces. Since dim R(D∗n) is ﬁnite, it follows that dim R(Dn) is ﬁnite, and we

fr and f

→

→

×

Hence dim R(Dn) = n, as required.

2 dim R(Dn) = dim R(Dn

Dn) = dim R(D∗n) = 2n

×

It follows that to prove Theorem 7.4.1 it sufﬁces to show that dim C(D∗n) = n.

There is one function that arises frequently in any discussion of differential equations. Given a complex
number w = a + ib (where a and b are real), we have ew = ea(cos b + i sin b). The law of exponents,
ewev = ew+v for all w, v in C is easily veriﬁed using the formulas for sin(b + b1) and cos(b + b1). If x is a
variable and w = a + ib is a complex number, deﬁne the exponential function ewx by

ewx = eax(cos bx + i sin bx)

Hence ewx is differentiable because its real and imaginary parts are differentiable for all x. Moreover, the
following can be proved using (7.4):

(ewx)′ = wewx

4Write

|

w
|

for the absolute value of any complex number w. As for functions R

R, we say that limt
0 f (t) = w if, for all
< δ. (Note that t represents a real number here.) In particular,
ε > 0 there exists δ > 0 such that
|
1
given a real number x, we deﬁne the derivative f ′ of a function f : R
t [ f (x + t)
and we say
that f is differentiable if f ′(x) exists for all x in R. Then we can prove that f is differentiable if and only if both fr and fi are
differentiable, and that f ′ = f ′r + i f ′i in this case.

C by f ′(x) = limt

whenever

f (x)]

w
|

f (t)

0
→

→

→

t
|

−

−

<

∈

(cid:8)

(cid:9)

→

|

400

Linear Transformations

In addition, (7.4) gives the product rule for differentiation:

If f and g are in D∞,

then ( f g)′ = f ′g + f g′

We omit the veriﬁcations.

To prove that dim C(D∗n) = n, two preliminary results are required. Here is the ﬁrst.

Lemma 7.4.2
Given f inD∞ and w in C,thereexists g inD∞ suchthat g′ −

wg = f .

wx. Then p is differentiable, whence pr and pi are both differentiable, hence
Proof. Deﬁne p(x) = f (x)e−
continuous, and so both have antiderivatives, say pr = q′r and pi = q′i. Then the function q = qr + iqi is in
D∞, and q′ = p by (7.4). Finally deﬁne g(x) = q(x)ewx. Then

g′ = q′ewx + qwewx = pewx + w(qewx) = f + wg

by the product rule, as required.

The second preliminary result is important in its own right.

Lemma 7.4.3: Kernel Lemma

LetV beavectorspace,andlet S and T belinearoperatorsV
and ker (T ) areﬁnitedimensional,then ker (T S) isalsoﬁnitedimensionaland
dim [ ker (T S)] = dim [ ker (T )] + dim [ ker (S)].

→

V. If S isontoandboth ker (S)

Proof. Let
is onto, let ui = S(wi) for some wi in V . It sufﬁces to show that

be a basis of ker (T ) and let

u1, u2, . . . , um

{

{

}

v1, v2, . . . , vn

be a basis of ker (S). Since S

}

B =

{

w1, w2, . . . , wm, v1, v2, . . . , vn

}

⊆

ker (T S) because T S(wi) = T (ui) = 0 for each i and T S(v j) = T (0) = 0

is a basis of ker (T S). Note B
for each j.
Spanning. If v is in ker (T S), then S(v) is in ker (T ), say S(v) = ∑ riui = ∑ riS (wi) = S (∑ riwi). It follows
that v
Independence. Let ∑ riwi + ∑ t jv j = 0. Applying S, and noting that S(v j) = 0 for each j, yields
0 = ∑ riS(wi) = ∑ riui. Hence ri = 0 for each i, and so ∑t jv j = 0. This implies that each t j = 0, and so
proves the independence of B.

, proving that v is in span (B).

∑ riwi is in ker (S) = span

v1, v2, . . . , vn

−

}

{

Proof of Theorem 7.4.1. By Lemma 7.4.1, it sufﬁces to prove that dim C(D∗n) = n. This holds for n = 1
because the proof of Theorem 3.5.1 goes through to show that D∗1 = Cea0x. Hence we proceed by induction
on n. With an eye on equation (7.3), consider the polynomial

p(t) = tn

an

1tn
1
−
−

an

2tn
2
−
−

−

−

− · · · −

a2t2

a1t

a0

−

−

(called the characteristic polynomial of equation (7.3)). Now deﬁne a map D : D∞
for all f in D∞. Then D is a linear operator, whence p(D) : D∞
since Dk( f ) = f (k) for each k

0, equation (7.3) takes the form p(D)( f ) = 0. In other words,

D∞ by D( f ) = f ′
→
D∞ is also a linear operator. Moreover,

→

≥

D∗n = ker [p(D)]

7.5. More on Linear Recurrences

401

By the fundamental theorem of algebra,5 let w be a complex root of p(t), so that p(t) = q(t)(t
complex polynomial q(t) of degree n
onto by Lemma 7.4.2, dim C[ ker (D
by induction. Hence Lemma 7.4.3 shows that ker [P(D)] is also ﬁnite dimensional and

1. It follows that p(D) = q(D)(D
−
w1D∞)] = 1 by the case n = 1 above, and dim C( ker [q(D)]) = n
−

w1D∞). Moreover D

w) for some
w1D∞ is
1

−

−

−

−

dim C( ker [p(D)]) = dim C( ker [q(D)]) + dim C( ker [D

−
Since D∗n = ker [p(D)], this completes the induction, and so proves Theorem 7.4.1.

−

w1D∞]) = (n

1) + 1 = n.

7.5 More on Linear Recurrences6

In Section 3.4 we used diagonalization to study linear recurrences, and gave several examples. We now
apply the theory of vector spaces and linear transformations to study the problem in more generality.

Consider the linear recurrence

xn+2 = 6xn

xn+1

for n

0

≥

−

If the initial values x0 and x1 are prescribed, this gives a sequence of numbers. For example, if x0 = 1 and
x1 = 1 the sequence continues

x2 = 5, x3 = 1, x4 = 29, x5 =

23, x6 = 197, . . .

−

as the reader can verify. Clearly, the entire sequence is uniquely determined by the recurrence and the two
initial values. In this section we deﬁne a vector space structure on the set of all sequences, and study the
subspace of those sequences that satisfy a particular recurrence.

Sequences will be considered entities in their own right, so it is useful to have a special notation for

them. Let

Example 7.5.1

[xn) denote the sequence x0, x1, x2, . . . , xn, . . .

[n)
[n + 1)
[2n)
[(
−
[5)

1)n)

is the sequence 0, 1, 2, 3, . . .
is the sequence 1, 2, 3, 4, . . .
is the sequence 1, 2, 22, 23, . . .
is the sequence 1,
is the sequence 5, 5, 5, 5, . . .

1, 1,

−

−

1, . . .

Sequences of the form [c) for a ﬁxed number c will be referred to as constant sequences, and those of the
form [λn), λ some number, are power sequences.

Two sequences are regarded as equal when they are identical:

[xn) = [yn) means

xn = yn

for all n = 0, 1, 2, . . .

5This is the reason for allowing our solutions to (7.3) to be complex valued.
6This section requires only Sections 7.1-7.3.

402

Linear Transformations

Addition and scalar multiplication of sequences are deﬁned by

[xn) + [yn) = [xn + yn)
r[xn) = [rxn)

These operations are analogous to the addition and scalar multiplication in Rn, and it is easy to check that
the vector-space axioms are satisﬁed. The zero vector is the constant sequence [0), and the negative of a
sequence [xn) is given by

[xn) = [

xn).

−
Now suppose k real numbers r0, r1, . . . , rk

−

determined by these numbers.

1 are given, and consider the linear recurrence relation

−

· · ·
= 0, we say this recurrence has length k.7 For example, the relation xn+2 = 2xn + xn+1 is of

−

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

1
−

(7.5)

When r0
length 2.

A sequence [xn) is said to satisfy the relation (7.5) if (7.5) holds for all n

all sequences that satisfy the relation. In symbols,

0. Let V denote the set of

≥

V =

[xn)

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

1 hold for all n

0

|

{

· · ·
It is easy to see that the constant sequence [0) lies in V and that V is closed under addition and scalar
multiplication of sequences. Hence V is vector space (being a subspace of the space of all sequences).
The following important observation about V is needed (it was used implicitly earlier): If the ﬁrst k terms
of two sequences agree, then the sequences are identical. More formally,

≥

}

−

−

Lemma 7.5.1

Let [xn) and [yn) denotetwosequencesinV. Then

[xn) = [yn)

ifandonlyif

x0 = y0, x1 = y1, . . . , xk

1 = yk

−

1
−

Proof. If [xn) = [yn) then xn = yn for all n = 0, 1, 2, . . . . Conversely, if xi = yi for all i = 0, 1, . . . , k
use the recurrence (7.5) for n = 0.

−

1,

xk = r0x0 + r1x1 +

+ rk

1xk

−

· · ·

1 = r0y0 + r1y1 +

−

+ rk

1yk

−

· · ·

1 = yk

−

Next the recurrence for n = 1 establishes xk+1 = yk+1. The process continues to show that xn+k = yn+k
holds for all n

0 by induction on n. Hence [xn) = [yn).

≥

This shows that a sequence in V is completely determined by its ﬁrst k terms. In particular, given a

k-tuple v = (v0, v1, . . . , vk

1) in Rk, deﬁne
−

T (v) to be the sequence in V whose ﬁrst k terms are v0, v1, . . . , vk

1
−

The rest of the sequence T (v) is determined by the recurrence, so T : Rk
an isomorphism.

→

V is a function. In fact, it is

7We shall usually assume that r0

= 0; otherwise, we are essentially dealing with a recurrence of shorter length than k.

6
6
7.5. More on Linear Recurrences

403

Theorem 7.5.1
Givenrealnumbers r0, r1, . . . , rk

1,let

−

V =

[xn)

{

|

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

−

· · ·

1, forall n

−

0

}

≥

denotethevectorspaceofallsequencessatisfyingthelinearrecurrencerelation(7.5)determined
by r0, r1, . . . , rk

1. Thenthefunction

−

T : Rk

V

→

deﬁnedaboveisanisomorphism. Inparticular:

1. dim V = k.

2. If

v1, . . . , vk
{

}

isanybasisof Rk,then

T (v1), . . . , T (vk)

{

}

isabasisofV.

Proof. (1) and (2) will follow from Theorem 7.3.1 and Theorem 7.3.2 as soon as we show that T is an
isomorphism. Given v and w in Rk, write v = (v0, v1, . . . , vk
1). The ﬁrst
k terms of T (v) and T (w) are v0, v1, . . . , vk
1, respectively, so the ﬁrst k terms of
T (v) + T (w) are v0 + w0, v1 + w1, . . . , vk
1. Because these terms agree with the ﬁrst k terms of
T (v + w), Lemma 7.5.1 implies that T (v + w) = T (v) + T (w). The proof that T (rv) + rT (v) is similar, so
T is linear.

1) and w = (w0, w1, . . . , wk

1 and w0, w1, . . . , wk

−
1 + wk

−

−

−

−

−

Now let [xn) be any sequence in V , and let v = (x0, x1, . . . , xk

1). Then the ﬁrst k terms of [xn) and
T (v) agree, so T (v) = [xn). Hence T is onto. Finally, if T (v) = [0) is the zero sequence, then the ﬁrst k
terms of T (v) are all zero (all terms of T (v) are zero!) so v = 0. This means that ker T =
, so T is
one-to-one.

}

0

{

−

Example 7.5.2

Show that the sequences [1), [n), and [(
recurrence

1)n) are a basis of the space V of all solutions of the

−

Then ﬁnd the solution satisfying x0 = 1, x1 = 2, x2 = 5.

xn+3 =

xn + xn+1 + xn+2

−

Solution. The veriﬁcations that these sequences satisfy the recurrence (and hence lie in V ) are left
to the reader. They are a basis because [1) = T (1, 1, 1), [n) = T (0, 1, 2), and
[(
sequence [xn) in V satisfying x0 = 1, x1 = 2, x2 = 5 is a linear combination of this basis:

is a basis of R3. Hence the

(1, 1, 1), (0, 1, 2), (1,

1)n) = T (1,

1, 1); and

1, 1)

−

−

−

}

{

The nth term is xn = t1 + nt2 + (

[xn) = t1[1) + t2[n) + t3[(

1)n)

−

1)nt3, so taking n = 0, 1, 2 gives

−

1 = x0 = t1 + 0 + t3
t3
2 = x1 = t1 + t2
5 = x2 = t1 + 2t2 + t3

−

This has the solution t1 = t3 = 1

2, t2 = 2, so xn = 1

2 + 2n + 1
2 (

1)n.

−

404

Linear Transformations

This technique clearly works for any linear recurrence of length k: Simply take your favourite basis
of Rk—perhaps the standard basis—and compute T (v1), . . . , T (vk). This is a basis of V all
v1, . . . , vk
{
right, but the nth term of T (vi) is not usually given as an explicit function of n. (The basis in Example 7.5.2
1)n, respectively, each
was carefully chosen so that the nth terms of the three sequences were 1, n, and (
a simple function of n.)

−

}

However, it turns out that an explicit basis of V can be given in the general situation. Given the

recurrence (7.5) again:

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

−

1
−

· · ·

the idea is to look for numbers λ such that the power sequence [λn) satisﬁes (7.5). This happens if and
only if

1λn+k
λn+k = r0λn + r1λn+1 +
−
0. This is true just when the case n = 0 holds; that is,
λk = r0 + r1λ +

+ rk

+ rk

1
−

· · ·

1
−

1λk
−

· · ·

holds for all n

≥

The polynomial

1xk
1
−
−
is called the polynomial associated with the linear recurrence (7.5). Thus every root λ of p(x) provides a
sequence [λn) satisfying (7.5). If there are k distinct roots, the power sequences provide a basis. Inciden-
tally, if λ = 0, the sequence [λn) is 1, 0, 0, . . . ; that is, we accept the convention that 00 = 1.

p(x) = xk

− · · · −

r1x

r0

rk

−

−

Theorem 7.5.2
Let r0, r1, . . . , rk

1 berealnumbers;let

−

V =

[xn)

{

|

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

−

· · ·

1 forall n

−

0

}

≥

denotethevectorspaceofallsequencessatisfyingthelinearrecurrencerelationdeterminedby
r0, r1, . . . , rk

1;andlet

−

p(x) = xk

rk

−

1xk
−

1
−

− · · · −

r1x

r0

−

denotethepolynomialassociatedwiththerecurrencerelation. Then

1. [λn) liesinV ifandonlyifλ isarootof p(x).

2. Ifλ1, λ2, . . . , λk aredistinctrealrootsof p(x),then

{

[λn

1 ), [λn

2 ), . . . , [λn
k )

isabasisofV.

}

Proof. It remains to prove (2). But [λn
), so (2) follows by
Theorem 7.5.1, provided that (v1, v2, . . . , vn) is a basis of Rk. This is true provided that the matrix with
the vi as its rows

i ) = T (vi) where vi = (1, λi, λ2

i , . . . , λk
i

1
−

1 λ1 λ2
1
1 λ2 λ2
2
...
...
...
1 λk λ2
k








· · ·
· · ·
. . .

· · ·

λk
1
λk
2
...
λk
k

1
−
1
−

1
−








is invertible. But this is a Vandermonde matrix and so is invertible if the λi are distinct (Theorem 3.2.7).
This proves (2).

Example 7.5.3

Find the solution of xn+2 = 2xn + xn+1 that satisﬁes x0 = a, x1 = b.

7.5. More on Linear Recurrences

405

Solution. The associated polynomial is p(x) = x2
and λ2 =
Theorem 7.5.2. Hence every solution [xn) is a linear combination

1, so the sequences [2n) and [(

2 = (x

−

−

−

−

−

x

1)n) are a basis for the space of solutions by

2)(x + 1). The roots are λ1 = 2

[xn) = t1[2n) + t2[(

1)n)

−

This means that xn = t12n + t2(
x1 = b give

1)n holds for n = 0, 1, 2, . . ., so (taking n = 0, 1) x0 = a and

−

These are easily solved: t1 = 1

t1 + t2 = a
t2 = b
2t1

−
3 (a + b) and t2 = 1
3(2a

−
3 [(a + b)2n + (2a

tn = 1

b), so

b)(

−

1)n]

−

The Shift Operator

If p(x) is the polynomial associated with a linear recurrence relation of length k, and if p(x) has k distinct
roots λ1, λ2, . . . , λk, then p(x) factors completely:

p(x) = (x

λ1)(x

λ2)

(x

−

· · ·

λk)

−

−

λ)mq(x), where q(λ)

Each root λi provides a sequence [λn
i ) satisfying the recurrence, and they are a basis of V by Theorem 7.5.2.
In general, a root λ has multiplicity m if
In this case, each λi has multiplicity 1 as a root of p(x).
p(x) = (x
= 0. In this case, there are fewer than k distinct roots and so fewer
than k sequences [λn) satisfying the recurrence. However, we can still obtain a basis because, if λ has
multiplicity m (and λ
= 0), it provides m linearly independent sequences that satisfy the recurrence. To
prove this, it is convenient to give another way to describe the space V of all sequences satisfying a given
linear recurrence relation.

−

Let S denote the vector space of all sequences and deﬁne a function

S : S

→

S by S[xn) = [xn+1) = [x1, x2, x3,

. . . )

S is clearly a linear transformation and is called the shift operator on S. Note that powers of S shift the
sequence further: S2[xn) = S[xn+1) = [xn+2). In general,

Sk[xn) = [xn+k) = [xk, xk+1,

. . . )

for all k = 0, 1, 2, . . .

But then a linear recurrence relation

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

−

1
−

· · ·

for all n = 0, 1, . . .

can be written

Sk[xn) = r0[xn) + r1S[xn) +

+ rk

1Sk
−

1[xn)
−

· · ·

(7.6)

6
6
406

Linear Transformations

Now let p(x) = xk
r0 denote the polynomial associated with the recurrence relation.
The set L[S, S] of all linear transformations from S to itself is a vector space (verify8) that is closed under
composition. In particular,

1xk
−

−· · ·−

r1x

1
−

rk

−

−

1Sk
−
is a linear transformation called the evaluation of p at S. The point is that condition (7.6) can be written
as

p(S) = Sk

− · · · −

r1S

1
−

r0

rk

−

−

In other words, the space V of all sequences satisfying the recurrence relation is just ker [p(S)]. This is the
ﬁrst assertion in the following theorem.

p(S)

[xn)

= 0

{

}

Theorem 7.5.3
Let r0, r1, . . . , rk

1 berealnumbers,andlet

−

V =

[xn)

{

|

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

−

1
−

· · ·

forall n

0

}

≥

denotethespaceofallsequencessatisfyingthelinearrecurrence relationdeterminedby
r0, r1, . . . , rk

1. Let

−

p(x) = xk

rk

−

1xk
−

1
−

− · · · −

r1x

r0

−

denotethecorrespondingpolynomial. Then:

1. V = ker [p(S)],where S istheshiftoperator.

2. If p(x) = (x

−

λ)mq(x),whereλ

= 0 and m > 1,thenthesequences

[λn), [nλn), [n2λn), . . . , [nm

1λn)
−

{

}

alllieinV andarelinearlyindependent.

= n(n
n
Proof (Sketch). It remains to prove (2). If
−
k
n
λn
is to use (1) to show that the sequence sk =
k
(cid:0)
(cid:1)
s0, s1, . . . , sm
(2) of Theorem 7.5.1 can be applied to show that
1
(cid:1)
(cid:2)(cid:0)
{
−
1, in the present theorem can be given by tk = ∑m
sequences tk = [nkλn), k = 0, 1, . . . , m
A =

denotes the binomial coefﬁcient, the idea
1. Then
is linearly independent. Finally, the
1
j=0 ak js j, where
−

(n
···
k!
is a solution for each k = 0, 1, . . . , m

−
is an invertible matrix. Then (2) follows. We omit the details.

k+1)

−

1)

}

(cid:1)

−

ai j

(cid:3)

This theorem combines with Theorem 7.5.2 to give a basis for V when p(x) has k real roots (not neces-
(cid:2)
= 0, a condition that is unimportant

sarily distinct) none of which is zero. This last requirement means r0
in practice (see Remark 1 below).

Theorem 7.5.4
Let r0, r1, . . . , rk

1 berealnumberswith r0

−

= 0;let

V =

[xn)

{

|

xn+k = r0xn + r1xn+1 +

+ rk

1xn+k

−

· · ·

1 forall n

−

0

}

≥

denotethespaceofallsequencessatisfyingthelinearrecurrencerelationoflength k determinedby

8See Exercises 9.1.19 and 9.1.20.

6
6
6
7.5. More on Linear Recurrences

407

r0, . . . , rk

1;andassumethatthepolynomial

−

p(x) = xk

rk

1xk
−

1
−

−

− · · · −

r1x

r0

−

factorscompletelyas

· · ·
whereλ1, λ2, . . . , λp aredistinctrealnumbersandeach mi

−

−

p(x) = (x

λ1)m1(x

λ2)m2

λp)mp

−
1. Thenλi

(x

≥

= 0 foreach i,and

λn
1
λn
(cid:2)
2

(cid:2)
λn
p

(cid:1)

(cid:1)

,

,

,

nλn
1
nλn
(cid:2)
2

(cid:2)
nλn
p

, . . . ,

nm1

(cid:1)

(cid:1)

, . . . ,
...
, . . . ,

nm2
(cid:2)

(cid:2)
nmp

1λn
−
1
1λn
−
2

1λn
−
p

(cid:2)

(cid:1)

(cid:2)

(cid:1)

(cid:2)

(cid:1)

(cid:1)

(cid:1)

isabasisofV.

Proof. There are m1 + m2 +
they are linearly independent. The assumption that r0
i ), . . . , [nmi
λi
whole set of sequences is linearly independent is omitted.

+ mp = k sequences in all so, because dim V = k, it sufﬁces to show that
= 0, implies that 0 is not a root of p(x). Hence each
is linearly independent by Theorem 7.5.3. The proof that the

i ), [nλn

1λn
i )
−

= 0, so

[λn

· · ·

{

}

Example 7.5.4

Find a basis for the space V of all sequences [xn) satisfying

Solution. The associated polynomial is

xn+3 =

9xn

−

−

3xn+1 + 5xn+2

p(x) = x3

−

5x2 + 3x + 9 = (x

3)2(x + 1)

−

Hence 3 is a double root, so [3n) and [n3n) both lie in V by Theorem 7.5.3 (the reader should verify
this). Similarly, λ =
1)n)
[3n), [n3n), [(

1 is a root of multiplicity 1, so [(
is a basis by Theorem 7.5.4.

1)n) lies in V . Hence

−

{

−
}

−

Remark1
If r0 = 0 [so p(x) has 0 as a root], the recurrence reduces to one of shorter length. For example, consider

If we set yn = xn+2, this recurrence becomes yn+2 = 3yn + 2yn+1, which has solutions [3n) and [(
These give the following solution to (7.5):

1)n).

−

xn+4 = 0xn + 0xn+1 + 3xn+2 + 2xn+3

(7.7)

0, 0, 1, 3, 32, . . .

0, 0, 1,
(cid:2)

1, (

−

1)2, . . .
(cid:1)

−

(cid:2)

(cid:1)

6
6
6
408

Linear Transformations

In addition, it is easy to verify that

[1, 0, 0, 0, 0, . . . )

[0, 1, 0, 0, 0, . . . )

are also solutions to (7.7). The space of all solutions of (7.5) has dimension 4 (Theorem 7.5.1), so these
sequences are a basis. This technique works whenever r0 = 0.

Remark2
Theorem 7.5.4 completely describes the space V of sequences that satisfy a linear recurrence relation for
which the associated polynomial p(x) has all real roots. However, in many cases of interest, p(x) has
complex roots that are not real. If p(µ) = 0, µ complex, then p(µ) = 0 too (µ the conjugate), and the
main observation is that [µn + µn) and [i(µn + µn)) are real solutions. Analogs of the preceding theorems
can then be proved.

Exercises for 7.5

Exercise 7.5.1 Find a basis for the space V of sequences
[xn) satisfying the following recurrences, and use it to
ﬁnd the sequence satisfying x0 = 1, x1 = 2, x2 = 1.

a. xn+2 =

b. xn+2 =

−

−

a2xn + 2axn+1, a

= 0

abxn + (a + b)xn+1, (a

= b)

a. xn+3 =

b. xn+3 =

c. xn+3 =

−

−

−

2xn + xn+1 + 2xn+2

6xn + 7xn+1

36xn + 7xn+2

Exercise 7.5.2 In each case, ﬁnd a basis for the space V
of all sequences [xn) satisfying the recurrence, and use it
to ﬁnd xn if x0 = 1, x1 =

1, and x2 = 1.

a. xn+3 = xn + xn+1

−
xn+2

−

b. xn+3 =

−

2xn + 3xn+1

4xn + 3xn+2

c. xn+3 =

−
d. xn+3 = xn

−
e. xn+3 = 8xn

3xn+1 + 3xn+2

12xn+1 + 6xn+2

−

Exercise 7.5.3 Find a basis for the space V of sequences
[xn) satisfying each of the following recurrences.

Exercise 7.5.4 In each case, ﬁnd a basis of V .

a. V =

b. V =

[xn)
{
[xn)
{

|

|

xn+4 = 2xn+2

xn+4 =

−

xn+3, for n

−

≥
xn+2 + 2xn+3, for n

0
}
0
}

≥

Exercise 7.5.5 Suppose that [xn) satisﬁes a linear recur-
rence relation of length k. If
e1 = (0, 1, . . . , 0), . . . , ek
−
dard basis of Rk, show that

e0 = (1, 0, . . . , 0),
{

1 = (0, 0, . . . , 1)
}

is the stan-

xn = x0T (e0) + x1T (e1) +

+ xk

1T (ek

1)

−

−

· · ·

holds for all n

≥

k. (Here T is as in Theorem 7.5.1.)

Exercise 7.5.6 Show that the shift operator S is onto but
not one-to-one. Find ker S.

Exercise 7.5.7 Find a basis for the space V of all se-
quences [xn) satisfying xn+2 =

xn.

−

6
6
Chapter 8

Orthogonality

{

f1, f2, . . . , fm

of nonzero vectors in Rn was called an orthogonal set if fi

In Section 5.3 we introduced the dot product in Rn and extended the basic geometric notions of length and
f j = 0 for
distance. A set
= j, and it was proved that every orthogonal set is independent. In particular, it was observed that
all i
the expansion of a vector as a linear combination of orthogonal basis vectors is easy to obtain because
formulas exist for the coefﬁcients. Hence the orthogonal bases are the “nice” bases, and much of this
chapter is devoted to extending results about bases to orthogonal bases. This leads to some very powerful
methods and theorems. Our ﬁrst task is to show that every subspace of Rn has an orthogonal basis.

}

·

8.1 Orthogonal Complements and Projections

v1, . . . , vm

is linearly independent in a general vector space, and if vm+1 is not in span

v1, . . . , vm

is independent (Lemma 6.4.1). Here is the analog for orthogonal sets in Rn.

{

If
{
then

{

}
v1, . . . , vm, vm+1

}

,

}

Lemma 8.1.1: Orthogonal Lemma

Let

f1, f2, . . . , fm
{

}

beanorthogonalsetin Rn. Givenxin Rn,write

fm+1 = x

f1
x
2 f1
·
f1
k
k

−

f2
x
2 f2
·
f2
k
k

−

− · · · −

fm
x
·
fm
k

2 fm
k

Then:

1. fm+1

·

fk = 0 for k = 1, 2, . . . , m.

2. Ifxisnotin span

f1, . . . , fm
{

,thenfm+1
}

= 0and

f1, . . . , fm, fm+1
{

}

isanorthogonalset.

Proof. For convenience, write ti = (x

fi)/

fi

k

k

·

2 for each i. Given 1

m:

k

≤

fm+1

·

fk = (x
= x

t1f1
−
fk −
fk
−

− · · · −
fk)
·
2

t1(f1
fk
tk

k

k

·

·

= x
= 0

tkfk

− · · · −
tk(fk ·

− · · · −

tmfm)
fk)

·
− · · · −

tm(fm

fk)

·

≤
fk

{

This proves (1), and (2) follows because fm+1

= 0 if x is not in span

f1, . . . , fm

.

}

The orthogonal lemma has three important consequences for Rn. The ﬁrst is an extension for orthog-

onal sets of the fundamental fact that any independent set is part of a basis (Theorem 6.4.1).

Theorem 8.1.1
LetU beasubspaceof Rn.

1. Everyorthogonalsubset

f1, . . . , fm
{
2. U hasanorthogonalbasis.

}

inU isasubsetofanorthogonalbasisofU.

409

6
6
6
410

Orthogonality

Proof.

1. If span

}

{

f1, . . . , fm

= U , it is already a basis. Otherwise, there exists x in U outside span

f1, . . . , fm
}
If fm+1 is as given in the orthogonal lemma, then fm+1 is in U and
is orthogonal.
= U , we are done. Otherwise, the process continues to create larger and
If span
larger orthogonal subsets of U . They are all independent by Theorem 5.3.5, so we have a basis when
we reach a subset containing dim U vectors.

f1, . . . , fm, fm+1

f1, . . . , fm, fm+1

{

}

}

{

{

.

2. If U =

0
{
follows from (1).

}

, the empty basis is orthogonal. Otherwise, if f

= 0 is in U , then

f
}

{

is orthogonal, so (2)

We can improve upon (2) of Theorem 8.1.1. In fact, the second consequence of the orthogonal lemma
of a subspace U of Rn can be systematically modiﬁed to

is a procedure by which any basis
yield an orthogonal basis

{
f1, . . . , fm
To start the process, take f1 = x1. Then x2 is not in span

x1, . . . , xm

{

}

}

of U . The fi are constructed one at a time from the xi.

because

f1

}

x1, x2

{

}

is independent, so take

{
f1
x2
2 f1
·
f1
k
k

f2 = x2

−

f1, f2
Thus
{
not in span

}
f1, f2
{

is orthogonal by Lemma 8.1.1. Moreover, span

. Hence

}

f1, f2, f3

{

}

is orthogonal where

f1, f2

{

}

= span

x1, x2

{

}

(verify), so x3 is

f3 = x3

f1
x3
2 f1
·
f1
k
k

−

f2
x3
2 f2
·
f2
k
k

−

Again, span
At the mth iteration we construct an orthogonal set

x1, x2, x3

f1, f2, f3

= span

{

}

}

{

, so x4 is not in span

f1, f2, f3

and the process continues.

}

{

such that

f1, . . . , fm

}

{
= span

span

{

f1, f2, . . . , fm

}

x1, x2, . . . , xm

= U

}

{

Hence

{

f1, f2, . . . , fm

}

is the desired orthogonal basis of U . The procedure can be summarized as follows.

6
8.1. Orthogonal Complements and Projections

411

Theorem 8.1.2: Gram-Schmidt Orthogonalization Algorithm1

x1, x2, . . . , xm
{

If
f1, f2, . . . , fm inU successivelyasfollows:

}

isanybasisofasubspaceU of Rn,construct

f1 = x1
f2 = x2
f3 = x3
...
fk = xk

f1
x2
2 f1
·
f1
k
k
f1
x3
2 f1
·
f1
k
k

−

−

f2
x3
2 f2
·
f2
k
k

−

f1
xk·
2 f1
f1
k
k

−

f2
xk·
2 f2
f2
k
k

−

− · · · −

fk
xk·
1
2 fk
−
fk
1k
k
−

1
−

foreach k = 2, 3, . . . , m. Then

1.

f1, f2, . . . , fm
{
2. span

k = 1, 2, . . . , m.

isanorthogonalbasisofU.

}

f1, f2, . . . , fk
{

= span

x1, x2, . . . , xk
{

}

}

foreach

0

0

x3

f2

f1

span

f1, f2
{

}

Gram-Schmidt

f3

f1

f2

span

f1, f2
{

}

The process (for k = 3) is depicted in the diagrams. Of course, the algorithm converts any basis of Rn

itself into an orthogonal basis.

Example 8.1.1

Find an orthogonal basis of the row space of A =

1 1
3 2
1 0

1
−
0
1



Solution. Let x1, x2, x3 denote the rows of A and observe that
Take f1 = x1. The algorithm gives



.

1
−
1
0 

x1, x2, x3

{

}

is linearly independent.

f2 = x2

f3 = x3

−

f1
x2
2 f1 = (3, 2, 0, 1)
·
f1
k
k
f2
x3
x3
f1
2 f2 = x3
2 f1
·
·
f2
f1
k
k
k
k
1), (2, 1, 1, 2), 1

−

−

4
4(1, 1,
1,
−
−
10f2 = 1
3
0
4 f1
10(4,

−

−

−

1) = (2, 1, 1, 2)

3, 7,

−

6)

−

{

1,

(1, 1,

Hence
−
algorithm. In hand calculations it may be convenient to eliminate fractions (see the Remark
below), so

is the orthogonal basis provided by the

is also an orthogonal basis for row A.

1), (2, 1, 1, 2), (4,

10 (4,

(1, 1,

3, 7,

3, 7,

6)

6)

−

−

−

1,

}

{

−

−

−

−

}

1Erhardt Schmidt (1876–1959) was a German mathematician who studied under the great David Hilbert and later developed
the theory of Hilbert spaces. He ﬁrst described the present algorithm in 1907. Jörgen Pederson Gram (1850–1916) was a Danish
actuary.

412

Orthogonality

Remark
Observe that the vector x
fi
2 fi is unchanged if a nonzero scalar multiple of fi is used in place of fi. Hence,
·
fi
k
k
if a newly constructed fi is multiplied by a nonzero scalar at some stage of the Gram-Schmidt algorithm,
the subsequent fs will be unchanged. This is useful in actual calculations.

Projections

x

p

x

−

0

p

Suppose a point x and a plane U through the origin in R3 are given, and
we want to ﬁnd the point p in the plane that is closest to x. Our geometric
intuition assures us that such a point p exists. In fact (see the diagram), p
must be chosen in such a way that x

p is perpendicular to the plane.

U

−
Now we make two observations: ﬁrst, the plane U is a subspace of R3
p
is perpendicular to the plane U means that x
p is orthogonal to every vector in U . In these terms the
whole discussion makes sense in Rn. Furthermore, the orthogonal lemma provides exactly what is needed
to ﬁnd p in this more general setting.

(because U contains the origin); and second, that the condition that x

−

−

Deﬁnition 8.1 Orthogonal Complement of a Subspace of Rn

IfU isasubspaceof Rn,deﬁnetheorthogonalcomplementU ⊥ ofU (pronounced“U-perp”)by

U ⊥ =

xin Rn
{

x

·

|

y= 0 forallyinU

}

The following lemma collects some useful properties of the orthogonal complement; the proof of (1)

and (2) is left as Exercise 8.1.6.

Lemma 8.1.2
LetU beasubspaceof Rn.

1. U ⊥ isasubspaceof Rn.
}⊥ = Rn and (Rn)⊥ =
0
{

2.

3. IfU = span

0
.
}
{
x1, x2, . . . , xk
,thenU ⊥ =
{
}

xin Rn
{

x
·

|

xi = 0 for i = 1, 2, . . . , k

.
}

Proof.

3. Let U = span

x1, x2, . . . , xk

; we must show that U ⊥ =

x

xi = 0 for each i

xi = 0 for all i because each xi is in U . Conversely, suppose that x

x

{

|

·

y = 0 for each y in U . Write y = r1x1 + r2x2 +

{

}

·

then x
show that x is in U ⊥, that is, x
ri is in R. Then, using Theorem 5.3.1,

·

. If x is in U ⊥
xi = 0 for all i; we must
+ rkxk, where each

}

·

· · ·

y = r1(x

x

·

·

x1) + r2(x

x2) +

·

· · ·

+ rk(x

·

xk) = r10 + r20 +

+ rk0 = 0

· · ·

as required.

U

d

p

x

0

Example 8.1.2

8.1. Orthogonal Complements and Projections

413

Find U ⊥ if U = span

(1,

1, 2, 0), (1, 0,

2, 3)

−
Solution. By Lemma 8.1.2, x = (x, y, z, w) is in U ⊥ if and only if it is orthogonal to both
(1,

1, 2, 0) and (1, 0,

2, 3); that is,

−

}

{

in R4.

−

−

x
x

−

y + 2z

= 0
2z + 3w = 0

−

Gaussian elimination gives U ⊥ = span

(2, 4, 1, 0), (3, 3, 0,

{

1)

−

.

}

Now consider vectors x and d

= 0 in R3. The projection p = projd x

of x on d was deﬁned in Section 4.2 as in the diagram.

The following formula for p was derived in Theorem 4.2.4

p = projd x =

d

d
x
·
2
d
k
k

(cid:16)

(cid:17)

where it is shown that x
U = Rd =

td
{
U ⊥ (by Theorem 4.2.4).

R

∈

}

t

|

−
is a subspace of R3, that

p is orthogonal to d. Now observe that the line
d
is an orthogonal basis
}

{

of U , and that p

U and x

p

∈

−

∈

In this form, this makes sense for any vector x in Rn and any subspace U of Rn, so we generalize it
is an orthogonal basis of U , we deﬁne the projection p of x on U by the

f1, f2, . . . , fm

(cid:16)
U and (by the orthogonal lemma) x

(cid:16)

(cid:17)

U ⊥, so it looks like we have a generalization of

fm

(8.1)

}

p =

x
·
f1
k

f1
2
k

f1 +

f2 +

+

· · ·

x
·
fm
k

fm
2
k

(cid:16)

(cid:17)

x
·
f2
k

f2
2
k
p

−

(cid:17)
∈

as follows. If
formula

{

Then p
Theorem 4.2.4.

∈

However there is a potential problem: the formula (8.1) for p must be shown to be independent of the
is another

. To verify this, suppose that

f1, f2, . . . , fm

choice of the orthogonal basis
{
orthogonal basis of U , and write

}

f′1, f′2, . . . , f′m}

{

p′ =

x
f′1
·
2
f′1k
k

f′1 +

x
f′2
·
2
f′2k
k

f′2 +

+

· · ·

x
f′m
·
2
f′mk
k

f′m

(cid:16)

(cid:16)
U ⊥, and we must show that p′ = p. To see this, write the vector p

(cid:16)

(cid:17)

(cid:17)

(cid:17)

As before, p′ ∈
follows:

U and x

p′ ∈

−

p′ as

−

This vector is in U (because p and p′ are in U ) and it is in U ⊥ (because x
−
so it must be zero (it is orthogonal to itself!). This means p′ = p as desired.

p

p′ = (x

p′)

(x

p)

−

−

−

−

p′ and x

−

p are in U ⊥), and

Hence, the vector p in equation (8.1) depends only on x and the subspace U , and not on the choice
of U used to compute it. Thus, we are entitled to make the following

f1, . . . , fm

of orthogonal basis
deﬁnition:

{

}

6
414

Orthogonality

Deﬁnition 8.2 Projection onto a Subspace of Rn

LetU beasubspaceof Rn withorthogonalbasis

f1, f2, . . . , fm
{
2 f1 + x
f1
·
f2
k
k

f2
2 f2 +
k

. Ifxisin Rn,thevector
}
+ x
·
fm
k

2 fm
k

· · ·

fm

projU x = x
·
f1
k

iscalledtheorthogonalprojectionofxonU. ForthezerosubspaceU =

0
,wedeﬁne
}
{

proj

0
{

}

x = 0

The preceding discussion proves (1) of the following theorem.

Theorem 8.1.3: Projection Theorem
IfU isasubspaceof Rn andxisin Rn,writep= projU x. Then:

1. pisinU andx

−

pisinU ⊥.

2. pisthevectorinU closesttoxinthesensethat

x
k

−

p
k

<

x
k

−

y
k

forally

∈

U , y

= p

Proof.

1. This is proved in the preceding discussion (it is clear if U =

).

0
}

{

2. Write x

p) + (p
y = (x
Pythagorean theorem gives

−

−

−

y). Then p

−

y is in U and so is orthogonal to x

p by (1). Hence, the

−

x

k

−

y

k

2 =

x

k

p
k

−

2 +

p

k

y

k

−

2 >

x

k

−

2

p

k

because p

−

y

= 0. This gives (2).

Example 8.1.3

x1, x2

Let U = span
1, 0, 2),
ﬁnd the vector in U closest to x and express x as the sum of a vector in U and a vector orthogonal
to U .

in R4 where x1 = (1, 1, 0, 1) and x2 = (0, 1, 1, 2). If x = (3,

−

}

{

x1, x2
Solution.
orthogonal basis

{

is independent but not orthogonal. The Gram-Schmidt process gives an
}
f1, f2
{

of U where f1 = x1 = (1, 1, 0, 1) and

}

Hence, we can compute the projection using

f2 = x2

−

f1
x2
2 f1 = x2
·
f1
k
k

3
3f1 = (

1, 0, 1, 1)

−

−
f1, f2

:

p = projU x = x
·
f1
k

f1
2 f1 + x
·
f2
k
k

{
f2
2 f2 = 4
k

}
3 f2 = 1
1
3 f1 + −
3

5 4

1 3

−

(cid:2)

(cid:3)

6
6
8.1. Orthogonal Complements and Projections

415

Thus, p is the vector in U closest to x, and x
U . (This can be veriﬁed by checking that it is orthogonal to the generators x1 and x2 of U .) The
required decomposition of x is thus

7, 1, 3) is orthogonal to every vector in

3(4,

−

−

p = 1

x = p + (x

p) = 1

3 (5, 4,

−

1, 3) + 1

3 (4,

−

7, 1, 3)

−

Example 8.1.4

Find the point in the plane with equation 2x + y

z = 0 that is closest to the point (2,

1,

−

3).

−

−

Solution. We write R3 as rows. The plane is the subspace U whose points (x, y, z) satisfy
z = 2x + y. Hence

U =

{

(s, t, 2s + t)

s, t in R

= span

{

}

|

(0, 1, 1), (1, 0, 2)

}

The Gram-Schmidt process produces an orthogonal basis
1, 1). Hence, the vector in U closest to x = (2,
f2 = (1,

−

projU x = x
·
f1
k
Thus, the point in U closest to (2,

f1
2 f1 + x
f2
2 f2 =
·
f2
k
k
k
3) is (0,

1,

−

−

−

2,

−

2).

−

of U where f1 = (0, 1, 1) and
}
3) is
−

f1, f2
1,

{
−
2f1 + 0f2 = (0,

2,

−

2)

−

The next theorem shows that projection on a subspace of Rn is actually a linear operator Rn

Rn.

→

Theorem 8.1.4
LetU beaﬁxedsubspaceof Rn. Ifwedeﬁne T : Rn

Rn by

→

T (x) = projU x forallxin Rn

1. T isalinearoperator.

2. im T = U and ker T = U ⊥.

3. dim U + dim U ⊥ = n.

, then U ⊥ = Rn, and so T (x) = proj
Proof. If U =
0
}
{
operator, so (1), (2), and (3) hold. Hence assume that U
=

{

0

}

x = 0 for all x. Thus T = 0 is the zero (linear)
0
}

{

.

1. If

{

f1, f2, . . . , fm

}

is an orthonormal basis of U , then

T (x) = (x

f1)f1 + (x

f2)f2 +

+ (x

fm)fm

·
by the deﬁnition of the projection. Thus T is linear because

· · ·

·

·

for all x in Rn

(8.2)

(x + y)

fi = x

fi + y

fi

·

·

·

and

(rx)

fi = r(x

fi)

·

·

for each i

6
416

Orthogonality

2. We have im T

⊆

U by (8.2) because each fi is in U . But if x is in U , then x = T (x) by (8.2) and the

expansion theorem applied to the space U . This shows that U

im T , so im T = U .

⊆

Now suppose that x is in U ⊥. Then x
·
ker T by (8.2). Hence U ⊥ ⊆
for all x in Rn, and it follows that ker T

ker T . On the other hand, Theorem 8.1.3 shows that x

fi = 0 for each i (again because each fi is in U ) so x is in
T (x) is in U ⊥

U ⊥. Hence ker T = U ⊥, proving (2).

⊆

−

3. This follows from (1), (2), and the dimension theorem (Theorem 7.2.4).

Exercises for 8.1

Exercise 8.1.1 In each case, use the Gram-Schmidt al-
gorithm to convert the given basis B of V into an orthog-
onal basis.

Exercise 8.1.4 In each case, use the Gram-Schmidt al-
gorithm to ﬁnd an orthogonal basis of the subspace U ,
and ﬁnd the vector in U closest to x.

a. V = R2, B =

b. V = R2, B =

c. V = R3, B =

d. V = R3, B =

−

1), (2, 1)
}

(1,
{
(2, 1), (1, 2)
}
{
(1,
{
(0, 1, 1), (1, 1, 1), (1,
{

1, 1), (1, 0, 1), (1, 1, 2)
}
2, 2)
}

−

−

Exercise 8.1.2
vector in U and a vector in U ⊥.

In each case, write x as the sum of a

a. x = (1, 5, 7), U = span

b. x = (2, 1, 6), U = span

(1,
{
(3,
{

−

−

c. x = (3, 1, 5, 9),

2, 3), (

−

1, 2), (2, 0,

1, 1, 1)
}
3)
}

−

U = span

(1, 0, 1, 1), (0, 1,
{

−

1, 1), (

−

2, 0, 1, 1)
}

d. x = (2, 0, 1, 6),
U = span

(1, 1, 1, 1), (1, 1,
{

e. x = (a, b, c, d),

1,

−

−

1), (1,

−

1, 1,

1)
}

−

U = span

(1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0)
}
{

f. x = (a, b, c, d),

U = span

(1,
{

−

1, 2, 0), (

−

1, 1, 1, 1)
}

Exercise 8.1.3 Let x = (1,
U = span
(2, 1, 3,
{

−

.
4), (1, 2, 0, 1)
}

−

2, 1, 6) in R4, and let

a. Compute projU x.

b. Show that

(1, 0, 2,
−
{
orthogonal basis of U .

3), (4, 7, 1, 2)
}

is another

a. U = span

b. U = span

c. U = span
x = (2, 0,

−

−

−

1, 0), (

1, 2, 1)

, x = (
(1, 1, 1), (0, 1, 1)
}
{
, x = (2, 1, 0)
1, 0, 1)
(1,
}
{
,
(1, 0, 1, 0), (1, 1, 1, 0), (1, 1, 0, 0)
{
}
−
(1,
{

,
1, 0, 1), (1, 1, 0, 0), (1, 1, 0, 1)
}

1, 3)

d. U = span

−
x = (2, 0, 3, 1)

Exercise 8.1.5 Let U = span
and let A be the k

v1, v2, . . . , vk
{

, vi in Rn,
}

n matrix with the vi as rows.

×

a. Show that U ⊥ =

x
{

|

x in Rn, AxT = 0
.
}

b. Use part (a) to ﬁnd U ⊥ if

U = span

(1,
{

−

1, 2, 1), (1, 0,

.
1, 1)
}

−

Exercise 8.1.6

a. Prove part 1 of Lemma 8.1.2.

b. Prove part 2 of Lemma 8.1.2.

Exercise 8.1.7 Let U be a subspace of Rn. If x in Rn
can be written in any way at all as x = p + q with p in U
and q in U ⊥, show that necessarily p = projU x.
Exercise 8.1.8 Let U be a subspace of Rn and let x be
a vector in Rn. Using Exercise 8.1.7, or otherwise, show
that x is in U if and only if x = projU x.
Exercise 8.1.9 Let U be a subspace of Rn.

a. Show that U ⊥ = Rn if and only if U =

0
.
}
{
if and only if U = Rn.

c. Use the basis in part (b) to compute projU x.

b. Show that U ⊥ =

0
{

}

x for all x in Rn.

is an orthogonal basis of

}
, show that
}

If U is a subspace of Rn, show that

If U is a subspace of Rn, show that

f1, . . . , fn
{
f1, . . . , fm
{
fm+1, . . . , fn
{

Exercise 8.1.10
projU x = x for all x in U .
Exercise 8.1.11
x = projU x + projU ⊥
Exercise 8.1.12 If
Rn and U = span
U ⊥ = span
.
}
Exercise 8.1.13 If U is a subspace of Rn, show that
U ⊥⊥ = U . [Hint: Show that U
U ⊥⊥, then use The-
orem 8.1.4 (3) twice.]
Exercise 8.1.14 If U is a subspace of Rn, show how to
ﬁnd an n
. [Hint:
}
Exercise 8.1.13.]
Exercise 8.1.15 Write Rn as rows. If A is an n
trix, write its null space as null A =
Show that:

n ma-
×
AxT = 0
.
}

n matrix A such that U =

x in Rn
{

Ax = 0

x
{

⊆

×

|

|

8.1. Orthogonal Complements and Projections

417

c. If EF = 0 = FE and E and F are projection ma-
trices, show that E + F is also a projection matrix.

d. If A is m

n and AAT is invertible, show that

E = AT (AAT )−

1A is a projection matrix.

×

×

Exercise 8.1.18 Let A be an n
n matrix of rank r. Show
n matrix U such that UA is a
that there is an invertible n
×
row-echelon matrix with the property that the ﬁrst r rows
[Hint: Let R be the row-echelon form
are orthogonal.
of A, and use the Gram-Schmidt process on the nonzero
rows of R from the bottom up. Use Lemma 2.4.1.]

Exercise 8.1.19 Let A be an (n
x1, x2, . . . , xn
(n
1)
umn i. Deﬁne the vector y in Rn by

×
1 and let Ai denote the
−
1) matrix obtained from A by deleting col-

n matrix with rows

(n

1)

×

−

−

−

a.

null A = ( row A)⊥;

b.

null AT = ( col A)⊥.

y =

det A1

−

det A2 det A3

1)n+1 det An

(
−

· · ·

Exercise 8.1.16 If U and W are subspaces, show that
(U +W )⊥ = U ⊥ ∩
Exercise 8.1.17 Think of Rn as consisting of rows.

W ⊥. [See Exercise 5.1.22.]

a. Let E be an n

n matrix, and let

×
x in Rn

xE
U =
|
{
equivalent.

. Show that the following are
}

i. E 2 = E = E T (E is a projection matrix).
(yE) = 0 for all x and y in Rn.
ii. (x
iii. projU x = xE for all x in Rn.

xE)

−

·

[Hint: For (ii) implies (iii): Write x = xE +
xE) and use the uniqueness argument
(x
preceding the deﬁnition of projU x. For (iii)
xE is in U ⊥ for all x in Rn.]
implies (ii): x

−

−

(cid:2)

Show that:

(cid:3)

1. [Hint: Write

−

a. xi

y = 0 for all i = 1, 2, . . . , n

and show that det Bi = 0.]

·
Bi =

xi
A

(cid:20)

(cid:21)

b. y

= 0 if and only if

x1, x2, . . . , xn
is linearly
1
{
−
independent. [Hint: If some det Ai
= 0, the rows
of Ai are linearly independent. Conversely, if the
xi are independent, consider A = U R where R is in
reduced row-echelon form.]

}

c. If

x1, x2, . . . , xn
{

is linearly independent, use
Theorem 8.1.3(3) to show that all solutions to the
system of n

1 homogeneous equations

}

−

1

−

AxT = 0

b. If E is a projection matrix, show that I

a projection matrix.

E is also

−

are given by ty, t a parameter.

6
6
418

Orthogonality

8.2 Orthogonal Diagonalization

Recall (Theorem 5.5.3) that an n
n matrix A is diagonalizable if and only if it has n linearly independent
eigenvectors. Moreover, the matrix P with these eigenvectors as columns is a diagonalizing matrix for A,
that is

×

P−

1AP is diagonal.

As we have seen, the really nice bases of Rn are the orthogonal ones, so a natural question is: which n
n
matrices have an orthogonal basis of eigenvectors? These turn out to be precisely the symmetric matrices,
and this is the main result of this section.

×

Before proceeding, recall that an orthogonal set of vectors is called orthonormal if

= 1 for each
v1, v2, . . . , vk
vector v in the set, and that any orthogonal set
can be “normalized”, that is converted into
{
1
vk
. In particular, if a matrix A has n orthogonal eigenvectors,
an orthonormal set
v1
}
k
they can (by normalizing) be taken to be orthonormal. The corresponding diagonalizing matrix P has
orthonormal columns, and such matrices are very easy to invert.

v2, . . . ,

1
vkk
k

1
v2
k

v1,

{

k

}

k

v

k

k

Theorem 8.2.1

Thefollowingconditionsareequivalentforan n

n matrix P.

×

1. P isinvertibleand P−

1 = PT.

2. Therowsof P areorthonormal.

3. Thecolumnsof P areorthonormal.

Proof. First recall that condition (1) is equivalent to PPT = I by Corollary 2.4.1 of Theorem 2.4.5. Let
x1, x2, . . . , xn denote the rows of P. Then xT
x j.
Thus PPT = I means that xi
x j = 1 if i = j. Hence condition (1) is equivalent to
·
(2). The proof of the equivalence of (1) and (3) is similar.

j is the jth column of PT , so the (i, j)-entry of PPT is xi

= j and xi

x j = 0 if i

·

·

Deﬁnition 8.3 Orthogonal Matrices

n matrix P iscalledanorthogonalmatrix2ifitsatisﬁesone(andhenceall)ofthe

An n
conditionsinTheorem8.2.1.

×

Example 8.2.1

The rotation matrix

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

(cid:20)

is orthogonal for any angle θ.

These orthogonal matrices have the virtue that they are easy to invert—simply take the transpose. But
Rn is a linear operator, we will prove
they have many other important properties as well. If T : Rn
(Theorem 10.4.3) that T is distance preserving if and only if its matrix is orthogonal. In particular, the
matrices of rotations and reﬂections about the origin in R2 and R3 are all orthogonal (see Example 8.2.1).

→

2In view of (2) and (3) of Theorem 8.2.1, orthonormal matrix might be a better name. But orthogonal matrix is standard.

6
It is not enough that the rows of a matrix A are merely orthogonal for A to be an orthogonal matrix.

8.2. Orthogonal Diagonalization

419

Here is an example.

Example 8.2.2

The matrix

2
1
−
0





1 1
1 1
1 1 


−

has orthogonal rows but the columns are not orthogonal. However, if

the rows are normalized, the resulting matrix 

now orthonormal as the reader can verify).






2
√6
1
−
√3

1
√6
1
√3
1
0 −
√2

1
√6
1
√3
1
√2








is orthogonal (so the columns are

Example 8.2.3

If P and Q are orthogonal matrices, then PQ is also orthogonal, as is P−

1 = PT .

Solution. P and Q are invertible, so PQ is also invertible and

(PQ)−

1 = Q−

1P−

1 = QT PT = (PQ)T

Hence PQ is orthogonal. Similarly,

(P−

1)−

1 = P = (PT )T = (P−

1)T

shows that P−

1 is orthogonal.

Deﬁnition 8.4 Orthogonally Diagonalizable Matrices

n matrix A issaidtobeorthogonallydiagonalizablewhenanorthogonalmatrix P canbe

An n
foundsuchthat P−

×

1AP = PT AP isdiagonal.

This condition turns out to characterize the symmetric matrices.

Theorem 8.2.2: Principal Axes Theorem

Thefollowingconditionsareequivalentforan n

n matrix A.

×

1. A hasanorthonormalsetof n eigenvectors.

2. A isorthogonallydiagonalizable.

3. A issymmetric.

420

Orthogonality

(2). Given (1), let x1, x2, . . . , xn be orthonormal eigenvectors of A. Then P =

Proof. (1)
⇔
is orthogonal, and P−
be diagonal where P is orthogonal. If x1, x2, . . . , xn are the columns of P then
orthonormal basis of Rn that consists of eigenvectors of A by Theorem 3.3.4. This proves (1).

1AP is diagonal by Theorem 3.3.4. This proves (2). Conversely, given (2) let P−

(cid:2)
x1, x2, . . . , xn

x1 x2

{

}

. . . xn
1AP
is an

(cid:3)

(2)

⇒

AT = PT T DT PT = PDPT = A.

(3). If PT AP = D is diagonal, where P−

1 = PT , then A = PDPT . But DT = D, so this gives

⇒

(3)

(2). If A is an n

diagonal. If n > 1, assume that (3)
(n
λ1 be a (real) eigenvalue of A, and let Ax1 = λ1x1, where
ﬁnd an orthonormal basis

n symmetric matrix, we proceed by induction on n. If n = 1, A is already
1) symmetric matrices. By Theorem 5.5.7 let
×
x1
= 1. Use the Gram-Schmidt algorithm to
for Rn. Let P1 =
x1 x2
, so P1 is an orthogonal

x1, x2, . . . , xn

(2) for (n

. . . xn

−
k

⇒

1)

−

×

k

{
λ1 B
0 A1

(cid:20)

(cid:21)

}

matrix and PT

1 AP1 =

in block form by Lemma 5.5.2. But PT

(cid:2)

follows that B = 0 and A1 is symmetric. Then, by induction, there exists an (n
matrix Q such that QT A1Q = D1 is diagonal. Observe that P2 =

(cid:3)

1 AP1 is symmetric (A is), so it
1) orthogonal

(n

1)

−

×

−

is orthogonal, and compute:

1 0
0 Q

(cid:20)

(cid:21)

1 AP1)P2

(P1P2)T A(P1P2) = PT
2 (PT
0
1
0 QT
0
λ1
0 D1

=

=

(cid:20)

(cid:20)

0
λ1
0 A1

1 0
0 Q

(cid:21)

(cid:21) (cid:20)

(cid:21) (cid:20)

(cid:21)

is diagonal. Because P1P2 is orthogonal, this proves (2).

A set of orthonormal eigenvectors of a symmetric matrix A is called a set of principal axes for A. The
name comes from geometry, and this is discussed in Section 8.9. Because the eigenvalues of a (real)
symmetric matrix are real, Theorem 8.2.2 is also called the real spectral theorem, and the set of distinct
eigenvalues is called the spectrum of the matrix. In full generality, the spectral theorem is a similar result
for matrices with complex entries (Theorem 8.7.8).

Example 8.2.4

Find an orthogonal matrix P such that P−

1AP is diagonal, where A =



1 0
0 1
1 2

−

1
−
2
.
5 




Solution. The characteristic polynomial of A is (adding twice row 1 to row 2):

cA(x) = det

x

−
0
1





1

0

x

1
−
2
−

1
2
−
5 
x
−


= x(x

1)(x

6)

−

−

Thus the eigenvalues are λ = 0, 1, and 6, and corresponding eigenvectors are

x1 =

1
2
−
1 






x2 =

2
1
0 






x3 =

1
−
2
5 






8.2. Orthogonal Diagonalization

421

respectively. Moreover, by what appears to be remarkably good luck, these eigenvectors are
orthogonal. We have

2 = 5, and

2 = 30, so

2 = 6,

x1

k

k

x2

k

k

x3

k

k

P =

1
√6

x1

1
√5

x2

1
√30

x3

h

is an orthogonal matrix. Thus P−

1 = PT and

= 1

√30 



i

2√6
√5
2√5 √6
√5
0

−

1
−
2
5





PT AP =

0 0 0
0 1 0
0 0 6 






by the diagonalization algorithm.

Actually, the fact that the eigenvectors in Example 8.2.4 are orthogonal is no coincidence. Theo-
rem 5.5.4 guarantees they are linearly independent (they correspond to distinct eigenvalues); the fact that
the matrix is symmetric implies that they are orthogonal. To prove this we need the following useful fact
about symmetric matrices.

Theorem 8.2.3

IfAisan n

×

n symmetricmatrix,then

forallcolumnsxandyin Rn.3

(Ax)

y= x

(Ay)

·

·

Proof. Recall that x

·

y = xT y for all columns x and y. Because AT = A, we get

(Ax)

·

y = (Ax)T y = xT AT y = xT Ay = x

(Ay)

·

Theorem 8.2.4

If A isasymmetricmatrix,theneigenvectorsof A correspondingtodistincteigenvaluesare
orthogonal.

Proof. Let Ax = λx and Ay = µy, where λ

= µ. Using Theorem 8.2.3, we compute

Hence (λ

µ)(x

·

−

λ(x

y) = (λx)

·
y) = 0, and so x

y = 0 because λ

·

·
= µ.

y = (Ax)

·

·

y = x

(Ay) = x

(µy) = µ(x

y)

·

·

Now the procedure for diagonalizing a symmetric n

n matrix is clear. Find the distinct eigenvalues
(all real by Theorem 5.5.7) and ﬁnd orthonormal bases for each eigenspace (the Gram-Schmidt algorithm

×

3The converse also holds (Exercise 8.2.15).

6
6
422

Orthogonality

may be needed). Then the set of all these basis vectors is orthonormal (by Theorem 8.2.4) and contains n
vectors. Here is an example.

Example 8.2.5

Orthogonally diagonalize the symmetric matrix A =

Solution. The characteristic polynomial is

8
2
−
2





−

2 2
5 4
.
4 5 


x

8

2

cA(x) = det

5
−
4
−
Hence the distinct eigenvalues are 0 and 9 of multiplicities 1 and 2, respectively, so dim (E0) = 1
and dim (E9) = 2 by Theorem 5.5.6 (A is diagonalizable, being symmetric). Gaussian elimination
gives

= x(x

−





x

9)2

−
2
2
−

2
−
4
5 
−
x
−


E0(A) = span

x1

{

}

, x1 =



,

and

E9(A) = span

1
2
2 


−



2
−
1
0 


,





2
0
1 












The eigenvectors in E9 are both orthogonal to x1 as Theorem 8.2.4 guarantees, but not to each
other. However, the Gram-Schmidt process yields an orthogonal basis





x2, x3

{

}

of E9(A) where

x2 =



Normalizing gives orthonormal vectors

1
3x1,

1
√5

x2,

{


1
3√5

and x3 =

2
−
1
0 

, so

x3

}

2
4
5 






P =

1
3 x1

1
√5

x2

1
3√5

x3

h

i

= 1

3√5 

√5
2√5
2√5

−

−

6 2
3 4
0 5







1AP is diagonal.
is an orthogonal matrix such that P−
It is worth noting that other, more convenient, diagonalizing matrices P exist. For example,

y2 =

2
1
2 
norm 3 (as does x1), so



and y3 =







lie in E9(A) and they are orthogonal. Moreover, they both have

2
−
2
1 


Q =

1
3x1

1
3y2

1
3y3

= 1

3 

(cid:2)

(cid:3)

1 2
2 1
2 2

−

2
−
2
1 




is a nicer orthogonal matrix with the property that Q−

1AQ is diagonal.

x2

x1

O

x1x2 = 1

y2

y1

O

y2
1 −

y2
2 = 1

8.2. Orthogonal Diagonalization

423

If A is symmetric and a set of orthogonal eigenvectors of A is given,
the eigenvectors are called principal axes of A. The name comes from
geometry. An expression q = ax2
2 is called a quadratic form
in the variables x1 and x2, and the graph of the equation q = 1 is called a
conic in these variables. For example, if q = x1x2, the graph of q = 1 is
given in the ﬁrst diagram.

1 +bx1x2 +cx2

−

y2, then q becomes q = y2

But if we introduce new variables y1 and y2 by setting x1 = y1 + y2 and
y2
x2 = y1
2, a diagonal form with no cross
1 −
term y1y2 (see the second diagram). Because of this, the y1 and y2 axes
are called the principal axes for the conic (hence the name). Orthogonal
diagonalization provides a systematic method for ﬁnding principal axes.
Here is an illustration.

Example 8.2.6

Find principal axes for the quadratic form q = x2

1 −

4x1x2 + x2
2.

Solution. In order to utilize diagonalization, we ﬁrst express q in matrix form. Observe that

The matrix here is not symmetric, but we can remedy that by writing

q =

x1 x2

(cid:2)

(cid:3)

1
0

4
−
1

x1
x2

(cid:21)

(cid:21) (cid:20)

(cid:20)

q = x2

1 −

2x1x2

−

2x2x1 + x2
2

Then we have

where x =

x1
x2

(cid:20)

(cid:21)

and A =

q =

x1 x2

(cid:2)
1
2
−

(cid:20)

2
−
1

(cid:21)

1
2
−

(cid:20)

2
−
1

x1
x2

(cid:21) (cid:20)

(cid:21)

= xT Ax

(cid:3)
is symmetric. The eigenvalues of A are λ1 = 3 and

1, with corresponding (orthogonal) eigenvectors x1 =

λ2 =

x1

k

k

−
=

= √2, so

x2

k

k

P = 1
√2

(cid:20)

1
1
−

(cid:21)

(cid:20)

and x2 =

1
1

. Since

(cid:21)

(cid:20)

(cid:21)

1 1
1 1

is orthogonal and PT AP = D =

(cid:21)

3
0

(cid:20)

0
1
−

Now deﬁne new variables

= y by y = PT x, equivalently x = Py (since P−

1 = PT ). Hence

(x1

−

x2)

and

y2 = 1
√2

(x1 + x2)

In terms of y1 and y2, q takes the form

q = xT Ax = (Py)T A(Py) = yT (PT AP)y = yT Dy = 3y2

Note that y = PT x is obtained from x by a counterclockwise rotation of π

y2
2

1 −
4 (see Theorem 2.4.6).

−
y1
y2

(cid:21)
(cid:20)
y1 = 1
√2

424

Orthogonality

Observe that the quadratic form q in Example 8.2.6 can be diagonalized in other ways. For example

q = x2

1 −

4x1x2 + x2

2 = z2

1 −

1

3 z2

2

2x2 and z2 = 3x2. We examine this more carefully in Section 8.9.

where z1 = x1

−

If we are willing to replace “diagonal” by “upper triangular” in the principal axes theorem, we can

weaken the requirement that A is symmetric to insisting only that A has real eigenvalues.

Theorem 8.2.5: Triangulation Theorem

If A isan n
uppertriangular.4

×

n matrixwith n realeigenvalues,anorthogonalmatrix P existssuchthat PT AP is

x1
Proof. We modify the proof of Theorem 8.2.2. If Ax1 = λ1x1 where
k
orthonormal basis of Rn, and let P1 =
. Then P1 is orthogonal and PT
x1 x2
in block form. By induction, let QT A1Q = T1 be upper triangular where Q is of size (n

= 1, let

· · ·

xn

k

{

x1, x2, . . . , xn

be an

1 AP1 =

}
λ1 B
0 A1

(cid:2)

(cid:20)
(n
1)
×
λ1 BQ
is orthogonal, so P = P1P2 is also orthogonal and PT AP =
T1
0

−

−

(cid:3)

(cid:21)
1) and

orthogonal. Then P2 =

is upper triangular.

1 0
0 Q

(cid:20)

(cid:21)

(cid:20)

(cid:21)

The proof of Theorem 8.2.5 gives no way to construct the matrix P. However, an algorithm will be given in
Section 11.1 where an improved version of Theorem 8.2.5 is presented. In a different direction, a version
of Theorem 8.2.5 holds for an arbitrary matrix with complex entries (Schur’s theorem in Section 8.7).

As for a diagonal matrix, the eigenvalues of an upper triangular matrix are displayed along the main
diagonal. Because A and PT AP have the same determinant and trace whenever P is orthogonal, Theo-
rem 8.2.5 gives:

Corollary 8.2.1

If A isan n
det A = λ1λ2 . . .λn and tr A = λ1 +λ2 +

×

n matrixwithrealeigenvaluesλ1, λ2, . . . , λn (possiblynotalldistinct),then

+λn.

· · ·

This corollary remains true even if the eigenvalues are not real (using Schur’s theorem).

Exercises for 8.2

Exercise 8.2.1 Normalize the rows to make each of the
following matrices orthogonal.

d.

A =

a b
b a

−

(cid:21)

(cid:20)

, (a, b)

= (0, 0)

a.

A =

1 1
1 1

(cid:21)

(cid:20)

−

b.

A =

3
4

4
−
3

(cid:21)

(cid:20)

c.

A =

1 2
4 2

(cid:21)
4There is also a lower triangular version.

−

(cid:20)

e.

A =





sinθ 0
cosθ
−
sinθ cosθ 0
2

0

0





6
8.2. Orthogonal Diagonalization

425

a)(x + a) and ﬁnd an orthogonal ma-
cA(x) = (x
−
1AP is diagonal.
trix P such that P−

b)(x

−

Exercise 8.2.8 Given A =

, show that

cA(x) = (x
trix P such that P−

−

−

a

b)(x + a

−
1AP is diagonal.

(cid:20)
b) and ﬁnd an orthogonal ma-

(cid:21)

b a
a b

Exercise 8.2.9 Consider A =

b 0 a
0 b 0
a 0 b 
b + a) and ﬁnd an orthog-



. Show that



cA(x) = (x
a)(x
onal matrix P such that P−

b)(x

−

−

−

b

−

1AP is diagonal.

Exercise 8.2.10 In each case ﬁnd new variables y1 and
y2 that diagonalize the quadratic form q.

a.

q = x2

1 + 6x1x2 + x2
2

b.

q = x2

1 + 4x1x2

2x2
2

−

Exercise 8.2.11 Show that the following are equivalent
for a symmetric matrix A.

a.

c.

A is orthogonal.

b.

A2 = I.

All eigenvalues of A are

1.

±

[Hint: For (b) if and only if (c), use Theorem 8.2.2.]

Exercise 8.2.12 We call matrices A and B orthogonally
B) if B = PT AP for an orthogonal
similar (and write A ◦
∼
matrix P.

a. Show that A ◦
∼
B and B ◦
∼

A ◦
∼

A for all A; A ◦
∼
C

B

B ◦
∼

⇒

A ◦
∼
b. Show that the following are equivalent for two

⇒

C.

A; and

f.

A =

g.

A =

h.

A =











2
1
0

1
1
1

−

1
−
1
1

−

1
2
2

2
1
2

−

2 6
3 2
6 3

−

−
3
−
6
2




2
2
1







−




Exercise 8.2.2 If P is a triangular orthogonal matrix,
show that P is diagonal and that all diagonal entries are 1
or

1.

Exercise 8.2.3 If P is orthogonal, show that kP is or-
thogonal if and only if k = 1 or k =

1.

Exercise 8.2.4 If the ﬁrst two rows of an orthogonal ma-
trix are ( 1
2
3 ), ﬁnd all possible third
3 , −
rows.

3 ) and ( 2

3 , 1

3 , 2

3 , 2

−

Exercise 8.2.5 For each matrix A, ﬁnd an orthogonal
matrix P such that P−

1AP is diagonal.

b.

A =

d.

A =

f.

A =

(cid:20)









1
1

−

1
1

−
3 0 7
0 5 0
7 0 3

(cid:21)



5
2
4

−
−


−
−

4
2
5

2
8
2

−

−





a.

A =

c.

A =

e.

A =

(cid:20)









g.

A = 





h.

A = 





0 1
1 0

(cid:21)

3 0 0
0 2 2
0 2 5

1 1 0
1 1 0
0 0 2








5 3 0 0
3 5 0 0
0 0 7 1
0 0 1 7

−

3
5
1
1

−

5
3
1
1

−






1
1
3
5

Exercise 8.2.6 Consider A =



−

1
1
5
3

symmetric matrices A and B.

i. A and B are similar.

ii. A and B are orthogonally similar.

iii. A and B have the same eigenvalues.

where one



Exercise 8.2.13 Assume that A and B are orthogonally
similar (Exercise 8.2.12).






0 a 0
a 0 c
0 c 0

= 0. Show that cA(x) = x(x

of a, c
k)(x + k), where
k = √a2 + c2 and ﬁnd an orthogonal matrix P such that
P−

1AP is diagonal.

−





Exercise 8.2.7 Consider A =

0 0 a
0 b 0
a 0 0









. Show that

a. If A and B are invertible, show that A−

1 and B−

1

are orthogonally similar.

b. Show that A2 and B2 are orthogonally similar.

c. Show that, if A is symmetric, so is B.

6
426

Orthogonality

Exercise 8.2.14
If A is symmetric, show that every
eigenvalue of A is nonnegative if and only if A = B2 for
some symmetric matrix B.

Exercise 8.2.15 Prove the converse of Theorem 8.2.3:

If (Ax)
A is symmetric.

·

y = x

(Ay) for all n-columns x and y, then

·

Exercise 8.2.16 Show that every eigenvalue of A is zero
if and only if A is nilpotent (Ak = 0 for some k

1).

≥

Exercise 8.2.17
A = B +C where B is symmetric and C is nilpotent.
[Hint: Theorem 8.2.5.]

If A has real eigenvalues, show that

Exercise 8.2.18 Let P be an orthogonal matrix.

a. Show that det P = 1 or det P =

1.

−

2 examples of P such that det P = 1 and
1.

b. Give 2
×
det P =
−
c. If det P =

1, show that I + P has no inverse.

[Hint: PT (I + P) = (I + P)T .]

−

d. If P is n

n and det P

1)n, show that I

= (

−

P

−

×

has no inverse.
[Hint: PT (I

a. Let A be an m
are equivalent.

×

n matrix. Show that the following

i. A has orthogonal rows.

ii. A can be factored as A = DP, where D is in-
vertible and diagonal and P has orthonormal
rows.

iii. AAT is an invertible, diagonal matrix.

b. Show that an n

n matrix A has orthogonal rows
if and only if A can be factored as A = DP, where
P is orthogonal and D is diagonal and invertible.

×

Exercise 8.2.23 Let A be a skew-symmetric matrix; that
is, AT =

A. Assume that A is an n

n matrix.

−

×

a. Show that I + A is invertible.

[Hint: By Theo-
rem 2.4.5, it sufﬁces to show that (I + A)x = 0,
x = xT x, and
x in Rn, implies x = 0. Compute x
use the fact that Ax =

x and A2x = x.]

·

−

P) =

−

(I

−

−

P)T .]

b. Show that P = (I

−

A)(I + A)−

1 is orthogonal.

Exercise 8.2.19 We call a square matrix E a projection
matrix if E 2 = E = E T . (See Exercise 8.1.17.)

a. If E is a projection matrix, show that P = I

is orthogonal and symmetric.

2E

−

b. If P is orthogonal and symmetric, show that

P) is a projection matrix.

E = 1

2 (I

−
c. If U is m

n and U TU = I (for example, a unit
column in Rn), show that E = UU T is a projection
matrix.

×

Exercise 8.2.20 A matrix that we obtain from the iden-
tity matrix by writing its rows in a different order is called
a permutation matrix. Show that every permutation
matrix is orthogonal.

Exercise 8.2.21 If the rows r1, . . . , rn of the n
n ma-
trix A = [ai j] are orthogonal, show that the (i, j)-entry of
A−

×

1 is

a ji
r j
k
Exercise 8.2.22

2 .
k

c. Show that every orthogonal matrix P such that
I + P is invertible arises as in part (b) from some
skew-symmetric matrix A.
[Hint: Solve P = (I

1 for A.]

A)(I + A)−

−

Exercise 8.2.24 Show that the following are equivalent
for an n

n matrix P.

×

a. P is orthogonal.

b.

c.

Px
k

k

=

x
k

k

for all columns x in Rn.

Px
k
Rn.

Py
k

−

=

x
k

−

y
k

for all columns x and y in

d. (Px)

·

(Py) = x

·

y for all columns x and y in Rn.

(d), see Exercise 5.3.14(a). For
(a), show that column i of P equals Pei,

[Hints: For (c)
(d)
where ei is column i of the identity matrix.]

⇒

⇒

6
Exercise 8.2.25

Show that every 2

onal matrix has

the form

cosθ
sinθ

(cid:20)

sinθ
cosθ

−

(cid:21)

for some angle θ.

cosθ
sinθ

(cid:20)

×
−

2 orthog-
sinθ
cosθ

or

(cid:21)

8.3. Positive Deﬁnite Matrices

427

[Hint: If a2 + b2 = 1, then a = cosθ and b = sinθ for
some angle θ.]

Exercise 8.2.26 Use Theorem 8.2.5 to show that every
symmetric matrix is orthogonally diagonalizable.

8.3 Positive Deﬁnite Matrices

All the eigenvalues of any symmetric matrix are real; this section is about the case in which the eigenvalues
are positive. These matrices, which arise whenever optimization (maximum and minimum) problems are
encountered, have countless applications throughout science and engineering. They also arise in statistics
(for example, in factor analysis used in the social sciences) and in geometry (see Section 8.9). We will
encounter them again in Chapter 10 when describing all inner products in Rn.

Deﬁnition 8.5 Positive Deﬁnite Matrices

Asquarematrixiscalledpositivedeﬁniteifitissymmetricandallitseigenvaluesλ arepositive,
thatisλ > 0.

Because these matrices are symmetric, the principal axes theorem plays a central role in the theory.

Theorem 8.3.1

If A ispositivedeﬁnite,thenitisinvertibleand det A > 0.

Proof. If A is n
axes theorem (or the corollary to Theorem 8.2.5).

n and the eigenvalues are λ1, λ2, . . . , λn, then det A = λ1λ2

×

λn > 0 by the principal

· · ·

If x is a column in Rn and A is any real n

n matrix, we view the 1

1 matrix xT Ax as a real number.

×
With this convention, we have the following characterization of positive deﬁnite matrices.

×

Theorem 8.3.2
Asymmetricmatrix A ispositivedeﬁniteifandonlyifxT Ax> 0 foreverycolumnx

= 0in Rn.

Proof. A is symmetric so, by the principal axes theorem, let PT AP = D = diag (λ1, λ2, . . . , λn) where
T
P−
Then

1 = PT and the λi are the eigenvalues of A. Given a column x in Rn, write y = PT x =

. . . yn

y1 y2

.

xT Ax = xT (PDPT )x = yT Dy = λ1y2

1 +λ2y2

2 +

+λny2
n

(cid:2)

(cid:3)
(8.3)

· · ·

= 0, then xT Ax > 0 by (8.3) because some y j

= 0 and every λi > 0. Con-
= 0 where e j is column j of In. Then y = e j, so (8.3)

= 0, let x = Pe j

If A is positive deﬁnite and x
versely, if xT Ax > 0 whenever x
reads λj = xT Ax > 0.

Note that Theorem 8.3.2 shows that the positive deﬁnite matrices are exactly the symmetric matrices A for
which the quadratic form q = xT Ax takes only positive values.

6
6
6
6
6
428

Orthogonality

Example 8.3.1

If U is any invertible n

×

n matrix, show that A = U TU is positive deﬁnite.

Solution. If x is in Rn and x

= 0, then

xT Ax = xT (U TU )x = (U x)T (U x) =

U x
k
k
= 0 (U is invertible). Hence Theorem 8.3.2 applies.

2 > 0

because U x

It is remarkable that the converse to Example 8.3.1 is also true. In fact every positive deﬁnite matrix
A can be factored as A = U TU where U is an upper triangular matrix with positive elements on the main
diagonal. However, before verifying this, we introduce another concept that is central to any discussion of
positive deﬁnite matrices.

If A is any n

n matrix, let (r)A denote the r
the matrix obtained from A by deleting the last n
(n)A = A are called the principal submatrices of A.

×
−

×

r submatrix in the upper left corner of A; that is, (r)A is
r rows and columns. The matrices (1)A, (2)A, (3)A, . . . ,

Example 8.3.2

If A =

10 5 2
5 3 2
2 2 3 






Lemma 8.3.1

then (1)A = [10], (2)A =

10 5
5 3

(cid:21)

(cid:20)

and (3)A = A.

If A ispositivedeﬁnite,soiseachprincipalsubmatrix (r)A for r = 1, 2, . . . , n.

Proof. Write A =

in block form. If y

= 0 in Rr, write x =

Then x

= 0, so the fact that A is positive deﬁnite gives

(r)A P
Q R

(cid:20)

(cid:21)

in Rn.

y
0

(cid:20)

(cid:21)

0 < xT Ax =

yT 0

(cid:2)

(cid:3)

(r)A P
Q R

y
0

(cid:21)

(cid:21) (cid:20)

(cid:20)

= yT ((r)A)y

This shows that (r)A is positive deﬁnite by Theorem 8.3.2.5

If A is positive deﬁnite, Lemma 8.3.1 and Theorem 8.3.1 show that det ((r)A) > 0 for every r. This
proves part of the following theorem which contains the converse to Example 8.3.1, and characterizes the
positive deﬁnite matrices among the symmetric ones.

5A similar argument shows that, if B is any matrix obtained from a positive deﬁnite matrix A by deleting certain rows and

deleting the same columns, then B is also positive deﬁnite.

6
6
6
6
8.3. Positive Deﬁnite Matrices

429

Theorem 8.3.3

Thefollowingconditionsareequivalentforasymmetric n

1. A ispositivedeﬁnite.

n matrix A:

×

2. det ((r)A) > 0 foreach r = 1, 2, . . . , n.

3. A = U TU whereU isanuppertriangularmatrixwithpositiveentriesonthemaindiagonal.

Furthermore,thefactorizationin(3)isunique(calledtheCholeskyfactorization6of A).

Proof. First, (3)

(1) by Example 8.3.1, and (1)

(2) by Lemma 8.3.1 and Theorem 8.3.1.

⇒

⇒

(2)

(3). Assume (2) and proceed by induction on n. If n = 1, then A = [a] where a > 0 by (2), so
take U = [√a]. If n > 1, write B =(n
1) A. Then B is symmetric and satisﬁes (2) so, by induction, we
−
have B = U TU as in (3) where U is of size (n
1). Then, as A is symmetric, it has block form

⇒

(n

1)

−

×

−

1 and b is in R. If we write x = (U T )−
where p is a column in Rn
−

1p and c = b

xT x,

−

A =

B p
pT b

(cid:20)

(cid:21)

block multiplication gives

A =

U TU p
pT
b

=

U T 0
xT
1

U x
0 c

(cid:20)
as the reader can verify. Taking determinants and applying Theorem 3.1.5 gives det A = det (U T ) det U
c = c( det U )2. Hence c > 0 because det A > 0 by (2), so the above factorization can be written

(cid:21) (cid:20)

(cid:21)

(cid:21)

(cid:20)

·

Since U has positive diagonal entries, this proves (3).

A =

(cid:20)

U T
0
xT √c

U x
0 √c

(cid:21)

(cid:21) (cid:20)

As to the uniqueness, suppose that A = U TU = U T

1 U1 are two Cholesky factorizations. Now write
1
1 = (U T )−
D = UU −
1 , and lower triangular, because
D = (U T )−
1U T
1 , and so it is a diagonal matrix. Thus U = DU1 and U1 = DU , so it sufﬁces to show that
D = I. But eliminating U1 gives U = D2U , so D2 = I because U is invertible. Since the diagonal entries
of D are positive (this is true of U and U1), it follows that D = I.

1
1 . Then D is upper triangular, because D = UU −

1U T

The remarkable thing is that the matrix U in the Cholesky factorization is easy to obtain from A using
row operations. The key is that Step 1 of the following algorithm is possible for any positive deﬁnite
matrix A. A proof of the algorithm is given following Example 8.3.3.

Algorithm for the Cholesky Factorization

If A isapositivedeﬁnitematrix,theCholeskyfactorization A = U TU canbeobtainedasfollows:

Step1. Carry A toanuppertriangularmatrixU1 withpositivediagonalentriesusingrow

operationseachofwhichaddsamultipleofarowtoalowerrow.

Step2. ObtainU fromU1 bydividingeachrowofU1 bythesquarerootofthediagonalentryin

thatrow.

6Andre-Louis Cholesky (1875–1918), was a French mathematician who died in World War I. His factorization was published

in 1924 by a fellow ofﬁcer.

430

Orthogonality

Example 8.3.3

Find the Cholesky factorization of A =



10 5 2
5 3 2
.
2 2 3 




Solution. The matrix A is positive deﬁnite by Theorem 8.3.3 because det (1)A = 10 > 0,
det (2)A = 5 > 0, and det (3)A = det A = 3 > 0. Hence Step 1 of the algorithm is carried out as
follows:

10 5 2
5 3 2
2 2 3 


→ 



A =





2
10 5
0 1
1
2
0 1 13
5

10 5 2
0 1
1
2
0 0 3
5

= U1





Now carry out Step 2 on U1 to obtain U = 





The reader can verify that U TU = A.



→ 



5
√10
1
√2
0


2
√10
√2

√3
√5


.






√10

0

0

Proof of the Cholesky Algorithm. If A is positive deﬁnite, let A = U TU be the Cholesky factorization,
and let D = diag (d1, . . . , dn) be the common diagonal of U and U T . Then U T D−
1 is lower triangular
with ones on the diagonal (call such matrices LT-1). Hence L = (U T D−
L
by a sequence of row operations each of which adds a multiple of a row to a lower row (verify; modify
LA by the same sequence of row operations (see the discussion
columns right to left). But then A
1][U TU ] = DU is upper triangular with positive entries
preceding Theorem 2.5.1). Since LA = [D(U T )−
on the diagonal, this shows that Step 1 of the algorithm is possible.

1 is also LT-1, and so In

1)−

→

→

U1 as in Step 1 so that U1 = L1A where L1 is LT-1. Since A is symmetric,

Turning to Step 2, let A

we get

→
L1U T

1 = L1(L1A)T = L1AT LT

1 = L1ALT

1 = U1LT
1

(8.4)

1
Let D1 = diag (e1, . . . , en) denote the diagonal of U1. Then (8.4) gives L1(U T
1 ) = U1LT
1 D−
both upper triangular (right side) and LT-1 (left side), and so must equal In. In particular, U T
Now let D2 = diag (√e1, . . . , √en), so that D2

1
2 U1 we have
2 = D1. If we write U = D−

1
1 D−
1 . This is
1
1
1 = L−
1 D−
1 .

U TU = (U T

2 U1) = U T
1
1
1 D−
2 )(D−

1 (D2

2)−

1U1 = (U T

1
1
1 )U1 = A
1 )U1 = (L−
1 D−

1
This proves Step 2 because U = D−
2 U1 is formed by dividing each row of U1 by the square root of its
diagonal entry (verify).

Exercises for 8.3

Exercise 8.3.1
each of the following matrices.

Find the Cholesky decomposition of

a.

c.

4 3
3 5

12
4
3

(cid:20)



(cid:21)
4
2
1

−


Exercise 8.3.2

b.

d.

(cid:20)





3
1
7

−





2
1

−

1
1

(cid:21)
−
20 4 5
4 2 3
5 3 5





8.4. QR-Factorization

431

Exercise 8.3.8 Let A0 be formed from A by deleting
rows 2 and 4 and deleting columns 2 and 4. If A is posi-
tive deﬁnite, show that A0 is positive deﬁnite.

Exercise 8.3.9 If A is positive deﬁnite, show that
A = CCT where C has orthogonal columns.

Exercise 8.3.10
A = C2 where C is positive deﬁnite.

If A is positive deﬁnite, show that

Exercise 8.3.11 Let A be a positive deﬁnite matrix. If a
is a real number, show that aA is positive deﬁnite if and
only if a > 0.

a. If A is positive deﬁnite, show that Ak is positive

Exercise 8.3.12

deﬁnite for all k

1.

≥
b. Prove the converse to (a) when k is odd.

c. Find a symmetric matrix A such that A2 is positive

deﬁnite but A is not.

Exercise 8.3.3 Let A =

. If a2 < b, show that

A is positive deﬁnite and ﬁnd the Cholesky factorization.

1 a
a b

(cid:20)

(cid:21)

Exercise 8.3.4 If A and B are positive deﬁnite and r > 0,
show that A + B and rA are both positive deﬁnite.

Exercise 8.3.5 If A and B are positive deﬁnite, show that

A 0
0 B

is positive deﬁnite.

(cid:21)

(cid:20)
Exercise 8.3.6 If A is an n
and U is an n
×
positive deﬁnite.

n positive deﬁnite matrix
m matrix of rank m, show that U T AU is

×

Exercise 8.3.7 If A is positive deﬁnite, show that each
diagonal entry is positive.

8.4 QR-Factorization7

a. Suppose an invertible matrix A can be factored in
Mnn as A = LDU where L is lower triangular with
1s on the diagonal, U is upper triangular with 1s
on the diagonal, and D is diagonal with positive
diagonal entries. Show that the factorization is
unique: If A = L1D1U1 is another such factoriza-
tion, show that L1 = L, D1 = D, and U1 = U .

b. Show that a matrix A is positive deﬁnite if and
only if A is symmetric and admits a factorization
A = LDU as in (a).

. . . , n.

Exercise 8.3.13 Let A be positive deﬁnite and write
dr = det (r)A for each r = 1, 2,
If U is the
upper triangular matrix obtained in step 1 of the algo-
rithm, show that the diagonal elements u11, u22, . . . , unn
of U are given by u11 = d1, u j j = d j/d j
1 if j > 1.
[Hint: If LA = U where L is lower triangular with 1s
on the diagonal, use block multiplication to show that
det (r)A = det (r)U for each r.]

−

One of the main virtues of orthogonal matrices is that they can be easily inverted—the transpose is the
inverse. This fact, combined with the factorization theorem in this section, provides a useful way to
simplify many matrix calculations (for example, in least squares approximation).

Deﬁnition 8.6 QR-factorization

Let A bean m
A = QR where Q is m

×

n matrixwithindependentcolumns. AQR-factorizationof A expressesitas

n withorthonormalcolumnsand R isaninvertibleanduppertriangular

×

7This section is not used elsewhere in the book

432

Orthogonality

matrixwithpositivediagonalentries.

The importance of the factorization lies in the fact that there are computer algorithms that accomplish it
with good control over round-off error, making it particularly useful in matrix calculations. The factoriza-
tion is a matrix version of the Gram-Schmidt process.

Suppose A =

n matrix with linearly independent columns c1, c2, . . . , cn.
The Gram-Schmidt algorithm can be applied to these columns to provide orthogonal columns f1, f2, . . . , fn
(cid:2)
where f1 = c1 and

is an m

c1 c2

· · ·

cn

×

(cid:3)

f1
ck·
f2
2 f1 + ck·
2 f2
f1
f2
k
k
k
k
for each k = 2, 3, . . . , n. Now write qk = 1
fkk
k
and the above equation becomes

fk = ck

−

− · · · −

ck·
fk
k

fk
1
2 fk
−
1k
−

1
−

fk for each k. Then q1, q2, . . . , qn are orthonormal columns,

·
−
Using these equations, express each ck as a linear combination of the qi:

k

k

·

·

fk

qk = ck

(ck

q1)q1 −

(ck

q2)q2 − · · · −

(ck

qk

1)qk
−

1
−

q1
q1)q1 +
f2
q1)q1 + (c3

k

k
·
·

q2
q2)q2 +

k
·

f3

k

k

q3

k

f1
c1 =
c2 = (c2
c3 = (c3
...
cn = (cn

...

· · ·
·
These equations have a matrix form that gives the required factorization:

·

·

q1)q1 + (cn

q2)q2 + (cn

q3)q3 +

+

fn

k

k

qn

q1
q2
q3

cn
cn
cn

k

· · ·
· · ·
· · ·
. . .

· · ·

·
·
·
...
fn

k










(8.5)

A =

c1 c2 c3

cn

· · ·

(cid:2)

(cid:2)

=

q1 q2 q3

qn

· · ·

(cid:3)

k



c2

q1 c3
c3

q1
q2

k

k

k

f1
0
0
...
0

·
f2
0
...
0

·
·
f3
...
0

k

k

(cid:3)








(cid:3)

has orthonormal columns, and the second factor is an
n upper triangular matrix R with positive diagonal entries (and so is invertible). We record this in the

q1 q2 q3

qn

· · ·

Here the ﬁrst factor Q =
n
following theorem.

×

(cid:2)

Theorem 8.4.1: QR-Factorization

Every m
hasorthonormalcolumnsand R isuppertriangularwithpositivediagonalentries.

n matrix A withlinearlyindependentcolumnshasaQR-factorization A = QR where Q

×

The matrices Q and R in Theorem 8.4.1 are uniquely determined by A; we return to this below.

Example 8.4.1

Find the QR-factorization of A = 

1 1 0
1 0 1
0 1 1
0 0 1

−

8.4. QR-Factorization

433

.





Solution. Denote the columns of A as c1, c2, and c3, and observe that
{
If we apply the Gram-Schmidt algorithm to these columns, the result is:





c1, c2, c3

is independent.

}

1
2
1
2
1
0

0

0



0

1







√2

1
1
−
0
0

,







f1 = c1 = 





f2 = c2

−



, and

f3 = c3 + 1

1
2f1 = 




is orthonormal. Then equation (8.5) preceding

f2 = 






2f1









−



.

0
0
0
1

Write q j = 1
2 f j for each j, so
f j
k
k
Theorem 8.4.1 gives A = QR where

{

q1, q2, q3}

q1 q2 q3

=



(cid:3)







q1 c3
·
f2
c3
0

k

·
·
f3

q1
q2

k

k

k

k

f1
0
0

c2

k

Q =

R =

(cid:2)





1
√2
1
−
√2
0

0

1
√6
1
√6
2
√6
0





0

0

= 





√3
0
1
√3 1
0
0
2
0
0 √6
0

−







= 1

√6 




1
1
√2 −
√2
√3
√3
√2
√2
1
0

1
1
2
−
0 √3 √3
0 √2
0





= 1

√2 










The reader can verify that indeed A = QR.

If a matrix A has independent rows and we apply QR-factorization to AT , the result is:

Corollary 8.4.1

If A hasindependentrows,then A factorsuniquelyas A = LP where P hasorthonormalrowsand L
isaninvertiblelowertriangularmatrixwithpositivemaindiagonalentries.

Since a square matrix with orthonormal columns is orthogonal, we have

Theorem 8.4.2

Everysquare,invertiblematrix A hasfactorizations A = QR and A = LP where Q and P are
orthogonal, R isuppertriangularwithpositivediagonalentries,and L islowertriangularwith
positivediagonalentries.

434

Orthogonality

Remark
In Section 5.6 we found how to ﬁnd a best approximation z to a solution of a (possibly inconsistent) system
Ax = b of linear equations: take z to be any solution of the “normal” equations (AT A)z = AT b. If A has
independent columns this z is unique (AT A is invertible by Theorem 5.4.3), so it is often desirable to com-
pute (AT A)−
1. This is particularly useful in least squares approximation (Section 5.6). This is simpliﬁed
if we have a QR-factorization of A (and is one of the main reasons for the importance of Theorem 8.4.1).
For if A = QR is such a factorization, then QT Q = In because Q has orthonormal columns (verify), so we
obtain

AT A = RT QT QR = RT R

Hence computing (AT A)−
gular. Thus the difﬁculty in computing (AT A)−

1 amounts to ﬁnding R−

1, and this is a routine matter because R is upper trian-

1 lies in obtaining the QR-factorization of A.

We conclude by proving the uniqueness of the QR-factorization.

Theorem 8.4.3

Let A bean m
QR-factorizationsof A,then Q1 = Q and R1 = R.

×

n matrixwithindependentcolumns. If A = QR and A = Q1R1 are

c1 c2

Proof. Write Q =
serve ﬁrst that QT Q = In = QT
that Q1 = Q (then R1 = QT
for convenience we write this matrix as

cn

(cid:2)

and Q1 =

dn
in terms of their columns, and ob-
· · ·
1 Q1 because Q and Q1 have orthonormal columns. Hence it sufﬁces to show
1;

(cid:2)
(cid:3)
1 Q1 = In, the equation QR = Q1R1 gives QT
1 A = QT A = R). Since QT

1 Q = R1R−

d1 d2

· · ·

(cid:3)

QT

1 Q = R1R−

1 =

ti j

This matrix is upper triangular with positive diagonal elements (since this is true for R and R1), so tii > 0
c j, so we have
for each i and ti j = 0 if i > j. On the other hand, the (i,
1). Hence the
di
expansion theorem gives

c j = ti j for all i and j. But each c j is in span

·
because Q = Q1(R1R−

d1, d2, . . . , dn

1 Q is dT

i c j = di

(cid:2)
j)-entry of QT

}

{

(cid:3)

·

c j = (d1

c j)d1 + (d2

c j)d2 +

+ (dn

c j)dn = t1 jd1 + t2 jd2 +

·
c j = ti j = 0 if i > j. The ﬁrst few equations here are

· · ·

·

·

+ t j jdi

· · ·

because di

·

c1 = t11d1
c2 = t12d1 + t22d2
c3 = t13d1 + t23d2 + t33d3
c4 = t14d1 + t24d2 + t34d3 + t44d4
...

...

= t11, whence c1 = d1. But then we
d1
The ﬁrst of these equations gives 1 =
have t12 = d1
c2 = 0, so the second equation becomes c2 = t22d2. Now a similar argument gives
c2 = c1
c2 = d2, and then t13 = 0 and t23 = 0 follows in the same way. Hence c3 = t33d3 and c3 = d3. Continue in
this way to get ci = di for all i. This means that Q1 = Q, which is what we wanted.

t11d1
k

t11
|

c1

|k

=

=

k

k

k

k

·

·

8.5. Computing Eigenvalues

435

Exercises for 8.4

Exercise 8.4.1 In each case ﬁnd the QR-factorization of
A.

c. If AB has a QR-factorization, show that the same

is true of B but not necessarily A.

a.

A =

(cid:20)

c.

A = 

1
1

−

1
0

−
1 1 1
1 1 0
1 0 0
0 0 0

(cid:21)











b.

A =

(cid:20)

2 1
1 1

d.

A = 

−

1
1
0
1





(cid:21)

−

1 0
0 1
1 1
1 0







[Hint: Consider AAT where A =

1 0 0
1 1 1

.]

(cid:21)

(cid:20)

Exercise 8.4.3 If R is upper triangular and invertible,
show that there exists a diagonal matrix D with diagonal
1 such that R1 = DR is invertible, upper trian-
entries
gular, and has positive diagonal entries.

±

Exercise 8.4.2 Let A and B denote matrices.

a. If A and B have independent columns, show
that AB has independent columns. [Hint: Theo-
rem 5.4.3.]

b. Show that A has a QR-factorization if and only if

A has independent columns.

Exercise 8.4.4 If A has independent columns, let
A = QR where Q has orthonormal columns and R is in-
vertible and upper triangular. [Some authors call this a
QR-factorization of A.] Show that there is a diagonal ma-
trix D with diagonal entries
1 such that A = (QD)(DR)
±
is the QR-factorization of A. [Hint: Preceding exercise.]

8.5 Computing Eigenvalues

In practice, the problem of ﬁnding eigenvalues of a matrix is virtually never solved by ﬁnding the roots
of the characteristic polynomial. This is difﬁcult for large matrices and iterative methods are much better.
Two such methods are described brieﬂy in this section.

The Power Method

In Chapter 3 our initial rationale for diagonalizing matrices was to be able to compute the powers of a
square matrix, and the eigenvalues were needed to do this. In this section, we are interested in efﬁciently
computing eigenvalues, and it may come as no surprise that the ﬁrst method we discuss uses the powers
of a matrix.

Recall that an eigenvalue λ of an n

1, and

×

n matrix A is called a dominant eigenvalue if λ has multiplicity

>

λ
|
|

µ
|
|

for all eigenvalues µ

= λ

Any corresponding eigenvector is called a dominant eigenvector of A. When such an eigenvalue exists,
one technique for ﬁnding it is as follows: Let x0 in Rn be a ﬁrst approximation to a dominant eigenvector
λ, and compute successive approximations x1, x2, . . . as follows:

x1 = Ax0

x2 = Ax1

x3 = Ax2

· · ·

In general, we deﬁne

xk+1 = Axk

for each k

0

≥

If the ﬁrst estimate x0 is good enough, these vectors xn will approximate the dominant eigenvector λ (see
below). This technique is called the power method (because xk = Akx0 for each k
1). Observe that if z
is any eigenvector corresponding to λ, then

≥

2 = z
z
(Az)
(λz)
·
·
z
z
k
k
k

2 = λ
k

6
436

Orthogonality

Because the vectors x1, x2, . . . , xn, . . . approximate dominant eigenvectors, this suggests that we deﬁne
the Rayleigh quotients as follows:

≥
Then the numbers rk approximate the dominant eigenvalue λ.

xk+1
rk = xk·
2
xkk
k

for k

1

Example 8.5.1

Use the power method to approximate a dominant eigenvector and eigenvalue of A =

1 1
2 0

.

(cid:21)

(cid:20)

Solution. The eigenvalues of A are 2 and

1, with eigenvectors

and

. Take

(cid:21)
as the ﬁrst approximation and compute x1, x2, . . . , successively, from

(cid:20)

(cid:21)

(cid:20)

−

1
1

1
2
−

x0 =

1
0

(cid:20)

(cid:21)

x1 = Ax0, x2 = Ax1, . . . . The result is

x1 =

1
2

(cid:21)

(cid:20)

, x2 =

3
2

(cid:20)

(cid:21)

, x3 =

5
6

(cid:21)

(cid:20)

, x4 =

11
10

(cid:21)

(cid:20)

, x3 =

21
22

(cid:21)

(cid:20)

,

. . .

These vectors are approaching scalar multiples of the dominant eigenvector

Rayleigh quotients are

1
1

(cid:21)

(cid:20)

. Moreover, the

13 , r3 = 115
and these are approaching the dominant eigenvalue 2.

5, r2 = 27

r1 = 7

61 , r4 = 451

221 , . . .

To see why the power method works, let λ1, λ2, . . . , λm be eigenvalues of A with λ1 dominant and
let y1, y2, . . . , ym be corresponding eigenvectors. What is required is that the ﬁrst approximation x0 be a
linear combination of these eigenvectors:

x0 = a1y1 + a2y2 +

+ amym with a1

= 0

· · ·

1, the fact that xk = Akx0 and Akyi = λk
1 y1 + a2λk

xk = a1λk

i yi for each i gives
+ amλk
2 y2 +

mym

· · ·

for k

1

≥

If k

≥

Hence

xk = a1y1 + a2

k

y2 +

λ2
λ1

+ am

k

ym

λm
λ1

· · ·

1
λk
1

(cid:16)
The right side approaches a1y1 as k increases because λ1 is dominant
a1

λi
λ1
= 0, this means that xk approximates the dominant eigenvector a1λk
(cid:16)(cid:12)
1 y1.
(cid:12)
(cid:12)
The power method requires that the ﬁrst approximation x0 be a linear combination of eigenvectors.
(In Example 8.5.1 the eigenvectors form a basis of R2.) But even in this case the method fails if a1 = 0,

< 1 for each i > 1

. Because

(cid:12)
(cid:12)
(cid:12)

(cid:17)

(cid:17)

(cid:16)

(cid:17)

where a1 is the coefﬁcient of the dominant eigenvector (try x0 =

in Example 8.5.1). In general,

the rate of convergence is quite slow if any of the ratios
is near 1. Also, because the method requires
repeated multiplications by A, it is not recommended unless these multiplications are easy to carry out (for
example, if most of the entries of A are zero).

λi
λ1

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1
−
2

(cid:20)

(cid:21)

6
6
8.5. Computing Eigenvalues

437

QR-Algorithm

A much better method for approximating the eigenvalues of an invertible matrix A depends on the factor-
ization (using the Gram-Schmidt algorithm) of A in the form

where Q is orthogonal and R is invertible and upper triangular (see Theorem 8.4.2). The QR-algorithm
uses this repeatedly to create a sequence of matrices A1 = A, A2, A3, . . . , as follows:

A = QR

1. Deﬁne A1 = A and factor it as A1 = Q1R1.

2. Deﬁne A2 = R1Q1 and factor it as A2 = Q2R2.

3. Deﬁne A3 = R2Q2 and factor it as A3 = Q3R3.

...

In general, Ak is factored as Ak = QkRk and we deﬁne Ak+1 = RkQk. Then Ak+1 is similar to Ak [in fact,
1
k Ak)Qk], and hence each Ak has the same eigenvalues as A. If the eigenvalues of A are
Ak+1 = RkQk = (Q−
real and have distinct absolute values, the remarkable thing is that the sequence of matrices A1, A2, A3, . . .
converges to an upper triangular matrix with these eigenvalues on the main diagonal. [See below for the
case of complex eigenvalues.]

Example 8.5.2

If A =

1 1
2 0

(cid:21)

(cid:20)

as in Example 8.5.1, use the QR-algorithm to approximate the eigenvalues.

Solution. The matrices A1, A2, and A3 are as follows:

= Q1R1 where Q1 = 1
√5

1
2

2
1
−

(cid:21)

and R1 = 1
√5

5 1
0 2

(cid:20)

(cid:21)

1 1
2 0

A1 =

(cid:20)
A2 = 1
5

7
4

(cid:21)
9
2
(cid:20)
−
where Q2 = 1
√65

=

(cid:21)

(cid:20)

(cid:20)
= Q2R2

1.8
0.4

(cid:21)

and R2 = 1
√65

1.4
0.8

−
−
4
7
−
2.08
0.62

(cid:21)

−
7
4

(cid:20)
=

13 11
0 10

(cid:21)

(cid:20)

A3 = 1
13

27
8

(cid:20)

5
−
14

−

(cid:21)

(cid:20)

0.38
1.08

−
−

(cid:21)

This is converging to

diagonal.

2
0

(cid:20)

∗
1
−

(cid:21)

and so is approximating the eigenvalues 2 and

1 on the main

−

It is beyond the scope of this book to pursue a detailed discussion of these methods. The reader is
referred to J. M. Wilkinson, The Algebraic Eigenvalue Problem (Oxford, England: Oxford University
Press, 1965) or G. W. Stewart, Introduction to Matrix Computations (New York: Academic Press, 1973).
We conclude with some remarks on the QR-algorithm.

438

Orthogonality

Shifting. Convergence is accelerated if, at stage k of the algorithm, a number sk is chosen and Ak
factored in the form QkRk rather than Ak itself. Then

−

skI is

1
1
k (QkRk + skI)Qk = RkQk + skI
k AkQk = Q−
Q−

so we take Ak+1 = RkQk + skI. If the shifts sk are carefully chosen, convergence can be greatly improved.
Preliminary Preparation. A matrix such as

∗ ∗ ∗ ∗ ∗
∗ ∗ ∗ ∗ ∗
0
∗ ∗ ∗ ∗
0 0
∗ ∗ ∗
0 0 0
∗ ∗

















B = HT

m · · ·

HT

1 AH1

Hm

· · ·

is said to be in upper Hessenberg form, and the QR-factorizations of such matrices are greatly simpliﬁed.
n matrix A, a series of orthogonal matrices H1, H2, . . . , Hm (called Householder matrices)
Given an n
can be easily constructed such that

×

is in upper Hessenberg form. Then the QR-algorithm can be efﬁciently applied to B and, because B is
similar to A, it produces the eigenvalues of A.

Complex Eigenvalues. If some of the eigenvalues of a real matrix A are not real, the QR-algorithm con-
verges to a block upper triangular matrix where the diagonal blocks are either 1
1 (the real eigenvalues)
2 (each providing a pair of conjugate complex eigenvalues of A).
or 2

×

×

Exercises for 8.5

Exercise 8.5.1 In each case, ﬁnd the exact eigenvalues
and determine corresponding eigenvectors. Then start

with x0 =

method.

1
1

(cid:21)

(cid:20)

and compute x4 and r3 using the power

a.

A =

c.

A =

2
3

−
1 2
2 1

(cid:20)

(cid:20)

−

4
3

(cid:21)

(cid:21)

b.

A =

d.

A =

5
3

−
3 1
1 0

(cid:20)

(cid:20)

2
2

−

(cid:21)

(cid:21)

Exercise 8.5.2 In each case, ﬁnd the exact eigenvalues
and then approximate them using the QR-algorithm.

a.

A =

1 1
1 0

(cid:20)

(cid:21)

b.

A =

3 1
1 0

(cid:21)

(cid:20)

Exercise 8.5.3 Apply the power method to

A =

(cid:21)
−
verge? Explain.

(cid:20)

0 1
1 0

, starting at x0 =

1
1

(cid:20)

(cid:21)

. Does it con-

Exercise 8.5.4 If A is symmetric, show that each matrix
Ak in the QR-algorithm is also symmetric. Deduce that
they converge to a diagonal matrix.

Exercise 8.5.5 Apply the QR-algorithm to

A =

2
1

(cid:20)

3
2

−
−

(cid:21)

. Explain.

≥

Exercise 8.5.6 Given a matrix A, let Ak, Qk, and Rk,
k
1, be the matrices constructed in the QR-algorithm.
Qk)(Rk
Show that Ak = (Q1Q2
1
· · ·
and hence that this is a QR-factorization of Ak.
1 for each k
[Hint: Show that QkRk = Rk
2, and
R2R1)
Qk)(Rk
use this equality to compute (Q1Q2
“from the centre out.” Use the fact that (AB)n+1 =
A(BA)nB for any square matrices A and B.]

R2R1) for each k

≥
· · ·

1Qk

· · ·

· · ·

≥

−

−

8.6. The Singular Value Decomposition

439

8.6 The Singular Value Decomposition

When working with a square matrix A it is clearly useful to be able to “diagonalize” A, that is to ﬁnd
1DQ where Q is invertible and D is diagonal. Unfortunately such a factorization
a factorization A = Q−
may not exist for A. However, even if A is not square gaussian elimination provides a factorization of
the form A = PDQ where P and Q are invertible and D is diagonal—the Smith Normal form (Theorem
2.5.3). However, if A is real we can choose P and Q to be orthogonal real matrices and D to be real. Such
a factorization is called a singular value decomposition (SVD) for A, one of the most useful tools in
applied linear algebra. In this Section we show how to explicitly compute an SVD for any real matrix A,
and illustrate some of its many applications.

We need a fact about two subspaces associated with an m
x in Rn

im A =

and

Ax

col A = span

n matrix A:

×

a

a is a column of A

}
Rm with x
Then im A is called the image of A (so named because of the linear transformation Rn
and col A is called the column space of A (Deﬁnition 5.10). Surprisingly, these spaces are equal:

→

}

{

{

|

|

Ax);

7→

Lemma 8.6.1

Forany m

×

n matrix A, im A = col A.

Proof. Let A =
y =
im A
(cid:2)
⊆

y1 y2

(cid:2)
· · ·

a1 a2
yn

(cid:3)

an

in terms of its columns. Let x

· · ·

T , then Ay = y1a1 + y2a2 +
(cid:3)

+ ynan

∈

· · ·

im A, say x = Ay, y in Rn.

If
col A by Deﬁnition 2.5. This shows that

∈

col A. For the other inclusion, each ak = Aek where ek is column k of In.

8.6.1 Singular Value Decompositions

We know a lot about any real symmetric matrix: Its eigenvalues are real (Theorem 5.5.7), and it is orthog-
onally diagonalizable by the Principal Axes Theorem (Theorem 8.2.2). So for any real matrix A (square
or not), the fact that both AT A and AAT are real and symmetric suggests that we can learn a lot about A by
studying them. This section shows just how true this is.

The following Lemma reveals some similarities between AT A and AAT which simplify the statement

and the proof of the SVD we are constructing.

Lemma 8.6.2

Let A beareal m

n matrix. Then:

×

1. Theeigenvaluesof AT A and AAT arerealandnon-negative.

2. AT A and AAT havethesamesetofpositiveeigenvalues.

Proof.

1. Let λ be an eigenvalue of AT A, with eigenvector 0

= q

Rn. Then:

∈

Aq
k

k

2 = (Aq)T (Aq) = qT (AT Aq) = qT (λq) = λ(qT q) = λ
k

2

q
k

Then (1.) follows for AT A, and the case AAT follows by replacing A by AT .

6
440

Orthogonality

2. Write N(B) for the set of positive eigenvalues of a matrix B. We must show that N(AT A) = N(AAT ).
Rn, then Aq

N(AT A) with eigenvector 0

Rm and

= q

If λ

∈

∈

∈

AAT (Aq) = A[(AT A)q] = A(λq) = λ(Aq)

Moreover, Aq
AAT , proving N(AT A)

⊆

= 0 since AT Aq = λq

= 0 and both λ

= 0 and q

= 0. Hence λ is an eigenvalue of

N(AAT ). For the other inclusion replace A by AT .

To analyze an m

n matrix A we have two symmetric matrices to work with: AT A and AAT . In view
of Lemma 8.6.2, we choose AT A (sometimes called the Gram matrix of A), and derive a series of facts
which we will need. This narrative is a bit long, but trust that it will be worth the effort. We parse it out in
several steps:

×

1. The n

n matrix AT A is real and symmetric so, by the Principal Axes Theorem 8.2.2, let

×

q1, q2, . . . , qn} ⊆
{
ues λ1, λ2, . . . , λn. By Lemma 8.6.2(1), λi is real for each i and λi
(and do) assume that

Rn be an orthonormal basis of eigenvectors of AT A, with corresponding eigenval-
0. By re-ordering the qi we may

≥

By Theorems 8.2.1 and 3.3.4, the matrix

λ1

λ2

≥

≥ · · · ≥

λr > 0

and 8 λi = 0 if i > r

Q =

q1 q2

qn

· · ·

is orthogonal

and

orthogonally diagonalizes AT A

(i)

(ii)

2. Even though the λi are the eigenvalues of AT A, the number r in (i) turns out to be rank A. To understand

(cid:3)

(cid:2)
why, consider the vectors Aqi ∈
Aqi ·

im A. For all i, j:

Aq j = (Aqi)T Aq j = qT

i (AT A)q j = qT

i (λjq j) = λj(qT

i q j) = λj(qi ·

q j)

Because

{

q1, q2, . . . , qn}
Aqi ·

is an orthonormal set, this gives
Aqik

= j

and

k

Aq j = 0 if i

2 = λi

2 = λi for each i

qik

k

We can extract two conclusions from (iii) and (i):

Aq1, Aq2, . . . , Aqr} ⊆

{

im A is an orthogonal set

and Aqi = 0 if i > r

(iii)

(iv)

With this write U = span
For this we must show that Ax
orthonormal), we can write xk = t1q1 +
obtain

Aq1, Aq2, . . . , Aqr} ⊆
U for each x
∈
+ trqr +

· · ·

∈

{

· · ·

im A; we claim that U = im A, that is im A

Rn. Since

q1, . . . , qr, . . . , qn}

{

+ tnqn where each t j

U .
is a basis of Rn (it is
R. Then, using (iv) we

⊆

Ax = t1Aq1 +

+ trAqr +

· · ·

· · ·

+ tnAqn = t1Aq1 +

· · ·

This shows that U = im A, and so

∈
+ trAqr ∈

U

Aq1, Aq2, . . . , Aqr}

{

is an orthogonal basis of im (A)

But col A = im A by Lemma 8.6.1, and rank A = dim ( col A) by Theorem 5.4.1, so

rank A = dim ( col A) = dim ( im A)

(v)
= r

(v)

(vi)

3. Before proceeding, some deﬁnitions are in order:

8Of course they could all be positive (r = n) or all zero (so AT A = 0, and hence A = 0 by Exercise 5.3.9).

6
6
6
6
6
6
8.6. The Singular Value Decomposition

441

Deﬁnition 8.7

Therealnumbersσi = √λi
matrix A.

(iii)
=

A ¯qik

k

for i = 1, 2, . . . , n,arecalledthesingularvaluesofthe

Clearly σ1, σ2, . . . , σr are the positive singular values of A. By (i) we have

σ1

σ2

≥

≥ · · · ≥

σr > 0

and

σi = 0 if i > r

(vii)

With (vi) this makes the following deﬁnitions depend only upon A.

Deﬁnition 8.8

Let A beareal, m
σi = 0 if i > r. Deﬁne:

×

n matrixofrank r,withpositivesingularvaluesσ1

σ2

≥

≥ · · · ≥

σr > 0 and

DA = diag (σ1, . . . , σr)

and

ΣA =

(cid:20)
Here ΣA isinblockformandiscalledthesingularmatrixof A.

DA 0
0
0

(cid:21)m

n
×

The singular values σi and the matrices DA and ΣA will be referred to frequently below.

4. Returning to our narrative, normalize the vectors Aq1, Aq2, . . . , Aqr, by deﬁning

pi = 1

Aqik
k

Aqi ∈

Rm for each i = 1, 2, . . . , r

By (v) and Lemma 8.6.1, we conclude that

p1, p2, . . . , pr}

{

is an orthonormal basis of col A

Rm

⊆

Employing the Gram-Schmidt algorithm (or otherwise), construct pr+1, . . . , pm so that

p1, . . . , pr, . . . , pm}
5. By (x) and (ii) we have two orthogonal matrices

{

is an orthonormal basis of Rm

(viii)

(ix)

(x)

P =

p1

pr

· · ·

· · ·

pm

of size m

×

m and Q =

q1

qr

· · ·

· · ·

qn

of size n

n

×

(cid:2)

(cid:3)

These matrices are related. In fact we have:

(cid:2)

(cid:3)

σipi =

λipi

(iii)
=

Aqik

k

pi

(viii)
= Aqi

for each i = 1, 2, . . . , r

(xi)

This yields the following expression for AQ in terms of its columns:

p

AQ =

Aq1

Aqr Aqr+1

· · ·

· · ·

(cid:2)

Aqn

(iv)
=

σ1p1

(cid:3)

(cid:2)

σrpr 0

0

· · ·

· · ·

(xii)

(cid:3)

442

Orthogonality

Then we compute:

PΣA =

p1

pr pr+1

pm

· · ·

· · ·

(cid:2)

σ1p1

=
(xii)
= AQ
(cid:2)

σrpr 0

0

· · ·

· · ·

σ1
...
0
0
...
0

· · ·
. . .

· · ·
· · ·

· · ·

0
...
σr
0
...
0

0
...
0
0
...
0

· · ·

· · ·
· · ·

· · ·

0
...
0
0
...
0























(cid:3)

(cid:3)

Finally, as Q−

1 = QT it follows that A = PΣAQT .

With this we can state the main theorem of this Section.

Theorem 8.6.1

Let A beareal m
≥ · · · ≥
Then r istherankof A andwehavethefactorization

n matrix,andletσ1

σ2

≥

×

σr > 0 bethepositivesingularvaluesof A.

A = PΣAQT

where P and Q areorthogonalmatrices

The factorization A = PΣAQT in Theorem 8.6.1, where P and Q are orthogonal matrices, is called a
Singular Value Decomposition (SVD) of A. This decomposition is not unique. For example if r < m then
to an orthonormal basis of Rm, and each
the vectors pr+1, . . . , pm can be any extension of
{
will lead to a different matrix P in the decomposition. For a more dramatic example, if A = In then ΣA = In,
and A = PΣAPT is a SVD of A for any orthogonal n

p1, . . ., pr}
n matrix P.

×

Example 8.6.1

Find a singular value decomposition for A =

1 0 1
1 1 0

−

.
(cid:21)

(cid:20)

Solution. We have AT A =

2
1
−
1





, so the characteristic polynomial is

−

1 1
1 0
0 1 

2

x

−
1
1
−



x

1

−
0

1

1
−
0

x

−

1 


cAT A(x) = det



= (x

3)(x

1)x

−

−

Hence the eigenvalues of AT A (in descending order) are λ1 = 3, λ2 = 1 and λ3 = 0 with,
respectively, unit eigenvectors

q1 = 1

√6 



,

2
1
−
1 


q2 = 1

√2 



,

0
1
1 


and q3 = 1

√3 



1
−
1
−
1 


It follows that the orthogonal matrix Q in Theorem 8.6.1 is

8.6. The Singular Value Decomposition

443

Q =

q1 q2 q3

(cid:2)

= 1

√6 



(cid:3)

√2
2
0
√2
1 √3
1 √3 √2

−
−

−





The singular values here are σ1 = √3, σ2 = 1 and σ3 = 0, so rank (A) = 2—clear in this
case—and the singular matrix is

ΣA =

(cid:20)

0
0
σ1
0 σ2 0

=

(cid:21)

(cid:20)

√3 0 0
1 0
0

(cid:21)

So it remains to ﬁnd the 2

×

2 orthogonal matrix P in Theorem 8.6.1. This involves the vectors

Aq1 = √6
2

(cid:20)
Normalize Aq1 and Aq2 to get

1
1
−

,

(cid:21)

Aq2 = √2
2

1
1

,

(cid:21)

(cid:20)

and Aq3 =

0
0

(cid:21)

(cid:20)

p1 = 1
√2

1
1
−

(cid:21)

(cid:20)

and p2 = 1
√2

1
1

(cid:21)

(cid:20)

is already a basis of R2 (so the Gram-Schmidt algorithm is not needed), and

In this case,
{
we have the 2

p1, p2}
×

2 orthogonal matrix

Finally (by Theorem 8.6.1) the singular value decomposition for A is

(cid:2)

(cid:3)

P =

p1 p2

= 1
√2

1 1
1 1

(cid:20)

−

(cid:21)

A = PΣAQT = 1
√2

1 1
1 1

−

(cid:20)

(cid:21) (cid:20)

√3 0 0
1 0
0

1
√6 

(cid:21)


Of course this can be conﬁrmed by direct matrix multiplication.

1
1
2
−
0 √3 √3
√2 √2

√2

−

−





Thus, computing an SVD for a real matrix A is a routine matter, and we now describe a systematic

procedure for doing so.

SVD Algorithm

Givenareal m

×

n matrix A,ﬁndanSVD A = PΣAQT asfollows:

1. UsetheDiagonalizationAlgorithm(seepage179)toﬁndthe(realandnon-negative)
eigenvaluesλ1, λ2, . . . , λn of AT A withcorresponding(orthonormal)eigenvectors
q1, q2, . . . , qn. Reordertheqi (ifnecessary)toensurethatthenonzeroeigenvaluesare
λ1

λr > 0 andλi = 0 if i > r.

λ2

≥

≥ · · · ≥

2. Theinteger r istherankofthematrix A.

444

Orthogonality

×
4. Deﬁnepi = 1

3. The n

n orthogonalmatrix Q intheSVDis Q =

q1 q2

qn

.

· · ·

Aqik
k

(cid:3)
Aqi for i = 1, 2, . . . , r (where r isasinstep1). Then

is
orthonormalin Rm so(usingGram-Schmidtorotherwise)extendittoanorthonormalbasis
p1, . . . , pr, . . . , pm}
{
5. The m

m orthogonalmatrix P intheSVDis P =

p1, p2, . . . , pr}
{

in Rm.

.

(cid:2)

p1

pr

· · ·

· · ·

pm

×

6. The singular values for A areσ1, σ2, . . . , σn whereσi = √λi foreach i. Hencethenonzero

singularvaluesareσ1

σ2

σr > 0,andsothesingularmatrixof A intheSVDis

(cid:3)

(cid:2)

ΣA =

≥

diag (σ1, . . . , σr) 0
0

0

≥ · · · ≥
.

(cid:20)

(cid:21)m
7. Thus A = PΣQT isaSVDfor A.

n
×

In practise the singular values σi, the matrices P and Q, and even the rank of an m

n matrix are not
calculated this way. There are sophisticated numerical algorithms for calculating them to a high degree of
accuracy. The reader is referred to books on numerical linear algebra.

×

So the main virtue of Theorem 8.6.1 is that it provides a way of constructing an SVD for every real
In particular it shows that every real matrix A has a singular value decomposition9 in the

matrix A.
following, more general, sense:

Deﬁnition 8.9

ASingularValueDecomposition(SVD)ofan m
×
A = U ΣV T whereU andV areorthogonaland Σ =

D = diag (d1, d2, . . . , dr) whereeach di > 0,and r

n matrix A ofrank r isafactorization
D 0
0 0
(cid:21)m
m and r

inblockformwhere

n
×
n.
≤

(cid:20)
≤

Note that for any SVD A = U ΣV T we immediately obtain some information about A:

Lemma 8.6.3
If A = U ΣV T isanySVDfor A asinDeﬁnition8.9,then:

1. r = rank A.

2. Thenumbers d1, d2, . . . , dr arethesingularvaluesof AT A insomeorder.

Proof. Use the notation of Deﬁnition 8.9. We have

AT A = (V ΣTU T )(U ΣV T ) = V (ΣT Σ)V T

so ΣT Σ and AT A are similar n
(1.). Furthermore, ΣT Σ and AT A have the same eigenvalues by Theorem 5.5.1; that is (using (1.)):

n matrices (Deﬁnition 5.11). Hence r = rank A by Corollary 5.4.3, proving

×

1, d2
d2

2, . . . , d2
r }

{

=

λ1, λ2, . . . , λr
{

}

are equal as sets

9In fact every complex matrix has an SVD [J.T. Scheick, Linear Algebra with Applications, McGraw-Hill, 1997]

8.6. The Singular Value Decomposition

445

where λ1, λ2, . . . , λr are the positive eigenvalues of AT A. Hence there is a permutation τ of
such that d2
proves (2.).

, r
}
i = λiτ for each i = 1, 2, . . . , r. Hence di = √λiτ = σiτ for each i by Deﬁnition 8.7. This

1, 2,

· · ·

{

We note in passing that more is true. Let A be m

n of rank r, and let A = U ΣV T be any SVD for A.
Using the proof of Lemma 8.6.3 we have di = σiτ for some permutation τ of
. In fact, it can
be shown that there exist orthogonal matrices U1 and V1 obtained from U and V by τ-permuting columns
and rows respectively, such that A = U1ΣAV T
1 is an SVD of A.

1, 2, . . . , r

×

{

}

8.6.2 Fundamental Subspaces

It turns out that any singular value decomposition contains a great deal of information about an m
×
n matrix A and the subspaces associated with A. For example, in addition to Lemma 8.6.3, the set
p1, p2, . . . , pr}
of vectors constructed in the proof of Theorem 8.6.1 is an orthonormal basis of col A
{
(by (v) and (viii) in the proof). There are more such examples, which is the thrust of this subsection.
n matrix A that have come to be called
In particular, there are four subspaces associated to a real m
fundamental:

×

Deﬁnition 8.10

Thefundamentalsubspacesofan m

n matrix A are:

×

row A = span

col A = span

xisarowof A

|
xisacolumnof A

}

}

x
{
x
{
|
Rn

null A =

x
{
null AT =

|
Rn

∈
x
{

∈

Ax= 0
}

AT x= 0
}

|

If A = U ΣV T is any SVD for the real m
n matrix A, any orthonormal bases of U and V provide orthonor-
mal bases for each of these fundamental subspaces. We are going to prove this, but ﬁrst we need three
properties related to the orthogonal complement U ⊥ of a subspace U of Rn, where (Deﬁnition 8.1):

×

U ⊥ =

Rn

x

{

∈

u

·

|

x = 0 for all u

U

}

∈

The orthogonal complement plays an important role in the Projection Theorem (Theorem 8.1.3), and we
return to it in Section 10.2. For now we need:

Lemma 8.6.4

If A isanymatrixthen:

1. ( row A)⊥ = null A and ( col A)⊥ = null AT.

2. IfU isanysubspaceof Rn thenU ⊥⊥ = U.

3. Let

f1, . . . , fm
{

}

beanorthonormalbasisof Rm. IfU = span

f1, . . . , fk
{

,then
}

U ⊥ = span

fk+1, . . . , fm
{

}

446

Orthogonality

Proof.

1. Assume A is m

n, and let b1, . . . , bm be the rows of A. If x is a column in Rn, then entry i of Ax is

x, so Ax = 0 if and only if bi

×

bi

·

x = 0 for each i. Thus:

·

x

null A

bi

x = 0 for each i

x

( span

b1, . . . , bm

)⊥ = ( row A)⊥

∈

⇔
Hence null A = ( row A)⊥. Now replace A by AT to get null AT = ( row AT )⊥ = ( col A)⊥, which is
the other identity in (1).

⇔

∈

}

{

·

2. If x

U then y

x = 0 for all y

U ⊥⊥. This proves that U
show that dim U = dim U ⊥⊥. By Theorem 8.1.4 we see that dim V ⊥ = n
V

∈
·
Rn. Hence

U ⊥, that is x

∈

∈

⊆

U ⊥⊥, so it is enough to
dim V for any subspace

⊆
−

dim U ⊥⊥ = n

dim U ⊥ = n

−

(n

dim U ) = dim U , as required

3. We have span

fk+1, . . . , fm

U ⊥ because

is orthogonal. For the other inclusion, let

{
x = 0 for i = 1, 2, . . . , k. By the Expansion Theorem 5.3.6:

} ⊆

}

{

U ⊥ so fi

x

∈

·

−
−
f1, . . . , fm

x = (f1
=

x)f1 +
·
0
+

· · ·
· · ·
fk+1, . . . , fm

}

Hence U ⊥ ⊆

span

{

+ (fk
+

x)fk + (fk+1
·
+ (fk+1
0

x)fk+1 +
x)fk+1 +

·
·

+ (fm
+ (fm

x)fm
x)fm

·
·

· · ·
· · ·

.

With this we can see how any SVD for a matrix A provides orthonormal bases for each of the four

fundamental subspaces of A.

Theorem 8.6.2

Let A bean m
size m

×
m and n

×

n respectively,andlet

n realmatrix,let A = U ΣV T be any SVDfor A whereU andV areorthogonalof

×
D 0
0 0

Σ =

(cid:20)

where

D = diag (λ1, λ2, . . . , λr), witheachλi > 0

(cid:21)m

n
×

WriteU =
and

ur
u1
v1, . . . , vr, . . . , vn
{
1. r = rank A,andthesingularvaluesof A are √λ1, √λ2, . . . , √λr.

· · ·
· · ·
areorthonormalbasesof Rm and Rn respectively. Then

andV =

, so

um

· · ·

· · ·

v1

vn

vr

}

(cid:3)

(cid:2)

(cid:2)

(cid:3)

u1, . . . , ur, . . . , um
{

}

2. Thefundamentalspacesaredescribedasfollows:

}

u1, . . . , ur
{
ur+1, . . . , um
{
vr+1, . . . , vn
{
v1, . . ., vr
{

}

isanorthonormalbasisof col A.

isanorthonormalbasisof null AT.

}
isanorthonormalbasisof null A.

}
isanorthonormalbasisof row A.

a.

b.

c.

d.

Proof.

8.6. The Singular Value Decomposition

447

1. This is Lemma 8.6.3.

2.

a. As col A = col (AV ) by Lemma 5.4.3 and AV = U Σ, (a.) follows from

U Σ =

u1

ur

· · ·

· · ·

um

(cid:2)
b. We have ( col A)⊥

(a.)
= ( span

(cid:20)

(cid:3)
u1, . . . , ur

diag (λ1, λ2, . . . , λr) 0
0

0

=

λ1u1

(cid:21)

(cid:2)

λrur 0

0

· · ·

· · ·

(cid:3)

{
proves (b.) because ( col A)⊥ = null AT by Lemma 8.6.4(1).

}

{

)⊥ = span

ur+1, . . . , um

by Lemma 8.6.4(3). This

}

c. We have dim ( null A) + dim ( im A) = n by the Dimension Theorem 7.2.4, applied to
Rm where T (x) = Ax. Since also im A = col A by Lemma 8.6.1, we obtain

T : Rn

→

dim ( null A) = n

dim ( col A) = n

r = dim ( span

vr+1, . . . , vn

)

}

{

−

−

So to prove (c.) it is enough to show that v j

∈

null A whenever j > r. To this end write

λr+1 =

· · ·

= λn = 0,

so ET E = diag (λ2

1 , . . . , λ2

r , λ2

r+1, . . . , λ2
n )

Observe that each λj is an eigenvalue of ΣT Σ with eigenvector e j = column j of In. Thus
v j = V e j for each j. As AT A = V ΣT ΣV T (proof of Lemma 8.6.3), we obtain

(AT A)v j = (V ΣT ΣV T )(V e j) = V (ΣT Σe j) = V

λ2

j e j

= λ2

j V e j = λ2

j v j

for 1

j

≤

≤

n. Thus each v j is an eigenvector of AT A corresponding to λ2

j . But then

(cid:0)

(cid:1)

Av j

k

k

2 = (Av j)T Av j = vT

j (AT Av j) = vT

j (λ2

j v j) = λ2
j k

2 = λ2
j

v j

k

for i = 1, . . . , n

In particular, Av j = 0 whenever j > r, so v j

null A if j > r, as desired. This proves (c).

∈

(c.)
= null A = ( row A)⊥ by Lemma 8.6.4(1). But then parts

d. Observe that span

vr+1, . . . , vn
}
(2) and (3) of Lemma 8.6.4 show

{

row A =

( row A)⊥

⊥ = ( span

(cid:16)
This proves (d.), and hence Theorem 8.6.2.

(cid:17)

vr+1, . . . , vn

)⊥ = span

v1, . . . , vr

{

}

}

{

Example 8.6.2

Consider the homogeneous linear system

Ax = 0 of m equations in n variables

Then the set of all solutions is null A. Hence if A = U ΣV T is any SVD for A then (in the notation
of Theorem 8.6.2)
such they are a set of basic solutions for the system, the most basic notion in Chapter 1.

is an orthonormal basis of the set of solutions for the system. As

vr+1, . . . , vn

}

{

448

Orthogonality

8.6.3 The Polar Decomposition of a Real Square Matrix

If A is real and n
in this case the decomposition is uniquely determined by A.

×

n the factorization in the title is related to the polar decomposition A. Unlike the SVD,

Recall (Section 8.3) that a symmetric matrix A is called positive deﬁnite if and only if xT Ax > 0 for

Rn. Before proceeding, we must explore the following weaker notion:

every column x

= 0

∈

Deﬁnition 8.11

Areal n

×

n matrix G iscalledpositive10ifitissymmetricand

xT Gx

≥

0 forallx

Rn

∈

Clearly every positive deﬁnite matrix is positive, but the converse fails. Indeed, A =

because, if x =
positive deﬁnite.
(cid:2)

a b

T in R2, then xT Ax = (a + b)2

0. But yT Ay = 0 if y =

(cid:20)

1

≥

(cid:3)

(cid:2)

(cid:3)

1 1
1 1
1
−

is positive

(cid:21)
T , so A is not

Lemma 8.6.5

Let G denotean n

n positivematrix.

×

1. If A isany m

×

n matrixand G ispositive,then AT GA ispositive(and m

m).

×

2. If G = diag (d1, d2,

, dn) andeach di

≥

· · ·

0 then G ispositive.

Proof.

1. xT (AT GA)x = (Ax)T G(Ax)

0 because G is positive.

≥
, then

2. If x =

x1 x2

(cid:2)

· · ·

T

xn

(cid:3)

because di

≥

0 for each i.

xT Gx = d1x2

1 + d2x2

2 +

+ dnx2

n ≥

0

· · ·

Deﬁnition 8.12

If A isareal n

×

n matrix,afactorization

A = GQ where G ispositiveand Q isorthogonal

iscalledapolardecompositionfor A.

Any SVD for a real square matrix A yields a polar form for A.

10Also called positive semi-deﬁnite.

6
8.6. The Singular Value Decomposition

449

Theorem 8.6.3

Everysquarerealmatrixhasapolarform.

Proof. Let A = U ΣV T be a SVD for A with Σ as in Deﬁnition 8.9 and m = n. Since U TU = In here we
have

A = U ΣV T = (U Σ)(U TU )V T = (U ΣU T )(UV T )

So if we write G = U ΣU T and Q = UV T , then Q is orthogonal, and it remains to show that G is positive.
But this follows from Lemma 8.6.5.

The SVD for a square matrix A is not unique (In = PInPT for any orthogonal matrix P). But given the

proof of Theorem 8.6.3 it is surprising that the polar decomposition is unique.11 We omit the proof.

The name “polar form” is reminiscent of the same form for complex numbers (see Appendix A). This
2 matrices. Write M2(R) for

is no coincidence. To see why, we represent the complex numbers as real 2
the set of all real 2

2 matrices, and deﬁne

×

×

σ : C

→

M2(R) by σ(a + bi) =

a
b

b
−
a

(cid:21)

(cid:20)

for all a + bi in C

One veriﬁes that σ preserves addition and multiplication in the sense that

σ(zw) = σ(z)σ(w)

and

σ(z + w) = σ(z) +σ(w)

for all complex numbers z and w. Since θ is one-to-one we may identify each complex number a + bi with
the matrix θ(a + bi), that is we write

a + bi =

a
b

b
−
a

(cid:20)

(cid:21)

for all a + bi in C

Thus 0 =

0 0
0 0

(cid:20)

, 1 =

(cid:21)

(cid:20)

1 0
0 1

(cid:21)

If z = a + bi is nonzero then the absolute value r =
position, then cosθ = a/r and sinθ = b/r. Observe:

0
1

1
−
0

(cid:20)

(cid:21)
z

|

|

r 0
0 r

(cid:20)

(cid:21)

= I2, i =

, and r =

if r is real.

= √a2 + b2

= 0. If θ is the angle of z in standard

a
b

b
−
a

r 0
0 r

=

(cid:21)

(cid:20)

(cid:20)

(cid:21) (cid:20)

a/r
b/r

−

b/r
a/r

r 0
0 r

=

(cid:21)

(cid:20)

(cid:21) (cid:20)

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

= GQ

(xiii)

where G =

is positive and Q =

is orthogonal. But in C we have G = r and

r 0
0 r

(cid:20)

(cid:21)

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

(cid:20)

Q = cosθ + i sinθ so (xiii) reads z = r(cosθ + i sinθ) = reiθ which is the classical polar form for the

complex number a + bi. This is why (xiii) is called the polar form of the matrix

8.12 simply adopts the terminology for n

n matrices.

×

11See J.T. Scheick, Linear Algebra with Applications, McGraw-Hill, 1997, page 379.

a
b

b
−
a

(cid:21)

(cid:20)

; Deﬁnition

6
450

Orthogonality

8.6.4 The Pseudoinverse of a Matrix

It is impossible for a non-square matrix A to have an inverse (see the footnote to Deﬁnition 2.11). Nonethe-
less, one candidate for an “inverse” of A is an m

n matrix B such that

×

Such a matrix B is called a middle inverse for A. If A is invertible then A−

1 is the unique middle inverse for

ABA = A

and

BAB = B

A, but a middle inverse is not unique in general, even for square matrices. For example, if A =

then B =

is a middle inverse for A for any b.

1 0 0
b 0 0

(cid:20)

(cid:21)

If ABA = A and BAB = B it is easy to see that AB and BA are both idempotent matrices. In 1955 Roger
Penrose observed that the middle inverse is unique if both AB and BA are symmetric. We omit the proof.

1 0
0 0
0 0 






Theorem 8.6.4: Penrose’ Theorem12
Givenanyreal m
followingconditions:

×

n matrix A,thereisexactlyone n

m matrix B suchthat A and B satisfythe

×

P1 ABA = A and BAB = B.

P2 Both AB and BA aresymmetric.

Deﬁnition 8.13

Let A beareal m
and A+ satisfyP1andP2,thatis:

×

n matrix. Thepseudoinverseof A istheunique n

m matrix A+ suchthat A

×

AA+A = A,

A+AA+ = A+,

andboth AA+ and A+A aresymmetric13

If A is invertible then A+ = A−

1 as expected. In general, the symmetry in conditions P1 and P2 shows

that A is the pseudoinverse of A+, that is A++ = A.

12R. Penrose, A generalized inverse for matrices, Proceedings of the Cambridge Philosophical Society 5l (1955), 406-413.
In fact Penrose proved this for any complex matrix, where AB and BA are both required to be hermitian (see Deﬁnition 8.18 in
the following section).

13Penrose called the matrix A+ the generalized inverse of A, but the term pseudoinverse is now commonly used. The matrix
A+ is also called the Moore-Penrose inverse after E.H. Moore who had the idea in 1935 as part of a larger work on “General
Analysis”. Penrose independently re-discovered it 20 years later.

8.6. The Singular Value Decomposition

451

Theorem 8.6.5

Let A bean m

n matrix.

×

1. If rank A = m then AAT isinvertibleand A+ = AT (AAT )−

1.

2. If rank A = n then AT A isinvertibleand A+ = (AT A)−

1AT.

Proof. Here AAT (respectively AT A) is invertible by Theorem 5.4.4 (respectively Theorem 5.4.3). The rest
is a routine veriﬁcation.

In general, given an m

n matrix A, the pseudoinverse A+ can be computed from any SVD for A. To
see how, we need some notation. Let A = U ΣV T be an SVD for A (as in Deﬁnition 8.9) where U and V

×

are orthogonal and Σ =

(cid:21)m
Hence D is invertible, so we make:

(cid:20)

D 0
0 0

in block form where D = diag (d1, d2, . . . , dr) where each di > 0.

n
×

Deﬁnition 8.14
1 0
0

D−
0

Σ′ =

(cid:20)

.

m

(cid:21)n

×

A routine calculation gives:

Lemma 8.6.6

• ΣΣ′Σ = Σ

• Σ′ΣΣ′ = Σ′

• ΣΣ′ =

• Σ′Σ =

Ir 0
0 0

Ir 0
0 0

(cid:20)

(cid:20)

(cid:21)m

m

×

(cid:21)n

n
×

That is, Σ′ is the pseudoinverse of Σ.

Now given A = U ΣV T , deﬁne B = V Σ′U T . Then

ABA = (U ΣV T )(V Σ′U T )(U ΣV T ) = U (ΣΣ′Σ)V T = U ΣV T = A

by Lemma 8.6.6. Similarly BAB = B. Moreover AB = U (ΣΣ′)U T and BA = V (Σ′Σ)V T are both symmetric
again by Lemma 8.6.6. This proves

Theorem 8.6.6

Let A berealand m
A+ = V Σ′U T.

×

n,andlet A = U ΣV T isanySVDfor A asinDeﬁnition8.9. Then

452

Orthogonality

Of course we can always use the SVD constructed in Theorem 8.6.1 to ﬁnd the pseudoinverse. If

A =



1 0
0 0
0 0 


AB is symmetric but BA is not, so B



= A+.

, we observed above that B =

1 0 0
b 0 0

(cid:20)

(cid:21)

is a middle inverse for A for any b. Furthermore

Example 8.6.3

Find A+ if A =




Solution. AT A =

q1 =

1
0

(cid:21)

(cid:20)

and q2 =

1 0
0 0
.
0 0 

1 0
0 0
0
1

(cid:21)

(cid:20)

(cid:20)

(cid:21)

with eigenvalues λ1 = 1 and λ2 = 0 and corresponding eigenvectors

. Hence Q =

q1 q2

= I2. Also A has rank 1 with singular values

1 0 0
0 0 0

(cid:20)

(cid:21)

= AT in this case.

σ1 = 1 and σ2 = 0, so ΣA =







Since Aq1 =


and Aq2 =

1
0
0 


of R3 where (say) p2 =
p1, p2, p3}
p1 p2 p3
P =
A+ = QΣ′APT = Σ′A =
(cid:3)

basis

1 0 0
0 0 0



{

(cid:2)

(cid:3)

(cid:2)
= A and Σ′A =

1 0
0 0
0 0 
0

, we have p1 =
0
0 




1
0
0 


and p3 =



0
1
0 


(cid:20)

. Note that A+ = AT in this case.
(cid:21)

. Hence



0
0
1 


= I, so the SVD for A is A = PΣAQT . Finally, the pseudoinverse of A is





which extends to an orthonormal

The following Lemma collects some properties of the pseudoinverse that mimic those of the inverse.

The veriﬁcations are left as exercises.

Lemma 8.6.7

Let A bean m

×
1. A++ = A.

n matrixofrank r.

2. If A isinvertiblethen A+ = A−

1.

3. (AT )+ = (A+)T.

4. (kA)+ = kA+ foranyreal k.

5. (UAV )+ = U T (A+)V T wheneverU andV areorthogonal.

6
8.6. The Singular Value Decomposition

453

Exercises for 8.6

Exercise 8.6.1 If ACA = A show that B = CAC is a mid-
dle inverse for A.

Exercise 8.6.10 Find an SVD for A =

0 1
1 0

.
(cid:21)

(cid:20)

−

Exercise 8.6.2 For any matrix A show that

ΣAT = (ΣA)T

Exercise 8.6.3 If A is m
itive, what is rank A?

×

n with all singular values pos-

Exercise 8.6.4 If A has singular values σ1, . . . , σr, what
are the singular values of:

Exercise 8.6.11 If A = U ΣV T is an SVD for A, ﬁnd an
SVD for AT .

Exercise 8.6.12 Let A be a real, m
itive singular values σ1, σ2, . . . , σr, and write

×

n matrix with pos-

s(x) = (x

σ1)(x

σ2)

−

−

a. Show that cAT A(x) = s(x)xn
−

cAT A(c) = s(x)xm

−

r.

(x

σr)

−

· · ·
r and

a.

c.

AT

A−

b.

tA where t > 0 is real

b. If m

≤

n conclude that cAT A(x) = s(x)xn
−

m.

1 assuming A is invertible.

Exercise 8.6.13 If G is positive show that:

Exercise 8.6.5 If A is square show that
product of the singular values of A.

|

det A

|

is the

a. rG is positive if r

0

≥

Exercise 8.6.6 If A is square and real, show that A = 0
if and only if every eigenvalue of AT A is 0.

Exercise 8.6.7 Given a SVD for an invertible matrix A,
1. How are ΣA and ΣA−
ﬁnd one for A−

1 related?

Exercise 8.6.8 Let A−
Given any orthogonal n
×
matrix V such that A = U ΣAV T is an SVD for A.

1 = A = AT where A is n
n.
n matrix U , ﬁnd an orthogonal

×

If A =

0 1
1 0

(cid:20)

a.

U = 1
5

(cid:20)

(cid:21)

3
4

do this for:

−

4
3

(cid:21)

b.

U = 1
√2

1
1

1
−
1

(cid:21)

(cid:20)

Exercise 8.6.9 Find a SVD for the following matrices:

a.

A =

1
0
1

−

1
1
0









1 1
1 0
1 2

−

1
2
0

−





b.





b. G + H is positive for any positive H.

0.

Exercise 8.6.14 If G is positive and λ is an eigenvalue,
show that λ
Exercise 8.6.15 If G is positive show that G = H 2 for
some positive matrix H. [Hint: Preceding exercise and
Lemma 8.6.5]

≥

Exercise 8.6.16 If A is n
are similar. [Hint: Start with an SVD for A.]

×

n show that AAT and AT A

Exercise 8.6.17 Find A+ if:

a. A =

b. A =

1
1

(cid:20)



−
1
0
1

2
2

(cid:21)



−
1
−
0
1

−


Exercise 8.6.18 Show that (A+)T = (AT )+.



454

Orthogonality

8.7 Complex Matrices

×

If A is an n
n matrix, the characteristic polynomial cA(x) is a polynomial of degree n and the eigenvalues
of A are just the roots of cA(x). In most of our examples these roots have been real numbers (in fact,
the examples have been carefully chosen so this will be the case!); but it need not happen, even when
then cA(x) = x2 + 1

the characteristic polynomial has real coefﬁcients. For example, if A =

i, where i is a complex number satisfying i2 =
has roots i and
possibility that the eigenvalues of a (real) square matrix might be complex numbers.

−

−

(cid:20)

−

(cid:21)

1. Therefore, we have to deal with the

0 1
1 0

In fact, nearly everything in this book would remain true if the phrase real number were replaced by
complex number wherever it occurs. Then we would deal with matrices with complex entries, systems
of linear equations with complex coefﬁcients (and complex solutions), determinants of complex matrices,
and vector spaces with scalar multiplication by any complex number allowed. Moreover, the proofs of
most theorems about (the real version of) these concepts extend easily to the complex case. It is not our
intention here to give a full treatment of complex linear algebra. However, we will carry the theory far
enough to give another proof that the eigenvalues of a real symmetric matrix A are real (Theorem 5.5.7)
and to prove the spectral theorem, an extension of the principal axes theorem (Theorem 8.2.2).

The set of complex numbers is denoted C . We will use only the most basic properties of these numbers

(mainly conjugation and absolute values), and the reader can ﬁnd this material in Appendix A.

If n

1, we denote the set of all n-tuples of complex numbers by Cn. As with Rn, these n-tuples will
≥
be written either as row or column matrices and will be referred to as vectors. We deﬁne vector operations
on Cn as follows:

(v1, v2, . . . , vn) + (w1, w2, . . . , wn) = (v1 + w1, v2 + w2, . . . , vn + wn)
for u in C

u(v1, v2, . . . , vn) = (uv1, uv2, . . . , uvn)

With these deﬁnitions, Cn satisﬁes the axioms for a vector space (with complex scalars) given in Chapter 6.
Thus we can speak of spanning sets for Cn, of linearly independent subsets, and of bases. In all cases,
the deﬁnitions are identical to the real case, except that the scalars are allowed to be complex numbers. In
particular, the standard basis of Rn remains a basis of Cn, called the standard basis of Cn.

A matrix A =

ai j

conjugation for complex numbers extends to matrices as follows: Deﬁne the conjugate of A =
the matrix

is called a complex matrix if every entry ai j is a complex number. The notion of
to be

ai j

(cid:3)

(cid:2)

(cid:2)

(cid:3)

A =

ai j

obtained from A by conjugating every entry. Then (using Appendix A)

(cid:2)

(cid:3)

holds for all (complex) matrices of appropriate size.

A + B = A + B

and

AB = A B

The Standard Inner Product

There is a natural generalization to Cn of the dot product in Rn.

8.7. Complex Matrices

455

Deﬁnition 8.15 Standard Inner Product in Rn
Givenz= (z1, z2, . . . , zn) andw= (w1, w2, . . . , wn) in Cn,deﬁnetheirstandardinnerproduct
z, w
i
h

by

z, w
i
h

= z1w1 + z2w2 +

+ znwn = z

w

· · ·

·

where w istheconjugateofthecomplexnumber w.

Clearly, if z and w actually lie in Rn, then

z, w

= z

·

i

h

w is the usual dot product.

Example 8.7.1

If z = (2, 1

i, 2i, 3

−

−

i) and w = (1

−
= 2(1 + i) + (1
2 + (1
= 2

·

−

i,

1,

−
i)(

z, w
i
z, z
i

h
h

−
i)(1 + i) + (2i)(

1) + (2i)(i) + (3
2i) + (3

−

2i) = 6
i)(3
i)(3 + i) = 20

−

−

6i

−
−

−

i, 3 + 2i), then

−

Note that
h
z, z
=
i

h

gives
explains the conjugation in the deﬁnition of

· · ·

zn

|

|

|

|

z, w
z1

i
2 +

is a complex number in general. However, if w = z = (z1, z2, . . . , zn), the deﬁnition
2 which is a nonnegative real number, equal to 0 if and only if z = 0. This

+

z, w
i

h

, and it gives (4) of the following theorem.

Theorem 8.7.1
Letz,z1,w,andw1 denotevectorsin Cn,andletλ denoteacomplexnumber.

1.

2.

3.

4.

=

z+ z1, w
z, w
h
i
h
i
z, w
λz, w
= λ
i
i
h
h
w, z
z, w
.
=
h
h
i
i
z, z
0, and
h

i ≥

+

and

z1, w
i
h
z, λw
and
i
h

z, w+ w1
h
z, w
.
= λ
i
h

=

z, w
i
h

+

z, w1
h

.
i

i

z, z
i
h

= 0 ifandonlyifz= 0.

Proof. We leave (1) and (2) to the reader (Exercise 8.7.10), and (4) has already been proved. To prove (3),
write z = (z1, z2, . . ., zn) and w = (w1, w2, . . . , wn). Then

w, z

h

i

= (w1z1 +

· · ·

+ wnzn) = w1z1 +
= z1w1 +

+ wnzn
+ znwn =

· · ·
· · ·

z, w

i

h

456

Orthogonality

Deﬁnition 8.16 Norm and Length in Cn

Asforthedotproducton Rn,property(4)enablesustodeﬁnethenormorlength
z= (z1, z2, . . . , zn) in Cn:

z
k
k

ofavector

=

z
k
k

=

z, z
i
h
p

q

2 +

z1

|

|

z2

|

|

2 +

+

zn

2

|

|

· · ·

The only properties of the norm function we will need are the following (the proofs are left to the reader):

Theorem 8.7.2
Ifzisanyvectorin Cn,then

1.

2.

z
k ≥
k
λz
k
k

0 and

=

λ
|

|k

= 0 ifandonlyifz= 0.

z
k
k
z
forallcomplexnumbersλ.
k

A vector u in Cn is called a unit vector if
= 0 is any nonzero vector in Cn, then u = 1
z
k
k

z

u
k
z is a unit vector.

k

= 1. Property (2) in Theorem 8.7.2 then shows that if

Example 8.7.2

In C4, ﬁnd a unit vector u that is a positive real multiple of z = (1

i, i, 2, 3 + 4i).

Solution.

z
k

k

= √2 + 1 + 4 + 25 = √32 = 4√2, so take u = 1
4√2

−
z.

Transposition of complex matrices is deﬁned just as in the real case, and the following notion is fun-

damental.

Deﬁnition 8.17 Conjugate Transpose in Cn

Theconjugatetranspose AH ofacomplexmatrix A isdeﬁnedby

AH = (A)T = (AT )

Observe that AH = AT when A is real.14

Example 8.7.3

i
3
1
2i 5 + 2i

−

(cid:20)

H

=

2 + i
i
−

(cid:21)





14Other notations for AH are A∗ and A†.

2i
2i

1 + i 5
i
2

−
−
i

3

−





6
8.7. Complex Matrices

457

The following properties of AH follow easily from the rules for transposition of real matrices and

extend these rules to complex matrices. Note the conjugate in property (3).

Theorem 8.7.3

Let A and B denotecomplexmatrices,andletλ beacomplexnumber.

1. (AH)H = A.

2. (A + B)H = AH + BH.

3. (λA)H = λAH.

4. (AB)H = BHAH.

Hermitian and Unitary Matrices

If A is a real symmetric matrix, it is clear that AH = A. The complex matrices that satisfy this condition
turn out to be the most natural generalization of the real symmetric matrices:

Deﬁnition 8.18 Hermitian Matrices
Asquarecomplexmatrix A iscalledhermitian15if AH = A,equivalentlyif A = AT.

Hermitian matrices are easy to recognize because the entries on the main diagonal must be real, and the
“reﬂection” of each nondiagonal entry in the main diagonal must be the conjugate of that entry.

Example 8.7.4

3

i
−
2
−

i

i
2
−
7
−





2 + i
7
1 
−


is hermitian, whereas

1
i

(cid:20)

i
2
−

and

(cid:21)

(cid:20)

1 i
i
i
−

(cid:21)

are not.

The following Theorem extends Theorem 8.2.3, and gives a very useful characterization of hermitian

matrices in terms of the standard inner product in Cn.

Theorem 8.7.4

An n

×

n complexmatrix A ishermitianifandonlyif

Az, w
i

h

=

z, Aw
i
h

forall n-tupleszandwin Cn.

15The name hermitian honours Charles Hermite (1822–1901), a French mathematician who worked primarily in analysis and
is remembered as the ﬁrst to show that the number e from calculus is transcendental—that is, e is not a root of any polynomial
with integer coefﬁcients.

458

Orthogonality

Proof. If A is hermitian, we have AT = A. If z and w are columns in Cn, then

z, w

= (Az)T w = zT AT w = zT Aw = zT (Aw) =
h
To prove the converse, let e j denote column j of the identity matrix. If A =

Az, w

h

i

= zT w, so

i

h
z, Aw

i
, the condition gives

ai j

Hence A = AT , so A is hermitian.

ai j =

ei, Ae j

Aei, e j

=

i

h

i

= ai j

h

(cid:2)

(cid:3)

Let A be an n

×
if Ax = λx holds for some column x
to λ. The characteristic polynomial cA(x) is deﬁned by

n complex matrix. As in the real case, a complex number λ is called an eigenvalue of A
= 0 in Cn. In this case x is called an eigenvector of A corresponding

This polynomial has complex coefﬁcients (possibly nonreal). However, the proof of Theorem 3.3.2 goes
through to show that the eigenvalues of A are the roots (possibly complex) of cA(x).

cA(x) = det (xI

A)

−

It is at this point that the advantage of working with complex numbers becomes apparent. The real
numbers are incomplete in the sense that the characteristic polynomial of a real matrix may fail to have
all its roots real. However, this difﬁculty does not occur for the complex numbers. The so-called funda-
mental theorem of algebra ensures that every polynomial of positive degree with complex coefﬁcients has
a complex root. Hence every square complex matrix A has a (complex) eigenvalue. Indeed (Appendix A),
cA(x) factors completely as follows:

−
where λ1, λ2, . . . , λn are the eigenvalues of A (with possible repetitions due to multiple roots).

· · ·

−

−

cA(x) = (x

λ1)(x

λ2)

(x

λn)

The next result shows that, for hermitian matrices, the eigenvalues are actually real. Because symmet-
ric real matrices are hermitian, this re-proves Theorem 5.5.7. It also extends Theorem 8.2.4, which asserts
that eigenvectors of a symmetric real matrix corresponding to distinct eigenvalues are actually orthogonal.
In the complex context, two n-tuples z and w in Cn are said to be orthogonal if

z, w

= 0.

h

i

Theorem 8.7.5

Let A denoteahermitianmatrix.

1. Theeigenvaluesof A arereal.

2. Eigenvectorsof A correspondingtodistincteigenvaluesareorthogonal.

Proof. Let λ and µ be eigenvalues of A with (nonzero) eigenvectors z and w. Then Az = λz and Aw = µw,
so Theorem 8.7.4 gives

λ
h

z, w

=

i

λz, w
h

If µ = λ and w = z, this becomes λ
h
Thus λ is real, proving (1). Similarly, µ is real, so equation (8.6) gives λ
h
implies

= 0, proving (2).

= λ
h

z, z
i

z, w

h

=

Az, w

z, Aw

=

z, µw

=

i
z, z
i

i

h
. Because

h

h

i
z, z
i

z, w

i

i
=

= µ
h
2

z
k
k
z, w
i

= 0, this implies λ = λ.
= µ, this
. If λ
= µ
h

z, w

i

(8.6)

h

i

The principal axes theorem (Theorem 8.2.2) asserts that every real symmetric matrix A is orthogonally
1 = PT ). The next theorem

diagonalizable—that is PT AP is diagonal where P is an orthogonal matrix (P−
identiﬁes the complex analogs of these orthogonal real matrices.

6
6
6
Deﬁnition 8.19 Orthogonal and Orthonormal Vectors in Cn

8.7. Complex Matrices

459

Asintherealcase,asetofnonzerovectors
zi, zj
h

z1, z2, . . . , zm
{
= j,anditisorthonormalif,inaddition,

= 0 whenever i

}

i

in Cn iscalledorthogonalif

= 1 foreach i.

zi
k

k

Theorem 8.7.6

Thefollowingareequivalentforan n

n complexmatrix A.

×

1. A isinvertibleand A−

1 = AH.

2. Therowsof A areanorthonormalsetin Cn.

3. Thecolumnsof A areanorthonormalsetin Cn.

Proof. If A =
Theorem 8.2.1. Now (1)

c1 c2

(cid:2)

cn

is a complex matrix with jth column c j, then AT A =

(2) follows, and (1)

(cid:3)

⇔

(3) is proved in the same way.

· · ·
⇔

ci, c j

i

, as in

(cid:3)

h

(cid:2)

Deﬁnition 8.20 Unitary Matrices

AsquarecomplexmatrixU iscalledunitaryifU −

1 = U H.

Thus a real matrix is unitary if and only if it is orthogonal.

Example 8.7.5

The matrix A =

1 + i 1
i
i
1

−

(cid:21)

(cid:20)

Normalizing the columns gives the unitary matrix 1
2

1 + i √2
i √2i
1

−

.
(cid:21)

(cid:20)

has orthogonal columns, but the rows are not orthogonal.

Given a real symmetric matrix A, the diagonalization algorithm in Section 3.3 leads to a procedure for
ﬁnding an orthogonal matrix P such that PT AP is diagonal (see Example 8.2.4). The following example
illustrates Theorem 8.7.5 and shows that the technique works for complex matrices.

Example 8.7.6

Consider the hermitian matrix A =

i
orthonormal eigenvectors, and so ﬁnd a unitary matrix U such that U HAU is diagonal.

−

(cid:21)

(cid:20)

2

. Find the eigenvalues of A, ﬁnd two

3

2 + i
7

Solution. The characteristic polynomial of A is

cA(x) = det (xI

A) = det

−

x
3
−
2 + i

2
−
x

i
−
7
−

= (x

2)(x

8)

−

−

−
Hence the eigenvalues are 2 and 8 (both real as expected), and corresponding eigenvectors are

(cid:21)

(cid:20)

6
460

Orthogonality

2 + i
1
−

and

1

2

i

(cid:21)

(cid:20)
−
diagonalization algorithm, let U = 1
√6

(cid:20)

(cid:21)

eigenvectors as columns.
Then U HAU =

2 0
0 8

(cid:20)

(cid:21)

is diagonal.

(orthogonal as expected). Each has length √6 so, as in the (real)

2 + i
1
−

(cid:20)

1

−

i

2

(cid:21)

be the unitary matrix with the normalized

Unitary Diagonalization

×

n complex matrix A is called unitarily diagonalizable if U HAU is diagonal for some unitary
An n
matrix U . As Example 8.7.6 suggests, we are going to prove that every hermitian matrix is unitarily
diagonalizable. However, with only a little extra effort, we can get a very important theorem that has this
result as an easy consequence.

A complex matrix is called upper triangular if every entry below the main diagonal is zero. We owe

the following theorem to Issai Schur.16

Theorem 8.7.7: Schur’s Theorem

If A isany n

×

n complexmatrix,thereexistsaunitarymatrixU suchthat

U HAU = T

isuppertriangular. Moreover,theentriesonthemaindiagonalof T aretheeigenvalues
λ1, λ2, . . . , λn of A (includingmultiplicities).

1)

(n

Proof. We use induction on n. If n = 1, A is already upper triangular. If n > 1, assume the theorem is valid
1) complex matrices. Let λ1 be an eigenvalue of A, and let y1 be an eigenvector with
for (n
−
= 1. Then y1 is part of a basis of Cn (by the analog of Theorem 6.4.1), so the (complex analog of
y1k
k
is an orthonormal basis of Cn.
the) Gram-Schmidt process provides y2, . . . , yn such that
If U1 =

is the matrix with these vectors as its columns, then (see Lemma 5.4.3)

y1, y2, . . . , yn}

×

−

{

y1 y2

yn

· · ·

(cid:2)

(cid:3)

U H

1 AU1 =

λ1 X1
0 A1

(cid:20)

in block form. Now apply induction to ﬁnd a unitary (n

1)

is upper triangular. Then U2 =

Theorem 8.7.6), and

1
0
0 W1

(cid:20)

(cid:21)

−
is a unitary n

×

(cid:21)
(n

1) matrix W1 such that W H

1 A1W1 = T1
×
n matrix. Hence U = U1U2 is unitary (using

−

2 (U H
U HAU = U H
1 AU1)U2
0
1
0 W H

=

(cid:20)

1 (cid:21) (cid:20)

λ1 X1
0 A1

0
1
0 W1

=

(cid:21)

(cid:20)

(cid:21) (cid:20)

λ1 X1W1
0

T1

(cid:21)

16Issai Schur (1875–1941) was a German mathematician who did fundamental work in the theory of representations of

groups as matrices.

is upper triangular. Finally, A and U HAU = T have the same eigenvalues by (the complex version of)
Theorem 5.5.1, and they are the diagonal entries of T because T is upper triangular.

The fact that similar matrices have the same traces and determinants gives the following consequence

8.7. Complex Matrices

461

of Schur’s theorem.

Corollary 8.7.1

Let A bean n
multiplicities. Then

×

n complexmatrix,andletλ1, λ2, . . . , λn denotetheeigenvaluesof A,including

det A = λ1λ2

· · ·

λn and tr A = λ1 +λ2 +

+λn

· · ·

Schur’s theorem asserts that every complex matrix can be “unitarily triangularized.” However, we

cannot substitute “unitarily diagonalized” here. In fact, if A =

, there is no invertible complex

matrix U at all such that U −

1AU is diagonal. However, the situation is much better for hermitian matrices.

1 1
0 1

(cid:20)

(cid:21)

Theorem 8.7.8: Spectral Theorem

If A ishermitian,thereisaunitarymatrixU suchthatU HAU isdiagonal.

Proof. By Schur’s theorem, let U HAU = T be upper triangular where U is unitary. Since A is hermitian,
this gives

T H = (U HAU )H = U HAHU HH = U HAU = T

This means that T is both upper and lower triangular. Hence T is actually diagonal.

The principal axes theorem asserts that a real matrix A is symmetric if and only if it is orthogonally
diagonalizable (that is, PT AP is diagonal for some real orthogonal matrix P). Theorem 8.7.8 is the complex
analog of half of this result. However, the converse is false for complex matrices: There exist unitarily
diagonalizable matrices that are not hermitian.

Example 8.7.7

Show that the non-hermitian matrix A =

0 1
1 0

is unitarily diagonalizable.

−
Solution. The characteristic polynomial is cA(x) = x2 + 1. Hence the eigenvalues are i and

(cid:21)

(cid:20)

i, and

−
are corresponding eigenvectors. Moreover, these

it is easy to verify that

and

i
1
−

1
−
i

(cid:20)

(cid:21)

(cid:20)

(cid:21)

eigenvectors are orthogonal and both have length √2, so U = 1
√2

i
1
−

(cid:20)

1
−
i

(cid:21)

is a unitary matrix

such that U HAU =

i
0

(cid:20)

0
i
−

(cid:21)

is diagonal.

There is a very simple way to characterize those complex matrices that are unitarily diagonalizable.
n complex matrix N is called normal if NNH = NH N. It is clear that every hermitian

To this end, an n

×

462

Orthogonality

or unitary matrix is normal, as is the matrix

result.

Theorem 8.7.9

0 1
1 0

−

(cid:21)

(cid:20)

in Example 8.7.7. In fact we have the following

An n

×

n complexmatrix A isunitarilydiagonalizableifandonlyif A isnormal.

Proof. Assume ﬁrst that U HAU = D, where U is unitary and D is diagonal. Then DDH = DHD as is
easily veriﬁed. Because DDH = U H(AAH)U and DHD = U H(AHA)U , it follows by cancellation that
AAH = AHA.

Conversely, assume A is normal—that is, AAH = AH A. By Schur’s theorem, let U HAU = T , where T

is upper triangular and U is unitary. Then T is normal too:

T T H = U H(AAH)U = U H(AHA)U = T HT

Hence it sufﬁces to show that a normal n
ti j
it is clear if n = 1. If n > 1 and T =

n upper triangular matrix T must be diagonal. We induct on n;

×
, then equating (1, 1)-entries in T T H and T H T gives

This implies t12 = t13 =
· · ·
1 = T1T H
T T H = T HT implies T1T H

= t1n = 0, so T =

(cid:2)
t11
|

(cid:3)
2 +
|

t12
|

|

2 +

+

t1n
|
· · ·
t11
0
0 T1

(cid:21)

(cid:20)

2 =

|

2

t11
|

|

in block form. Hence T =

t11
0

(cid:20)

0
T H
1 (cid:21)

so

1 . Thus T1 is diagonal by induction, and the proof is complete.

We conclude this section by using Schur’s theorem (Theorem 8.7.7) to prove a famous theorem about
A),

matrices. Recall that the characteristic polynomial of a square matrix A is deﬁned by cA(x) = det (xI
and that the eigenvalues of A are just the roots of cA(x).

−

Theorem 8.7.10: Cayley-Hamilton Theorem17

If A isan n

×

n complexmatrix,then cA(A) = 0;thatis, A isarootofitscharacteristicpolynomial.

1 p(A)P for any invertible
Proof. If p(x) is any polynomial with complex coefﬁcients, then p(P−
complex matrix P. Hence, by Schur’s theorem, we may assume that A is upper triangular. Then the
eigenvalues λ1, λ2, . . . , λn of A appear along the main diagonal, so

1AP) = P−

cA(x) = (x

λ1)(x

λ2)(x

λ3)

(x

−

· · ·

−

λn)

−

−

Thus

Note that each matrix A

−

cA(A) = (A

λ1I)(A

λ2I)(A

λ3I)

−

−

−

λiI is upper triangular. Now observe:

(A

−

· · ·

λnI)

1. A

−

λ1I has zero ﬁrst column because column 1 of A is (λ1, 0, 0, . . . , 0)T .

17Named after the English mathematician Arthur Cayley (1821–1895) and William Rowan Hamilton (1805–1865), an Irish

mathematician famous for his work on physical dynamics.

8.7. Complex Matrices

463

2. Then (A

λ1I)(A

λ2I) has the ﬁrst two columns zero because the second column of (A

−
(b, 0, 0, . . . , 0)T for some constant b.

−

3. Next (A

λ2I)(A
(c, d, 0, . . . , 0)T for some constants c and d.

λ1I)(A

−

−

−

λ3I) has the ﬁrst three columns zero because column 3 of (A

λ2I) is

λ3I) is

−

−

Continuing in this way we see that (A
is, cA(A) = 0.

−

λ1I)(A

λ2I)(A

λ3I)

(A

−

· · ·

−

−

λnI) has all n columns zero; that

Exercises for 8.7

Exercise 8.7.1 In each case, compute the norm of the
complex vector.

a. (1, 1

i,

2, i)

−
−
i, 1 + i, 1,

1)

b. (1

−
c. (2 + i, 1

d. (

−

2,

−

−
i, 2, 0,

−
i, 1 + i, 1

i)

−
i, 2i)

−

Exercise 8.7.2 In each case, determine whether the two
vectors are orthogonal.

a. (4,

b. (i,

−

3i, 2 + i), (i, 2, 2

−

4i)

i, 2 + i), (i, i, 2

−
i)

−
i, 1)

c. (1, 1, i, i), (1, i,

−
d. (4 + 4i, 2 + i, 2i), (

1 + i, 2, 3

2i)

−

−

Exercise 8.7.3 A subset U of Cn is called a complex
subspace of Cn if it contains 0 and if, given v and w in
U , both v + w and zv lie in U (z any complex number).
In each case, determine whether U is a complex subspace
of C3.

a. U =

b. U =

(w, v + w, v
{
−
(iv + w, 0, 2v
{
(u, v, w)
{
u, v, w in C

iu

|

−

c. U =

iw)

|
w)

v, w in C

}
v, w in C

−

|
3v + (1

}

i)w = 0;

−

}

d. U =

(u, v, w)
{
u, v, w in C

}

2u + (1 + i)v

iw = 0;

−

|

Exercise 8.7.5
given matrix is hermitian, unitary, or normal.

In each case, determine whether the

a.

c.

e.

g.

(cid:20)

1
i

i
−
i

i
1
i 2

−

(cid:20)
1
√2

1
1

(cid:20)
1 + i
i

−

(cid:20)

b.

d.

f.

h.

(cid:21)

(cid:21)

−

1
1

(cid:21)

1
1 + i

−

(cid:21)

2 3
3 2

i
−
1
−

−
1
i

1
1 + i

(cid:21)

(cid:21)
1 + i
i

(cid:20)

(cid:20)

(cid:20)

1
√2
z
|

| (cid:20)

z
z

z
z

−

(cid:21)

, z

= 0

(cid:21)

Exercise 8.7.6 Show that a matrix N is normal if and
only if NNT = NT N.

Exercise 8.7.7 Let A =

where v, w, and z are

complex numbers. Characterize in terms of v, w, and z
when A is

v
z
v w

(cid:20)

(cid:21)

b. U =

a. U =

(w, w, 0)
|
{
(w, 2w, a)
{
c. U = R3

|

w in C

}
w in C, a in R

}

a.

c.

hermitian

normal.

b.

unitary

d. U =

(v + w, v
{

−

2w, v)

|

v, w in C

}

Exercise 8.7.4 In each case, ﬁnd a basis over C, and
determine the dimension of the complex subspace U of
C3 (see the previous exercise).

Exercise 8.7.8
such that U HAU is diagonal.

In each case, ﬁnd a unitary matrix U

a. A =

i
1
i 1

−

(cid:21)

(cid:20)

6
464

Orthogonality

b. A =

c. A =

d. A =

e. A =

f. A =

(cid:20)

(cid:20)

(cid:20)









4
3 + i

3

i

−
1

(cid:21)

; a, b, real

(cid:21)
1 + i
3

(cid:21)

a b
b a

−
2

i

1

−
1
0

1

−

0 1 + i
2
i 0

0
0

1
0
0 1

0
1

i

−

0
1 + i
2









Exercise 8.7.9 Show that
all n

Ax, y
h
n matrices A and for all n-tuples x and y in Cn.

x, AHy
h

holds for

=

i

i

×

Exercise 8.7.10

b. If B is skew-hermitian, show that B2 and iB are

hermitian.

c. If B is skew-hermitian, show that the eigenvalues

of B are pure imaginary (iλ for real λ).

d. Show that every n

n complex matrix Z can be
written uniquely as Z = A + B, where A is hermi-
tian and B is skew-hermitian.

×

Exercise 8.7.15 Let U be a unitary matrix. Show that:

a.

b.

=

x
k

for all columns x in Cn.

k

k
= 1 for every eigenvalue λ of U .

U x
k
λ
|
|

Exercise 8.7.16

a. If Z is an invertible complex matrix, show that ZH

is invertible and that (ZH)−

1 = (Z−

1)H.

b. Show that the inverse of a unitary matrix is again

a. Prove (1) and (2) of Theorem 8.7.1.

unitary.

b. Prove Theorem 8.7.2.

c. Prove Theorem 8.7.3.

Exercise 8.7.11

c. If U is unitary, show that U H is unitary.

Exercise 8.7.17 Let Z be an m
ZHZ = In (for example, Z is a unit column in Cn).

n matrix such that

×

a. Show that V = ZZH is hermitian and satisﬁes

a. Show that A is hermitian if and only if A = AT .

V 2 = V .

b. Show that the diagonal entries of any hermitian

matrix are real.

Exercise 8.7.12

a. Show that every complex matrix Z can be written
uniquely in the form Z = A + iB, where A and B
are real matrices.

b. If Z = A + iB as in (a), show that Z is hermi-
tian if and only if A is symmetric, and B is skew-
symmetric (that is, BT =

B).

−

Exercise 8.7.13 If Z is any complex n
that ZZH and Z + ZH are hermitian.

×

n matrix, show

Exercise 8.7.14 A complex matrix B is called skew-
hermitian if BH =

B.

−

b. Show that U = I
mitian (so U −

−

1 = U H = U ).

2ZZH is both unitary and her-

Exercise 8.7.18

a. If N is normal, show that zN is also normal for all

complex numbers z.

b. Show that (a) fails if normal is replaced by hermi-

tian.

Exercise 8.7.19 Show that a real 2

either symmetric or has the form

×

2 normal matrix is
a b
b a

.

(cid:20)

−

(cid:21)

Exercise 8.7.20 If A is hermitian, show that all the co-
efﬁcients of cA(x) are real numbers.

Exercise 8.7.21

a. Show that Z

complex matrix Z.

−

ZH is skew-hermitian for any square

a. If A =

, show that U −

1AU is not diagonal

1 1
0 1

for any invertible complex matrix U .

(cid:20)

(cid:21)

8.8. An Application to Linear Codes over Finite Fields

465

b. If A =

, show that U −

1AU is not upper

triangular for any real invertible matrix U .

(cid:20)

−

(cid:21)

0 1
1 0

0 u v
0 0 0
0 0 0

.





Exercise 8.7.22
If A is any n
U HAU is lower triangular for some unitary matrix U .

n matrix, show that

×

Exercise 8.7.23
3 matrix, show that
A2 = 0 if and only if there exists a unitary matrix U

If A is a 3

×

such that U HAU has the form

0 0 u
0 0 v
0 0 0





or the form




Exercise 8.7.24
[Hint: Use Schur’s theorem.]

If A2 = A, show that rank A = tr A.

Exercise 8.7.25 Let A be any n
n complex matrix
. . . , λn. Show that A = P + N
with eigenvalues λ1,
where Nn = 0 and P = U DU T where U is unitary and
D = diag (λ1, . . . , λn). [Hint: Schur’s theorem]

×


8.8 An Application to Linear Codes over Finite Fields



For centuries mankind has been using codes to transmit messages. In many cases, for example transmit-
ting ﬁnancial, medical, or military information, the message is disguised in such a way that it cannot be
understood by an intruder who intercepts it, but can be easily “decoded” by the intended receiver. This
subject is called cryptography and, while intriguing, is not our focus here. Instead, we investigate methods
for detecting and correcting errors in the transmission of the message.

The stunning photos of the planet Saturn sent by the space probe are a very good example of how
successful these methods can be. These messages are subject to “noise” such as solar interference which
causes errors in the message. The signal is received on Earth with errors that must be detected and cor-
rected before the high-quality pictures can be printed. This is done using error-correcting codes. To see
how, we ﬁrst discuss a system of adding and multiplying integers while ignoring multiples of a ﬁxed
integer.

Modular Arithmetic

We work in the set Z =
of integers, that is the set of whole numbers. Everyone is
familiar with the process of “long division” from arithmetic. For example, we can divide an integer a by 5
0, 1, 2, 3, 4
and leave a remainder “modulo 5” in the set

. As an illustration

3, . . .

±

±

±

2,

1,

0,

{

}

{

19 = 3

}
5 + 4

·
so the remainder of 19 modulo 5 is 4. Similarly, the remainder of 137 modulo 5 is 2 because we have
137 = 27

5 + 2. This works even for negative integers: For example,

·

17 = (

−

4)

−

·

5 + 3

so the remainder of

17 modulo 5 is 3.

−

This process is called the division algorithm. More formally, let n

integer a can be written uniquely in the form

2 denote an integer. Then every

≥

a = qn + r where q and r are integers and 0

r

n

1

−

≤

≤

Here q is called the quotient of a modulo n, and r is called the remainder of a modulo n. We refer to n
as the modulus. Thus, if n = 6, the fact that 134 = 22
6 + 2 means that 134 has quotient 22 and remainder
2 modulo 6.

·

466

Orthogonality

Our interest here is in the set of all possible remainders modulo n. This set is denoted

Zn =

{

0, 1, 2, 3, . . . , n

1

}

−

and is called the set of integers modulo n. Thus every integer is uniquely represented in Zn by its remain-
der modulo n.

We are going to show how to do arithmetic in Zn by adding and multiplying modulo n. That is, we
add or multiply two numbers in Zn by calculating the usual sum or product in Z and taking the remainder
modulo n. It is proved in books on abstract algebra that the usual laws of arithmetic hold in Zn for any
modulus n
2. This seems remarkable until we remember that these laws are true for ordinary addition
and multiplication and all we are doing is reducing modulo n.

≥

To illustrate, consider the case n = 6, so that Z6 =
{
leaves a remainder of 1 when divided by 6. Similarly, 2
this way we can ﬁll in the addition and multiplication tables for Z6; the result is:

. Then 2 + 5 = 1 in Z6 because 7
5 = 4 in Z6, while 3 + 5 = 2, and 3 + 3 = 0. In

0, 1, 2, 3, 4, 5

}

·

Tables for Z6

+ 0 1 2 3 4 5
0 1 2 3 4 5
0
1 2 3 4 5 0
1
2
2 3 4 5 0 1
3 4 5 0 1 2
3
4 5 0 1 2 3
4
5
5 0 1 2 3 4

0 1 2 3 4 5
0 0 0 0 0 0
0 1 2 3 4 5
0 2 4 0 2 4
0 3 0 3 0 3
0 4 2 0 4 2
0 5 4 3 2 1

×
0
1
2
3
4
5

Calculations in Z6 are carried out much as in Z . As an illustration, consider the familiar “distributive law”
a(b + c) = ab + ac from ordinary arithmetic. This holds for all a, b, and c in Z6; we verify a particular
case:

3(5 + 4) = 3

5 + 3

4

·

·

in Z6

In fact, the left side is 3(5 + 4) = 3
doing arithmetic in Z6 is familiar. However, there are differences. For example, 3
to the fact that a
Z.

4) = 3 + 0 = 3 too. Hence
4 = 0 in Z6, in contrast
b = 0 in Z can only happen when either a = 0 or b = 0. Similarly, 32 = 3 in Z6, unlike

3 = 3, and the right side is (3

5) + (3

·

·

·

·

·

Note that we will make statements like

30 = 19 in Z7; it means that

remainder 5 when divided by 7, and so are equal in Z7 because they both equal 5. In general, if n
any modulus, the operative fact is that

≥

30 and 19 leave the same
2 is

−

−

a = b in Zn

if and only if

b is a multiple of n

a

−

In this case we say that a and b are equal modulo n, and write a = b( mod n).

Arithmetic in Zn is, in a sense, simpler than that for the integers. For example, consider negatives.
8? The answer lies in the observation that 8 + 9 = 0 in Z17, so

Given the element 8 in Z17, what is

9 = 8). In the same way, ﬁnding negatives is not difﬁcult in Zn for any modulus n.

−

8 = 9 (and

−

−

Finite Fields

8.8. An Application to Linear Codes over Finite Fields

467

In our study of linear algebra so far the scalars have been real (possibly complex) numbers. The set R
of real numbers has the property that it is closed under addition and multiplication, that the usual laws of
arithmetic hold, and that every nonzero real number has an inverse in R. Such a system is called a ﬁeld.
Hence the real numbers R form a ﬁeld, as does the set C of complex numbers. Another example is the set
Q of all rational numbers (fractions); however the set Z of integers is not a ﬁeld—for example, 2 has no
inverse in the set Z because 2

x = 1 has no solution x in Z .

Our motivation for isolating the concept of a ﬁeld is that nearly everything we have done remains valid
if the scalars are restricted to some ﬁeld: The gaussian algorithm can be used to solve systems of linear
equations with coefﬁcients in the ﬁeld; a square matrix with entries from the ﬁeld is invertible if and only
if its determinant is nonzero; the matrix inversion algorithm works in the same way; and so on. The reason
is that the ﬁeld has all the properties used in the proofs of these results for the ﬁeld R, so all the theorems
remain valid.

·

It turns out that there are ﬁnite ﬁelds—that is, ﬁnite sets that satisfy the usual laws of arithmetic and in
which every nonzero element a has an inverse, that is an element b in the ﬁeld such that ab = 1. If n
2 is
an integer, the modular system Zn certainly satisﬁes the basic laws of arithmetic, but it need not be a ﬁeld.
1 = 2(3a) = 0a = 0
For example we have 2
·
in Z6, a contradiction). The problem is that 6 = 2

3 = 0 in Z6 so 3 has no inverse in Z6 (if 3a = 1 then 2 = 2

3 can be properly factored in Z.

≥

·

≥

An integer p

2 is called a prime if p cannot be factored as p = ab where a and b are positive integers
and neither a nor b equals 1. Thus the ﬁrst few primes are 2, 3, 5, 7, 11, 13, 17, . . . . If n
2 is not a
1, then ab = 0 in Zn and it follows (as above in the case n = 6)
prime and n = ab where 2
that b cannot have an inverse in Zn, and hence that Zn is not a ﬁeld. In other words, if Zn is a ﬁeld, then n
must be a prime. Surprisingly, the converse is true:

a, b

≥

≤

≤

−

n

·

Theorem 8.8.1
If p isaprime,then Zp isaﬁeldusingadditionandmultiplicationmodulo p.

The proof can be found in books on abstract algebra.18 If p is a prime, the ﬁeld Zp is called the ﬁeld of
integers modulo p.

For example, consider the case n = 5. Then Z5 =

0, 1, 2, 3, 4

and the addition and multiplication

tables are:

{

+ 0 1 2 3 4
0 1 2 3 4
0
1 2 3 4 0
1
2 3 4 0 1
2
3 4 0 1 2
3
4 0 1 2 3
4

×
0
1
2
3
4

}
0 1 2 3 4
0 0 0 0 0
0 1 2 3 4
0 2 4 1 3
0 3 1 4 2
0 4 3 2 1

Hence 1 and 4 are self-inverse in Z5, and 2 and 3 are inverses of each other, so Z5 is indeed a ﬁeld. Here
is another important example.

18See, for example, W. Keith Nicholson, Introduction to Abstract Algebra, 4th ed., (New York: Wiley, 2012).

468

Orthogonality

Example 8.8.1

If p = 2, then Z2 =

0, 1

}

{

is a ﬁeld with addition and multiplication modulo 2 given by the tables

+ 0 1
0 0 1
1 1 0

and

0 1
0 0
0 1

×
0
1

This is binary arithmetic, the basic algebra of computers.

While it is routine to ﬁnd negatives of elements of Zp, it is a bit more difﬁcult to ﬁnd inverses in Zp.
14 = 1 in Z17, we are looking for an
For example, how does one ﬁnd 14−
14 = 1 modulo 17. Of course we can try all possibilities in Z17 (there are
integer a with the property that a
only 17 of them!), and the result is a = 11 (verify). However this method is of little use for large primes
p, and it is a comfort to know that there is a systematic procedure (called the euclidean algorithm) for
ﬁnding inverses in Zp for any prime p. Furthermore, this algorithm is easy to program for a computer. To
illustrate the method, let us once again ﬁnd the inverse of 14 in Z17.

1
1 in Z17? Since we want 14−

·

·

Example 8.8.2

Find the inverse of 14 in Z17.

Solution. The idea is to ﬁrst divide p = 17 by 14:

Now divide (the previous divisor) 14 by the new remainder 3 to get

17 = 1

14 + 3

·

and then divide (the previous divisor) 3 by the new remainder 2 to get

14 = 4

3 + 2

·

3 = 1

2 + 1

·

It is a theorem of number theory that, because 17 is a prime, this procedure will always lead to a
remainder of 1. At this point we eliminate remainders in these equations from the bottom up:

1 = 3
= 3
= 5

2
(14
1

1
·
−
1
−
·
(17
·

3) = 5
1

4
−
·
14)
·
14 = 1 in Z17, that is, 11

−

−

·

14

3
−
·
14 = 5

1
·
17

6

14

·

·

−
14 = 1. So 14−

·

Hence (

6)

−

·

1 = 11 in Z17.

since 3 = 1
·
since 2 = 14
since 3 = 17

2 + 1
4
1

−
−

·
·

3
14

As mentioned above, nearly everything we have done with matrices over the ﬁeld of real numbers can
be done in the same way for matrices with entries from Zp. We illustrate this with one example. Again
the reader is referred to books on abstract algebra.

8.8. An Application to Linear Codes over Finite Fields

469

Example 8.8.3

Determine if the matrix A =

1 4
6 5

(cid:20)

(cid:21)

from Z7 is invertible and, if so, ﬁnd its inverse.

Solution. Working in Z7 we have det A = 1
·
5
6
−

Hence Example 2.4.4 gives A−

1
1 = 2−

(cid:20)

5

6
−
4
−
1

(cid:21)

4 = 5

3 = 2

·

−
. Note that 2−

= 0 in Z7, so A is invertible.
1 = 4 in Z7 (because 2

4 = 1 in

·

Z7). Note also that

4 = 3 and

−

−

6 = 1 in Z7, so ﬁnally A−

1 = 4

can verify that indeed

1 4
6 5

(cid:20)

(cid:21) (cid:20)

6 5
4 4

=

(cid:21)

(cid:20)

1 0
0 1

(cid:21)

in Z7.

5 3
1 1

(cid:20)

=

(cid:21)

(cid:20)

6 5
4 4

. The reader

(cid:21)

While we shall not use them, there are ﬁnite ﬁelds other than Zp for the various primes p. Surprisingly,
1, there exists a ﬁeld with exactly pn elements, and this ﬁeld is

for every prime p and every integer n
unique.19 It is called the Galois ﬁeld of order pn, and is denoted GF(pn).

≥

Error Correcting Codes

Coding theory is concerned with the transmission of information over a channel that is affected by noise.
The noise causes errors, so the aim of the theory is to ﬁnd ways to detect such errors and correct at least
some of them. General coding theory originated with the work of Claude Shannon (1916–2001) who
showed that information can be transmitted at near optimal rates with arbitrarily small chance of error.

Let F denote a ﬁnite ﬁeld and, if n

1, let

≥
F n denote the F-vector space of 1

n row matrices over F

×

In this context, the rows in F n are
with the usual componentwise addition and scalar multiplication.
called words (or n-words) and, as the name implies, will be written as [a b c d] = abcd. The individual
components of a word are called its digits. A nonempty subset C of F n is called a code (or an n-code),
and the elements in C are called code words. If F = Z2, these are called binary codes.

If a code word w is transmitted and an error occurs, the resulting word v is decoded as the code word
“closest” to v in F n. To make sense of what “closest” means, we need a distance function on F n analogous
to that in Rn (see Theorem 5.3.3). The usual deﬁnition in Rn does not work in this situation. For example,
if w = 1111 in (Z2)4 then the square of the distance of w from 0 is

even though w

= 0.

0)2 + (1

(1

−

−

0)2 + (1

−

0)2 + (1

−

0)2 = 0

However there is a satisfactory notion of distance in F n due to Richard Hamming (1915–1998). Given
an in F n, we ﬁrst deﬁne the Hamming weight wt(w) to be the number of nonzero

a word w = a1a2
digits in w:

· · ·

Clearly, 0
distance d(v, w) between v and w is deﬁned by

wt(w)

≤

≤

n for every word w in F n. Given another word v = b1b2

wt(w) = wt(a1a2

an) =

i

ai

= 0

· · ·

|{

|

}|

bn in F n, the Hamming

· · ·

19See, for example, W. K. Nicholson, Introduction to Abstract Algebra, 4th ed., (New York: Wiley, 2012).

d(v, w) = wt(v

w) =

i

|

|{

−

bi

= ai

}|

6
6
6
6
470

Orthogonality

In other words, d(v, w) is the number of places at which the digits of v and w differ. The next result
justiﬁes using the term distance for this function d.

Theorem 8.8.2
Letu,v,andwdenotewordsin F n. Then:

1. d(v, w)

0.

≥

2. d(v, w) = 0 ifandonlyifv= w.

3. d(v, w) = d(w, v).

4. d(v, w)

≤

d(v, u) + d(u, w)

Proof. (1) and (3) are clear, and (2) follows because wt(v) = 0 if and only if v = 0. To prove (4), write
x = v
u and y = u
bn,
this follows because ai + bi

wt(x) + wt(y). If x = a1a2
= 0 or bi

≤
= 0 implies that either ai

w. Then (4) reads wt(x + y)

an and y = b1b2

= 0.

· · ·

· · ·

−

−

Given a word w in F n and a real number r > 0, deﬁne the ball Br(w) of radius r (or simply the r-ball)

about w as follows:

}
Using this we can describe one of the most useful decoding methods.

≤

∈

{

|

Br(w) =

x

F n

d(w, x)

r

Nearest Neighbour Decoding

LetC bean n-code,andsupposeawordvistransmittedandwisreceived. Thenwisdecodedas
thecodewordinC closesttoit. (Ifthereisatie,choosearbitrarily.)

Using this method, we can describe how to construct a code C that can detect (or correct) t errors.
Suppose a code word c is transmitted and a word w is received with s errors where 1
t. Then s is
the number of places at which the c- and w-digits differ, that is, s = d(c, w). Hence Bt(c) consists of all
possible received words where at most t errors have occurred.

≤

≤

s

Assume ﬁrst that C has the property that no code word lies in the t-ball of another code word. Because
w is in Bt(c) and w
= c, this means that w is not a code word and the error has been detected. If we
strengthen the assumption on C to require that the t-balls about code words are pairwise disjoint, then w
belongs to a unique ball (the one about c), and so w will be correctly decoded as c.

To describe when this happens, let C be an n-code. The minimum distance d of C is deﬁned to be the

smallest distance between two distinct code words in C; that is,

d = min

d(v, w)

{

|

v and w in C; v

= w

}

Theorem 8.8.3

LetC bean n-codewithminimumdistance d. Assumethatnearestneighbourdecodingisused.
Then:

6
6
6
6
6
8.8. An Application to Linear Codes over Finite Fields

471

1. Ift < d,thenC candetect t errors.20

2. If 2t < d,thenC cancorrect t errors.

Proof.

1. Let c be a code word in C. If w

contains no other code word, so C can detect t errors by the preceding discussion.

∈

≤

Bt(c), then d(w, c)

t < d by hypothesis. Thus the t-ball Bt(c)

2. If 2t < d, it sufﬁces (again by the preceding discussion) to show that the t-balls about distinct code
Bt(c), then

= c′ are code words in C and w is in Bt(c′)

words are pairwise disjoint. But if c
Theorem 8.8.2 gives

∩

by hypothesis, contradicting the minimality of d.

d(c, c′)

d(c, w) + d(w, c′)

≤

t + t = 2t < d

≤

Example 8.8.4

If F = Z3 =
, the 6-code
0, 1, 2
detect 2 errors and correct 1 error.

{

}

111111, 111222, 222111

}

{

has minimum distance 3 and so can

(cid:1)

(cid:0)
(q

Let c be any word in F n. A word w satisﬁes d(w, c) = r if and only if w and c differ in exactly r digits.
F
is the binomial coefﬁcient. Indeed, choose
1)r ways. It follows that

1)r such words where
ways, and then ﬁll those places in w in (q

= q, there are exactly
If
the r places where they differ in
(cid:1)
the number of words in the t-ball about c is

−
n
r

(q

−

n
r

n
r

(cid:0)

(cid:1)

(cid:0)

|

|

Bt(c)

=

|

|

n
0

+

n
1

1) +

n
2

(q

1)2 +

+

n
t

(q

−

· · ·

−

−

1)t = ∑t

i=0

n
i

(q

1)i

−

(cid:1)
(cid:1)
This leads to a useful bound on the size of error-correcting codes.

(cid:0)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:0)

(cid:1)

Theorem 8.8.4: Hamming Bound

LetC bean n-codeoveraﬁeld F thatcancorrect t errorsusingnearestneighbourdecoding. If
F

= q,then

|

|

C
|

| ≤

qn
i=0 (n
∑t
i)(q

1)i

−

n
i

i=0

Proof. Write k = ∑t
C
there are
|
discussion preceding Theorem 8.8.3). Hence they contain k
proving the theorem.

1)i. The t-balls centred at distinct code words each contain k words, and
of them. Moreover they are pairwise disjoint because the code corrects t errors (see the
= qn,

distinct words, and so k

| ≤ |

F n

(q

−

· |

· |

C

C

(cid:1)

(cid:0)

|

|

|

A code is called perfect if there is equality in the Hamming bound; equivalently, if every word in F n
lies in exactly one t-ball about a code word. For example, if F = Z2, n = 3, and t = 1, then q = 2 and
3
4 = 2. The 3-code C =
has minimum distance 3 and
0
so can correct 1 error by Theorem 8.8.3. Hence C is perfect.
(cid:0)

= 4, so the Hamming bound is 23

(cid:1)
20We say that C detects (corrects) t errors if C can detect (or correct) t or fewer errors.

000, 111

+

3
1

}

{

(cid:0)

(cid:1)

6
472

Orthogonality

Linear Codes

Up to this point we have been regarding any nonempty subset of the F-vector space F n as a code. However
many important codes are actually subspaces. A subspace C
1 over F is called an
(n, k)-linear code, or simply an (n, k)-code. We do not regard the zero subspace (that is, k = 0) as a code.

F n of dimension k

≥

⊆

Example 8.8.5

If F = Z2 and n
2, the n-parity-check code is constructed as follows: An extra digit is added to
≥
each word in F n
1 to make the number of 1s in the resulting word even (we say such words have
−
even parity). The resulting (n, n
1)-code is linear because the sum of two words of even parity
again has even parity.

−

Many of the properties of general codes take a simpler form for linear codes. The following result gives
a much easier way to ﬁnd the minimal distance of a linear code, and sharpens the results in Theorem 8.8.3.

Theorem 8.8.5

LetC bean (n, k)-codewithminimumdistance d overaﬁniteﬁeld F,andusenearestneighbour
decoding.

1. d = min

wt(w)

{

2. C candetectt

≥

0

= w

C

.

∈

|
1 errorsifandonlyift < d.

}

3. C cancorrect t

≥
4. IfC cancorrect t

Proof.

≥
n
0

1 errorsifandonlyif 2t < d.

1 errorsand

F

= q,then

|
|
1) +

+

n
1

(q

−

n
2

(q

−

1)2 +

· · ·

+

n
t

(q

1)t

−

≤

k
qn
−

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

1. Write d′ = min

wt(w)

. If v
w is in the subspace C. Hence d

= w in C

0

{

}

|

because v
C, we have wt(w) = d(w, 0)

−

≥

= w are words in C, then d(v, w) = wt(v

d′
= 0 in C then, since 0 is in

w)

≥

−

d′. Conversely, given w

d by the deﬁnition of d. Hence d′ ≥

≥

d and (1) is proved.

2. Assume that C can detect t errors. Given w

= 0 in C, the t-ball Bt(w) about w contains no other
code word (see the discussion preceding Theorem 8.8.3). In particular, it does not contain the code
word 0, so t < d(w, 0) = wt(w). Hence t < d by (1). The converse is part of Theorem 8.8.3.

3. We require a result of interest in itself.

Claim. Suppose c in C has wt(c)

2t. Then Bt(0)

≤

Bt(c) is nonempty.
Bt(c). So assume t < wt(c)

∩

Proof. If wt(c)
2t. Then c has more than
t nonzero digits, so we can form a new word w by changing exactly t of these nonzero digits to zero.

t, then c itself is in Bt(0)

≤

≤

∩

6
6
6
6
6
8.8. An Application to Linear Codes over Finite Fields

473

∩

Bt(c), proving the Claim.

Then d(w, c) = t, so w is in Bt(c). But wt(w) = wt(c)
Bt(0)
If C corrects t errors, the t-balls about code words are pairwise disjoint (see the discussion preceding
= 0 in C, from which d > 2t by (1).
Theorem 8.8.3). Hence the claim shows that wt(c) > 2t for all c
The other inequality comes from Theorem 8.8.3.

t, so w is also in Bt(0). Hence w is in

−

≤

t

= qk because dim F C = k, so this assertion restates Theorem 8.8.4.

4. We have

C
|

|

Example 8.8.6

If F = Z2, then

C =

{

0000000, 0101010, 1010101, 1110000, 1011010, 0100101, 0001111, 1111111

}

is a (7, 3)-code; in fact C = span
3, the minimum weight of a nonzero word in C.

{

0101010, 1010101, 1110000

. The minimum distance for C is

}

Matrix Generators

Given a linear n-code C over a ﬁnite ﬁeld F, the way encoding works in practice is as follows. A message
n called messages. Each message u in F k is encoded as a
stream is blocked off into segments of length k
code word, the code word is transmitted, the receiver decodes the received word as the nearest code word,
and then re-creates the original message. A fast and convenient method is needed to encode the incoming
messages, to decode the received word after transmission (with or without error), and ﬁnally to retrieve
messages from code words. All this can be achieved for any linear code using matrix multiplication.

≤

×

Let G denote a k

n matrix over a ﬁnite ﬁeld F, and encode each message u in F k as the word uG in
F n using matrix multiplication (thinking of words as rows). This amounts to saying that the set of code
of F n. This subspace need not have dimension k for every
words is the subspace C =
}
is the standard basis of F k, then eiG is row i of G for each I and
n matrix G. But, if
k
{
×
spans C. Hence dim C = k if and only if the rows of G are independent in F n, and
e1G, e2G, . . . , ekG
{
these matrices turn out to be exactly the ones we need. For reference, we state their main properties in
Lemma 8.8.1 below (see Theorem 5.4.4).

uG
e1, e2, . . . , ek

u in F k

}

}

{

|

Lemma 8.8.1

Thefollowingareequivalentfora k

n matrix G overaﬁniteﬁeld F:

×

1. rank G = k.

2. Thecolumnsof G span F k.

3. Therowsof G areindependentin F n.

4. Thesystem GX = B isconsistentforeverycolumn B in Rk.

5. GK = Ik forsome n

k matrix K.

×

6
474

Orthogonality

T

· · ·

xn
kk

· · ·
· · ·

Proof. (1)

⇒
⇒
⇒

⇒
(4). G
(5). G
(3).

(2). This is because dim ( col G) = k by (1).
x1
= x1c1 +
Gk1
k1
=

= 0. Hence each ai = 0, proving (3).

· · ·
(cid:2)
· · ·
If a1R1 +
(cid:2)
ak

· · ·
(1). rank G = dim ( row G) = k by (3).

+ xncn where c j is column j of G.
for columns k j.

Gkk
(cid:3)
+ akRk = 0 where Ri is row i of G, then
(cid:3)

(2)
(4)
(5)
a1
(3)
(cid:2)
Note that Theorem 5.4.4 asserts that, over the real ﬁeld R, the properties in Lemma 8.8.1 hold if and only if
GGT is invertible. But this need not be true in general. For example, if F = Z2 and G =
then GGT = 0. The reason is that the dot product w
even though GGT is not invertible, we do have GK = I2 for some 4

1 0 1 0
0 1 0 1
= 0. However,
2 matrix K over F as Lemma 8.8.1

(cid:20)
w can be zero for w in F n even if w

G = 0, so by (5),

· · ·

⇒

a1

ak

(cid:21)

(cid:3)

(cid:2)

(cid:3)

(cid:3)

(cid:2)

·

,

asserts (in fact, K =

1 0 0 0
0 1 0 0

(cid:20)

is one such matrix).

(cid:21)

T

×

Let C

⊆

F n be an (n, k)-code over a ﬁnite ﬁeld F. If

w1, . . . , wk}

{

is a basis of C, let G = 

be the k
Then wi = eiG for each i, so C = span

n matrix with the wi as its rows. Let

×

w1, . . . , wk

{

e1, . . . , ek

}
= span


is the standard basis of F k regarded as rows.

e1G, . . . , ekG

. It follows (verify) that




{

}

w1
...
wk



{

}
uG

C =

{

u in F k

}

|

Because of this, the k
because its rows wi are independent.

×

n matrix G is called a generator of the code C, and G has rank k by Lemma 8.8.1

R by row operations, Theorem 2.5.1 shows that these same row operations

In fact, every linear code C in F n has a generator of a simple, convenient form. If G is a generator
matrix for C, let R be the reduced row-echelon form of G. We claim that C is also generated by R. Since
G
,
performed on
.
(cid:3)
}
[In fact, if u is in F k, then uG = u1R where u1 = uW −
1 is in F k, and uR = u2G where u2 = uW is in F k].
Thus R is a generator of C, so we may assume that G is in reduced row-echelon form.

G Ik
→
uR
k matrix W such that R = W G. Then C =
(cid:3)
{

, produce an invertible k

R W
u in F k

G Ik

→

×

(cid:2)
|

(cid:2)

(cid:2)

(cid:3)

In that case, G has no row of zeros (since rank G = k) and so contains all the columns of Ik. Hence a
Ik A
k) matrix
series of column interchanges will carry G to the block form G′′ =
A. Hence the code C′′ =
is essentially the same as C; the code words in C′′ are obtained
from those in C by a series of column interchanges. Hence if C is a linear (n, k)-code, we may (and shall)
assume that the generator matrix G has the form

for some k

uG′′ |

u in F k

(n

×

−

}

{

(cid:2)

(cid:3)

G =

Ik A

for some

(n

k

×

−

k) matrix A

Such a matrix is called a standard generator, or a systematic generator, for the code C. In this case,
if u is a message word in F k, the ﬁrst k digits of the encoded word uG are just the ﬁrst k digits of u, so
retrieval of u from uG is very simple indeed. The last n

k digits of uG are called parity digits.

(cid:2)

(cid:3)

−

6
8.8. An Application to Linear Codes over Finite Fields

475

Parity-Check Matrices

We begin with an important theorem about matrices over a ﬁnite ﬁeld.

Theorem 8.8.6

Let F beaﬁniteﬁeld,let G bea k
×
andletC =
and D =
|
followingconditionsareequivalent:

uin F k

uG
{

}

vH
{

|

n matrixofrank k,let H bean (n
k
Vin F n
−

k)
bethecodestheygenerate. Thenthe

n matrixofrank n

×

−

k,

−

}

1. GHT = 0.

2. HGT = 0.

3. C =

4. D =

win F n
{

win F n
{

wHT = 0
.
}
wGT = 0
.
}

|

|

Proof. First, (1)

(2) holds because HGT and GHT are transposes of each other.

⇒

(1)

k deﬁned by T (w) = wHT for all w in F n.
F n
(3) Consider the linear transformation T : F n
−
→
ker T by (1) because T (uG) = uGHT = 0 for all
To prove (3) we must show that C = ker T . We have C
u in F k. Since dim C = rank G = k, it is enough (by Theorem 6.4.2) to show dim ( ker T ) = k. However
dim ( im T ), so it is enough to show
the dimension theorem (Theorem 7.2.4) shows that dim ( ker T ) = n
that dim ( im T ) = n

k. But if R1, . . . , Rn are the rows of HT , then block multiplication gives

−

⊆

⇔

−

im T =

wHT

{

|

w in Rn

= span

{

}

R1, . . . , Rn

}

= row (HT )

Hence dim ( im T ) = rank (HT ) = rank H = n

k, as required. This proves (3).

(3)

(1) If u is in F k, then uG is in C so, by (3), u(GHT ) = (uG)HT = 0. Since u is arbitrary in F k,

−

it follows that GHT = 0.

⇒

(2)

⇔

(4) The proof is analogous to (1)

(3).

⇔

The relationship between the codes C and D in Theorem 8.8.6 will be characterized in another way in the
next subsection.
If C is an (n, k)-code, an (n
as in Theorem 8.8.6. Such matrices are easy to ﬁnd for a given code C. If G =
k), the (n
generator for C where A is k

n matrix H is called a parity-check matrix for C if C =
Ik A

wHT = 0
w
}
|
is a standard

n matrix

(n

k)

k)

×

−

{

(cid:2)

(cid:3)

×

−

−
H =

×

AT

In

k
−

−
is a parity-check matrix for C. Indeed, rank H = n
presence of In

k), and

(cid:2)

−

−

GHT =

Ik A

A

−
In
k
−

(cid:20)

(cid:21)

=

A + A = 0

−

k because the rows of H are independent (due to the

(cid:3)

(cid:2)
(cid:3)
by block multiplication. Hence H is a parity-check matrix for C and we have C =
Since wHT and HwT are transposes of each other, this shows that C can be characterized as follows:

w in F n

wHT = 0

{

|

.

}

w in F n

C =

{

HwT = 0
}

|

476

Orthogonality

by Theorem 8.8.6.

This is useful in decoding. The reason is that decoding is done as follows: If a code word c is trans-
c is called the error. Since HcT = 0, we have HzT = HvT and this

mitted and v is received, then z = v
word

−

s = HzT = HvT

is called the syndrome. The receiver knows v and s = HvT , and wants to recover c. Since c = v
enough to ﬁnd z. But the possibilities for z are the solutions of the linear system

−

z, it is

HzT = s

where s is known. Now recall that Theorem 2.2.3 shows that these solutions have the form z = x + s where
x is any solution of the homogeneous system HxT = 0, that is, x is any word in C (by Lemma 8.8.1). In
other words, the errors z are the elements of the set

C + s =

c + s

{

|

c in C

}

The set C + s is called a coset of C. Let

k the search for z is reduced
= qn
C
−
|
|
from qn possibilities in F n to qn
k possibilities in C + s. This is called syndrome decoding, and various
−
methods for improving efﬁciency and accuracy have been devised. The reader is referred to books on
coding for more details.21

= q. Since

C + s
|

=

F

|

|

|

Orthogonal Codes

Let F be a ﬁnite ﬁeld. Given two words v = a1a2
deﬁned (as in Rn) by

· · ·

an and w = b1b2

· · ·

bn in F n, the dot product v

w is

·

Note that v
If C

w is an element of F, and it can be computed as a matrix product: v
·
F n is an (n, k)-code, the orthogonal complement C⊥ is deﬁned as in Rn:

·

w = vwT .

⊆

v

w = a1b1 + a2b2 +

+ anbn

· · ·

·

C⊥ =

{

v in F n

v

·

|

c = 0 for all c in C

}

This is easily seen to be a subspace of F n, and it turns out to be an (n, n
k)-code. This follows when
F = R because we showed (in the projection theorem) that n = dim U ⊥ + dim U for any subspace U of
Rn. However the proofs break down for a ﬁnite ﬁeld F because the dot product in F n has the property that
= 0. Nonetheless, the result remains valid.
w

w = 0 can happen even if w

−

·

Theorem 8.8.7

LetC bean (n, k)-codeoveraﬁniteﬁeld F,let G =
A is k

k),andwrite H =

AT

(n

In

×

−

−

k
−

fortheparity-checkmatrix. Then:

(cid:2)

(cid:3)

Ik A

beastandardgeneratorforC where

1. H isageneratorofC⊥.

(cid:2)

(cid:3)

2. dim (C⊥) = n

k = rank H.

−

3. C⊥⊥ = C and dim (C⊥) + dim C = n.

21For an elementary introduction, see V. Pless, Introduction to the Theory of Error-Correcting Codes, 3rd ed., (New York:

Wiley, 1998).

6
8.8. An Application to Linear Codes over Finite Fields

477

Proof. As in Theorem 8.8.6, let D =
{
for all w in F n and all u in F k, we have

vH

|

k
v in F n
−

}

denote the code generated by H. Observe ﬁrst that,

w

(uG) = w(uG)T = w(GT uT ) = (wGT )

u
, this shows that w is in C⊥ if and only if (wGT )

·

·

uG

u in F k

u = 0 for all u in F k; if and
Since C =
only if22 wGT = 0; if and only if w is in D (by Theorem 8.8.6). Thus C⊥ = D and a similar argument
shows that D⊥ = C.

{

}

·

|

1. H generates C⊥ because C⊥ = D =

vH

{

|

k
v in F n
−

.

}

2. This follows from (1) because, as we observed above, rank H = n

k.

−

3. Since C⊥ = D and D⊥ = C, we have C⊥⊥ = (C⊥)⊥ = D⊥ = C. Finally the second equation in (3)

restates (2) because dim C = k.

We note in passing that, if C is a subspace of Rk, we have C + C⊥ = Rk by the projection theorem
x
x = 0. How-
because any vector x in C
(Theorem 8.1.3), and C
∩
{
k
in F 4 then C⊥ = C, so
ever, this fails in general. For example, if F = Z2 and C = span
{
C +C⊥ = C = C

C⊥ satisﬁes
1010, 0101

C⊥ =

2 = x

C⊥.

k
}

∩

0

}

·

We conclude with one more example.

If F = Z2, consider the standard matrix G below, and the

∩

corresponding parity-check matrix H:

1 0 0 0 1 1 1
0 1 0 0 1 1 0
0 0 1 0 1 0 1
0 0 0 1 0 1 1

G = 




u in F 4







and H =

1 1 1 0 1 0 0
1 1 0 1 0 1 0
1 0 1 1 0 0 1 






uG

The code C =
The vectors in C are listed in the ﬁrst table below. The dual code generated by H has dimension n
and is listed in the second table.

generated by G has dimension k = 4, and is called the Hamming (7, 4)-code.
k = 3

−

{

}

|

C :

u

uG

0000 0000000
0001 0001011
0010 0010101
0011 0011110
0100 0100110
0101 0101101
0110 0110011
0111 0111000
1000 1000111
1001 1001100
1010 1010010
1011 1011001
1100 1100001
1101 1101010
1110 1110100
1111 1111111

C⊥ :

vH

v
000 0000000
001 1011001
010 1101010
011 0110011
100 1110100
101 0101101
110 0011110
111 1000111

22If v

·

u = 0 for every u in F k, then v = 0—let u range over the standard basis of F k.

478

Orthogonality

Clearly each nonzero code word in C has weight at least 3, so C has minimum distance d = 3. Hence C
can detect two errors and correct one error by Theorem 8.8.5. The dual code has minimum distance 4 and
so can detect 3 errors and correct 1 error.

Exercises for 8.8

Exercise 8.8.1 Find all a in Z10 such that:

a.

Z5

b.

Z7

a. a2 = a.

b. a has an inverse (and ﬁnd the inverse).

c. ak = 0 for some k

d. a = 2k for some k

1.

1.

≥

≥

e. a = b2 for some b in Z10.

Exercise 8.8.2

1, t
{

Exercise 8.8.8 Let K be a vector space over Z2 with ba-
a, b,
sis
. It is known that
}
K becomes a ﬁeld of four elements if we deﬁne t2 = 1+t.
Write down the multiplication table of K.

, so K =
}

a + bt
{

in Z2

|

1, t
{

Exercise 8.8.9 Let K be a vector space over Z3 with ba-
a, b,
. It is known that
sis
}
|
K becomes a ﬁeld of nine elements if we deﬁne t2 =
1
−
in Z3. In each case ﬁnd the inverse of the element x of K:

, so K =
}

a + bt
{

in Z3

a. Show that if 3a = 0 in Z10, then necessarily a = 0

in Z10.

a.

x = 1 + 2t

b.

x = 1 + t

b. Show that 2a = 0 in Z10 holds in Z10 if and only

if a = 0 or a = 5.

Exercise 8.8.10 How many errors can be detected or
corrected by each of the following binary linear codes?

Exercise 8.8.3 Find the inverse of:

a.

8 in Z13;

b.

11 in Z19.

Exercise 8.8.4 If ab = 0 in a ﬁeld F, show that either
a = 0 or b = 0.

Exercise 8.8.5 Show that the entries of the last column
of the multiplication table of Zn are

0, n

1, n

−

−

2, . . . , 2, 1

in that order.

Exercise 8.8.6 In each case show that the matrix A is
invertible over the given ﬁeld, and ﬁnd A−

1.

a. C =

b. C =

0000000, 0011110, 0100111, 0111001,
{
1001011, 1010101, 1101100, 1110010
}
0000000000, 0010011111, 0101100111,
{
0111111000, 1001110001, 1011101110,
1100010110, 1110001001
}

Exercise 8.8.11

a. If a binary linear (n, 2)-code corrects one error,

show that n

≥

5. [Hint: Hamming bound.]

b. Find a (5, 2)-code that corrects one error.

a. A =

b. A =

1 4
2 1

5 6
4 3

(cid:21)

(cid:21)

(cid:20)

(cid:20)

over Z5.

over Z7.

the

8.8.7

Consider

Exercise
3x + y + 4z = 3
4x + 3y + z = 1
reducing the augmented matrix to reduced row-echelon
form over the given ﬁeld:

In each case solve the system by

system

linear

.

Exercise 8.8.12

a. If a binary linear (n, 3)-code corrects two errors,

show that n

≥

9. [Hint: Hamming bound.]

b. If G =

1 0 0 1 1 1 1 0 0 0
0 1 0 1 1 0 0 1 1 0
0 0 1 1 0 1 0 1 1 1

,




show that the binary (10, 3)-code generated by


G corrects two errors.
[It can be shown that no
binary (9, 3)-code corrects two errors.]

Exercise 8.8.13

d. The code in Exercise 8.8.10(b).

8.9. An Application to Quadratic Forms

479

a. Show that no binary linear (4, 2)-code can correct

single errors.

b. Find a binary linear (5, 2)-code that can correct

one error.

Exercise 8.8.14 Find the standard generator matrix G
and the parity-check matrix H for each of the following
systematic codes:

a.

00000, 11111
}
{
b. Any systematic (n, 1)-code where n

over Z2.

2.

≥

c. The code in Exercise 8.8.10(a).

Exercise 8.8.15 Let c be a word in F n. Show that
Bt(c) = c + Bt(0), where we write

c + Bt(0) =

c + v
{

|

v in Bt(0)
}

Exercise 8.8.16 If a (n, k)-code has two standard gen-
erator matrices G and G1, show that G = G1.

Exercise 8.8.17 Let C be a binary linear n-code (over
Z2). Show that either each word in C has even weight, or
half the words in C have even weight and half have odd
weight. [Hint: The dimension theorem.]

8.9 An Application to Quadratic Forms

3 −

1 + x2

2 + x2

An expression like x2
2x1x3 + x2x3 is called a quadratic form in the variables x1, x2, and x3.
In this section we show that new variables y1, y2, and y3 can always be found so that the quadratic form,
when expressed in terms of the new variables, has no cross terms y1y2, y1y3, or y2y3. Moreover, we do this
for forms involving any ﬁnite number of variables using orthogonal diagonalization. This has far-reaching
applications; quadratic forms arise in such diverse areas as statistics, physics, the theory of functions of
several variables, number theory, and geometry.

Deﬁnition 8.21 Quadratic Form

Aquadraticform q inthe n variables x1, x2, . . . , xn isalinearcombinationofterms
1, x2
x2

n,andcrossterms x1x2, x1x3, x2x3, . . . .

2, . . . , x2

If n = 3, q has the form

q = a11x2

1 + a22x2

2 + a33x2

3 + a12x1x2 + a21x2x1 + a13x1x3 + a31x3x1 + a23x2x3 + a32x3x2

In general

+ annx2
This sum can be written compactly as a matrix product

1 + a22x2

q = a11x2

2 +

· · ·

n + a12x1x2 + a13x1x3 +

· · ·

q = q(x) = xT Ax

where x = (x1, x2, . . . , xn) is thought of as a column, and A =
n matrix. Note that if
i
= j, two separate terms ai jxix j and a jix jxi are listed, each of which involves xix j, and they can (rather
cleverly) be replaced by

is a real n

ai j

×

(cid:2)

(cid:3)

1
2 (ai j + a ji)xix j

and

1
2 (ai j + a ji)x jxi

respectively, without altering the quadratic form. Hence there is no loss of generality in assuming that xix j
and x jxi have the same coefﬁcient in the sum for q. In other words, we may assume that A is symmetric.

6
480

Orthogonality

Example 8.9.1

Write q = x2

1 + 3x2

3 + 2x1x2

x1x3 in the form q(x) = xT Ax, where A is a symmetric 3

−
Solution. The cross terms are 2x1x2 = x1x2 + x2x1 and
Of course, x2x3 and x3x2 both have coefﬁcient zero, as does x2

−

x1x3 =

1
2x1x3

1
2x3x1.

−

−
2. Hence

3 matrix.

×

q(x) =

x1 x2 x3

(cid:2)

is the required form (verify).

1 1
1 0
1
2 0

−

1
2
0
3

x1
x2
x3

















(cid:3)

−

We shall assume from now on that all quadratic forms are given by

q(x) = xT Ax

where A is symmetric. Given such a form, the problem is to ﬁnd new variables y1, y2, . . . , yn, related to
x1, x2, . . . , xn, with the property that when q is expressed in terms of y1, y2, . . . , yn, there are no cross
terms. If we write

y = (y1, y2, . . . , yn)T
this amounts to asking that q = yT Dy where D is diagonal. It turns out that this can always be accomplished
and, not surprisingly, that D is the matrix obtained when the symmetric matrix A is orthogonally diagonal-
1 = PT ) and
ized. In fact, as Theorem 8.2.2 shows, a matrix P can be found that is orthogonal (that is, P−
diagonalizes A:

PT AP = D = 





0
λ1
0 λ2
...
...
0
0

0
0
...
λn

· · ·
· · ·

· · ·








The diagonal entries λ1, λ2, . . . , λn are the (not necessarily distinct) eigenvalues of A, repeated according
to their multiplicities in cA(x), and the columns of P are corresponding (orthonormal) eigenvectors of A.
As A is symmetric, the λi are real by Theorem 5.5.7.
Now deﬁne new variables y by the equations

Then substitution in q(x) = xT Ax gives

x = Py

equivalently

y = PT x

q = (Py)T A(Py) = yT (PT AP)y = yT Dy = λ1y2

1 +λ2y2

2 +

+λny2
n

· · ·

Hence this change of variables produces the desired simpliﬁcation in q.

Theorem 8.9.1: Diagonalization Theorem

Let q = xT Axbeaquadraticforminthevariables x1, x2, . . . , xn,wherex= (x1, x2, . . . , xn)T and
A isasymmetric n

n matrix. Let P beanorthogonalmatrixsuchthat PT AP isdiagonal,and

×

8.9. An Application to Quadratic Forms

481

deﬁnenewvariablesy= (y1, y2, . . . , yn)T by

x= Py equivalently y= PT x

If q isexpressedintermsofthesenewvariables y1, y2, . . . , yn,theresultis

whereλ1, λ2, . . . , λn aretheeigenvaluesof A repeatedaccordingtotheirmultiplicities.

q = λ1y2

1 +λ2y2

2 +

+λny2
n

· · ·

Let q = xT Ax be a quadratic form where A is a symmetric matrix and let λ1, . . . , λn be the (real) eigen-
values of A repeated according to their multiplicities. A corresponding set
of orthonormal
eigenvectors for A is called a set of principal axes for the quadratic form q. (The reason for the name
will become clear later.) The orthogonal matrix P in Theorem 8.9.1 is given as P =
, so the
variables X and Y are related by

f1, . . . , fn

· · ·

f1

fn

{

}

(cid:2)

(cid:3)

x = Py =

f1

f2

fn





= y1f1 + y2f2 +

+ ynfn

· · ·

· · ·

(cid:2)





Thus the new variables yi are the coefﬁcients when x is expanded in terms of the orthonormal basis
fi by the expansion theorem
f1, . . . , fn
{
(Theorem 5.3.6). Hence q itself is easily computed from the eigenvalues λi and the principal axes fi:

of Rn. In particular, the coefﬁcients yi are given by yi = x






}

(cid:3)

·

y1
y2
...
yn

q = q(x) = λ1(x

f1)2 +

·

· · ·

+λn(x

fn)2

·

Example 8.9.2

Find new variables y1, y2, y3, and y4 such that

q = 3(x2

1 + x2

2 + x2

3 + x2

4) + 2x1x2

10x1x3 + 10x1x4 + 10x2x3

10x2x4 + 2x3x4

−

−

has diagonal form, and ﬁnd the corresponding principal axes.

Solution. The form can be written as q = xT Ax, where

x = 

A routine calculation yields





x1
x2
x3
x4







and

A = 





3
1
5
−
5

1
3
5
5
−

5
−
5
3
1

5
5
−
1
3







cA(x) = det (xI

A) = (x

−

−

12)(x + 8)(x

4)2

−

so the eigenvalues are λ1 = 12, λ2 =

8, and λ3 = λ4 = 4. Corresponding orthonormal

−

482

Orthogonality

eigenvectors are the principal axes:

1
1
−
1
−
1







1
1
−
1
1
−







f2 = 1

2 





1
1
1
1







f3 = 1

2 





f1 = 1

2 





f4 = 1

2 

1
1
1
−
1
−







The matrix

P =

f1

f2

f3

f4

= 1

2 

(cid:2)

(cid:3)











1
1
−
1
−
1

1 1
1 1
1 1
1 1

−

−

1
1
1
−
1
−





is thus orthogonal, and P−
variables x are related by y = PT x and x = Py. Explicitly,

1AP = PT AP is diagonal. Hence the new variables y and the old

y1 = 1
y2 = 1
y3 = 1
y4 = 1

−

2(x1
x2
x3 + x4)
−
x4)
x2 + x3
2(x1
2(x1 + x2 + x3 + x4)
x4)
2(x1 + x2

x3

−

−

x1 = 1
2(y1 + y2 + y3 + y4)
x2 = 1
2(
x3 = 1
2(
−
x4 = 1
2(y1

y1
y1 + y2 + y3
y2 + y3

y2 + y3 + y4)
y4)

−
y4)

−

−

−

−
If these xi are substituted in the original expression for q, the result is

−

−

This is the required diagonal form.

q = 12y2

1 −

8y2

2 + 4y2

3 + 4y2
4

It is instructive to look at the case of quadratic forms in two variables x1 and x2. Then the principal
axes can always be found by rotating the x1 and x2 axes counterclockwise about the origin through an
R2, and it is shown in Theorem 2.6.4 that Rθ
angle θ. This rotation is a linear transformation Rθ : R2
denotes the standard basis of R2, the rotation produces a
has matrix P =

. If

→

e1, e2

cosθ
sinθ
given by

−

sinθ
cosθ

(cid:21)

{

}

new basis

(cid:20)
f1, f2
}

{

f1 = Rθ(e1) =

cosθ
sinθ

(cid:21)

(cid:20)

and

f2 = Rθ(e2) =

sinθ
cosθ

−

(cid:21)

(cid:20)

(8.7)

y2

x2

x2

y2

θ

x1

O

p

y1

y1

x1

x1
x2

(cid:20)

(cid:21)

Given a point p =

= x1e1 + x2e2 in the original system, let y1

and y2 be the coordinates of p in the new system (see the diagram). That
is,

x1
x2

(cid:20)

(cid:21)

Writing x =

= p = y1f1 + y2f2 =

cosθ
sinθ

sinθ
cosθ

−

y1
y2

(cid:21)

(cid:21) (cid:20)

(cid:20)

(8.8)

x1
x2

(cid:20)

(cid:21)

and y =

y1
y2

(cid:20)

, this reads x = Py so, since P is or-
(cid:21)

thogonal, this is the change of variables formula for the rotation as in Theorem 8.9.1.

8.9. An Application to Quadratic Forms

483

If r

= 0

= s, the graph of the equation rx2
rs < 0. More generally, given a quadratic form

1 + sx2

2 = 1 is called an ellipse if rs > 0 and a hyperbola if

q = ax2

1 + bx1x2 + cx2

2 where not all of a, b, and c are zero

the graph of the equation q = 1 is called a conic. We can now completely describe this graph. There are
two special cases which we leave to the reader.

1. If exactly one of a and c is zero, then the graph of q = 1 is a parabola.

So we assume that a
the discriminant of the quadratic form q.

= 0 and c

= 0. In this case, the description depends on the quantity b2

4ac, called

−

2. If b2

−

4ac = 0, then either both a

0 and c

Hence q = (√ax1 + √cx2)2 or q = (√
lines in either case.

−

≥

−

≥
ax1 + √

0, or both a

0.
cx2)2, so the graph of q = 1 is a pair of straight

0 and c

≤

≤

So we also assume that b2
4ac
the plane about the origin which transforms the equation ax2
hyperbola, and the theorem also provides a simple way to decide which conic it is.

= 0. But then the next theorem asserts that there exists a rotation of
2 = 1 into either an ellipse or a

1 + bx1x2 + cx2

−

Theorem 8.9.2
Considerthequadraticform q = ax2

1 + bx1x2 + cx2

2 where a, c,and b2

4ac areallnonzero.

−

1. Thereisacounterclockwiserotationofthecoordinateaxesabouttheoriginsuchthat,inthe

newcoordinatesystem, q hasnocrossterm.

2. Thegraphoftheequation

isanellipseif b2

−

4ac < 0 andanhyperbolaif b2

−

ax2

1 + bx1x2 + cx2

2 = 1
4ac > 0.

Proof. If b = 0, q already has no cross term and (1) and (2) are clear. So assume b

= 0. The matrix

A =

d =

(cid:20)

1
2b
a
1
c
2b
b2 + (a

of q has characteristic polynomial cA(x) = x2

(a + c)x

−
(cid:21)
c)2 for convenience; then the quadratic formula gives the eigenvalues
−

−

−

1
4(b2

4ac).

If we write

p

λ1 = 1

2[a + c

d]

−

and λ2 = 1

2 [a + c + d]

with corresponding principal axes

f1 =

f2 =

1
√b2+(a

d)2

c
−
−

1
√b2+(a

d)2

c
−
−

(cid:20)

(cid:20)

a

−

d

c
b

−

b
−
c

−

−

d

a

and

(cid:21)

(cid:21)

6
6
6
6
6
6
484

Orthogonality

as the reader can verify. These agree with equation (8.7) above if θ is an angle such that

cosθ =

a
d
c
−
−
√b2+(a
c
−
−

d)2

and

sinθ =

b
√b2+(a

d)2

c
−
−

cosθ
sinθ

sinθ
cosθ

−

Then P =

f1

f2

=

(cid:20)

(cid:2)

in Theorem 8.9.1. This proves (1).
0
λ1
0 λ2

Finally, A is similar to

(cid:3)

diagonalizes A and equation (8.8) becomes the formula x = Py

(cid:21)

so λ1λ2 = det A = 1

4 (4ac

b2). Hence the graph of λ1y2

1 +λ2y2

2 = 1

−

is an ellipse if b2 < 4ac and an hyperbola if b2 > 4ac. This proves (2).

(cid:20)

(cid:21)

Example 8.9.3

Consider the equation x2 + xy + y2 = 1. Find a rotation so that the equation has no cross term.

Solution.

y1

y2

x2

3π
4

x1

and sinθ = 1
√2
(x2

Here a = b = c = 1 in the notation of Theorem 8.9.2, so
1
. Hence θ = 3π
cosθ = −
4 will do it. The new
√2
variables are y1 = 1
1
x1) and y2 = −
(x2 + x1) by (8.8),
√2
√2
1 + 3y2
and the equation becomes y2
2 = 2. The angle θ has been
chosen such that the new y1 and y2 axes are the axes of symmetry
of the ellipse (see the diagram). The eigenvectors f1 = 1
√2

−

1
−
1

and f2 = 1
√2
this is the reason for the name principal axes.

(cid:21)

(cid:20)

point along these axes of symmetry, and

1
−
1
−

(cid:20)

(cid:21)

The determinant of any orthogonal matrix P is either 1 or

1 (because PPT = I). The orthogonal

−

arising from rotations all have determinant 1. More generally, given any

matrices

(cid:20)

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

quadratic form q = xT Ax, the orthogonal matrix P such that PT AP is diagonal can always be chosen so
that det P = 1 by interchanging two eigenvalues (and hence the corresponding columns of P). It is shown
2 matrices with determinant 1 correspond to rotations. Similarly,
in Theorem 10.4.4 that orthogonal 2
×
it can be shown that orthogonal 3
3 matrices with determinant 1 correspond to rotations about a line
through the origin. This extends Theorem 8.9.2: Every quadratic form in two or three variables can be
diagonalized by a rotation of the coordinate system.

×

8.9. An Application to Quadratic Forms

485

Congruence

We return to the study of quadratic forms in general.

Theorem 8.9.3
If q(x) = xT Axisaquadraticformgivenbyasymmetricmatrix A,then A isuniquelydetermined
by q.

Proof. Let q(x) = xT Bx for all x where BT = B. If C = A
must show that C = 0. Given y in Rn,

−

B, then CT = C and xTCx = 0 for all x. We

0 = (x + y)TC(x + y) = xTCx + xTCy + yTCx + yTCy

= xTCy + yTCx

But yTCx = (xTCy)T = xTCy (it is 1

1). Hence xTCy = 0 for all x and y in Rn. If e j is column j of

In, then the (i, j)-entry of C is eT

×
i Ce j = 0. Thus C = 0.

Hence we can speak of the symmetric matrix of a quadratic form.

On the other hand, a quadratic form q in variables xi can be written in several ways as a linear combi-
nation of squares of new variables, even if the new variables are required to be linear combinations of the
xi. For example, if q = 2x2

4x1x2 + x2

2 then

1 −
q = 2(x1

x2)2

x2
2

−

−

and

q =

2x2

1 + (2x1

−

x2)2

−

The question arises: How are these changes of variables related, and what properties do they share? To
investigate this, we need a new concept.

Let a quadratic form q = q(x) = xT Ax be given in terms of variables x = (x1, x2, . . . , xn)T . If the new
variables y = (y1, y2, . . . , yn)T are to be linear combinations of the xi, then y = Ax for some n
n matrix
A. Moreover, since we want to be able to solve for the xi in terms of the yi, we ask that the matrix A be
invertible. Hence suppose U is an invertible matrix and that the new variables y are given by

×

y = U −

1x,

equivalently x = U y

In terms of these new variables, q takes the form

q = q(x) = (U y)T A(U y) = yT (U T AU )y

That is, q has matrix U T AU with respect to the new variables y. Hence, to study changes of variables
n matrices A and B are
in quadratic forms, we study the following relationship on matrices: Two n
called congruent, written A c
B, if B = U T AU for some invertible matrix U . Here are some properties of
∼
congruence:

×

A for all A.

1. A c
∼
2. If A c
∼
3. If A c
∼

B, then B c
A.
∼
C, then A c
B and B c
∼
∼

C.

486

Orthogonality

4. If A c
∼
5. If A c
∼

B, then A is symmetric if and only if B is symmetric.

B, then rank A = rank B.

The converse to (5) can fail even for symmetric matrices.

Example 8.9.4

The symmetric matrices A =
congruent. Indeed, if A c
∼

(cid:20)

1 = det B = ( det U )2, a contradiction.

1 0
0 1

(cid:21)

−

and B =

have the same rank but are not

1
0

(cid:20)

0
1
−

(cid:21)

B, an invertible matrix U exists such that B = U T AU = U TU . But then

The key distinction between A and B in Example 8.9.4 is that A has two positive eigenvalues (counting

multiplicities) whereas B has only one.

Theorem 8.9.4: Sylvester’s Law of Inertia
If A c
∼

B,then A and B havethesamenumberofpositiveeigenvalues,countingmultiplicities.

The proof is given at the end of this section.

The index of a symmetric matrix A is the number of positive eigenvalues of A. If q = q(x) = xT Ax
is a quadratic form, the index and rank of q are deﬁned to be, respectively, the index and rank of the
matrix A. As we saw before, if the variables expressing a quadratic form q are changed, the new matrix is
congruent to the old one. Hence the index and rank depend only on q and not on the way it is expressed.
Now let q = q(x) = xT Ax be any quadratic form in n variables, of index k and rank r, where A is

symmetric. We claim that new variables z can be found so that q is completely diagonalized—that is,
+ z2

q(z) = z2

1 +

· · ·

z2
k+1 − · · · −

z2
r

k −

If k
by r

≤
−

r
n, let Dn(k, r) denote the n
≤
k minus ones, followed by n

n diagonal matrix whose main diagonal consists of k ones, followed
×
r zeros. Then we seek new variables z such that
−

q(z) = zT Dn(k, r)z

To determine z, ﬁrst diagonalize A as follows: Find an orthogonal matrix P0 such that

PT
0 AP0 = D = diag (λ1, λ2, . . . , λr, 0, . . . , 0)

r
is diagonal with the nonzero eigenvalues λ1, λ2, . . . , λr of A on the main diagonal (followed by n
zeros). By reordering the columns of P0, if necessary, we may assume that λ1, . . . , λk are positive and
λk+1, . . . , λr are negative. This being the case, let D0 be the n

n diagonal matrix

−

D0 = diag

1
√λ1

, . . . ,

1
√λk

,

1
λk+1

√

, 1, . . . , 1

1

−

√

λr

Then DT

(cid:19)
0 DD0 = Dn(k, r), so if new variables z are given by x = (P0D0)z, we obtain

(cid:18)

−

×
, . . . ,

1 +
as required. Note that the change-of-variables matrix P0D0 from z to x has orthogonal columns (in fact,
scalar multiples of the columns of P0).

q(z) = zT Dn(k, r)z = z2

z2
k+1 − · · · −

+ z2

k −

z2
r

· · ·

8.9. An Application to Quadratic Forms

487

Example 8.9.5

Completely diagonalize the quadratic form q in Example 8.9.2 and ﬁnd the index and rank .

Solution. In the notation of Example 8.9.2, the eigenvalues of the matrix A of q are 12,
8, 4, 4; so
the index is 3 and the rank is 4. Moreover, the corresponding orthogonal eigenvectors are f1, f2, f3
f3
(see Example 8.9.2), and f4. Hence P0 =

is orthogonal and

f1

f2

f4

−

(cid:3)
PT
0 AP0 = diag (12, 4, 4,

(cid:2)

8)

−

As before, take D0 = diag ( 1
2, 1
, 1
1
) and deﬁne the new variables z by x = (P0D0)z. Hence
2,
√8
√12
1
0 PT
0 x. The result is
the new variables are given by z = D−

x3 + x4)

−

−

x2

z1 = √3(x1
z2 = x1 + x2 + x3 + x4
x3
z3 = x1 + x2
x4
−
z4 = √2(x1
x2 + x3

−

−

x4)

−

This discussion gives the following information about symmetric matrices.

Theorem 8.9.5

Let A and B besymmetric n

n matrices,andlet 0

k

r

≤

≤

≤

n.

×

1. A hasindex k and rank r ifandonlyif A c
∼

Dn(k, r).

B ifandonlyiftheyhavethesame rank andindex.

2. A c
∼

Proof.

1. If A has index k and rank r, take U = P0D0 where P0 and D0 are as described prior to Example 8.9.5.
Then U T AU = Dn(k, r). The converse is true because Dn(k, r) has index k and rank r (using
Theorem 8.9.4).

2. If A and B both have index k and rank r, then A c
∼

earlier.

Dn(k, r)

c
∼

B by (1). The converse was given

488

Orthogonality

Proof of Theorem 8.9.4.
D1 and B c
By Theorem 8.9.1, A c
D2 where D1 and D2 are diagonal and have the same eigenvalues as A
∼
∼
D2, (because A c
c
and B, respectively. We have D1
B), so we may assume that A and B are both diagonal.
∼
∼
Consider the quadratic form q(x) = xT Ax. If A has k positive eigenvalues, q has the form

q(x) = a1x2

1 +

+ akx2

k −

· · ·

ak+1x2

k+1 − · · · −

arx2

r , ai > 0

where r = rank A = rank B. The subspace W1 =
and satisﬁes q(x) > 0 for all x

= 0 in W1.

x

{

|

xk+1 =

= xr = 0

}

· · ·

of Rn has dimension n

r + k

−

On the other hand, if B = U T AU , deﬁne new variables y by x = U y. If B has k′ positive eigenvalues, q

has the form

q(x) = b1y2

1 +

+ bk′

y2
k′ −

· · ·

bk′+1y2

k′+1 − · · · −

bry2

r , bi > 0

Let f1, . . . , fn denote the columns of U . They are a basis of Rn and

x = U y =

f1

fn



· · ·

= y1f1 +



· · ·

+ ynfn

(cid:2)

(cid:3)






satisﬁes q(x) < 0 for all x

y1
...
yn

Hence the subspace W2 = span
k′.
It follows that W1 and W2 have only the zero vector in common. Hence, if B1 and B2 are bases of W1 and
W2, respectively, then (Exercise 6.3.33) B1
k′
vectors in Rn. This implies that k

B2 is an independent set of (n
k′, and a similar argument shows k′ ≤

= 0 in W2. Note dim W2 = r

fk′+1, . . . , fr

k′) = n + k

r + k) + (r

−
k.

−

−

−

≤

∪

}

{

Exercises for 8.9

Exercise 8.9.1 In each case, ﬁnd a symmetric matrix A
such that q = xT Bx takes the form q = xT Ax.

1 1
0 1

(cid:21)

1 0 1
1 1 0
0 1 1

a.

c.

(cid:20)









b.

d.

1 1
1 2

2
1
2

−

−
1
4
5

(cid:20)





(cid:21)

−

1
0
3





f. q = 5x2

1 + 8x2

2 + 5x2

3 −

4(x1x2 + 2x1x3 + x2x3)

g. q = x2

1 −

x2
3 −

4x1x2 + 4x2x3

h. q = x2

1 + x2

3 −

2x1x2 + 2x2x3

Exercise 8.9.2 In each case, ﬁnd a change of variables
that will diagonalize the quadratic form q. Determine the
index and rank of q.

Exercise 8.9.3 For each of the following, write the equa-
tion in terms of new variables so that it is in standard
position, and identify the curve.

a. q = x2

b. q = x2

c. q = x2

d. q = 7x2

e. q = 2(x2

1 + 2x1x2 + x2
2
1 + 4x1x2 + x2
2
2 + x2
1 + x2
3 −
2 + x2
1 + x2
3 + 8x1x2 + 8x1x3
2 + x2
1 + x2

x1x2 + x1x3

3 −

4(x1x2 + x1x3 + x2x3)

16x2x3

−
x2x3)

−

a.

xy = 1

c.

6x2 + 6xy

−

b.

3x2

−

4xy = 2

2y2 = 5

d.

2x2 + 4xy + 5y2 = 1

6
6
8.10. An Application to Constrained Optimization

489

Exercise 8.9.4 Consider the equation ax2 + bxy + cy2 =
d, where b
= 0. Introduce new variables x1 and y1 by
rotating the axes counterclockwise through an angle θ.
Show that the resulting equation has no x1y1-term if θ is
given by

cos 2θ =

sin 2θ =

a
c
−
√b2+(a

b
√b2+(a

c)2

−

c)2

−

[Hint: Use equation (8.8) preceding Theorem 8.9.2

to get x and y in terms of x1 and y1, and substitute.]

Exercise 8.9.5 Prove properties (1)–(5) preceding Ex-
ample 8.9.4.
Exercise 8.9.6 If A c
∼
only if B is invertible.

B show that A is invertible if and

Exercise 8.9.7 If x = (x1, . . . , xn)T is a column of vari-
ables, A = AT is n
n, and c is a constant,
xT Ax+Bx = c is called a quadratic equation in the vari-
ables xi.

n, B is 1

×

×

Exercise 8.9.8 Given a symmetric matrix A, deﬁne
qA(x) = xT Ax. Show that B c
A if and only if B is
∼
symmetric and there is an invertible matrix U such that
qB(x) = qA(U x) for all x. [Hint: Theorem 8.9.3.]
Exercise 8.9.9 Let q(x) = xT Ax be a quadratic form
where A = AT .

a. Show that q(x) > 0 for all x

= 0, if and only if A is
positive deﬁnite (all eigenvalues are positive). In
this case, q is called positive deﬁnite.

b. Show that new variables y can be found such that
2 and y = U x where U is upper triangu-
k
[Hint: Theo-

q =
lar with positive diagonal entries.
rem 8.3.3.]

y
k

Exercise 8.9.10 A bilinear form β on Rn is a function
that assigns to every pair x, y of columns in Rn a number
β(x, y) in such a way that

β(rx + sy, z) = rβ(x, z) + sβ(y, z)
β(x, ry + sz) = rβ(x, z) + sβ(x, z)

a. Show that new variables y1, . . . , yn can be found

such that the equation takes the form

for all x, y, z in Rn and r, s in R. If β(x, y) = β(y, x) for
all x, y, β is called symmetric.

λ1y2

1 +

· · ·

+ λry2

r + k1y1 +

· · ·

+ knyn = c

b. Put x2

1 + 3x2

6x3 = 7
in this form and ﬁnd variables y1, y2, y3 as in (a).

4x1x3 + 5x1

3 + 4x1x2

2 + 3x2

−

−

a. If β is a bilinear form, show that an n

n matrix

A exists such that β(x, y) = xT Ay for all x, y.

×

b. Show that A is uniquely determined by β.

c. Show that β is symmetric if and only if A = AT .

8.10 An Application to Constrained Optimization

It is a frequent occurrence in applications that a function q = q(x1, x2, . . . , xn) of n variables, called an
objective function, is to be made as large or as small as possible among all vectors x = (x1, x2, . . . , xn)
lying in a certain region of Rn called the feasible region. A wide variety of objective functions q arise in
practice; our primary concern here is to examine one important situation where q is a quadratic form. The
next example gives some indication of how such problems arise.

6
6
490

Orthogonality

Example 8.10.1

x2

√5
2

1

5x2

1 + 3x2

2 ≤

c = 2

A politician proposes to spend x1 dollars annually on health
care and x2 dollars annually on education. She is constrained
in her spending by various budget pressures, and one model of this
is that the expenditures x1 and x2 should satisfy a constraint like

15

5x2

1 + 3x2

15

2 ≤
0 for each i, the feasible region is the shaded area

1

2

O

x1

√3

≥

c = 1

Since xi
shown in the diagram. Any choice of feasible point (x1, x2) in this
region will satisfy the budget constraints. However, these choices
have different effects on voters, and the politician wants to choose
x = (x1, x2) to maximize some measure q = q(x1, x2) of voter satisfaction. Thus the assumption is
that, for any value of c, all points on the graph of q(x1, x2) = c have the same appeal to voters.
Hence the goal is to ﬁnd the largest value of c for which the graph of q(x1, x2) = c contains a
feasible point.
The choice of the function q depends upon many factors; we will show how to solve the problem
for any quadratic form q (even with more than two variables). In the diagram the function q is
given by

q(x1, x2) = x1x2

and the graphs of q(x1, x2) = c are shown for c = 1 and c = 2. As c increases the graph of
q(x1, x2) = c moves up and to the right. From this it is clear that there will be a solution for some
value of c between 1 and 2 (in fact the largest value is c = 1
2

√15 = 1.94 to two decimal places).

15 in Example 8.10.1 can be put in a standard form. If we divide through

1 + 3x2
The constraint 5x2
2
x1
+
√3
≤
. Then the constraint becomes

by 15, it becomes
y1 = x1
√3
variables, the objective function is q = √15y1y2, and we want to maximize this subject to
y
this is done, the maximizing values of x1 and x2 are obtained from x1 = √3y1 and x2 = √5y2.

1. This suggests that we introduce new variables y = (y1, y2) where
1. In terms of these new

and y2 = x2
(cid:16)
√5

1, equivalently

2 ≤
x2
√5

y
k ≤

1. When

k ≤

≤

(cid:17)

(cid:16)

(cid:17)

k

k

y

k

k

2

2

Hence, for constraints like that in Example 8.10.1, there is no real loss in generality in assuming that
1. In this case the principal axes theorem solves the problem. Recall

the constraint takes the form
that a vector in Rn of length 1 is called a unit vector.

x
k ≤

k

Theorem 8.10.1
Considerthequadraticform q = q(x) = xT Axwhere A isan n
andλn denotethelargestandsmallesteigenvaluesof A,respectively. Then:

×

n symmetricmatrix,andletλ1

1. max

2. min

q(x)

{
q(x)

x

| k
x

{

| k

k ≤

k ≤

= λ1,and q(f1) = λ1 wheref1 isanyunitλ1-eigenvector.

= λn,and q(fn) = λn wherefn isanyunitλn-eigenvector.

1

}

1

}

Proof. Since A is symmetric, let the (real) eigenvalues λi of A be ordered as to size as follows:

λ1

λ2

≥

≥ · · · ≥

λn

8.10. An Application to Constrained Optimization

491

By the principal axes theorem, let P be an orthogonal matrix such that PT AP = D = diag (λ1, λ2, . . . , λn).
Deﬁne y = PT x, equivalently x = Py, and note
2.
=
If we write y = (y1, y2, . . . , yn)T , then

2 = yT y = xT (PPT )x = xT x =

because

y
k

y
k

k

k

k

x

x

k

k

k

q(x) = q(Py) = (Py)T A(Py)
= yT (PT AP)y = yT Dy
1 +λ2y2
= λ1y2

2 +

+λny2
n

· · ·

(8.9)

Now assume that

1. Since λi

λ1 for each i, (8.9) gives

k

x
k ≤
q(x) = λ1y2

≤
2 +

1 +λ2y2

+λny2

n ≤

λ1y2

1 +λ1y2

2 +

+λ1y2

n = λ1

2

y
k

k

≤

λ1

· · ·

· · ·

y

because
is actually achieved, let f1 be a unit eigenvector corresponding to λ1. Then

1. This shows that q(x) cannot exceed λ1 when

k ≤

=

k

x

k

x

k

k

k ≤

1. To see that this maximum

q(f1) = fT

1 Af1 = fT

1 (λ1f1) = λ1(fT

1 f1) = λ1

2 = λ1

f1

k

k

Hence λ1 is the maximum value of q(x) when

x

k

k ≤

1, proving (1). The proof of (2) is analogous.

The set of all vectors x in Rn such that

1 is called the unit ball. If n = 2, it is often called the
unit disk and consists of the unit circle and its interior; if n = 3, it is the unit sphere and its interior. It is
worth noting that the maximum value of a quadratic form q(x) as x ranges throughout the unit ball is (by
Theorem 8.10.1) actually attained for a unit vector x on the boundary of the unit ball.

k ≤

k

x

Theorem 8.10.1 is important for applications involving vibrations in areas as diverse as aerodynamics
and particle physics, and the maximum and minimum values in the theorem are often found using advanced
calculus to minimize the quadratic form on the unit ball. The algebraic approach using the principal axes
theorem gives a geometrical interpretation of the optimal values because they are eigenvalues.

Example 8.10.2

Maximize and minimize the form q(x) = 3x2

1 + 14x1x2 + 3x2

2 subject to

x

k

k ≤

1.

Solution. The matrix of q is A =

3 7
7 3
corresponding unit eigenvectors f1 = 1
√2
x in R2, q(x) takes its maximal value 10 at x = f1, and the minimum value of q(x) is
x = f2.

, with eigenvalues λ1 = 10 and λ2 =
(cid:21)
(1, 1) and f2 = 1
√2

(1,

−

(cid:20)

4, and

−
1). Hence, among all unit vectors

4 when

−

As noted above, the objective function in a constrained optimization problem need not be a quadratic
form. We conclude with an example where the objective function is linear, and the feasible region is
determined by linear constraints.

492

Orthogonality

Example 8.10.3

x2

1200x1 + 1300x2 = 8700

p

p

p

=

=

=

4

5

5

3

0

7

0

0

0

(4, 3)

O
2000x1 + 1100x2 = 11300

x1

A manufacturer makes x1 units of product 1, and x2 units
of product 2, at a proﬁt of $70 and $50 per unit respectively,
and wants to choose x1 and x2 to maximize the total proﬁt
p(x1, x2) = 70x1 + 50x2. However x1 and x2 are not arbitrary; for
example, x1
0. Other conditions also come into play.
Each unit of product 1 costs $1200 to produce and requires 2000
square feet of warehouse space; each unit of product 2 costs $1300
to produce and requires 1100 square feet of space. If the total
warehouse space is 11 300 square feet, and if the total production
budget is $8700, x1 and x2 must also satisfy the conditions

0 and x2

≥

≥

2000x1 + 1100x2
1200x1 + 1300x2

11300
8700

≤
≤
0, x2

The feasible region in the plane satisfying these constraints (and x1
diagram. If the proﬁt equation 70x1 + 50x2 = p is plotted for various values of p, the resulting
lines are parallel, with p increasing with distance from the origin. Hence the best choice occurs for
the line 70x1 + 50x2 = 430 that touches the shaded region at the point (4, 3). So the proﬁt p has a
maximum of p = 430 for x1 = 4 units and x2 = 3 units.

0) is shaded in the

≥

≥

Example 8.10.3 is a simple case of the general linear programming problem23 which arises in eco-
nomic, management, network, and scheduling applications. Here the objective function is a linear com-
+ anxn of the variables, and the feasible region consists of the vectors
bination q = a1x1 + a2x2 +
x = (x1, x2, . . . , xn)T in Rn which satisfy a set of linear inequalities of the form b1x1 +b2x2 +
b.
There is a good method (an extension of the gaussian algorithm) called the simplex algorithm for ﬁnding
the maximum and minimum values of q when x ranges over such a feasible set. As Example 8.10.3 sug-
gests, the optimal values turn out to be vertices of the feasible set. In particular, they are on the boundary
of the feasible region, as is the case in Theorem 8.10.1.

+bnxn

· · ·

· · ·

≤

8.11 An Application to Statistical Principal Component

Analysis

Linear algebra is important in multivariate analysis in statistics, and we conclude with a very short look
at one application of diagonalization in this area. A main feature of probability and statistics is the idea
of a random variable X , that is a real-valued function which takes its values according to a probability
law (called its distribution). Random variables occur in a wide variety of contexts; examples include the
number of meteors falling per square kilometre in a given region, the price of a share of a stock, or the
duration of a long distance telephone call from a certain city.

The values of a random variable X are distributed about a central number µ, called the mean of X .
The mean can be calculated from the distribution as the expectation E(X ) = µ of the random variable X .
µ)2 is a random variable,
Functions of a random variable are again random variables. In particular, (X

−

23More information is available in “Linear Programming and Extensions” by N. Wu and R. Coppins, McGraw-Hill, 1981.

8.11. An Application to Statistical Principal Component Analysis

493

and the variance of the random variable X , denoted var (X ), is deﬁned to be the number

var (X ) = E

(X

{

−

µ)2

}

where µ = E(X )

var (X ) is called
It is not difﬁcult to see that var (X )
the standard deviation of X , and is a measure of how much the values of X are spread about the mean
µ of X . A main goal of statistical inference is ﬁnding reliable methods for estimating the mean and the
standard deviation of a random variable X by sampling the values of X .

0 for every random variable X . The number σ =

p

≥

If two random variables X and Y are given, and their joint distribution is known, then functions of X
and Y are also random variables. In particular, X +Y and aX are random variables for any real number a,
and we have

E(X +Y ) = E(X ) + E(Y )

and

E(aX ) = aE(X ).24

An important question is how much the random variables X and Y depend on each other. One measure of
this is the covariance of X and Y , denoted cov (X , Y ), deﬁned by

cov (X , Y ) = E

(X

µ)(Y

υ)

where µ = E(X ) and υ = E(Y )

}
Clearly, cov (X , X ) = var (X ). If cov (X , Y ) = 0 then X and Y have little relationship to each other and
are said to be uncorrelated.25

−

−

{

Multivariate statistical analysis deals with a family X1, X2, . . . , Xn of random variables with means
i = var (Xi) for each i. Let σi j = cov (Xi, X j) denote the covariance of Xi and

µi = E(Xi) and variances σ2
X j. Then the covariance matrix of the random variables X1, X2, . . . , Xn is deﬁned to be the n

n matrix

×

Σ = [σi j]

whose (i,
semideﬁnite in the sense that λ
cases of interest.) So suppose that the eigenvalues of Σ are λ1
theorem (Theorem 8.2.2) shows that an orthogonal matrix P exists such that

j)-entry is σi j. The matrix Σ is clearly symmetric; in fact it can be shown that Σ is positive
0 for every eigenvalue λ of Σ. (In reality, Σ is positive deﬁnite in most
0. The principal axes

≥ · · · ≥

λ2

λn

≥

≥

≥

PT ΣP = diag (λ1, λ2, . . . , λn)

If we write X = (X1, X2, . . . , Xn), the procedure for diagonalizing a quadratic form gives new variables
Y = (Y1, Y2, . . . , Yn) deﬁned by

Y = PT X
These new random variables Y1, Y2, . . . , Yn are called the principal components of the original random
variables Xi, and are linear combinations of the Xi. Furthermore, it can be shown that

cov (Yi, Yj) = 0 if i

= j

and

var (Yi) = λi

for each i

Of course the principal components Yi point along the principal axes of the quadratic form q = X T ΣX.

The sum of the variances of a set of random variables is called the total variance of the variables, and
determining the source of this total variance is one of the beneﬁts of principal component analysis. The
fact that the matrices Σ and diag (λ1, λ2, . . . , λn) are similar means that they have the same trace, that is,

24Hence E( ) is a linear transformation from the vector space of all random variables to the space of real numbers.
25If X and Y are independent in the sense of probability theory, then they are uncorrelated; however, the converse is not true

σ11 +σ22 +

+σnn = λ1 +λ2 +

+λn

· · ·

· · ·

in general.

6
494

Orthogonality

λ2

This means that the principal components Yi have the same total variance as the original random variables
Xi. Moreover, the fact that λ1
0 means that most of this variance resides in the ﬁrst few
Yi. In practice, statisticians ﬁnd that studying these ﬁrst few Yi (and ignoring the rest) gives an accurate
analysis of the total system variability. This results in substantial data reduction since often only a few Yi
sufﬁce for all practical purposes. Furthermore, these Yi are easily obtained as linear combinations of the
Xi. Finally, the analysis of the principal components often reveals relationships among the Xi that were not
previously suspected, and so results in interpretations that would not otherwise have been made.

≥ · · · ≥

λn

≥

≥

Chapter 9

Change of Basis

If A is an m

×

n matrix, the corresponding matrix transformation TA : Rn

Rm is deﬁned by

→

TA(x) = Ax

for all columns x in Rn

It was shown in Theorem 2.6.2 that every linear transformation T : Rn
that is, T = TA for some m
A is given in terms of its columns by

Rm is a matrix transformation;
n matrix A. Furthermore, the matrix A is uniquely determined by T . In fact,

→

×

where

e1, e2, . . . , en

{

}

A =

T (e1) T (e2)

T (en)

· · ·

is the standard basis of Rn.

(cid:2)

(cid:3)

In this chapter we show how to associate a matrix with any linear transformation T : V

W where V
and W are ﬁnite-dimensional vector spaces, and we describe how the matrix can be used to compute T (v)
for any v in V . The matrix depends on the choice of a basis B in V and a basis D in W , and is denoted
MDB(T ). The case when W = V is particularly important. If B and D are two bases of V , we show that the
1MBB(T )P for some invertible matrix P.
matrices MBB(T ) and MDD(T ) are similar, that is MDD(T ) = P−
Moreover, we give an explicit method for constructing P depending only on the bases B and D. This leads
to some of the most important theorems in linear algebra, as we shall see in Chapter 11.

→

9.1 The Matrix of a Linear Transformation

→

W be a linear transformation where dim V = n and dim W = m. The aim in this section is to
Let T : V
n matrix A. The idea is to convert a vector v in V into
describe the action of T as multiplication by an m
a column in Rn, multiply that column by A to get a column in Rm, and convert this column back to get
T (v) in W .

×

Converting vectors to columns is a simple matter, but one small change is needed. Up to now the order
of the vectors in a basis has been of no importance. However, in this section, we shall speak of an ordered
basis
, which is just a basis where the order in which the vectors are listed is taken into
{
account. Hence
If B =

b1, b2, b3
is a different ordered basis from
}
is an ordered basis in a vector space V , and if
}

}
b2, b1, b3
b1, b2, . . . , bn

b1, b2, . . . , bn

}

{

{

{

.

· · ·
is a vector in V , then the (uniquely determined) numbers v1, v2, . . . , vn are called the coordinates of v
with respect to the basis B.

∈

v = v1b1 + v2b2 +

+ vnbn,

vi

R

495

496

Change of Basis

Deﬁnition 9.1 Coordinate Vector CB(v) of v for a basis B

Thecoordinatevectorofvwithrespectto B isdeﬁnedtobe

CB(v) = (v1b1 + v2b2 +

· · ·

+ vnbn) = 





v1
v2
...
vn








The reason for writing CB(v) as a column instead of a row will become clear later. Note that CB(bi) = ei
is column i of In.

Example 9.1.1

The coordinate vector for v = (2, 1, 3) with respect to the ordered basis

B =

{

(1, 1, 0), (1, 0, 1), (0, 1, 1)

of R3 is CB(v) =

}



because

v = (2, 1, 3) = 0(1, 1, 0) + 2(1, 0, 1) + 1(0, 1, 1)

0
2
1 




Theorem 9.1.1

IfV hasdimension n and B =
transformationCB : V

b1, b2, . . . , bn
{

}

1
B : Rn
Rn isanisomorphism. Infact,C−

isanyorderedbasisofV,thecoordinate

V isgivenby

→

1
C−
B

→
v1
v2
...
vn








= v1b1 + v2b2 +

· · ·








+ vnbn

forall 






in Rn.

v1
v2
...
vn








Proof. The veriﬁcation that CB is linear is Exercise 9.1.13. If T : Rn
in
the theorem, one veriﬁes (Exercise 9.1.13) that TCB = 1V and CBT = 1Rn. Note that CB(b j) is column
j of the identity matrix, so CB carries the basis B to the standard basis of Rn, proving again that it is an
isomorphism (Theorem 7.3.1)

1
V is the map denoted C−
B

→

T

TA

V

CB

Rn

W

CD

Rm

→

Now let T : V

dim W = m, and let B =
W , respectively. Then CB : V
→
and we have the situation shown in the diagram where A is an m
(to be determined). In fact, the composite

W be any linear transformation where dim V = n and
and D be ordered bases of V and
Rm are isomorphisms
n matrix

}
Rn and CD : W

b1, b2, . . . , bn

→

×

{

so Theorem 2.6.2 shows that a unique m

B : Rn
1
CDTC−
n matrix A exists such that

→

Rm is a linear transformation

1
CDTC−

×
B = TA,

equivalently CDT = TACB

9.1. The Matrix of a Linear Transformation

497

TA acts by left multiplication by A, so this latter condition is

CD[T (v)] = ACB(v) for all v in V

This requirement completely determines A. Indeed, the fact that CB(b j) is column j of the identity matrix
gives

column j of A = ACB(b j) = CD[T (b j)]

for all j. Hence, in terms of its columns,

A =

CD[T (b1)] CD[T (b2)]

CD[T (bn)]

· · ·

(cid:2)

(cid:3)

Deﬁnition 9.2 Matrix MDB(T ) of T : V

W for bases D and B

→

Thisiscalledthematrixof T corresponding totheordered bases B and D,andweusethe
followingnotation:

MDB(T ) =

CD[T (b1)] CD[T (b2)]

(cid:2)

CD[T (bn)]

· · ·

(cid:3)

This discussion is summarized in the following important theorem.

Theorem 9.1.2

Let T : V
→
b1, . . . , bn
B =
{
givenistheunique m

}

W bealineartransformationwhere dim V = n and dim W = m,andlet

and D beorderedbasesofV andW,respectively. Thenthematrix MDB(T ) just

n matrix A thatsatisﬁes

×

CDT = TACB

Hencethedeﬁningpropertyof MDB(T ) is

CD[T (v)] = MDB(T )CB(v) forallvinV

Thematrix MDB(T ) isgivenintermsofitscolumnsby

MDB(T ) =

CD[T (b1)] CD[T (b2)]

(cid:2)

CD[T (bn)]

· · ·

(cid:3)

1
D TACB means that the action of T on a vector v in V can be performed by ﬁrst taking
The fact that T = C−
coordinates (that is, applying CB to v), then multiplying by A (applying TA), and ﬁnally converting the
1
resulting m-tuple back to a vector in W (applying C−
D ).

Example 9.1.2

Deﬁne T : P2
B =

→
b1, b2, b3
}

{

R2 by T (a + bx + cx2) = (a + c, b
and D =

where

d1, d2

{

}

c) for all polynomials a + bx + cx2. If

a

−

−

b1 = 1, b2 = x, b3 = x2

and

d1 = (1, 0), d2 = (0, 1)

498

Change of Basis

compute MDB(T ) and verify Theorem 9.1.2.

Solution. We have T (b1) = d1

d2, T (b2) = d2, and T (b3) = d1

d2. Hence

−

−

MDB(T ) =

CD[T (b1)] CD[T (b2)] CD[T (bn)]

=

(cid:3)
If v = a + bx + cx2 = ab1 + bb2 + cb3, then T (v) = (a + c)d1 + (b

(cid:2)

1 0
1 1

−

(cid:20)

1
1
−

(cid:21)

a

−

−

c)d2, so

CD[T (v)] =

b

(cid:20)

as Theorem 9.1.2 asserts.

a + c
a

−

−

c

=

(cid:21)

(cid:20)

1 0
1 1

−

1
1
−

a
b
c 


(cid:21)





= MDB(T )CB(v)

The next example shows how to determine the action of a transformation from its matrix.

Example 9.1.3

Suppose T : M22(R)

→

R3 is linear with matrix MDB(T ) =

B =

1 0
0 0

0 1
0 0

,

(cid:21)

(cid:20)

(cid:26)(cid:20)

Compute T (v) where v =

(cid:20)

,

(cid:21)

(cid:20)

a b
c d

0 0
1 0

,

(cid:21)

(cid:20)

0 0
0 1

(cid:21)(cid:27)

.

(cid:21)

1
0
0

1
−
1
0

0
1
−
1





0
0
1 


−

where

and D =

{

(1, 0, 0), (0, 1, 0), (0, 0, 1)

}

Solution. The idea is to compute CD[T (v)] ﬁrst, and then obtain T (v). We have

CD[T (v)] = MDB(T )CB(v) =

1
0
0

1
−
1
0

0
1
−
1





a
b
c
d

0
0
1 


−









=





a
b
c

−
−
−

b
c
d 






Hence T (v) = (a
= (a

−
−

b)(1, 0, 0) + (b
d)
c, c
b, b

−

−

c)(0, 1, 0) + (c

d)(0, 0, 1)

−

−

The next two examples will be referred to later.

Example 9.1.4

Rm be the matrix transformation induced by
Let A be an m
→
A : TA(x) = Ax for all columns x in Rn. If B and D are the standard bases of Rn and Rm,

n matrix, and let TA : Rn

×

9.1. The Matrix of a Linear Transformation

499

respectively (ordered as usual), then

MDB(TA) = A

In other words, the matrix of TA corresponding to the standard bases is A itself.

Solution. Write B =
{
CD(y) = y for all columns y in Rm. Hence

e1, . . . , en

}

. Because D is the standard basis of Rm, it is easy to verify that

MDB(TA) =

TA(e1) TA(e2)

TA(en)

=

Ae1 Ae2

· · ·

because Ae j is the jth column of A.

(cid:2)

(cid:3)

(cid:2)

Aen

= A

· · ·

(cid:3)

Example 9.1.5

Let V and W have ordered bases B and D, respectively. Let dim V = n.

1. The identity transformation 1V : V

V has matrix MBB(1V ) = In.

→

2. The zero transformation 0 : V

W has matrix MDB(0) = 0.

→

The ﬁrst result in Example 9.1.5 is false if the two bases of V are not equal. In fact, if B is the standard
basis of Rn, then the basis D of Rn can be chosen so that MDB(1Rn) turns out to be any invertible matrix
we wish (Exercise 9.1.14).

The next two theorems show that composition of linear transformations is compatible with multiplica-

tion of the corresponding matrices.

Theorem 9.1.3

T

V

W

ST

S

U

LetV T
→
ﬁniteorderedbasesofV,W,andU,respectively. Then

U be linear transformationsand let B, D, and E be

W S
→

MEB(ST ) = MED(S)

MDB(T )

·

Proof. We use the property in Theorem 9.1.2 three times. If v is in V ,

MED(S)MDB(T )CB(v) = MED(S)CD[T (v)] = CE[ST (v)] = MEB(ST )CB(v)

e1, . . . , en

If B =
MEB(ST ) have equal jth columns. The theorem follows.

{

}

, then CB(e j) is column j of In. Hence taking v = e j shows that MED(S)MDB(T ) and

500

Change of Basis

Theorem 9.1.4

Let T : V
→
equivalent.

W bealineartransformation,where dim V = dim W = n. Thefollowingare

1. T isanisomorphism.

2. MDB(T ) isinvertibleforallorderedbases B and D ofV andW.

3. MDB(T ) isinvertibleforsomepairoforderedbases B and D ofV andW.

Whenthisisthecase, [MDB(T )]−

1 = MBD(T −

1).

Proof. (1)

⇒

1

(2). We have V T
→
MBD(T −

W T −
→
1)MDB(T ) = MBB(T −

1T ) = MBB(1v) = In

V , so Theorem 9.1.3 and Example 9.1.5 give

1) = In, proving (2) (and the last statement in the theorem).

Similarly, MDB(T )MBD(T −
(3). This is clear.

(2)

⇒

TA−

Rn

1

TA

Rn

Rn
TATA−

(3)

(1). Suppose that TDB(T ) is invertible for some bases B and D and, for
convenience, write A = MDB(T ). Then we have CDT = TACB by Theorem 9.1.2,
so

⇒

1

by Theorem 9.1.1 where (CD)−

we can demonstrate that TA : Rn
that TATA−
1 = 1Rn = TA−

T = (CD)−
1 and CB are isomorphisms. Hence (1) follows if
Rn is also an isomorphism. But A is invertible by (3) and one veriﬁes
1 = TA−
In Section 7.2 we deﬁned the rank of a linear transformation T : V
n matrix and TA : Rn

W by rank T = dim ( im T ).
Rm is the matrix transformation, we showed that

Moreover, if A is any m
rank (TA) = rank A. So it may not be surprising that rank T equals the rank of any matrix of T .

1TA. So TA is indeed invertible (and (TA)−

→

→

→

1).

×

1TACB

Theorem 9.1.5

Let T : V
orderedbasesofV andW,then rank T = rank [MDB(T )].

→

W bealineartransformationwhere dim V = n and dim W = m. If B and D areany

Proof. Write A = MDB(T ) for convenience. The column space of A is U =
. This means
rank A = dim U and so, because rank T = dim ( im T ), it sufﬁces to ﬁnd an isomorphism S : im T
U .
Now every vector in im T has the form T (v), v in V . By Theorem 9.1.2, CD[T (v)] = ACB(v) lies in U . So
deﬁne S : im T

U by

Ax

→

{

}

|

x in Rn

→

S[T (v)] = CD[T (v)] for all vectors T (v)

im T

∈

The fact that CD is linear and one-to-one implies immediately that S is linear and one-to-one. To see that
S is onto, let Ax be any member of U , x in Rn. Then x = CB(v) for some v in V because CB is onto. Hence
Ax = ACB(v) = CD[T (v)] = S[T (v)], so S is onto. This means that S is an isomorphism.

9.1. The Matrix of a Linear Transformation

501

Example 9.1.6

Deﬁne T : P2
rank T .

→

R3 by T (a + bx + cx2) = (a

2b, 3c

−

−

2a, 3c

−

4b) for a, b, c

∈

R. Compute

Solution. Since rank T = rank [MDB(T )] for any bases B
convenient ones: B =
MDB(T ) =

CD[T (1)] CD[T (x)] CD[T (x2)]

and D =

1, x, x2

= A where

{

{

}

⊆
⊆
(1, 0, 0), (0, 1, 0), (0, 0, 1)

P2 and D

R3, we choose the most
. Then

}

(cid:2)

A =



1
2
−
0



−

−

2 0
0 3
4 3 


. Since A

(cid:3)

→ 

1
0
0



we have rank A = 2. Hence rank T = 2 as well.

−
−
−

2 0
4 3
4 3 


→ 



1
0
0

2
−
1
0

0
3
4
0

−





We conclude with an example showing that the matrix of a linear transformation can be made very

simple by a careful choice of the two bases.

Example 9.1.7

Let T : V
→
basis B =
{
empty. Then
basis D =

W be a linear transformation where dim V = n and dim W = m. Choose an ordered
b1, . . . , br, br+1, . . . , bn
is a basis of ker T , possibly
T (b1), . . . , T (br)
is a basis of im T by Theorem 7.2.5, so extend it to an ordered
= T (bn) = 0, we have

T (b1), . . . , T (br), fr+1, . . . , fm

of W . Because T (br+1) =

br+1, . . . , bn

of V in which

{

}

}

{

}

}

· · ·

{

MDB(T ) =

CD[T (b1)]

CD[T (br)] CD[T (br+1)]

· · ·

· · ·

Incidentally, this shows that rank T = r by Theorem 9.1.5.

(cid:2)

CD[T (bn)]

=

(cid:20)

(cid:3)

Ir 0
0 0

(cid:21)

Exercises for 9.1

Exercise 9.1.1 In each case, ﬁnd the coordinates of v
with respect to the basis B of the vector space V .

a. V = P2, v = 2x2 + x

x + 1, x2, 3
}
{
b. V = P2, v = ax2 + bx + c, B =

1, B =

−

x2, x + 1, x + 2
}
{

1, 2),

−

c. V = R3, v = (1,
(1,
{

B =

−
d. V = R3, v = (a, b, c),
1, 2), (1, 1,

B =

(1,
{

−

1, 0), (1, 1, 1), (0, 1, 1)
}

1), (0, 0, 1)
}

−

Exercise 9.1.2 Suppose T : P2
formation. If B =
ﬁnd the action of T given:

1, x, x2
{

}

→
and D =

R2 is a linear trans-
,
(1, 1), (0, 1)
}
{

a. MDB(T ) =

b. MDB(T ) =

1 2
1 0

−

2 1
1 0

−

(cid:20)

(cid:20)

1
1

3
2

−

−

(cid:21)

(cid:21)

e. V = M22, v =

1 1
0 0

(cid:20)

−

,

(cid:21)

(cid:20)

B =

(cid:26)(cid:20)

1 2
1 0
1 0
1 0

,

(cid:21)

,

(cid:21)

(cid:20)

0 0
1 1

1 0
0 1

,

(cid:21)

(cid:20)

Exercise 9.1.3 In each case, ﬁnd the matrix of the linear
transformation T : V
W corresponding to the bases B
and D of V and W , respectively.

→

(cid:21)(cid:27)

502

Change of Basis

a. T : M22

B =

→
1 0
0 0
(cid:26)(cid:20)
D =
1
}
{
b. T : M22
B = D

→
1 0
0 0

=

(cid:26)(cid:20)
c. T : P2
D =

R, T (A) = tr A;
0 1
0 0

,

,

(cid:21)

(cid:20)

(cid:21)

(cid:20)

0 0
1 0

,

(cid:21)

(cid:20)

0 0
0 1

,
(cid:21)(cid:27)

M22, T (A) = AT ;

,

(cid:21)

(cid:20)

0 1
0 0

0 0
1 0

,

(cid:21)

(cid:20)

P3, T [p(x)] = xp(x); B =

,

0 0
0 1

(cid:21)
(cid:20)
1, x, x2
{

}

(cid:21)(cid:27)
and

→
1, x, x2, x3
{
d. T : P2

}

→
B = D =

P2, T [p(x)] = p(x + 1);
1, x, x2
{

}

Exercise 9.1.4 In each case, ﬁnd the matrix of
T : V
W corresponding to the bases B and D, respec-
tively, and use it to compute CD[T (v)], and hence T (v).

→

a. T : R3

R4, T (x, y, z) = (x+z, 2z, y

→

B and D standard; v = (1,

1, 3)

z, x+2y);

−

−
R4, T (x, y) = (2x

y, 3x + 2y, 4y, x);

−

b. T : R2
B =

→

, D standard; v = (a, b)
(1, 1), (1, 0)
}
{
c. T : P2
→
1, x, x2
B =
{
v = a + bx + cx2

R2, T (a + bx + cx2) = (a + c, 2b);
(1, 0), (1,
{

, D =
}

;
1)
}

−

d. T : P2
→
1, x, x2
B =
{
v = a + bx + cx2

R2, T (a + bx + cx2) = (a + b, c);

, D =
}

(1,
{

;
1), (1, 1)
}

−

e. T : M22

= a + b + c + d;

R, T

,

→
1 0
0 0

B =

(cid:26)(cid:20)
D =

(cid:21)
; v =
1
}
{

a b
c d

(cid:20)
0 1
0 0
a b
c d

(cid:20)

(cid:21)
,

(cid:21)

(cid:20)

(cid:21)

(cid:20)
M22,

f. T : M22

→

(cid:20)

T

a b
c d
B = D =
1 0
0 0

=

(cid:21)

,

(cid:26)(cid:20)
v =

(cid:21)
a b
c d

(cid:20)

(cid:21)

(cid:20)

a
b + c

b + c
d

(cid:20)
0 1
0 0

(cid:21)
0 0
1 0

,

(cid:21)

(cid:20)

,

(cid:21)

(cid:20)

0 0
0 1

;
(cid:21)(cid:27)

Exercise 9.1.5 In each case, verify Theorem 9.1.3. Use
the standard basis in Rn and

in P2.

1, x, x2
{

}

a. R3 T
→

R2 S
→

S(a, b) = (a, b

R4; T (a, b, c) = (a + b, b

2a, 3b, a + b)

c),

−

b. R3 T
→

R4 S
→

−
R2;

T (a, b, c) = (a + b, c + b, a + c, b
S(a, b, c, d) = (a + b, c

d)

a),

−

−
P2; T (a+bx+cx2) = (a, b

c. P2

T
→

R3 S
→

S(a, b, c) = b + cx + (a

c)x2

−

d. R3 T
→

S
R2;
P2
→
T (a, b, c) = (a
S(a + bx + cx2) = (a

−

b) + (c

a)x + bx2,

−
b, c)

−

c, c

a),

−

−

Exercise 9.1.6 Verify Theorem 9.1.3 for
S
P2 where T (A) = AT and
M22
→
= b + (a + d)x + cx2. Use the bases

M22

S

T
→
a b
c d

,

0 1
0 0

,

0 0
1 0

0 0
0 1

,

(cid:21)

(cid:20)

(cid:21)(cid:27)

(cid:20)

B = D =

(cid:21)

1 0
0 0
(cid:26)(cid:20)
1, x, x2
{

(cid:20)

and E =

(cid:21)
.
}
Exercise 9.1.7 In each case, ﬁnd T −
[MDB(T )]−

1 = MBD(T −

1).

(cid:21)

(cid:20)

1 and verify that

a. T : R2

R2, T (a, b) = (a + 2b, 2a + 5b);

→
B = D = standard

b. T : R3

R3, T (a, b, c) = (b + c, a + c, a + b);

→
B = D = standard

R3, T (a + bx + cx2) = (a
, D = standard
}

c, b, 2a

c);

−

−

c. T : P2
B =

→
1, x, x2
{
d. T : P2

R3,

→

0 0
1 0

,

(cid:21)

(cid:20)

0 0
0 1

,
(cid:21)(cid:27)

T (a + bx + cx2) = (a + b + c, b + c, c);
B =

, D = standard
}

1, x, x2
{

Exercise 9.1.8 In each case, show that MDB(T ) is in-
1 to
vertible and use the fact that MBD(T −
determine the action of T −

1) = [MBD(T )]−

1.

;

R3, T (a + bx + cx2) = (a + c, c, b

c);

−

a. T : P2
B =

→
1, x, x2
{

b. T : M22

, D = standard
}
R4,

T

(cid:20)

B =

→

a b
c d
1 0
0 0

(cid:21)

,

(cid:26)(cid:20)

(cid:21)
D = standard

= (a + b + c, b + c, c, d);

0 1
0 0

,

(cid:21)

(cid:20)

0 0
1 0

,

(cid:21)

(cid:20)

0 0
0 1

,
(cid:21)(cid:27)

(cid:20)

P2 be the differentiation
Exercise 9.1.9 Let D : P3
map given by D[p(x)] = p′(x). Find the matrix of D cor-
responding to the bases B =
E =
D(a + bx + cx2 + dx3).

, and use it to compute
}

1, x, x2, x3
{

1, x, x2
{

and

→

}

Exercise 9.1.10 Use Theorem 9.1.4 to show that
T : V
V is not an isomorphism if ker T
= 0 (assume
dim V = n). [Hint: Choose any ordered basis B contain-
ing a vector in ker T .]

→

Exercise 9.1.11 Let T : V
tion, and let D =
basis B =
MDB(T ) = [T (e1)

1
{
}
e1, . . . , en
{

}
T (en)].

· · ·

R be a linear transforma-
be the basis of R. Given any ordered

→

of V , show that

Exercise 9.1.12 Let T : V
W be an isomorphism,
e1, . . . , en
be an ordered basis of V , and let
let B =
{
T (e1), . . . , T (en)
D =
. Show that MDB(T ) = In—the
{
}
n identity matrix.
n

→

}

×

Exercise 9.1.13 Complete the proof of Theorem 9.1.1.

Exercise 9.1.14 Let U be any invertible n
let D =
that MBD(1Rn ) = U when B is the standard basis of Rn.

n matrix, and
where f j is column j of U . Show

f1, f2, . . . , fn
{

×

}

Exercise 9.1.15 Let B be an ordered basis of the n-
Rn be the coor-
dimensional space V and let CB : V
dinate transformation. If D is the standard basis of Rn,
show that MDB(CB) = In.

→

Exercise 9.1.16 Let T : P2
T (p) = (p(0), p(1), p(2)) for all p in P2. Let
B =

and D =

R3 be deﬁned by

→

.
(1, 0, 0), (0, 1, 0), (0, 0, 1)
}
{

1, x, x2
{

}

a. Show that MDB(T ) =

1 0 0
1 1 1
1 2 4



that T is an isomorphism.



and conclude





Rn+1 where

b. Generalize to T : Pn
T (p) = (p(a0),
a0, a1, . . . , an are distinct real numbers.
[Hint: Theorem 3.2.7.]

→
p(a1),

. . . ,

p(an))

Let T : Pn

Exercise 9.1.17
Pn be deﬁned by
T [p(x)] = p(x) + xp′(x), where p′(x) denotes the deriva-
tive. Show that T is an isomorphism by ﬁnding MBB(T )
when B =
.
}

1, x, x2, . . . , xn
{

→

Exercise 9.1.18 If k is any number, deﬁne
Tk : M22

M22 by Tk(A) = A + kAT .

→

9.1. The Matrix of a Linear Transformation

503

,

a. If B =
1 0
0 0
(cid:21)(cid:27)
MBB(Tk), and conclude that Tk is invertible if k
and k

0 1
1 0

0 0
0 1

0 1
1 0

−

1.

(cid:20)

(cid:21)

(cid:21)

(cid:21)

(cid:20)

(cid:20)

,

,

=

(cid:26)(cid:20)

ﬁnd

= 1

−
b. Repeat for Tk : M33

M33. Can you generalize?

→

The remaining exercises require the following
deﬁnitions. If V and W are vector spaces, the set
of all linear transformations from V to W will be
denoted by

L(V , W ) =

T : V

T

{

|

→

W is a linear transformation

}

Given S and T in L(V , W ) and a in R, deﬁne
S + T : V

W and aT : V

W by

→

→
(S + T )(v) = S(v) + T (v)

(aT )(v) = aT (v)

for all v in V
for all v in V

Exercise 9.1.19 Show that L(V , W ) is a vector space.

Exercise 9.1.20 Show that the following properties hold
provided that the transformations link together in such a
way that all the operations are deﬁned.

a. R(ST ) = (RS)T

b. 1W T = T = T 1V

c. R(S + T ) = RS + RT

d. (S + T )R = SR + T R

e. (aS)T = a(ST ) = S(aT )

Exercise 9.1.21 Given S and T in L(V , W ), show that:

a. ker S

ker T

∩
im (S + T )

ker (S + T )

⊆
im S + im T

⊆

b.

and

Exercise 9.1.22 Let V and W be vector spaces. If X is a
subset of V , deﬁne

X 0 =

T in L(V , W )
{

|

T (v) = 0 for all v in X

}

a. Show that X 0 is a subspace of L(V , W ).

b. If X

⊆

X1, show that X 0

1 ⊆

X 0.

c. If U and U1 are subspaces of V , show that
U 0
1 .

(U +U1)0 = U 0

∩

6
6
6
504

Change of Basis

L(Rn, Rm) by
Exercise 9.1.23 Deﬁne R : Mmn
Rm
R(A) = TA for each m
is given by TA(x) = Ax for all x in Rn. Show that R is an
isomorphism.

n matrix A, where TA : Rn

→

→

×

If V is a vector space,

Exercise 9.1.27
the space
V ∗ = L(V , R) is called the dual of V . Given a basis
R for each
B =
i = 1, 2, . . . , n be the linear transformation satisfying

b1, b2, . . . , bn
{

of V , let Ei : V

→

}

Exercise 9.1.24 Let V be any vector space (we do not
assume it is ﬁnite dimensional). Given v in V , deﬁne
Sv : R

V by Sv(r) = rv for all r in R.

→

Ei(b j) =

0
1

if i
= j
if i = j

(cid:26)

a. Show that Sv lies in L(R, V ) for each v in V .

(each Ei exists by Theorem 7.1.3). Prove the following:

b. Show that the map R : V

L(R, V ) given by
R(v) = Sv is an isomorphism. [Hint: To show that
R is onto, if T lies in L(R, V ), show that T = Sv
where v = T (1).]

→

Exercise 9.1.25 Let V be a vector space with ordered
b1, b2, . . . , bn
basis B =
. For each i = 1, 2, . . . , m,
}
{
deﬁne Si : R

V by Si(r) = rbi for all r in R.

→

a. Show that each Si lies in L(R, V ) and Si(1) = bi.

a. Ei(r1b1 +

· · ·

+ rnbn) = ri for each i = 1, 2, . . . , n

b. v = E1(v)b1 + E2(v)b2 +

V

+ En(v)bn for all v in

· · ·

c. T = T (b1)E1 + T (b2)E2 +

in V ∗

+ T (bn)En for all T

· · ·

d.

E1, E2, . . . , En
{
basis of B).

}

is a basis of V ∗ (called the dual

b. Given T in L(R, V ), let
T (1) = a1b1 + a2b2 +
that T = a1S1 + a2S2 +

· · ·
· · ·
S1, S2, . . . , Sn
{

c. Show that

+ anbn, ai in R. Show
+ anSn.

Given v in V , deﬁne v∗ : V
v∗(w) = E1(v)E1(w) + E2(v)E2(w) +
En(v)En(w) for all w in V . Show that:

R by

→

+

· · ·

is a basis of L(R, V ).

}

e. v∗ : V

→

R is linear, so v∗ lies in V ∗.

Exercise 9.1.26 Let dim V = n, dim W = m, and let
B and D be ordered bases of V and W , respectively.
Mmn is an isomorphism
Show that MDB : L(V , W )
b1,
[Hint: Let B =
and
of vector spaces.
{
. Given A = [ai j] in Mmn, show that
d1, . . . , dm
D =
}
{
W is deﬁned by
A = MDB(T ) where T : V
+ am jdm for each j.]
T (b j) = a1 jd1 + a2 jd2 +

. . . , bn

→

}

→
· · ·

f. b∗i = Ei for each i = 1, 2, . . . , n.

g. The map R : V

→

V ∗ with R(v) = v∗ is an isomor-
phism. [Hint: Show that R is linear and one-to-
one and use Theorem 7.3.3. Alternatively, show
that R−

1(T ) = T (b1)b1 +

+ T (bn)bn.]

· · ·

9.2 Operators and Similarity

While the study of linear transformations from one vector space to another is important, the central prob-
lem of linear algebra is to understand the structure of a linear transformation T : V
V from a space
V to itself. Such transformations are called linear operators. If T : V
V is a linear operator where
dim (V ) = n, it is possible to choose bases B and D of V such that the matrix MDB(T ) has a very simple

→

→

Ir 0
0 0

(cid:20)

(cid:21)

form: MDB(T ) =

where r = rank T (see Example 9.1.7). Consequently, only the rank of T

is revealed by determining the simplest matrices MDB(T ) of T where the bases B and D can be chosen
arbitrarily. But if we insist that B = D and look for bases B such that MBB(T ) is as simple as possible, we
learn a great deal about the operator T . We begin this task in this section.

6
The B-matrix of an Operator

Deﬁnition 9.3 Matrix MDB(T ) of T : V

W for basis B

→

9.2. Operators and Similarity

505

If T : V
MB(T ) = MBB(T ) andcallthisthe BBB-matrixof T.

→

V isanoperatoronavectorspaceV,andif B isanorderedbasisofV,deﬁne

Recall that if T : Rn

Rn, then CE (x) = x for every x
Theorem 2.6.2. Hence ME(T ) will be called the standard matrix of the operator T .

Rn is a linear operator and E =

e1, e2, . . . , en
is the standard basis of
Rn, so ME(T ) = [T (e1), T (e2), . . . , T (en)] is the matrix obtained in

→

∈

{

}

For reference the following theorem collects some results from Theorem 9.1.2, Theorem 9.1.3, and
Theorem 9.1.4, specialized for operators. As before, CB(v) denoted the coordinate vector of v with respect
to the basis B.

Theorem 9.2.1

Let T : V

→

V beanoperatorwhere dim V = n,andlet B beanorderedbasisofV.

1. CB(T (v)) = MB(T )CB(v) forallvinV.

2. If S : V

→

V isanotheroperatoronV,then MB(ST ) = MB(S)MB(T ).

3. T isanisomorphismifandonlyif MB(T ) isinvertible. Inthiscase MD(T ) isinvertiblefor

everyorderedbasis D ofV.

4. If T isanisomorphism,then MB(T −

1) = [MB(T )]−

1.

5. If B =

b1, b2, . . . , bn
{

,then MB(T ) =
}

CB[T (b1)] CB[T (b2)]

CB[T (bn)]

.

· · ·

(cid:2)

(cid:3)

For a ﬁxed operator T on a vector space V , we are going to study how the matrix MB(T ) changes when
the basis B changes. This turns out to be closely related to how the coordinates CB(v) change for a vector
v in V . If B and D are two ordered bases of V , and if we take T = 1V in Theorem 9.1.2, we obtain

CD(v) = MDB(1V )CB(v)

for all v in V

Deﬁnition 9.4 Change Matrix PD

B for bases B and D

←

Withthisinmind,deﬁnethechangematrix PD

B by

←

PD

←

B = MDB(1V )

foranyorderedbases B and D ofV

506

Change of Basis

This proves equation 9.2 in the following theorem:

Theorem 9.2.2

Let B =
PD

←

b1, b2, . . . , bn
{

}

B isgivenintermsofitscolumnsby

and D denoteorderedbasesofavectorspaceV. Thenthechangematrix

andhasthepropertythat

PD

←

B =

CD(b1) CD(b2)

CD(bn)

· · ·

(cid:2)
CD(v) = PD

←

BCB(v) forallvinV

(cid:3)

(9.1)

(9.2)

Moreover,if E isanotherorderedbasisofV,wehave

1. PB

←

B = In

2. PD

←

B isinvertibleand (PD

B)−

1 = PB

D

←

←

3. PE

DPD

←

←

B = PE

B

←

Proof. The formula 9.2 is derived above, and 9.1 is immediate from the deﬁnition of PD
for MDB(T ) in Theorem 9.1.2.

←

B and the formula

1. PB

←

B = MBB(1V ) = In as is easily veriﬁed.

2. This follows from (1) and (3).

3. Let V T
→

W S
→

U be operators, and let B, D, and E be ordered bases of V , W , and U respectively.

We have MEB(ST ) = MED(S)MDB(T ) by Theorem 9.1.3. Now (3) is the result of specializing
V = W = U and T = S = 1V .

Property (3) in Theorem 9.2.2 explains the notation PD

B.

←

Example 9.2.1

1, x, x2
In P2 ﬁnd PD
x), (1
p = p(x) = a + bx + cx2 as a polynomial in powers of (1

and D =

B if B =

1, (1

−

←

{

}

{

x)2
−
x).

−

. Then use this to express

}

Solution. To compute the change matrix PD

B, express 1, x, x2 in the basis D:

←

1 = 1 + 0(1

x = 1
x2 = 1

1(1

2(1

−

−

−

−

−

x) + 0(1

x) + 0(1

x) + 1(1

x)2
x)2
x)2

−

−

−

9.2. Operators and Similarity

507

Hence PD

←

B =

CD(1), CD(x), CD(x)2

=

(cid:2)

(cid:3)

CD(p) = PD

BCB(p) =

←






1
0
0

Hence p(x) = (a + b + c)

(b + 2c)(1

−


x) + c(1

−

−

1
0
0

1
1
−
0

1
2
1 
−


. We have CB(p) =

1
1
−
0

=

1
2
1 
−


a
b
c 

x)2 by Deﬁnition 9.1.1

b
−









a + b + c
2c

−
c

, so

a
b
c 










Now let B =

V
has different matrices MB[T ] and MB0[T ] with respect to B and B0. We can now determine how these
matrices are related. Theorem 9.2.2 asserts that

and B0 be two ordered bases of a vector space V . An operator T : V

b1, b2, . . . , bn

→

}

{

On the other hand, Theorem 9.2.1 gives

CB0(v) = PB0

←

BCB(v) for all v in V

CB[T (v)] = MB(T )CB(v) for all v in V

Combining these (and writing P = PB0

B for convenience) gives

←

PMB(T )CB(v) = PCB[T (v)]
= CB0[T (v)]
= MB0(T )CB0(v)
= MB0(T )PCB(v)

This holds for all v in V . Because CB(b j) is the jth column of the identity matrix, it follows that

PMB(T ) = MB0(T )P

Moreover P is invertible (in fact, P−

1 = PB

B0 by Theorem 9.2.2), so this gives

←

MB(T ) = P−

1MB0(T )P

This asserts that MB0(T ) and MB(T ) are similar matrices, and proves Theorem 9.2.3.

Theorem 9.2.3: Similarity Theorem

Let B0 and B betwoorderedbasesofaﬁnitedimensionalvectorspaceV. If T : V
linearoperator,thematrices MB(T ) and MB0(T ) of T withrespecttothesebasesaresimilar. More
precisely,

V isany

→

MB(T ) = P−

1MB0(T )P

where P = PB0

←

B isthechangematrixfrom B to B0.

1This also follows from Taylor’s theorem (Corollary 6.5.3 of Theorem 6.5.1 with a = 1).

508

Change of Basis

Example 9.2.2

Let T : R3
basis of R3 and B =
P−

→
1MB0(T )P = MB(T ).

{

R3 be deﬁned by T (a, b, c) = (2a

−
(1, 1, 0), (1, 0, 1), (0, 1, 0)

b, b + c, c

3a). If B0 denotes the standard

, ﬁnd an invertible matrix P such that

−

}

Solution. We have

MB0(T ) =

CB0(2, 0,

3) CB0(

−

1, 1, 0) CB0(0, 1, 1)

−

=

(cid:2)

MB(T ) =

CB(1, 1,

3) CB(2, 1,

−

2) CB(

−

1, 1, 0)

−

(cid:2)

P = PB0

←

B =

CB0(1, 1, 0) CB0(1, 0, 1) CB0(0, 1, 0)

(cid:2)

(cid:3)

(cid:3)

=

(cid:3)





2
0
3
−
4
3
−
3
−

−

1 0
1 1
0 1 

1
−
0
2 


4
2
−
3
−
1 1 0
1 0 1
0 1 0 









=

The reader can verify that P−

1MB0(T )P = MB(T ); equivalently that MB0(T )P = PMB(T ).

A square matrix is diagonalizable if and only if it is similar to a diagonal matrix. Theorem 9.2.3 comes
into this as follows: Suppose an n
V with
respect to an ordered basis B0. If another ordered basis B of V can be found such that MB(T ) = D is
1AP = D. In other words, the
diagonal, then Theorem 9.2.3 shows how to ﬁnd an invertible P such that P−
1AP is diagonal comes down to the “geometric” problem
“algebraic” problem of ﬁnding P such that P−
of ﬁnding a basis B such that MB(T ) is diagonal. This shift of emphasis is one of the most important
techniques in linear algebra.

n matrix A = MB0(T ) is the matrix of some operator T : V

→

×

Each n

×

n matrix A can be easily realized as the matrix of an operator. In fact, (Example 9.1.4),

ME(TA) = A

→

where TA : Rn
Rn is the matrix operator given by TA(x) = Ax, and E is the standard basis of Rn. The
ﬁrst part of the next theorem gives the converse of Theorem 9.2.3: Any pair of similar matrices can be
realized as the matrices of the same linear operator with respect to different bases. This is part 1 of the
following theorem.

Theorem 9.2.4

Let A bean n

×

n matrixandlet E bethestandardbasisof Rn.

1. Let A′ besimilarto A,say A′ = P−
columnsof P inorder. Then TA : Rn

Rn islinearand

→

1AP,andlet B betheorderedbasisof Rn consistingofthe

ME(TA) = A and MB(TA) = A′

2. If B isanyorderedbasisof Rn,let P bethe(invertible)matrixwhosecolumnsarethevectors

in B inorder. Then

MB(TA) = P−

1AP

Proof.

9.2. Operators and Similarity

509

1. We have ME(TA) = A by Example 9.1.4. Write P =

b1

bn

in terms of its columns so

B =

{

b1, . . . , bn

}

· · ·
is a basis of Rn. Since E is the standard basis,

PE

←

B =

CE (b1)

· · ·

CE(bn)

Hence Theorem 9.2.3 (with B0 = E) gives MB(TA) = P−

(cid:2)

(cid:2)
=

b1

(cid:3)
bn

= P

· · ·
(cid:3)
(cid:2)
1AP = A′.
1ME(TA)P = P−

(cid:3)

2. Here P and B are as above, so again PE

B = P and MB(TA) = P−

1AP.

←

Example 9.2.3

Given A =

10
18

6
11

, P =

1
−
2

2
3
−

, and D =

(cid:20)
use this fact to ﬁnd a basis B of R2 such that MB(TA) = D.

−

−

(cid:21)

(cid:20)

(cid:21)

(cid:20)

1
0

0
2
−

, verify that P−
(cid:21)

1AP = D and

Solution. P−

1AP = D holds if AP = PD; this veriﬁcation is left to the reader. Let B consist of the

columns of P in order, that is B =

2
3
−
1AP = D. More explicitly,

(cid:26)(cid:20)

MB(TA) = P−

1
−
2

,

(cid:21)

(cid:20)

(cid:21)(cid:27)

. Then Theorem 9.2.4 gives

MB(TA) =

CB
(cid:20)

TA

(cid:18)

(cid:20)

2
3
−

CB

TA

(cid:21)(cid:19)

(cid:18)

(cid:20)

1
−
2

=

CB
(cid:20)

(cid:20)

(cid:21)(cid:19)(cid:21)

2
3
−

(cid:21)

CB

2
4
−

(cid:20)

=

(cid:21)(cid:21)

(cid:20)

1
0

0
2
−

(cid:21)

= D

Let A be an n
matrix P such that P−
MB(TA) = D is diagonal and take P =
by Theorem 9.2.4,

×

n matrix. As in Example 9.2.3, Theorem 9.2.4 provides a new way to ﬁnd an invertible
of Rn such that
to be the matrix with the b j as columns. Then,

1AP is diagonal. The idea is to ﬁnd a basis B =

b1, b2, . . . , bn

b1 b2

bn

{

}

· · ·

1AP = MB(TA) = D
As mentioned above, this converts the algebraic problem of diagonalizing A into the geometric problem of
ﬁnding the basis B. This new point of view is very powerful and will be explored in the next two sections.
Theorem 9.2.4 enables facts about matrices to be deduced from the corresponding properties of oper-

P−

(cid:3)

(cid:2)

ators. Here is an example.

Example 9.2.4

1. If T : V

V is an operator where V is ﬁnite dimensional, show that T ST = T for some

invertible operator S : V

→

V .

→

2. If A is an n

×

n matrix, show that AUA = A for some invertible matrix U .

Solution.

1. Let B =

ker T = span
complete it to a basis

b1, . . . , br, br+1, . . . , bn
}
br+1, . . . , bn
. Then

be a basis of V chosen so that
T (b1), . . . , T (br)
T (b1), . . . , T (br), fr+1, . . . , fn

}
of V .

}

{

{

{

{

}

is independent (Theorem 7.2.5), so

510

Change of Basis

By Theorem 7.1.3, deﬁne S : V

V by

→
S[T (bi)] = bi
S(f j) = b j

i
for 1
for r < j

≤

r
n

≤
≤

Then S is an isomorphism by Theorem 7.3.1, and T ST = T because these operators agree on
the basis B. In fact,

(T ST )(bi) = T [ST (bi)] = T (bi) if 1

i

r, and

≤

≤

(T ST )(b j) = T S[T (b j)] = T S(0) = 0 = T (b j) for r < j

n

≤

2. Given A, let T = TA : Rn

Rn is an isomorphism.
Rn. By (1) let T ST = T where S : Rn
If E is the standard basis of Rn, then A = ME(T ) by Theorem 9.2.4. If U = ME(S) then, by
Theorem 9.2.1, U is invertible and

→

→

AUA = ME(T )ME(S)ME(T ) = ME(T ST ) = ME(T ) = A

as required.

The reader will appreciate the power of these methods if he/she tries to ﬁnd U directly in part 2 of Exam-
ple 9.2.4, even if A is 2
A property of n

n matrix A has
the property, every matrix similar to A also has the property. Theorem 5.5.1 shows that rank , determinant,
trace, and characteristic polynomial are all similarity invariants.

n matrices is called a similarity invariant if, whenever a given n

×

×

×

2.

→

To illustrate how such similarity invariants are related to linear operators, consider the case of rank .
If T : V
V is a linear operator, the matrices of T with respect to various bases of V all have the same
rank (being similar), so it is natural to regard the common rank of all these matrices as a property of T
itself and not of the particular matrix used to describe T . Hence the rank of T could be deﬁned to be the
rank of A, where A is any matrix of T . This would be unambiguous because rank is a similarity invariant.
Of course, this is unnecessary in the case of rank because rank T was deﬁned earlier to be the dimension
of im T , and this was proved to equal the rank of every matrix representing T (Theorem 9.1.5). This
deﬁnition of rank T is said to be intrinsic because it makes no reference to the matrices representing T .
However, the technique serves to identify an intrinsic property of T with every similarity invariant, and
some of these properties are not so easily deﬁned directly.

In particular, if T : V
of T (denoted det T ) by

→

V is a linear operator on a ﬁnite dimensional space V , deﬁne the determinant

det T = det MB(T ),

B any basis of V

This is independent of the choice of basis B because, if D is any other basis of V , the matrices MB(T ) and
MD(T ) are similar and so have the same determinant. In the same way, the trace of T (denoted tr T ) can
be deﬁned by

tr T = tr MB(T ),

B any basis of V

This is unambiguous for the same reason.

Theorems about matrices can often be translated to theorems about linear operators. Here is an exam-

ple.

9.2. Operators and Similarity

511

Example 9.2.5

Let S and T denote linear operators on the ﬁnite dimensional space V . Show that

Solution. Choose a basis B of V and use Theorem 9.2.1.

det (ST ) = det S det T

det (ST ) = det MB(ST ) = det [MB(S)MB(T )]

= det [MB(S)] det [MB(T )] = det S det T

Recall next that the characteristic polynomial of a matrix is another similarity invariant: If A and A′ are
similar matrices, then cA(x) = cA′(x) (Theorem 5.5.1). As discussed above, the discovery of a similarity
invariant means the discovery of a property of linear operators. In this case, if T : V
V is a linear
operator on the ﬁnite dimensional space V , deﬁne the characteristic polynomial of T by

→

cT (x) = cA(x) where A = MB(T ), B any basis of V

In other words, the characteristic polynomial of an operator T is the characteristic polynomial of any
matrix representing T . This is unambiguous because any two such matrices are similar by Theorem 9.2.3.

Example 9.2.6

Compute the characteristic polynomial cT (x) of the operator T : P2
T (a + bx + cx2) = (b + c) + (a + c)x + (a + b)x2.

→

P2 given by

Solution. If B =

1, x, x2

{

}

, the corresponding matrix of T is

MB(T ) =

CB[T (1)] CB[T (x)] CB[T (x2)]

=

Hence cT (x) = det [xI

MB(T )] = x3

3x

−

−

−

2 = (x + 1)2(x

(cid:2)

(cid:3)

2).

−

0 1 1
1 0 1
1 1 0 






In Section 4.4 we computed the matrix of various projections, reﬂections, and rotations in R3. How-
ever, the methods available then were not adequate to ﬁnd the matrix of a rotation about a line through the
origin. We conclude this section with an example of how Theorem 9.2.3 can be used to compute such a
matrix.

Example 9.2.7

Let L be the line in R3 through the origin with (unit) direction vector d = 1
3
Compute the matrix of the rotation about L through an angle θ measured counterclockwise when
viewed in the direction of d.

2 1 2

(cid:2)

(cid:3)

T .

512

Change of Basis

L

d = R(d)

0

R(g)

θ

g

R(f)

θ

f

R(g)

g

0

θ

R(f)

f

θ

→

Solution. Let R : R3
R3 be the rotation. The idea is to ﬁrst ﬁnd
a basis B0 for which the matrix of MB0(R) of R is easy to compute,
and then use Theorem 9.2.3 to compute the “standard” matrix
ME(R) with respect to the standard basis E =
To construct the basis B0, let K denote the plane through the
origin with d as normal, shaded in the diagram. Then the vectors
f = 1
3
(they are orthogonal to d) and are independent (they are orthogonal
(cid:2)
(cid:2)
to each other).
is an orthonormal basis of R3, and the
Hence B0 =
effect of R on B0 is easy to determine. In fact R(d) = d and
(as in Theorem 2.6.4) the second diagram gives

T are both in K

T and g = 1
3

e1, e2, e3

2 2 1

of R3.

d, f, g

2
−

1 2

−

}

}

{

{

(cid:3)

(cid:3)

R(f) = cosθf + sinθg

and

R(g) =

sinθf + cosθg

−

because

f
k

k

= 1 =

g

k

k

. Hence

MB0(R) =

CB0(d) CB0(f) CB0(g)

=



(cid:2)

(cid:3)

Now Theorem 9.2.3 (with B = E) asserts that
ME(R) = P−

1MB0(R)P where

0

0
sinθ

1
0 cosθ
0 sinθ cosθ 
−




P = PB0

←

E =

CB0(e1) CB0(e2) CB0(e3)

= 1

3 

−

(cid:2)

2 1
2 2
1 2

2
1
2 


(cid:3)

−
1 = PT (P is orthogonal), the matrix of R



using the expansion theorem (Theorem 5.3.6). Since P−
with respect to E is

ME(R) = PT MB0(R)P

6 sinθ

2 cosθ + 2 4

3 sinθ

5 cosθ + 4
6 sinθ

2
−
3 sinθ

2 cosθ
4 cosθ + 4 2

−

−

−

8 cosθ + 1
6 sinθ

2 cosθ

−

−

−
6 sinθ

4 cosθ
2 cosθ + 2

−

−

5 cosθ + 4





= 1

9 



As a check one veriﬁes that this is the identity matrix when θ = 0, as it should.

Note that in Example 9.2.7 not much motivation was given to the choices of the (orthonormal) vectors
f and g in the basis B0, which is the key to the solution. However, if we begin with any basis containing
d the Gram-Schmidt algorithm will produce an orthogonal basis containing d, and the other two vectors
will automatically be in L⊥ = K.

Exercises for 9.2

9.2. Operators and Similarity

513

Exercise 9.2.1 In each case ﬁnd PD
←
are ordered bases of V . Then verify that
CD(v) = PD

BCB(v).

←

B, where B and D

B if B =

Exercise 9.2.6 Find PD
and
b2, b3, b1, b4
D =
. Change matrices arising when the
}
{
bases differ only in the order of the vectors are called
permutation matrices.

b1, b2, b3, b4
{

←

}

D =

a. V = R2, B =

(0,
,
1), (2, 1)
−
{
}
, v = (3,
(0, 1), (1, 1)
−
}
{
x, 1+ x, x2
b. V = P2, B =
, D =
}
{
v = 1 + x + x2

5)

Exercise 9.2.7 In each case, ﬁnd P = PB0
that P−

1MB0(T )P = MB(T ) for the given operator T .

B and verify

←

2, x + 3, x2
{

,
1
}

−

R3, T (a, b, c) = (2a
a. T : R3
→
B0 =
(1, 1, 0), (1, 0, 1), (0, 1, 0)
}
{
standard basis.

−

b, b + c, c

3a);
−
and B is the

c. V = M22,
1 0
0 0

B =

(cid:26)(cid:20)

D =

(cid:26)(cid:20)
v =

(cid:20)

,

(cid:20)
,

(cid:21)
1 1
0 0
3
1

(cid:21)
1
−
4

,

(cid:20)
,

0 0
0 1

(cid:21)
1 0
0 1

,

(cid:20)
,

(cid:21)

(cid:20)

(cid:21)

(cid:20)

0 0
1 0

,
(cid:21)(cid:27)

0 1
1 0

,
(cid:21)(cid:27)

0 1
0 0

(cid:21)
1 0
1 0

(cid:20)

(cid:21)

←

B, where
and

Exercise 9.2.2 In R3 ﬁnd PD
B =
D =

(1, 0, 0), (1, 1, 0), (1, 1, 1)
}
{
(1, 0, 1), (1, 0,
{

and CB(v) =

show that CD(v) = 1

and verify that CD(v) = PD

. If v = (a, b, c),
1), (0, 1, 0)
}
a
b

−
a + c
c
a
2b 
−
BCB(v).


b
−
c
c 
−

1, x, x2, x3
Exercise 9.2.3 In P3 ﬁnd PD
{
}
and D =
. Then express
}
p = a + bx + cx2 + dx3 as a polynomial in powers of
(1

B if B =
x)3

1, (1
{

←
x)2, (1

x), (1

2 

x).

−

−

−







←

,

−

Exercise 9.2.4 In each case verify that PD
verse of PB
and E are ordered bases of V .

D and that PE

B = PE

DPD

←

←

←

←

B is the in-
B, where B, D,

←

(1, 1, 1), (1,
{

2, 1), (1, 0,

−

,
1)
}

−

a. V = R3, B =

D = standard basis,
E =
(1, 1, 1), (1,
{
b. V = P2, B =
x,

−
1, x, x2
{
1 + x2
, E =
}

−

−

1

1, 0), (

1, 0, 1)
−
}
1 + x + x2,
, D =
{
}
x2, x, 1
}
{

Exercise 9.2.5 Use property (2) of Theorem 9.2.2, with
D the standard basis of Rn, to ﬁnd the inverse of:

a.

A =

1 1 0
1 0 1
0 1 1









b.

A =

1 2 1
2 3 0
1 0 2









−

b. T : P2

P2,

→

1, x, x2
{
c. T : M22

}
M22,

T (a + bx + cx2) = (a + b) + (b + c)x + (c + a)x2;
B0 =

x2, 1 + x, 2x + x2

and B =

1
{

−

}

T

(cid:20)

B0 =

→

a b
c d
1 0
0 0

(cid:26)(cid:20)
and

1 1
0 0

B =

(cid:26)(cid:20)

;

b + c
a + d
a + c b + d
0 1
0 0

,

(cid:21)
0 0
1 0

(cid:20)
0 0
1 1

(cid:20)
1 0
0 1

(cid:21)

,

(cid:21)

(cid:20)

=

(cid:21)

(cid:20)

,

(cid:21)

,

(cid:21)

(cid:20)

0 0
0 1

,
(cid:21)(cid:27)

,

(cid:21)

,

(cid:20)
0 1
1 1

(cid:21)

(cid:20)

(cid:21)(cid:27)

Exercise 9.2.8 In each case, verify that P−
ﬁnd a basis B of R2 such that MB(TA) = D.

1AP = D and

a. A =

b. A =

11
12

29
70

(cid:20)

(cid:20)

6
6

−
−

12
29

−
−

P =

(cid:21)

(cid:20)

P =

(cid:21)

(cid:20)

2 3
3 4

3 2
7 5

D =

(cid:21)

(cid:21)

D =

2 0
0 3

1
0

−

(cid:21)
0
1

(cid:21)

(cid:20)

(cid:20)

Exercise 9.2.9 In each case, compute the characteristic
polynomial cT (x).

a. T : R2

b. T : R2

→

→

R2, T (a, b) = (a

b, 2b

a)

−

−

R2, T (a, b) = (3a + 5b, 2a + 3b)

c. T : P2

P2,

→
T (a + bx + cx2)

= (a

−

2c) + (2a + b + c)x + (c

a)x2

−

d. T : P2

P2,

→
T (a + bx + cx2)
= (a + b

e. T : R3

→

2c) + (a

2b + c)x + (b

−

−
R3, T (a, b, c) = (b, c, a)

2a)x2

−

514

Change of Basis

f. T : M22

M22, T

→

a b
c d

=

(cid:21)

(cid:20)

a
a

−
−

c b
c b

d
d

−
−

(cid:21)

(cid:20)

Exercise 9.2.10
If V is ﬁnite dimensional, show that
a linear operator T on V has an inverse if and only if
det T

= 0.

Exercise 9.2.11 Let S and T be linear operators on V
where V is ﬁnite dimensional.

a. Show that tr (ST ) = tr (T S). [Hint: Lemma 5.5.1.]

b. [See Exercise 9.1.19.] For a in R, show that
tr (S + T ) = tr S + tr T , and tr (aT ) = a tr (T ).

Exercise 9.2.12 If A and B are n
n matrices, show that
they have the same null space if and only if A = U B for
some invertible matrix U . [Hint: Exercise 7.3.28.]

×

Exercise 9.2.13 If A and B are n
n matrices, show that
they have the same column space if and only if A = BU
for some invertible matrix U . [Hint: Exercise 7.3.28.]

×

Exercise 9.2.14 Let E =
ordered basis of Rn, written as columns. If
D =
PE

d1, . . . , dn
{
d1
D =

e1, . . . , en
{

dn

}

.

is any ordered basis, show that

be the standard

←

}
· · ·

Exercise 9.2.15
. . . , bn
Let B =
(cid:2)
(cid:3)
any ordered basis of Rn, written as columns.
bn
Q =
columns, show that QCB(v) = v for all v in Rn.

be
If
is the matrix with the bi as

b1, b2,
{

b1 b2

· · ·

}

(cid:2)

(cid:3)

Exercise 9.2.16 Given a complex number w, deﬁne
Tw : C

C by Tw(z) = wz for all z in C.

→

a. Show that Tw is a linear operator for each w in C,

viewing C as a real vector space.

b. If B is any ordered basis of C, deﬁne S : C

M22
by S(w) = MB(Tw) for all w in C. Show that S
is a one-to-one linear transformation with the ad-
ditional property that S(wv) = S(w)S(v) holds for
all w and v in C.

→

c. Taking B =

S(a + bi) =

show that

1, i
}
{
a
b

(cid:20)

−

b
a

(cid:21)

for all complex numbers

a + bi. This is called the regular representation
of the complex numbers as 2
2 matrices. If θ
is any angle, describe S(eiθ) geometrically. Show
that S(w) = S(w)T for all w in C; that is, that con-
jugation corresponds to transposition.

×

}

}

and

b1, b2, . . . , bn
{

Exercise 9.2.17 Let B =
d1, d2, . . . , dn
D =
be two ordered bases of a vec-
{
tor space V . Prove that CD(v) = PD
BCB(v) holds for
all v in V as follows: Express each b j in the form
+ pn jdn and write P = [pi j].
b j = p1 jd1 + p2 jd2 +
CD(b1) CD(b1)
Show that P =
and
that CD(v) = PCB(v) for all v in B.
(cid:2)

CD(b1)

· · ·

· · ·

←

(cid:3)

Exercise 9.2.18 Find the standard matrix of the rotation
R about the line through the origin with direction vector
T
d =
. [Hint: Consider f =
and g =
(cid:2)

2 3 6

6 2

−

.]

3

3

(cid:2)

(cid:3)

T

T

6 2
(cid:3)
−

(cid:2)
9.3 Invariant Subspaces and Direct Sums

(cid:3)

A fundamental question in linear algebra is the following: If T : V
V is a linear operator, how can a
basis B of V be chosen so the matrix MB(T ) is as simple as possible? A basic technique for answering
such questions will be explained in this section. If U is a subspace of V , write its image under T as

→

T (U ) =

T (u)

{

|

u in U

}

6
9.3. Invariant Subspaces and Direct Sums

515

Deﬁnition 9.5 T -invariant Subspace

V beanoperator. AsubspaceU

V iscalled

Let T : V
→
TTT -invariantif T (U )
u

U,thatis, T (u)
U. Hence T isalinearoperatoronthevectorspaceU.

U foreveryvector

⊆

∈

⊆

∈

V

U

T

T

V

U

This is illustrated in the diagram, and the fact that T : U

U is an op-
erator on U is the primary reason for our interest in T -invariant subspaces.

→

Example 9.3.1

Let T : V

→

V be any linear operator. Then:

1.

0

}

{

and V are T -invariant subspaces.

2. Both ker T and im T = T (V ) are T -invariant subspaces.

3. If U and W are T -invariant subspaces, so are T (U ), U

W , and U +W .

∩

Solution. Item 1 is clear, and the rest is left as Exercises 9.3.1 and 9.3.2.

Example 9.3.2

Deﬁne T : R3
(a, b, a)
U =

R3 by T (a, b, c) = (3a + 2b, b
−
a, b in R
is T -invariant because

}

c, 4a + 2b

c). Then

−

→
|

{

−
is in U for all a and b (the ﬁrst and last entries are equal).

T (a, b, a) = (3a + 2b, b

a, 3a + 2b)

If a spanning set for a subspace U is known, it is easy to check whether U is T -invariant.

Example 9.3.3

Let T : V
V . Show that U is T -invariant if and only if T (ui) lies in U for each i = 1, 2, . . . , k.

V be a linear operator, and suppose that U = span

u1, u2, . . . , uk

→

{

}

is a subspace of

Solution. Given u in U , write it as u = r1u1 +

+ rkuk, ri in R. Then

· · ·

T (u) = r1T (u1) +

+ rkT (uk)

· · ·

and this lies in U if each T (ui) lies in U . This shows that U is T -invariant if each T (ui) lies in U ;
the converse is clear.

516

Change of Basis

Example 9.3.4

Deﬁne T : R2
0 and R2.

→

R2 by T (a, b) = (b,

a). Show that R2 contains no T -invariant subspace except

−

= R2. Then U has dimension 1
= 0. Now T (x) lies in U —say T (x) = rx, r in R. If we write x = (a, b), this is

Solution. Suppose, if possible, that U is T -invariant, but U
so U = Rx where x
a = rb. Eliminating b gives r2a = rb =
(b,
(r2 + 1)a = 0. Hence a = 0. Then b = ra = 0 too, contrary to the assumption that x
one-dimensional T -invariant subspace exists.

a) = r(a, b), which gives b = ra and

−
= 0. Hence no

= 0, U

a, so

−

−

Deﬁnition 9.6 Restriction of an Operator

Let T : V

→

V bealinearoperator. IfU isany T-invariantsubspaceofV,then

isalinearoperatoronthesubspaceU,calledtherestrictionof T toU.

T : U

U

→

This is the reason for the importance of T -invariant subspaces and is the ﬁrst step toward ﬁnding a basis
that simpliﬁes the matrix of T .

Theorem 9.3.1

V bealinearoperatorwhereV hasdimension n andsupposethatU isany T-invariant

Let T : V
→
subspaceofV. Let B1 =
B =

b1, . . . , bk, bk+1, . . . , bn
{

}

b1, . . . , bk
{

}

beanybasisofU andextendittoabasis

ofV inanyway. Then MB(T ) hastheblocktriangularform

MB(T ) =

MB1(T ) Y
Z

0

(cid:20)

(cid:21)

where Z is (n

k)

−

×

(n

−

k) and MB1(T ) isthematrixoftherestrictionof T toU.

Proof. The matrix of (the restriction) T : U

U with respect to the basis B1 is the k

k matrix

×

→

MB1(T ) =

CB1[T (b1)] CB1[T (b2)]

CB1[T (bk)]

· · ·

(cid:2)
Now compare the ﬁrst column CB1[T (b1)] here with the ﬁrst column CB[T (b1)] of MB(T ). The fact that
T (b1) lies in U (because U is T -invariant) means that T (b1) has the form

(cid:3)

T (b1) = t1b1 + t2b2 +

+ tkbk + 0bk+1 +

+ 0bn

· · ·

· · ·

6
6
6
6
9.3. Invariant Subspaces and Direct Sums

517

Consequently,

CB1[T (b1)] = 





t1
t2
...
tk








in Rk whereas CB[T (b1)] =

in Rn

t1
t2
...
tk
0
...
0



























This shows that the matrices MB(T ) and

have identical ﬁrst columns.

(cid:20)
Similar statements apply to columns 2, 3, . . . , k, and this proves the theorem.

(cid:21)

MB1(T ) Y
Z

0

The block upper triangular form for the matrix MB(T ) in Theorem 9.3.1 is very useful because the
determinant of such a matrix equals the product of the determinants of each of the diagonal blocks. This
is recorded in Theorem 9.3.2 for reference, together with an important application to characteristic poly-
nomials.

Theorem 9.3.2

Let A beablockuppertriangularmatrix,say

A11 A12 A13
A22 A23
0
A33
0
0
...
...
...
0
0
0

· · ·
· · ·
· · ·

· · ·

A1n
A2n
A3n
...
Ann










A =










wherethediagonalblocksaresquare. Then:

1. det A = ( det A11)( det A22)( det A33)

( det Ann).

2. cA(x) = cA11(x)cA22(x)cA33(x)

· · ·
cAnn(x).

· · ·

Proof. If n = 2, (1) is Theorem 3.1.5; the general case (by induction on n) is left to the reader. Then (2)
follows from (1) because

A =

xI

−

xI










A11
−
0
0
...
0

xI

A12

−
A22
−
0
...
0

A13
A23
A33

xI

−
−
−
...
0

· · · −
· · · −
· · · −

A1n
A2n
A3n
...

Ann

xI

−

· · ·










where, in each diagonal block, the symbol I stands for the identity matrix of the appropriate size.

518

Change of Basis

Example 9.3.5

Consider the linear operator T : P2

P2 given by

→

T (a + bx + cx2) = (

2a

b + 2c) + (a + b)x + (

6a

2b + 5c)x2

−

−

−
is T -invariant, use it to ﬁnd a block upper triangular matrix for T ,

−

x, 1 + 2x2
Show that U = span
and use that to compute cT (x).

{

}

Solution. U is T -invariant by Example 9.3.3 because U = span
T (1 + 2x2) lie in U :

{

x, 1 + 2x2

}

and both T (x) and

T (x) =

(1 + 2x2)
−
T (1 + 2x2) = 2 + x + 4x2 = x + 2(1 + 2x2)

2x2 = x

1 + x

−

−

Extend the basis B1 =
B =

x, 1 + 2x2, x2

{
. Then

x, 1 + 2x2

{

}

of U to a basis B of P2 in any way at all—say,

}

CB[T (x)] CB[T (1 + 2x2)] CB[T (x2)]
CB(

2x2) CB(2 + x + 4x2) CB(2 + 5x2)

(cid:3)

(cid:3)

MB(T ) =
=

=

(cid:2)

(cid:2)



−

1 + x
−
1 1 0
1 2 2
0 0 1 


−



is in block upper triangular form as expected. Finally,

cT (x) = det

1

x

−
1
0

1
−
x
2
−
0





0
2
−
1 
x
−


= (x2

−

3x + 3)(x

1)

−

Eigenvalues

V be a linear operator. A one-dimensional subspace Rv, v

= 0, is T -invariant if and only if
Let T : V
T (rv) = rT (v) lies in Rv for all r in R. This holds if and only if T (v) lies in Rv; that is, T (v) = λv for
some λ in R. A real number λ is called an eigenvalue of an operator T : V

V if

→

→

T (v) = λv

holds for some nonzero vector v in V . In this case, v is called an eigenvector of T corresponding to λ.
The subspace

Eλ(T ) =

v in V

{

|

T (v) = λv

}

is called the eigenspace of T corresponding to λ. These terms are consistent with those used in Section 5.5
Rn
for matrices. If A is an n
if and only if λ is an eigenvalue of the matrix A. Moreover, the eigenspaces agree:

n matrix, a real number λ is an eigenvalue of the matrix operator TA : Rn

→

×

Eλ(TA) =

x in Rn

{

|

Ax = λx

= Eλ(A)

}

6
9.3. Invariant Subspaces and Direct Sums

519

The following theorem reveals the connection between the eigenspaces of an operator T and those of the
matrices representing T .

Theorem 9.3.3

Let T : V
CB : V

→

V bealinearoperatorwhere dim V = n,let B denoteanyorderedbasisofV,andlet

→
Rn denotethecoordinateisomorphism. Then:

1. Theeigenvaluesλ of T arepreciselytheeigenvaluesofthematrix MB(T ) andthusarethe

rootsofthecharacteristicpolynomialcT (x).

2. Inthiscasetheeigenspaces Eλ(T ) and Eλ[MB(T )] areisomorphicviatherestriction

CB : Eλ(T )

→

Eλ[MB(T )].

Proof. Write A = MB(T ) for convenience. If T (v) = λv, then λCB(v) = CB[T (v)] = ACB(v) because CB
is linear. Hence CB(v) lies in Eλ(A), so we do have a function CB : Eλ(T )
Eλ(A). It is clearly linear
and one-to-one; we claim it is onto. If x is in Eλ(A), write x = CB(v) for some v in V (CB is onto). This v
actually lies in Eλ(T ). To see why, observe that

→

CB[T (v)] = ACB(v) = Ax = λx = λCB(v) = CB(λv)

Hence T (v) = λv because CB is one-to-one, and this proves (2). As to (1), we have already shown that
eigenvalues of T are eigenvalues of A. The converse follows, as in the foregoing proof that CB is onto.

Theorem 9.3.3 shows how to pass back and forth between the eigenvectors of an operator T and the
eigenvectors of any matrix MB(T ) of T :

v lies in Eλ(T )

if and only if CB(v) lies in Eλ[MB(T )]

Example 9.3.6

Find the eigenvalues and eigenspaces for T : P2

P2 given by

→

T (a + bx + cx2) = (2a + b + c) + (2a + b

2c)x

−

−

(a + 2c)x2

Solution. If B =

1, x, x2

{

, then

}

MB(T ) =

CB[T (1)] CB[T (x)] CB[T (x2)]

=



2 1
2 1
1 0

1
2
−
2 
−


(cid:3)
3) as the reader can verify.

−



Hence cT (x) = det [xI

−

Moreover, E

1[MB(T )] = R

and E3[MB(T )] = R

, so Theorem 9.3.3 gives

−

E

1(T ) = R(

−

−

1 + 2x + x2) and E3(T ) = R(5 + 6x



x2).



−

5
6
1 




−

(cid:2)
MB(T )] = (x + 1)2(x
1
−
2
1 




−

520

Change of Basis

Theorem 9.3.4

Eacheigenspaceofalinearoperator T : V

V isa T-invariantsubspaceofV.

→

Proof. If v lies in the eigenspace Eλ(T ), then T (v) = λv, so T [T (v)] = T (λv) = λT (v). This shows that
T (v) lies in Eλ(T ) too.

Direct Sums

Sometimes vectors in a space V can be written naturally as a sum of vectors in two subspaces. For example,
in the space Mnn of all n

n matrices, we have subspaces

×
P in Mnn

U =

{

|
where a matrix Q is called skew-symmetric if QT =
the sum of a matrix in U and a matrix in W ; indeed,

}

P is symmetric

and W =

Q in Mnn

{

Q is skew symmetric
}

|

Q. Then every matrix A in Mnn can be written as

−

A = 1

2(A + AT ) + 1
where 1
2(A + AT ) is symmetric and 1
AT ) is skew symmetric. Remarkably, this representation is
Q, then AT = PT + QT = P
unique: If A = P + Q where PT = P and QT =
Q; adding this to A = P + Q
gives P = 1
AT ). In addition, this uniqueness turns out to be
2(A
closely related to the fact that the only matrix in both U and W is 0. This is a useful way to view matrices,
and the idea generalizes to the important notion of a direct sum of subspaces.

2(A + AT ), and subtracting gives Q = 1

AT )

2 (A

2(A

−

−

−

−

−

If U and W are subspaces of V , their sum U + W and their intersection U

tion 6.4 as follows:

W were deﬁned in Sec-

∩

U +W =
W =
U

∩

{
{

|

u + w
v

u in U and w in W
v lies in both U and W

|

}
}

These are subspaces of V , the sum containing both U and W and the intersection contained in both U and
W . It turns out that the most interesting pairs U and W are those for which U
W is as small as possible
and U +W is as large as possible.

∩

Deﬁnition 9.7 Direct Sum of Subspaces

AvectorspaceV issaidtobethedirectsumofsubspacesU andW if

W =

U

∩

0
}
{

and U +W = V

InthiscasewewriteV = U
calledacomplementofU inV.

⊕

W. GivenasubspaceU,anysubspaceW suchthatV = U

W is

⊕

Example 9.3.7

In the space R5, consider the subspaces U =
W =

(0, 0, 0, d, e)

d and e in R

{
. Show that R5 = U

(a, b, c, 0, 0)
|
W .

{

|

}

⊕

a, b, and c in R

and

}

9.3. Invariant Subspaces and Direct Sums

521

Solution. If x = (a, b, c, d, e) is any vector in R5, then x = (a, b, c, 0, 0) + (0, 0, 0, d, e), so x
lies in U +W . Hence R5 = U +W . To show that U
Then d = e = 0 because x lies in U , and a = b = c = 0 because x lies in W . Thus
x = (0, 0, 0, 0, 0) = 0, so 0 is the only vector in U

, let x = (a, b, c, d, e) lie in U

W . Hence U

W =

W =

∩

∩

{

0

}

.

W .

∩

∩

0
}

{

Example 9.3.8

If U is a subspace of Rn, show that Rn = U

U ⊥.

⊕

Solution. The equation Rn = U +U ⊥ holds because, given x in Rn, the vector projU x lies in U
and x
U ⊥ =
∩
orthogonal to itself and hence must be zero.

projU x lies in U ⊥. To see that U

, observe that any vector in U

U ⊥ is

−

∩

0

{

}

Example 9.3.9

e1, e2, . . . , en

Let
ek+1, . . . , en

{

}

. If U = span

be a basis of a vector space V , and partition it into two parts:

e1, . . . , ek

and

and W = span

ek+1, . . . , en

, show that V = U

{

e1, . . . , ek}
W , then v = a1e1 +

}

{
Solution. If v lies in U
some ai and b j in R. The fact that the ei are linearly independent forces all ai = b j = 0, so v = 0.
+ vnen where the vi are in R. Then
Hence U
W =
v = u + w, where u = v1e1 +
+ vnen lies in W . This
proves that V = U +W .

+ vkek lies in U and w = vk+1ek+1 +

. Now, given v in V , write v = v1e1 +

⊕
+ bnen hold for

+ akek and v = bk+1ek+1 +

0
}

· · ·

· · ·

· · ·

· · ·

· · ·

∩

∩

{

}

{

{

}
W .

Example 9.3.9 is typical of all direct sum decompositions.

Theorem 9.3.5

LetU andW besubspacesofaﬁnitedimensionalvectorspaceV. Thefollowingthreeconditions
areequivalent:

1. V = U

W.

⊕

2. EachvectorvinV canbewrittenuniquelyintheform

v= u+ w

uinU , winW

3. If

u1, . . . , uk
{
B =

and
u1, . . . , uk, w1, . . . , wm
{

w1, . . . , wm
{

}

}

}

isabasisofV.

arebasesofU andW,respectively,then

(Theuniquenessin(2)meansthatifv= u1 + w1 isanothersuchrepresentation,thenu1 = uand
w1 = w.)

Proof. Example 9.3.9 shows that (3)

(1).

⇒

522

Change of Basis

(1)

(2). Given v in V , we have v = u + w, u in U , w in W , because V = U +W .
, so u = u1 and w = w1.

w lies in U

u1 = w1

W =

If also v = u1 + w1, then u

⇒

0

−

−

∩

{

}

(2)

⇒

(3). Given v in V , we have v = u+w, u in U , w in W . Hence v lies in span B; that is, V = span B.
+ akuk
· · ·
+ bmwm. Then u + w = 0, and so u = 0 and w = 0 by the uniqueness in (2). Hence

+ bmwm = 0. Write u = a1u1 +

+ akuk + b1w1 +

· · ·

· · ·

To see that B is independent, let a1u1 +
and w = b1w1 +
· · ·
ai = 0 for all i and b j = 0 for all j.

Condition (3) in Theorem 9.3.5 gives the following useful result.

Theorem 9.3.6

IfaﬁnitedimensionalvectorspaceV isthedirectsumV = U

W ofsubspacesU andW,then

⊕

dim V = dim U + dim W

These direct sum decompositions of V play an important role in any discussion of invariant subspaces.

If T : V

→

V is a linear operator and if U1 is a T -invariant subspace, the block upper triangular matrix

MB(T ) =

(cid:20)

MB1(T ) Y
Z

0

(cid:21)
b1, . . . , bk}

(9.3)

b1, . . . , bk, bk+1, . . . , bn

in Theorem 9.3.1 is achieved by choosing any basis B1 =
B =
}
ﬁrst k columns of MB(T ) have the form in (9.3) (that is, the last n
arises whether the additional basis vectors bk+1, . . . , bn can be chosen such that

of U1 and completing it to a basis
of V in any way at all. The fact that U1 is T -invariant ensures that the
k entries are zero), and the question

−

{

{

U2 = span

{

bk+1, . . . , bn

}

is also T -invariant. In other words, does each T -invariant subspace of V have a T -invariant complement?
Unfortunately the answer in general is no (see Example 9.3.11 below); but when it is possible, the matrix
MB(T ) simpliﬁes further. The assumption that the complement U2 = span
is T -invariant
too means that Y = 0 in equation 9.3 above, and that Z = MB2(T ) is the matrix of the restriction of T to
U2 (where B2 =

). The veriﬁcation is the same as in the proof of Theorem 9.3.1.

bk+1, . . . , bn

bk+1, . . . , bn

}

{

{

}

Theorem 9.3.7

Let T : V
andU2 are T-invariant. If B1 =
respectively,then

→

V bealinearoperatorwhereV hasdimension n. SupposeV = U1

U2 wherebothU1

⊕

arebasesofU1 andU2

b1, . . . , bk
{

}

and B2 =

bk+1, . . . , bn
{

}

B =

b1, . . . , bk, bk+1, . . . , bn
{

}

isabasisofV,and MB(T ) hastheblockdiagonalform

MB(T ) =

(cid:20)

MB1(T )
0

0
MB2(T )

(cid:21)

where MB1(T ) and MB2(T ) arethematricesoftherestrictionsof T toU1 andtoU2 respectively.

Deﬁnition 9.8 Reducible Linear Operator

9.3. Invariant Subspaces and Direct Sums

523

Thelinearoperator T : V
canbefoundsuchthatV = U1

→

U2.

⊕

V issaidtobereducibleifnonzero T-invariantsubspacesU1 andU2

Then T has a matrix in block diagonal form as in Theorem 9.3.7, and the study of T is reduced to
studying its restrictions to the lower-dimensional spaces U1 and U2. If these can be determined, so can T .
Here is an example in which the action of T on the invariant subspaces U1 and U2 is very simple indeed.
The result for operators is used to derive the corresponding similarity theorem for matrices.

Example 9.3.10

Let T : V
Deﬁne

→

V be a linear operator satisfying T 2 = 1V (such operators are called involutions).

U1 =

v

T (v) = v

}

|

and U2 =

T (v) =

v

{

|

v
−

}

a. Show that V = U1

⊕

{
U2.

b. If dim V = n, ﬁnd a basis B of V such that MB(T ) =

Ik
0

(cid:20)

0
In

−

k
−

for some k.

(cid:21)

c. Conclude that, if A is an n

some k.

Solution.

n matrix such that A2 = I, then A is similar to

×

Ik
0

(cid:20)

0
In

−

k
−

for

(cid:21)

a. The veriﬁcation that U1 and U2 are subspaces of V is left to the reader. If v lies in U1

then v = T (v) =

v, and it follows that v = 0. Hence U1

U2 =

U2,
. Given v in V , write

∩

0
}

{

−

v = 1
2 {

[v + T (v)] + [v

−

∩
T (v)]

}

Then v + T (v) lies in U1, because T [v + T (v)] = T (v) + T 2(v) = v + T (v). Similarly,
v

T (v) lies in U2, and it follows that V = U1 +U2. This proves part (a).

−

b. U1 and U2 are easily shown to be T -invariant, so the result follows from Theorem 9.3.7 if
bk+1, . . . , bn

of U1 and U2 can be found such that

bases B1 =
MB1(T ) = Ik and MB2(T ) =

b1, . . . , bk}

and B2 =
In

{

{

k. But this is true for any choice of B1 and B2:

}

−

−
CB1[T (b1)] CB1[T (b2)]
CB1(b1) CB1(b2)

· · ·

MB1(T ) =
=
(cid:2)
= Ik
(cid:2)

CB1[T (bk)]

· · ·
CB1(bk)

(cid:3)

(cid:3)

A similar argument shows that MB2(T ) =
B =
.

b1, b2, . . . , bn

{

}

In

k, so part (b) follows with

−

−

524

Change of Basis

c. Given A such that A2 = I, consider TA : Rn

Rn. Then (TA)2(x) = A2x = x for all x in Rn,

so (TA)2 = 1V . Hence, by part (b), there exists a basis B of Rn such that

→

MB(TA) =

Ir
0

0
In

r
−

−

(cid:20)
1AP for some invertible matrix P, and this

(cid:21)

But Theorem 9.2.4 shows that MB(TA) = P−
proves part (c).

Note that the passage from the result for operators to the analogous result for matrices is routine and can
be carried out in any situation, as in the veriﬁcation of part (c) of Example 9.3.10. The key is the analysis
of the operators. In this case, the involutions are just the operators satisfying T 2 = 1V , and the simplicity
of this condition means that the invariant subspaces U1 and U2 are easy to ﬁnd.

Unfortunately, not every linear operator T : V

V is reducible. In fact, the linear operator in Exam-
→
ple 9.3.4 has no invariant subspaces except 0 and V . On the other hand, one might expect that this is the
only type of nonreducible operator; that is, if the operator has an invariant subspace that is not 0 or V , then
some invariant complement must exist. The next example shows that even this is not valid.

Example 9.3.11

Consider the operator T : R2

a + b
b
→
T -invariant but that U1 has not T -invariant complement in R2.

R2 given by T

a
b

=

(cid:21)

(cid:20)

(cid:20)

. Show that U1 = R
(cid:21)

(cid:20)

1
0

is

(cid:21)

1
Solution. Because U1 = span
0
(cid:20)
U1 is T -invariant. Now assume, if possible, that U1 has a T -invariant complement U2 in R2. Then
U1

, it follows (by Example 9.3.3) that

U2. Theorem 9.3.6 gives

U2 = R2 and T (U2)

and T

(cid:21)(cid:27)

(cid:26)(cid:20)

1
0

1
0

=

(cid:20)

(cid:21)

(cid:21)

⊕

⊆
2 = dim R2 = dim U1 + dim U2 = 1 + dim U2

so dim U2 = 1. Let U2 = Ru2, and write u2 =

. We claim that u2 is not in U1. For if u2

U1,

∈

p
q

(cid:20)

(cid:21)

0

, a contradiction, as dim U2 = 1. So
U2 = Ru2 (because U2 is T -invariant), say

{

}

then u2
u2 /
∈

∈

U1

U2 =

, so u2 = 0. But then U2 = Ru2 =

0
}
{
∩
= 0. On the other hand, T (u2)
U1, from which q
p
q

∈

.

T (u2) = λu2 = λ

Thus

(cid:20)

(cid:21)

p + q
q

= T

= λ

p
q

p
q

where λ

R

∈

(cid:20)
Hence p + q = λp and q = λq. Because q
= 0, the second of these equations implies that λ = 1, so
the ﬁrst equation implies q = 0, a contradiction. So a T -invariant complement of U1 does not exist.

(cid:21)

(cid:21)

(cid:20)

(cid:20)

(cid:21)

This is as far as we take the theory here, but in Chapter 11 the techniques introduced in this section will

be reﬁned to show that every matrix is similar to a very nice matrix indeed—its Jordan canonical form.

6
6
Exercises for 9.3

Exercise 9.3.1 If T : V
that ker T and im T are T -invariant subspaces.

V is any linear operator, show

→

Exercise 9.3.2 Let T be a linear operator on V . If U and
W are T -invariant, show that

a. U

∩

W and U +W are also T -invariant.

b. T (U ) is T -invariant.

Exercise 9.3.3 Let S and T be linear operators on V and
assume that ST = T S.

9.3. Invariant Subspaces and Direct Sums

525

Exercise 9.3.8 In each case, show that U is T -invariant,
use it to ﬁnd a block upper triangular matrix for T , and
use that to compute cT (x).

a. T : P2

P2,

→
T (a + bx + cx2)

a + 2b + c) + (a + 3b + c)x + (a + 4b)x2,

= (
U = span

−

1, x + x2
{
P2,

}

b. T : P2

→
T (a + bx + cx2)

= (5a
U = span

2b + c) + (5a
−
−
2x2, x + x2
1
}
{

−

b + c)x + (a + 2c)x2,

a. Show that im S and ker S are T -invariant.

b. If U is T -invariant, show that S(U ) is T -invariant.

Exercise 9.3.9 In each case, show that TA : R2
no invariant subspaces except 0 and R2.

→

R2 has

Exercise 9.3.4 Let T : V
V be a linear operator. Given
v in V , let U denote the set of vectors in V that lie in every
T -invariant subspace that contains v.

→

a. A =

b. A =

1
1

−
cosθ
sinθ

(cid:20)

(cid:20)

2
1

−

(cid:21)

−

sinθ
cosθ

(cid:21)

, 0 < θ < π

a. Show that U is a T -invariant subspace of V con-

taining v.

Exercise 9.3.10 In each case, show that V = U

W .

⊕

b. Show that U is contained in every T -invariant sub-

space of V that contains v.

Exercise 9.3.5

a. If T is a scalar operator (see Example 7.1.1) show

that every subspace is T -invariant.

b. Conversely, if every subspace is T -invariant, show

that T is scalar.

Exercise 9.3.6 Show that the only subspaces of V that
V are 0 and
are T -invariant for every operator T : V
V . Assume that V is ﬁnite dimensional.
[Hint: Theo-
rem 7.1.3.]

→

Exercise 9.3.7 Suppose that T : V
V is a linear oper-
→
ator and that U is a T -invariant subspace of V . If S is an
1. Show that S(U ) is a
invertible operator, put T ′ = ST S−
T ′-invariant subspace.

a. V = R4, U = span

W = span

,
(1, 1, 0, 0), (0, 1, 1, 0)
}
{

(0, 1, 0, 1), (0, 0, 1, 1)
}
{
a, b in R

W =

b. V = R4, U =

(a, a, b, b)
{
(c, d, c,
{
c. V = P3, U =

|
−
a + bx
{
ax2 + bx3
{

|
a, b in R

W =

d)

|

|
c, d in R

}
a, b in R

,
}

}

a, b in R

a a
b b

d. V = M22, U =

W =

(cid:26)(cid:20)

−

(cid:26)(cid:20)

a b
a b

(cid:21)(cid:12)
(cid:12)
(cid:12)
a, b in R
(cid:12)

(cid:27)

,
}

,
(cid:27)

(cid:21)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Exercise 9.3.11 Let U = span
in R4. Show that R4 = U
where W1 = span
W2 = span

(1, 0, 0, 0), (0, 1, 0, 0)
{
}
W1 and R4 = U
W2,
⊕
⊕
and
(0, 0, 1, 0), (0, 0, 0, 1)
}
{

(1, 1, 1, 1), (1, 1, 1,
{

.
1)
}

−

Exercise 9.3.12 Let U be a subspace of V , and suppose
that V = U
W2 hold for subspaces W1
and W2. Show that dim W1 = dim W2.

W1 and V = U

⊕

⊕

526

Change of Basis

Exercise 9.3.13
If U and W denote the subspaces of
even and odd polynomials in Pn, respectively, show that
Pn = U
x)
is even.]

W . (See Exercise 6.3.36.) [Hint: f (x) + f (

⊕

−

Exercise 9.3.14 Let E be an n
Show that Mnn = U
W =
X .]

n matrix with E 2 = E.
and
. [Hint: X E lies in U for every matrix
BE = 0
}

W , where U =

AE = A

B
{

A
{

⊕

×

}

|

|

0
{

if and only if

Exercise 9.3.15 Let U and W be subspaces of V . Show
that U
W =
is independent
∩
}
for all u
= 0 in U and all w
Exercise 9.3.16 Let V T
→
tions, and assume that dim V and dim W are ﬁnite.

= 0 in W .
W S
→

V be linear transforma-

u, w
}
{

→

Exercise 9.3.20 Let T : V
V be a linear operator
where dim V = n. If U is a T -invariant subspace of V ,
let T1 : U
U denote the restriction of T to U
(so T1(u) = T (u) for all u in U ). Show that cT (x) =
cT1(x)
[Hint: Theo-
rem 9.3.1.]

q(x) for some polynomial q(x).

→

·

Exercise 9.3.21 Let T : V
V be a linear operator
where dim V = n. Show that V has a basis of eigen-
vectors if and only if V has a basis B such that MB(T ) is
diagonal.

→

In each case, show that T 2 = 1 and
Exercise 9.3.22
ﬁnd (as in Example 9.3.10) an ordered basis B such that
MB(T ) has the given block form.

a. If ST = 1V , show that W = im T
[Hint: Given w in W , show that w
ker S.]

⊕
−

ker S.
T S(w) lies in

b. Illustrate with R2 T
→

R3 S
→

R2 where

T (x, y) = (x, y, 0) and S(x, y, z) = (x, y).

Exercise 9.3.17 Let U and W be subspaces of V , let
dim V = n, and assume that dim U + dim W = n.

W =

a. If U

, show that V = U
}
b. If U +W = V , show that V = U

0
{

∩

W .

⊕
W . [Hint: The-

⊕

orem 6.4.5.]

Exercise 9.3.18 Let A =

TA : R2

R2.

→

0 1
0 0

(cid:21)

(cid:20)

and consider

a. Show that the only eigenvalue of TA is λ = 0.

b. Show that ker (TA) = R

is the unique

TA-invariant subspace of R2 (except for 0 and
R2).

1
0

(cid:20)

(cid:21)

Exercise 9.3.19

−
−

2
1
0
0

If A = 

5
2
0
0
R4 has two-dimensional T -invariant sub-
W , but A has no real

, show

0
0
2
1

0
0
1
1









−

−



that TA : R4
spaces U and W such that R4 = U
eigenvalue.

→

⊕

a. T : M22

→

MB(T ) =

b. T : P3

→
MB(T ) =

c. T : C

→
MB(T ) =

M22 where T (A) = AT ,
I3
0

0
1

−

(cid:20)
(cid:21)
P3 where T [p(x)] = p(

I2
0

0
I2 (cid:21)
(cid:20)
C where T (a + bi) = a

−

x),

−

bi,

−

1
0

0
1

−

(cid:20)
R3 where

(cid:21)

a + 2b + c, b + c,

c),

−

d. T : R3

→
T (a, b, c) = (
1
0

MB(T ) =

−

e. T : V

→
MB(T ) =

0
I2 (cid:21)
(cid:20)
V where T (v) =
In

−

−

v, dim V = n,

−

Exercise 9.3.23 Let U and W denote subspaces of a vec-
tor space V .

a. If V = U

⊕

W , deﬁne T : V

V by T (v) = w
where v is written (uniquely) as v = u + w with
u in U and w in W . Show that T is a linear trans-
formation, U = ker T , W = im T , and T 2 = T .

→

b. Conversely, if T : V

→
such that T 2 = T , show that V = ker T
[Hint: v

⊕
T (v) lies in ker T for all v in V .]

V is a linear transformation
im T .

−

Exercise 9.3.24 Let T : V
V be a linear operator sat-
→
isfying T 2 = T (such operators are called idempotents).
v
Deﬁne U1 =
,
}
{
T (v) = 0
U2 = ker T =
.
}
U2.

|
v
{
a. Show that V = U1

T (v) = v

|

⊕

6
6
b. If dim V = n, ﬁnd a basis B of V such that

b. If dim V = n, show that V has a basis B such that

9.3. Invariant Subspaces and Direct Sums

527

MB(T ) =

, where r = rank T .

MB(T ) =

Ir 0
0 0

(cid:21)

(cid:20)
c. If A is an n

×
A is similar to

(cid:21)
[Hint: Example 9.3.10.]

(cid:20)

n matrix such that A2 = A, show that

Ir 0
0 0

, where r = rank A.

cIk
0

0
cIn

−

−

k (cid:21)

for some k.

n matrix such that A2 = c2I, c

= 0,

(cid:20)
c. If A is an n

×
show that A is similar to

cIk
0

(cid:20)

0
cIn

−

−

k (cid:21)

for

some k.

Exercise 9.3.25 In each case, show that T 2 = T and ﬁnd
(as in the preceding exercise) an ordered basis B such that
MB(T ) has the form given (0k is the k

k zero matrix).

×

If P is a ﬁxed n

Exercise 9.3.28
n matrix, deﬁne
T : Mnn
Mnn by T (A) = PA. Let U j denote the sub-
space of Mnn consisting of all matrices with all columns
zero except possibly column j.

→

×

b + c)(1 + x + x2),

−

a. Show that each U j is T -invariant.

b. Show that Mnn has a basis B such that MB(T ) is
block diagonal with each block on the diagonal
equal to P.

a. T : P2

P2 where

T (a + bx + cx2) = (a

→

MB(T ) =

b. T : R3

→

1
0
0 02 (cid:21)
(cid:20)
R3 where

T (a, b, c) = (a + 2b, 0, 4b + c),
I2 0
0 0

MB(T ) =

(cid:20)

(cid:21)

c. T : M22

M22 where

→

a b
c d

T

(cid:20)

MB(T ) =

−

15
6

a b
c d

,

(cid:21)

(cid:21) (cid:20)

(cid:20)

−

5
2

0
02 (cid:21)

=

I2
0

(cid:21)

(cid:20)

Exercise 9.3.26 Let T : V
T 2 = cT , c
= 0.

→

V be an operator satisfying

a. Show that V = U
u
{

ker T , where
⊕
T (u) = cu
U =
.
}
[Hint: Compute T (v
−

1
c T (v)).]

|

b. If dim V = n, show that V has a basis B such that

cIr 0
0 0

, where r = rank T .

(cid:21)

n matrix of rank r such that
= 0, show that A is similar to

×

MB(T ) =

(cid:20)
c. If A is any n
A2 = cA, c
cIr 0
0 0

.

(cid:20)

(cid:21)

Exercise 9.3.27 Let T : V
T 2 = c2, c
= 0.

→

V be an operator such that

a. Show that V = U1
v
U1 =
{
[Hint: v = 1

⊕
T (v) = cv
}
[T (v) + cv]

U2, where
and U2 =

v
{
[T (v)

|

2c {

−

cv
.
}

−

T (v) =
cv]
.]
}

|
−

R
Exercise 9.3.29 Let V be a vector space. If f : V
is a linear transformation and z is a vector in V , deﬁne
V by Tf , z(v) = f (v)z for all v in V . Assume
Tf , z : V
that f

= 0 and z

= 0.

→

→

a. Show that Tf , z is a linear operator of rank 1.

b. If f

= 0, show that Tf , z is an idempotent if and
V is called

only if f (z) = 1. (Recall that T : V
an idempotent if T 2 = T .)

→

c. Show that every idempotent T : V

→

V of rank 1
R and

has the form T = Tf , z for some f : V
some z in V with f (z) = 1. [Hint: Write
im T = Rz and show that T (z) = z. Then use Ex-
ercise 9.3.23.]

→

Exercise 9.3.30 Let U be a ﬁxed n
sider the operator T : Mnn

n matrix, and con-
Mnn given by T (A) = UA.

×

→

a. Show that λ is an eigenvalue of T if and only if it

is an eigenvalue of U .

P1 P2

b. If λ is an eigenvalue of T , show that Eλ(T ) con-
sists of all matrices whose columns lie in Eλ(U ):
Eλ(T )
=

{
(cid:2)
c. Show if dim [Eλ(U )] = d, then dim [Eλ(T )] = nd.
[Hint: If B =
is a basis of Eλ(U ),
consider the set of all matrices with one column
from B and the other columns zero.]

(cid:3)
x1, . . . , xd
{

Pi in Eλ(U ) for each i
}

· · ·

Pn

}

|

6
6
6
6
6
6
6
528

Change of Basis

→

V be a linear operator
V is a subspace, let

Exercise 9.3.31 Let T : V
where V is ﬁnite dimensional. If U
u0 + T (u1) + T 2(u2) +
U =
{
≥
. Show that U is the smallest T -invariant subspace
0
}
containing U (that is, it is T -invariant, contains U , and
is contained in every such subspace).

⊆
+ T k(uk)

ui in U , k

· · ·

|

Exercise 9.3.32 Let U1, . . . , Um be subspaces of V and
+Um; that is, every v in V can be
assume that V = U1 +
· · ·
written (in at least one way) in the form v = u1 +
+ um,
ui in Ui. Show that the following conditions are equiva-
lent.

· · ·

i. If u1 +

i.

· · ·

+ um = 0, ui in Ui, then ui = 0 for each

ii. If u1 +

+ um = u′1 +
then ui = u′i for each i.

· · ·

+ u′m, ui and u′i in Ui,

· · ·

iv. Ui

(Ui+1 +
· · ·
i = 1, 2, . . . , m

∩

for each

+Um) =
1.

−

0
{

}

When these conditions are satisﬁed, we say that V is

the direct sum of the subspaces Ui, and write
Um.
V = U1

U2

⊕
Exercise 9.3.33

⊕ · · · ⊕

a. Let B be a basis of V and let B = B1

Bm
where the Bi are pairwise disjoint, nonempty sub-
sets of B.
If Ui = span Bi for each i, show that
V = U1

Um (preceding exercise).

∪ · · · ∪

U2

B2

∪

⊕
⊕ · · · ⊕
b. Conversely if V = U1

of Ui for each i, show that B = B1
basis of V as in (a).

⊕ · · · ⊕

Um and Bi is a basis
Bm is a

∪ · · · ∪

iii. Ui

(U1 +

+Ui

1 +Ui+1 +

∩

each i = 1, 2, . . . , m.

· · ·

−

+Um) =

for

0
{

}

· · ·

Exercise 9.3.34 Let T : V
T 3 = 0. If u
that U is T -invariant and has dimension 3.

→
V and U = span

V be an operator where
u, T (u), T 2(u)
, show
}
{

∈

Chapter 10

Inner Product Spaces

10.1

Inner Products and Norms

The dot product was introduced in Rn to provide a natural generalization of the geometrical notions of
length and orthogonality that were so important in Chapter 4. The plan in this chapter is to deﬁne an inner
product on an arbitrary real vector space V (of which the dot product is an example in Rn) and use it to
introduce these concepts in V . While this causes some repetition of arguments in Chapter 8, it is well
worth the effort because of the much wider scope of the results when stated in full generality.

Deﬁnition 10.1 Inner Product Spaces

AninnerproductonarealvectorspaceV isafunctionthatassignsarealnumber
pairv,wofvectorsinV insuchawaythatthefollowingaxiomsaresatisﬁed.

v, w
i
h

toevery

v, w

i

is a real number for all v and w in V .

w, v

for all v and w in V .

P1.

P2.

P3.

P4.

P5.

h

h

h

h

h

v, w

=

h
i
v + w, u

=

i
v, u

i
= r

h
v, w

i
h
i
> 0 for all v

rv, w

v, v

i

+

w, u
i

for all u, v, and w in V .

h

i
for all v and w in V and all r in R.

= 0 in V .

A real vector space V with an inner product
i
subspace of an inner product space is again an inner product space using the same inner product.1

will be called an inner product space. Note that every

h

,

Example 10.1.1
Rn is an inner product space with the dot product as inner product:

v, w

= v

·

i

h

w for all v, w

Rn

∈

See Theorem 5.3.1. This is also called the euclidean inner product, and Rn, equipped with the dot
product, is called euclidean n-space.

Example 10.1.2

If A and B are m
matrix X . Show that

×

n matrices, deﬁne

A, B
is an inner product in Mmn.

h

i

,

h

i

= tr (ABT ) where tr (X ) is the trace of the square

1If we regard Cn as a vector space over the ﬁeld C of complex numbers, then the “standard inner product” on Cn deﬁned in

Section 8.7 does not satisfy Axiom P4 (see Theorem 8.7.1(3)).

529

6
530

Inner Product Spaces

Solution. P1 is clear. Since tr (P) = tr (PT ) for every square matrix P, we have P2:

A, B

i

h

= tr (ABT ) = tr [(ABT )T ] = tr (BAT ) =

B, A

i

h

Next, P3 and P4 follow because trace is a linear transformation Mmn
Turning to P5, let r1, r2, . . . , rm denote the rows of the matrix A. Then the (i, j)-entry of AAT is
ri

R (Exercise 10.1.19).

r j, so

→

·

= tr (AAT ) = r1
r j is the sum of the squares of the entries of r j, so this shows that

r1 + r2

+ rm

r2 +

But r j
squares of all nm entries of A. Axiom P5 follows.

A, A

· · ·

i

h

·

·

·

·

rm

A, A

i

h

is the sum of the

The importance of the next example in analysis is difﬁcult to overstate.

Example 10.1.3: 2

Let C[a, b] denote the vector space of continuous functions from [a, b] to R, a subspace of
F[a, b]. Show that

deﬁnes an inner product on C[a, b].

f , g

=

i

h

b

a

Z

f (x)g(x)dx

Solution. Axioms P1 and P2 are clear. As to axiom P4,

r f , g

=

i

h

b

a

Z

r f (x)g(x)dx = r

b

a

Z

f (x)g(x)dx = r

f , g

i

h

Axiom P3 is similar. Finally, theorems of calculus show that
continuous, that this is zero if and only if f is the zero function. This gives axiom P5.

f , f

=

≥

h

i

0 and, if f is

b
a f (x)2dx
R

If v is any vector, then, using axiom P3, we get

0, v

=

0 + 0, v

=

0, v

+

0, v

i
and it follows that the number
must be zero. This observation is recorded for reference in the
following theorem, along with several other properties of inner products. The other proofs are left as
Exercise 10.1.20.

h
0, v

i

h

i

i

i

h

h

h

Theorem 10.1.1

beaninnerproductonaspaceV;letv,u,andwdenotevectorsinV;andlet r denotea

,
Let
i
realnumber.

h

1.

2.

u, v+ w
i
h
v, rw
= r
i
h

=

u, v
i
h
v, w
=
i
h

u, w
i
h
rv, w
i

h

+

2This example (and others later that refer to it) can be omitted with no loss of continuity by students with no calculus

background.

10.1. Inner Products and Norms

531

3.

4.

v, 0
h
i
v, v
i
h

= 0 =

0, v
i
h
= 0 ifandonlyifv= 0

If

,

h

i

is an inner product on a space V , then, given u, v, and w in V ,

h
for all r and s in R by axioms P3 and P4. Moreover, there is nothing special about the fact that there are
two terms in the linear combination or that it is in the ﬁrst component:

h

h

i

h

h

i

ru + sv, w
i

=

ru, w

+

= r

u, w

+ s

sv, w
i

v, w
i

r1v1 + r2v2 +
h
· · ·
v, s1w1 + s2w2 +

+ rnvn, w
+ smwm

· · ·

= r1
= s1

v1, w
v, w1

+ r2
+ s2

h
h

i
i

h
h

v2, w
v, w2

+
+

i
i

· · ·
· · ·

i
i

, and

+ rn
h
+ sm
h

vn, w
i
v, wm
i

h

hold for all ri and si in R and all v, w, vi, and w j in V . These results are described by saying that inner
products “preserve” linear combinations. For example,

2u

h

−

v, 3u + 2v

i

=
h
= 6
= 6

2u, 3u
u, u
u, u

+
i
+ 4
+

2u, 2v
h
u, v
h
u, v

i
i −
2

+
h−
v, u
3
h
v, v

h

i −

h

i

i
i

h
h

v, 2v

v, 3u
i
2
h

i −

+
h−
v, v
i

n matrix and x and y are columns in Rn, we regard the 1

i

×

1 matrix xT Ay as a

If A is a symmetric n

number. If we write

×

x, y

= xT Ay

for all columns x, y in Rn

h

i

then axioms P1–P4 follow from matrix arithmetic (only P2 requires that A is symmetric). Axiom P5 reads

xT Ax > 0

for all columns x

= 0 in Rn

and this condition characterizes the positive deﬁnite matrices (Theorem 8.3.2). This proves the ﬁrst asser-
tion in the next theorem.

Theorem 10.1.2

If A isany n

×

n positivedeﬁnitematrix,then

deﬁnesaninnerproducton Rn,andeveryinnerproducton Rn arisesinthisway.

= xT Ayforallcolumnsx, yin Rn

x, y
i
h

Proof. Given an inner product

on Rn, let

,

h

i

{

e1, e2, . . . , en

}

be the standard basis of Rn. If x =

and y =

n
∑
j=1

y je j are two vectors in Rn, compute

x, y

i

h

by adding the inner product of each term xiei to

n
∑
i=1

xiei

each term y je j. The result is a double sum.

x, y

=

i

h

n
∑
i=1

n
∑
j=1h

xiei, y je j

n
∑
i=1

n
∑
j=1

=

i

ei, e j

xi

h

y j

i

6
532

Inner Product Spaces

As the reader can verify, this is a matrix product:

x, y

=

i

h

x1 x2

· · ·

xn



(cid:2)

(cid:3)

e1, e1
e2, e1
...
en, e1

h
h

h

i
i

i

e1, e2
e2, e2
...
en, e2

h
h

h

i
i

i

· · ·
· · ·
. . .

· · ·

e1, en
e2, en
...
en, en

h
h

h

i
i

i

y1
y2
...
yn



























Hence

x, y

i

h

= xT Ay, where A is the n

×

n matrix whose (i, j)-entry is

. The fact that

ei, e j

h

i

shows that A is symmetric. Finally, A is positive deﬁnite by Theorem 8.3.2.

ei, e j

e j, ei

=

i

h

i

h

Thus, just as every linear operator Rn
corresponds to a positive deﬁnite n
matrix In.

×

Rn corresponds to an n

n matrix, every inner product on Rn
→
n matrix. In particular, the dot product corresponds to the identity

×

Remark
If we refer to the inner product space Rn without specifying the inner product, we mean that the dot
product is to be used.

Example 10.1.4

Let the inner product

,

h

i

be deﬁned on R2 by

v1
v2

w1
w2

,

(cid:21)

(cid:20)

(cid:28)(cid:20)

(cid:21)(cid:29)

= 2v1w1

v1w2

−

−

v2w1 + v2w2

Find a symmetric 2

2 matrix A such that

x, y

= xT Ay for all x, y in R2.

h
Solution. The (i, j)-entry of the matrix A is the coefﬁcient of viw j in the expression, so

×

i

A =

2
1
−

(cid:20)

1
−
1

. Incidentally, if x =
(cid:21)

(cid:20)
= 2x2

x, x

i

h

x
y

−

, then

(cid:21)
2xy + y2 = x2 + (x

y)2

0

≥

−

for all x, so
deﬁnite.

h

x, x

i

= 0 implies x = 0. Hence

,

h

i

is indeed an inner product, so A is positive

Let

,
h
i
x1 x2

be an inner product on Rn given as in Theorem 10.1.2 by a positive deﬁnite matrix A. If
= xT Ax is an expression in the variables x1, x2, . . . , xn called a

T , then

x, x

xn

x =
quadratic form. These are studied in detail in Section 8.9.

· · ·

i

h

(cid:2)

(cid:3)

Norm and Distance

Deﬁnition 10.2 Norm and Distance
Asin Rn,if

,

isaninnerproductonaspaceV,thenorm3

h

i

10.1. Inner Products and Norms

533

ofavectorvinV isdeﬁnedby

v
k
k

WedeﬁnethedistancebetweenvectorsvandwinaninnerproductspaceV tobe

p

=

v
k
k

v, v
i
h

d (v, w) =

v
k

−

w
k

Note that axiom P5 guarantees that

v, v

i ≥

h

0, so

v
k

k

is a real number.

Example 10.1.5

y

y = f (x)2

2

f

||

||

O

a

b

x

The norm of a continuous function f = f (x) in C[a, b]
(with the inner product from Example 10.1.3) is given by

b

f (x)2dx

=

f

k

k

sZ

a

Hence
between x = a and x = b (shaded in the diagram).

2 is the area beneath the graph of y = f (x)2

k

k

f

Example 10.1.6

Show that

u + v, u

h

v

i

−

=

2

u
k

k

v
k

− k

2 in any inner product space.

Solution.

u + v, u

v

i

−

h

=

=

=

u, v

+

v, u

i
+

h
u, v

h

i − h
v
k

i − k

v, v
2

i

u, u
2

h

u
k
u
k

k

k

i − h

− h

2

u, v
2

v

i

− k

k

A vector v in an inner product space V is called a unit vector if

= 1. The set of all unit vectors in

V is called the unit ball in V . For example, if V = R2 (with the dot product) and v = (x, y), then

v

k

k

2 = 1

v

k

k

if and only if

x2 + y2 = 1

Hence the unit ball in R2 is the unit circle x2 + y2 = 1 with centre at the origin and radius 1. However, the
shape of the unit ball varies with the choice of inner product.

3If the dot product is used in Rn, the norm

x
k

k

of a vector x is usually called the length of x.

534

Inner Product Spaces

Example 10.1.7

y

(0, b)

(a, 0)

(0,

b)

−

a, 0)

(

−

O

x

Let a > 0 and b > 0. If v = (x, y) and w = (x1, y1), deﬁne an
inner product on R2 by

v, w
i
The reader can verify (Exercise 10.1.5) that this is indeed an
inner product. In this case

= xx1

a2 + yy1
b2

h

a2 + y2
x2
so the unit ball is the ellipse shown in the diagram.

if and only if

2 = 1

v
k

k

b2 = 1

Example 10.1.7 graphically illustrates the fact that norms and distances in an inner product space V vary
with the choice of inner product in V .

Theorem 10.1.3

= 0isanyvectorinaninnerproductspaceV,then 1
v
k
k

Ifv
positivemultipleofv.

vistheuniqueunitvectorthatisa

The next theorem reveals an important and useful fact about the relationship between norms and inner

products, extending the Cauchy inequality for Rn (Theorem 5.3.2).

Theorem 10.1.4: Cauchy-Schwarz Inequality4

IfvandwaretwovectorsinaninnerproductspaceV,then

2

v, w
i
h

v
k

≤ k

2

2

w
k
k

Moreover,equalityoccursifandonlyifoneofvandwisascalarmultipleoftheother.

w

k
aw

k
2 = b2
2 = b2

k

bv

−
bv + aw

k

k

Proof. Write

= a and

v

k

k

= b. Using Theorem 10.1.1 we compute:

k

k

v

v

2

2ab
−
2 + 2ab

h

v, w

v, w

i

+ a2
+ a2

k

k

w

w

2 = 2ab(ab
− h
2 = 2ab(ab +

v, w

v, w

i

)

)

k

k

k
0 and ab +

h
v, w

k

i
k
0, and hence that

h

i ≥

h

i
v, w

ab

−

≤ h

(10.1)

ab. But then

i ≤

v, w

v, w
It follows that ab
− h
w
v
kk
k
v, w
|h

k
Conversely, if

ab =

i| ≤

|h

i|

i ≥

, as desired.
w
v
k

kk

=

h
bv + aw = 0. It follows that one of v and w is a scalar multiple of the other, even if a = 0 or b = 0.

−

±

k

i

= ab then

v, w

=

ab. Hence (10.1) shows that bv

aw = 0 or

4Hermann Amandus Schwarz (1843–1921) was a German mathematician at the University of Berlin. He had strong geo-

metric intuition, which he applied with great ingenuity to particular problems. A version of the inequality appeared in 1885.

6
10.1. Inner Products and Norms

535

Example 10.1.8

If f and g are continuous functions on the interval [a, b], then (see Example 10.1.3)

b

a

(cid:18)Z

f (x)g(x)dx

(cid:19)

2

b

≤

a

Z

f (x)2dx

a

Z

b

g(x)2dx

Another famous inequality, the so-called triangle inequality, also comes from the Cauchy-Schwarz

inequality. It is included in the following list of basic properties of the norm of a vector.

Theorem 10.1.5

IfV isaninnerproductspace,thenorm

hasthefollowingproperties.

k · k

0 foreveryvectorvinV.

= 0 ifandonlyifv= 0.

1.

2.

3.

4.

k ≥

v
k
v
k
k
rv
k
k
v+ w
k

=

r

|

|k

v
k
v
k

k ≤ k

foreveryvinV andevery r in R.

+

w
k
k

forallvandwinV (triangleinequality).

Proof. Because
v, v
rem 10.1.1. As to (3), compute

=

k

v

k

h

p

, properties (1) and (2) follow immediately from (3) and (4) of Theo-
i

k
k
Hence (3) follows by taking positive square roots. Finally, the fact that
Schwarz inequality gives

k

h

i

i

h

2 =

rv
k

rv, rv

= r2

v, v

= r2

v

2

v, w

w

v
kk

k

i ≤ k

h

by the Cauchy-

v + w

k

2 =

k

h

v + w, v + w

=

i

k

v

v

k

2 + 2
2 + 2
+

h

v

k
w

kk
)2

k

k

≤ k
= (

k
v
k

k

v, w

+

w

2

k
+

i
w
k

k

k
w

2

k

Hence (4) follows by taking positive square roots.

It is worth noting that the usual triangle inequality for absolute values,

|
is a special case of (4) where V = R = R1 and the dot product

| ≤ |

|

|

|

r + s

r

+

s

for all real numbers r and s

r, s

= rs is used.

h

i

In many calculations in an inner product space, it is required to show that some vector v is zero. This

is often accomplished most easily by showing that its norm

is zero. Here is an example.

v

k

k

Example 10.1.9

v1, . . . , vn

Let
each i = 1, 2, . . . , n, show that v = 0.

{

}

be a spanning set for an inner product space V . If v in V satisﬁes

v, vi

h

i

= 0 for

536

Inner Product Spaces

Solution. Write v = r1v1 +
Compute:

· · ·

+ rnvn, ri in R. To show that v = 0, we show that

2 =

v

k

k

h

v, v

i

= 0.

h
by hypothesis, and the result follows.

i

h

v, v

=

v, r1v1 +

+ rnvn

= r1

v, v1

+

i

· · ·

+ rn

h

v, vn

i

h

i

= 0

· · ·

The norm properties in Theorem 10.1.5 translate to the following properties of distance familiar from

geometry. The proof is Exercise 10.1.21.

Theorem 10.1.6

LetV beaninnerproductspace.

1. d (v, w)

≥

0 forallv,winV.

2. d (v, w) = 0 ifandonlyifv= w.

3. d (v, w) = d (w, v) forallvandwinV.

4. d (v, w)

≤

d (v, u) + d (u, w) forallv,u,andwinV.

Exercises for 10.1

Exercise 10.1.1 In each case, determine which of ax-
ioms P1–P5 fail to hold.

a. V = R2,

b. V = R3,

(x1, y1), (x2, y2)
i
h

= x1y1x2y2

(x1, x2, x3), (y1, y2, y3)
i
h

= x1y1

−

x2y2 + x3y3

c. V = C,
tion

z, w
h

i

= zw, where w is complex conjuga-

p(x), q(x)
i

= p(1)q(1)

= det (AB)

d. V = P3,

h
e. V = M22,

A, B
h
f. V = F[0, 1],

i
f , g
i

h

= f (1)g(0) + f (0)g(1)

If
V is a subspace, show that U is an inner product

Exercise 10.1.2 Let V be an inner product space.
U
space using the same inner product.

⊆

Exercise 10.1.3 In each case, ﬁnd a scalar multiple of v
that is a unit vector.

a. v = f in C[0, 1] where f (x) = x2
1
0 f (x)g(x)dx

f , g
i

h

R

π, π] where f (x) = cos x

b. v = f in C[
f , g
i

h

−

π
π f (x)g(x)dx
−
R
1
3

in R2 where

c. v =

(cid:20)

d. v =

(cid:21)
3
1

v, w
h

i

= vT

1 1
1 2

−

1
2

w

w

(cid:21)

(cid:21)

(cid:20)
1
1

in R2,

v, w
i
h

= vT

(cid:20)

(cid:21)

−

(cid:20)

−

Exercise 10.1.4 In each case, ﬁnd the distance between
u and v.

a. u = (3,

−
b. u = (1, 2,

1, 2, 0), v = (1, 1, 1, 3);

1, 2), v = (2, 1,

−

−

u, v
h
1, 3);

i
u, v
h

= u

·
= u

i

v

v

·

g(x) = 1

c. u = f , v = g in C[0, 1] where f (x) = x2 and
1
0 f (x)g(x)dx
R
π, π] where f (x) = 1 and

f , g
i
d. u = f , v = g in C[

−

=

x;

h

g(x) = cos x;

f , g
i

h

−
=

π
π f (x)g(x)dx
−
R

Exercise 10.1.5 Let a1, a2, . . . , an be positive numbers.
Given v = (v1, v2, . . . , vn) and w = (w1, w2, . . . , wn),
+ anvnwn. Show that this is
deﬁne
i
an inner product on Rn.

= a1v1w1 +

v, w
h

· · ·

Exercise 10.1.6 If
v = v1b1 +
tors in V , deﬁne

· · ·

b1, . . . , bn
{

}

+ vnbn and w = w1b1 +

is a basis of V and if
+ wnbn are vec-

· · ·

v, w
i
h
Show that this is an inner product on V .

= v1w1 +

· · ·

+ vnwn.

Exercise 10.1.7 If p = p(x) and q = q(x) are polynomi-
als in Pn, deﬁne

p, q
i

= p(0)q(0) + p(1)q(1) +

+ p(n)q(n)

h

· · ·
Show that this is an inner product on Pn.
[Hint for P5: Theorem 6.5.4 or Appendix D.]

,

Exercise 10.1.8 Let Dn denote the space of all func-
to R with pointwise
1, 2, 3, . . . , n
tions from the set
}
{
addition and scalar multiplication (see Exercise 6.3.35).
is an inner product on Dn if
Show that
f, g
h
Exercise 10.1.9 Let re (z) denote the real part of the
complex number z. Show that
is an inner product on
C if

= f (1)g(1) + f (2)g(2) +

+ f (n)g(n).

= re (zw).

· · ·

h

i

i

h

i

,

z, w
i
h

Exercise 10.1.10 If T : V
inner product space V , show that

→

V is an isomorphism of the

v, w
1 =
i
h

T (v), T (w)
i
h

deﬁnes a new inner product

,

1 on V .
i

h

Exercise 10.1.11 Show that every inner product
i
on Rn has the form
(U y) for some upper
triangular matrix U with positive diagonal entries. [Hint:
Theorem 8.3.3.]

x, y
h

= (U x)

h

i

·

,

= vT Aw
Exercise 10.1.12 In each case, show that
deﬁnes an inner product on R2 and hence show that A is
positive deﬁnite.

v, w
h

i

a.

A =

c.

A =

2 1
1 1

3 2
2 3

(cid:20)

(cid:20)

(cid:21)

(cid:21)

b.

A =

d.

A =

5
3

−
3 4
4 6

(cid:20)

(cid:20)

3
−
2

(cid:21)

(cid:21)

Exercise 10.1.13 In each case, ﬁnd a symmetric matrix
A such that

= vT Aw.

10.1. Inner Products and Norms

537

v1
v2
v3

v1
v2
v3

w1
w2
w3

w1
w2
w3

,

,



















+





+



c.

d.

*


*


= 2v1w1 + v2w2 + v3w3

v1w2

−

v2w1 + v2w3 + v3w2

−

= v1w1 + 2v2w2 + 5v3w3

2v1w3

2v3w1

−

−

Exercise 10.1.14 If A is symmetric and xT Ax = 0 for
all columns x in Rn, show that A = 0. [Hint: Consider
x + y, x + y
h
i
Exercise 10.1.15 Show that the sum of two inner prod-
ucts on V is again an inner product.

= xT Ay.]

x, y
i
h

where

Exercise 10.1.16 Let
u, v
1,
i
h

u
k
k
= 0 and

=

−

u, w
i
h
v
i

−

a.

v + w, 2u
h

= √3,

k

= 1,

= 2,

w
k

v
k
k
v, w
= 3. Compute:
i
h
u
h

w, 3w

2v

−

−

b.

v
i

−

Exercise 10.1.17 Given the data in Exercise 10.1.16,
show that u + v = w.

Exercise 10.1.18 Show that no vectors exist such that
u, v
u
i
h
k
Exercise 10.1.19 Complete Example 10.1.2.

= 2, and

= 1,

v
k

=

−

3.

k

k

Exercise 10.1.20 Prove Theorem 10.1.1.

Exercise 10.1.21 Prove Theorem 10.1.6.

Exercise 10.1.22 Let u and v be vectors in an inner
product space V .

a. Expand

2u
h
3u
h
c. Show that

b. Expand

d. Show that

−

7v, 3u + 5v
.
i
4v, 5u + v
.
i
u
k
u
k

−
2 =
u + v
k
k
2 =
v
u
k
k

−

2 + 2
u, v
h
k
i
2
u, v
2
i
h
k

−

+

+

2.
v
k
k
2.
v
k
k

Exercise 10.1.23 Show that

v
k

2 +
k

2 = 1
w
k
k

2 {k

2 +
v + w
k

−
for any v and w in an inner product space.

v
k

2
w
k

}

,

Exercise 10.1.24 Let
be an inner product on a vec-
tor space V . Show that the corresponding distance func-
tion is translation invariant. That is, show that
d (v, w) = d (v + u, w + u) for all v, w, and u in V .

i

h

i

v, w
h
v1
v2

,

(cid:21)

(cid:20)

v1
v2

,

(cid:21)

(cid:20)

a.

b.

(cid:28)(cid:20)

(cid:28)(cid:20)

w1
w2

w1
w2

(cid:21)(cid:29)

(cid:21)(cid:29)

= v1w1 + 2v1w2 + 2v2w1 + 5v2w2

Exercise 10.1.25

= v1w1

v1w2

−

−

v2w1 + 2v2w2

a. Show that

u, v
i
h
u, v in an inner product space V .

= 1
2
u + v
4 [
k
k

− k

u

v

2] for all
k

−

538

Inner Product Spaces

b. If

,

,

h

h

i

and

i′ are two inner products on V that
have equal associated norm functions, show that
i′ holds for all u and v.
u, v
h

u, v
h

=

i

Exercise 10.1.26 Let v denote a vector in an inner prod-
uct space V .

Exercise 10.1.31

a. Show that AAT =

(cid:20)
b. Show that det (AAT )

u
v
·
2
v
k
k

.

(cid:21)

2
k
v
·

0.

u
k
u

≥

a. Show that W =
space of V .

w
{

|

w in V ,

v, w = 0
}
h

is a sub-

b. Let W be as in (a). If V = R3 with the dot product,

and if v = (1,

−

1, 2), ﬁnd a basis for W .

a. If v and w are nonzero vectors in an inner product
1
1, and hence

v, w
space V , show that
i
h
w
v
k ≤
kk
k
that a unique angle θ exists such that

≤

−

v, w
h
i
w
v
k
k
called the angle between v and w.

= cosθ and 0

≤

≤

θ

kk

π. This angle θ is

v, wi
h

= 0 for each i. Show that

Exercise 10.1.27 Given vectors w1, w2, . . . , wn and v,
assume that
= 0
for all w in span

.
}
v1, v2, . . . , vn
}
{
holds for each i. Show that v = w.

i
w1, w2, . . . , wn
{
Exercise 10.1.28
v, vi
=
h
Exercise 10.1.29 Use the Cauchy-Schwarz inequality in
an inner product space to show that:

If V = span

v, w
i
h

w, vi
h

and

i

i

u
k

a. If

1, then

u, v
h
k ≤
b. (x cosθ + y sinθ)2

v
≤ k

2 for all v in V .
k

2
i
x2 + y2 for all real x, y, and

θ.

≤

c.

r1v1 +
+
k
all vectors vi, and all ri > 0 in R.

+ rnvn

v1
k

[r1

· · ·

≤

k

2
k

· · ·

+ rn

vn
k

]2 for
k

Exercise 10.1.30 If A is a 2
note the rows of A.

×

n matrix, let u and v de-

10.2 Orthogonal Sets of Vectors

b. Find the angle between v = (1, 2,

1, 1 3) and

w = (2, 1, 0, 2, 0) in R5 with the dot product.

−

c. If θ is the angle between v and w, show that the

law of cosines is valid:

v
k

w
k

−

=

v
k

2 +
k

2
w
k
k

v
2
k

−

w

kk

k

cosθ.

Exercise 10.1.32 If V = R2, deﬁne

(x, y)
k
k

=

+

x
|
|

y
.
|
|

a. Show that
rem 10.1.5.

k · k

satisﬁes the conditions in Theo-

b. Show that

k · k

does not arise from an inner prod-
uct on R2 given by a matrix A. [Hint: If it did, use
Theorem 10.1.2 to ﬁnd numbers a, b, and c such
that

2 = ax2 + bxy + cy2 for all x and y.]
(x, y)
k
k

The idea that two lines can be perpendicular is fundamental in geometry, and this section is devoted to
introducing this notion into a general inner product space V . To motivate the deﬁnition, recall that two
nonzero geometric vectors x and y in Rn are perpendicular (or orthogonal) if and only if x
y = 0. In
general, two vectors v and w in an inner product space V are said to be orthogonal if

·

h
of vectors is called an orthogonal set of vectors if

i

v, w

= 0

A set

{

f1, f2, . . . , fn

}

1. Each fi

= 0.

2.

fi, f j

h

i

= 0 for all i

= j.

If, in addition,

fi

k

k

= 1 for each i, the set

f1, f2, . . . , fn

{

}

is called an orthonormal set.

6
6
10.2. Orthogonal Sets of Vectors

539

Example 10.2.1

sin x, cos x
}

{

is orthogonal in C[

π, π] because

−

π

π

Z

−

sin x cos x dx =

−
(cid:2)

π

π = 0
−

1
4 cos 2x
(cid:3)

The ﬁrst result about orthogonal sets extends Pythagoras’ theorem in Rn (Theorem 5.3.4) and the same

proof works.

Theorem 10.2.1: Pythagoras’ Theorem

If

f1, f2, . . . , fn
{

}

isanorthogonalsetofvectors,then

f1 + f2 +
k

· · ·

+ fn

k

2 =

f1
k

k

2 +

f2
k

k

2 +

+

2

fn
k

k

· · ·

The proof of the next result is left to the reader.

Theorem 10.2.2

Let

f1, f2, . . . , fn
{

}

beanorthogonalsetofvectors.

{

1.

2.

r1f1, r2f2, . . . , rnfn

}

isalsoorthogonalforany ri

= 0 in R.

f1,

1
f1
k

k

1
f2
k

k

f2, . . . ,

fn

1
fn
k

k

o

isanorthonormalset.

n

As before, the process of passing from an orthogonal set to an orthonormal one is called normalizing the
orthogonal set. The proof of Theorem 5.3.5 goes through to give

Theorem 10.2.3

Everyorthogonalsetofvectorsislinearlyindependent.

Example 10.2.2

,

2
1
−
0 






,

0
1
1 






0
1
−
2 


Show that










Solution. We have



= vT Aw, where A =

v, w

i

h

1 1 0
1 2 0
0 0 1 






is an orthogonal basis of R3 with inner product




,

2
1
−
0 






0
1
1 


+

*


=

2

1 0

−

(cid:2)

1 1 0
1 2 0
0 0 1 






0
1
1 






(cid:3)

=

1 0 0

(cid:2)

= 0

0
1
1 






(cid:3)

6
540

Inner Product Spaces

and the reader can verify that the other pairs are orthogonal too. Hence the set is orthogonal, so it
is linearly independent by Theorem 10.2.3. Because dim R3 = 3, it is a basis.

The proof of Theorem 5.3.6 generalizes to give the following:

Theorem 10.2.4: Expansion Theorem

Let

f1, f2, . . . , fn
{

}

beanorthogonalbasisofaninnerproductspaceV. IfvisanyvectorinV,then

2 fn
i
k
istheexpansionofvasalinearcombinationofthebasisvectors.

2 f2 +
i
k

v, f2
2 f1 + h
i
f2
k
k

v, f1
v= h
f1
k

v, fn
+ h
fn
k

· · ·

v, f1
The coefﬁcients h
f1
k

v, f2
2 in the expansion theorem are sometimes called the Fourier
2 , h
i
i
f2
k
k
k
coefﬁcients of v with respect to the orthogonal basis
. This is in honour of the French
mathematician J.B.J. Fourier (1768–1830). His original work was with a particular orthogonal set in the
space C[a, b], about which there will be more to say in Section 10.5.

f1, f2, . . . , fn

2 , . . . ,
i
k

v, fn
h
fn
k

{

}

Example 10.2.3

If a0, a1, . . . , an are distinct numbers and p(x) and q(x) are in Pn, deﬁne

p(x), q(x)

i

h

= p(a0)q(a0) + p(a1)q(a1) +

+ p(an)q(an)

· · ·

This is an inner product on Pn. (Axioms P1–P4 are routinely veriﬁed, and P5 holds because 0 is
the only polynomial of degree n with n + 1 distinct roots. See Theorem 6.5.4 or Appendix D.)
Recall that the Lagrange polynomials δ0(x), δ1(x), . . . , δn(x) relative to the numbers
a0, a1, . . . , an are deﬁned as follows (see Section 6.5):

δk(x) = ∏i
∏i

=k(x
−
=k(ak−

ai)
ai)

k = 0, 1, 2, . . . , n

where ∏i

=k(x

−

ai) means the product of all the terms

except that the kth term is omitted. Then

a0), (x

(x

−

,

because δk(ai) = 0 if i
h
so the expansion theorem gives

i

a1), (x

a2), . . . , (x

−

−

−
δ0(x), δ1(x), . . . , δn(x)
{

}

an)

= k and δk(ak) = 1. These facts also show that

is orthonormal with respect to
= p(ak)
p(x), δk(x)

h

i

p(x) = p(a0)δ0(x) + p(a1)δ1(x) +

+ p(an)δn(x)

· · ·

for each p(x) in Pn. This is the Lagrange interpolation expansion of p(x), Theorem 6.5.3, which
is important in numerical integration.

6
6
6
6
Lemma 10.2.1: Orthogonal Lemma

10.2. Orthogonal Sets of Vectors

541

f1, f2, . . . , fm
Let
{
vectornotin span

beanorthogonalsetofvectorsinaninnerproductspaceV,andletvbeany
}
f1, f2, . . . , fm
{

. Deﬁne
}

fm+1 = v

−

v, f1
h
f1
k

2 f1
i
k

−

v, f2
h
f2
k

2 f2
i
k

− · · · −

v, fm
h
fm
k

2 fm
i
k

Then

f1, f2, . . . , fm, fm+1
{

}

isanorthogonalsetofvectors.

The proof of this result (and the next) is the same as for the dot product in Rn (Lemma 8.1.1 and

Theorem 8.1.2).

Theorem 10.2.5: Gram-Schmidt Orthogonalization Algorithm

LetV beaninnerproductspaceandlet
f1, f2, . . . , fn inV successivelyasfollows:

v1, v2, . . . , vn
{

}

beanybasisofV. Deﬁnevectors

f1 = v1
f2 = v2
f3 = v3
...
fk = vk

−

−
...

−

v2, f1
2 f1
h
i
f1
k
k
v3, f1
2 f1
i
h
f1
k
k

v3, f2
h
f2
k

2 f2
i
k

−

vk, f1
h
f1
k

2 f1
i
k

−

vk, f2
h
f2
k

2 f2
i
k

− · · · −

vk, fk
1i
2 fk
h
−
fk
1k
k
−

1
−

foreach k = 2, 3, . . . , n. Then

1.

f1, f2, . . . , fn
{
2. span

isanorthogonalbasisofV.

}

f1, f2, . . . , fk
{

= span

v1, v2, . . . , vk
{

}

}

holdsforeach k = 1, 2, . . . , n.

The purpose of the Gram-Schmidt algorithm is to convert a basis of an inner product space into an or-
thogonal basis. In particular, it shows that every ﬁnite dimensional inner product space has an orthogonal
basis.

Example 10.2.4

Consider V = P3 with the inner product
is applied to the basis

1, x, x2, x3

h

p, q

=

i

{

, show that the result is the orthogonal basis

1

1 p(x)q(x)dx. If the Gram-Schmidt algorithm
−
R
1), 1

3x)

5 (5x3

−

}

}
1, x, 1

3(3x2

{

−
Solution. Take f1 = 1. Then the algorithm gives
x, f1
2 f1 = x
i
h
f1
k
k
x2, f1
f1
k
2
3
2 1

−
f3 = x2

2 f1
i
k

f2 = x

= x2

−

−

x

h

−

−

0
2
3

0
2 f1 = x
−
x2, f2
f2
k

2 f2
i
k

h

542

Inner Product Spaces

The veriﬁcation that f4 = 1

5(5x3

3x) is omitted.

−

= 1

3(3x2

1)

−

The polynomials in Example 10.2.4 are such that the leading coefﬁcient is 1 in each case. In other contexts
(the study of differential equations, for example) it is customary to take multiples p(x) of these polynomials
such that p(1) = 1. The resulting orthogonal basis of P3 is

and these are the ﬁrst four Legendre polynomials, so called to honour the French mathematician A. M.
Legendre (1752–1833). They are important in the study of differential equations.

1, x, 1

3(3x2

{

−

1), 1

5 (5x3

3x)

}

−

If V is an inner product space of dimension n, let E =

f1, f2, . . . , fn

(by Theorem 10.2.5). If v = v1f1 + v2f2 +
V , we have CE(v) =

vn

}
{
+ vnfn and w = w1f1 + w2f2 +
wn
w1 w2

T and CE(w) =

· · ·

be an orthonormal basis of V
+ wnfn are two vectors in

v1 v2
h∑

=

i

i

· · ·
vifi, ∑
j

(cid:2)
v, w

h

(cid:3)
w jf j

= ∑
i, j

i

viw j

h

(cid:2)
fi, f j

= ∑
i

i

· · ·
T . Hence

· · ·
viwi = CE(v)

(cid:3)

CE(w)

·

This shows that the coordinate isomorphism CE : V

Rn preserves inner products, and so proves

→

Corollary 10.2.1
IfV isany n-dimensionalinnerproductspace,thenV isisomorphicto Rn asinnerproductspaces.
Moreprecisely,if E isanyorthonormalbasisofV,thecoordinateisomorphism

CE : V

→

Rn satisﬁes

v, w
i
h

= CE(v)

CE(w)

·

forallvandwinV.

The orthogonal complement of a subspace U of Rn was deﬁned (in Chapter 8) to be the set of all vectors
in Rn that are orthogonal to every vector in U . This notion has a natural extension in an arbitrary inner
product space. Let U be a subspace of an inner product space V . As in Rn, the orthogonal complement
U ⊥ of U in V is deﬁned by

U ⊥ =

v

v

{

|

∈

V ,

h

v, u

i

= 0 for all u

U

}

∈

Theorem 10.2.6

LetU beaﬁnitedimensionalsubspaceofaninnerproductspaceV.

1. U ⊥ isasubspaceofV andV = U

U ⊥.

⊕

2. If dim V = n,then dim U + dim U ⊥ = n.

3. If dim V = n,thenU ⊥⊥ = U.

Proof.

10.2. Orthogonal Sets of Vectors

543

1. U ⊥ is a subspace by Theorem 10.1.1. If v is in U

rem 10.1.1. Hence U
show that v is in U +U ⊥, and this is clear if v is in U . If v is not in U , let
thogonal basis of U . Then the orthogonal lemma shows that v

= 0, so v = 0 again by Theo-
v, v
, and it remains to show that U +U ⊥ = V . Given v in V , we must
be an or-
}
v, fm
2 fm
+ h
i
fm
k
k

f1, f2, . . . , fm
{
v, f2
2 f2 +
2 f1 + h
i
i
f2
k
k
k

U ⊥, then

v, f1
h
f1
k

U ⊥ =

· · ·

−

∩

∩

0

}

{

h

i

is in U ⊥, so v is in U +U ⊥ as required.

(cid:16)

(cid:17)

2. This follows from Theorem 9.3.6.

3. We have dim U ⊥⊥ = n

dim U ⊥ = n

−
holds (verify), (3) follows by Theorem 6.4.2.

−

−

(n

dim U ) = dim U , using (2) twice. As U

U ⊥⊥ always

⊆

We digress brieﬂy and consider a subspace U of an arbitrary vector space V . As in Section 9.3, if W
W , then each vector v in V has a unique representation as a

is any complement of U in V , that is, V = U
sum v = u + w where u is in U and w is in W . Hence we may deﬁne a function T : V

⊕

V as follows:

→

T (v) = u where v = u + w, u in U , w in W

Thus, to compute T (v), express v in any way at all as the sum of a vector u in U and a vector in W ; then
T (v) = u.

This function T is a linear operator on V . Indeed, if v1 = u1 + w1 where u1 is in U and w1 is in W ,

then v + v1 = (u + u1) + (w + w1) where u + u1 is in U and w + w1 is in W , so

T (v + v1) = u + u1 = T (v) + T (v1)

Similarly, T (av) = aT (v) for all a in R, so T is a linear operator. Furthermore, im T = U and ker T = W
as the reader can verify, and T is called the projection on U with kernel W .

If U is a subspace of V , there are many projections on U , one for each complementary subspace W
W . If V is an inner product space, we single out one for special attention. Let U be a ﬁnite

with V = U
dimensional subspace of an inner product space V .

⊕

Deﬁnition 10.3 Orthogonal Projection on a Subspace

TheprojectiononU withkernelU ⊥ iscalledtheorthogonalprojectiononU (orsimplythe
projectiononU)andisdenoted projU : V
V.

→

Theorem 10.2.7: Projection Theorem

LetU beaﬁnitedimensionalsubspaceofaninnerproductspaceV andletvbeavectorinV.

1. projU : V

→

V isalinearoperatorwithimageU andkernelU ⊥.

2. projU visinU andv

−

projU visinU ⊥.

3. If

f1, f2, . . . , fm
{

}

isanyorthogonalbasisofU,then

v, f1
projU v= h
f1
k

v, f2
2 f1 + h
i
f2
k
k

2 f2 +
i
k

v, fm
+ h
fm
k

2 fm
i
k

· · ·

544

Inner Product Spaces

Proof. Only (3) remains to be proved. But since
is an orthogonal basis of U and since
projU v is in U , the result follows from the expansion theorem (Theorem 10.2.4) applied to the ﬁnite
dimensional space U .

f1, f2, . . . , fn

{

}

Note that there is no requirement in Theorem 10.2.7 that V is ﬁnite dimensional.

Example 10.2.5

v = v

projU v for all v

Let U be a subspace of the ﬁnite dimensional inner product space V . Show that
projU ⊥
−
Solution. We have V = U ⊥ ⊕
v = (v
p) + p where v
projU ⊥

U ⊥⊥ by Theorem 10.2.6. If we write p = projU v, then
p is in U ⊥ and p is in U = U ⊥⊥ by Theorem 10.2.7. Hence

−
p. See Exercise 8.1.7.

−
v = v

V .

∈

−

v

v

−

projU v

0

U

projU v

The vectors v, projU v, and v

projU v in Theorem 10.2.7 can be visu-
alized geometrically as in the diagram (where U is shaded and dim U = 2).
This suggests that projU v is the vector in U closest to v. This is, in fact,
the case.

−

Theorem 10.2.8: Approximation Theorem

LetU beaﬁnitedimensionalsubspaceofaninnerproductspaceV. IfvisanyvectorinV,then
projU visthevectorinU thatisclosesttov. Hereclosestmeansthat

foralluinU,u

= projU v.

v
k

−

projU v
k

<

v
k

−

u
k

Proof. Write p = projU v, and consider v
U , Pythagoras’ theorem gives

−

u = (v

−

p) + (p

−

u). Because v

p is in U ⊥ and p

u is in

−

−

u
k
= 0. The result follows.

−

v

k

2 =

v

k

p
k

−

2 +

p

k

u
k

−

2 >

v

k

−

2

p

k

because p

u

−

Example 10.2.6

Consider the space C[
f , g
product
approximates the absolute-value function f given by f (x) =

1 f (x)g(x)dx. Find the polynomial p = p(x) of degree at most 2 that best
−
R

1, 1] of real-valued continuous functions on the interval [

−

−

=

h

i

1

.

1, 1] with inner

x
|

|

Solution. Here we want the vector p in the subspace U = P2 of C[
Example 10.2.4 the Gram-Schmidt algorithm was applied to give an orthogonal basis
f1 = 1, f2 = x, f3 = 3x2
{
factor). Hence the required polynomial is

of P2 (where, for convenience, we have changed f3 by a numerical

1, 1] that is closest to f . In

−

−

}

1

6
6
10.2. Orthogonal Sets of Vectors

545

y

y = p(x)

-1

O

y = f (x)

x

1

p = projP2

f

f , f1
f , f2
f , f3
2 f1 + h
2 f2 + h
= h
i
i
f2
f1
f3
k
k
k
k
k
2f1 + 0f2 + 1/2
= 1
8/5 f3
16(5x2 + 1)
= 3

2 f3
i
k

The graphs of p(x) and f (x) are given in the diagram.

If polynomials of degree at most n are allowed in Example 10.2.6, the polynomial in Pn is projPn f ,
and it is calculated in the same way. Because the subspaces Pn get larger as n increases, it turns out that the
approximating polynomials projPn f get closer and closer to f . In fact, solving many practical problems
comes down to approximating some interesting vector v (often a function) in an inﬁnite dimensional inner
U2 are ﬁnite
product space V by vectors in ﬁnite dimensional subspaces (which can be computed). If U1
dimensional subspaces of V , then

⊆

by Theorem 10.2.8 (because projU1
to v than projU1
v, use it to construct a sequence of ﬁnite dimensional subspaces

v is a better approximation
v. Hence a general method in approximation theory might be described as follows: Given

v

projU2

v
k ≤ k

v

−

−
k
v lies in U1 and hence in U2). Thus projU2

k

projU1

v

U1

U2

U3

⊆

⊆

⊆ · · ·

v is a suitable ap-
of V in such a way that
proximation to v if k is large enough. For more information, the interested reader may wish to consult
Interpolation and Approximation by Philip J. Davis (New York: Blaisdell, 1963).

approaches zero as k increases. Then projUk

projUk

v
k

−

v

k

Exercises for 10.2

Use the dot product in Rn unless otherwise in-
structed.
Exercise 10.2.1 In each case, verify that B is an orthog-
onal basis of V with the given inner product and use the
expansion theorem to express v as a linear combination
of the basis vectors.

a
b

a. v =

(cid:20)
v, w
h

i

, B =

(cid:21)

(cid:21)
(cid:26)(cid:20)
= vT Aw where A =

−

1
1

, V = R2,

1
0
2 2
2 5

(cid:21)(cid:27)

(cid:21)

,

(cid:20)

(cid:20)

b. v =

, B =

a
b
c


















1
1
1

−

1
0
1

1
6
1

,







−



,









,










V = R3,

v, w
i
h

= vT Aw where A =

2 0 1
0 1 0
1 0 2



c. v = a + bx + cx2, B =

3x2
= p(0)q(0) + p(1)q(1) + p(

1 x, 2
{

−






, V = P2,
}
1)q(
1)
−

−

p, q
i

h

d. v =

(cid:20)

B =

a b
c d

,

(cid:21)
1 0
0 1

(cid:21)
X , Y
h

,

i

(cid:26)(cid:20)
V = M22,

1
0

0
1

0 1
1 0

,

(cid:20)

,

(cid:21)

(cid:20)

−

0 1
1 0

,

(cid:21)(cid:27)

(cid:20)
(cid:21)
−
= tr (XY T )

Let R3 have the inner product
Exercise 10.2.2
= 2xx′ + yy′ + 3zz′. In each case,
(x, y, z), (x′, y′, z′)
h
i
use the Gram-Schmidt algorithm to transform B into an
orthogonal basis.

546

Inner Product Spaces

a. B =

b. B =

(1, 1, 0), (1, 0, 1), (0, 1, 1)
{
}
1, 1), (1, 1, 0)
(1, 1, 1), (1,
}
{

−

Let M22 have the inner product
= tr (XY T ). In each case, use the Gram-Schmidt

Exercise 10.2.3
X , Y
h
algorithm to transform B into an orthogonal basis.

i

a. B =

b. B =

1 1
0 0

1 1
0 1

(cid:20)

(cid:21)

,

,

(cid:21)

(cid:20)

1 0
1 0

1 0
1 1

0 1
0 1

1 0
0 1

(cid:20)

(cid:21)

,

,

(cid:21)

(cid:20)

(cid:20)

(cid:21)

,

,

(cid:21)

(cid:20)

1 0
0 1

1 0
0 0

(cid:26)(cid:20)

(cid:26)(cid:20)

(cid:21)(cid:27)

(cid:21)(cid:27)

Exercise 10.2.4
process to convert the basis B =
thogonal basis of P2.

In each case, use the Gram-Schmidt
into an or-

1, x, x2
{

}

= p(0)q(0) + p(1)q(1) + p(2)q(2)

a.

b.

p, q
i
p, q
i

h

h

=

2
0 p(x)q(x)dx
R

Exercise 10.2.5 Show that
−
orthogonal basis of P2 with the inner product

1, x
{

−

1
2 , x2

x + 1
6 }

, is an

p, q
i

h

=

1

0
Z

p(x)q(x)dx

and ﬁnd the corresponding orthonormal basis.

Exercise 10.2.6
dim U and dim U ⊥.

In each case ﬁnd U ⊥ and compute

= p(0)q(0) + p(1)q(1) + p(2)q(2)

1, 2, 1),

−

3,

−

(1,

a. U = span

(1, 1, 2, 0), (3,
{
in R4
2, 1)
}
−
b. U = span
(1, 1, 0, 0)
}
{
in P2 with
1, x
}
{

c. U = span
p, q
i

h

d. U = span

in P2 with

x
}
{

h

in R4

1 0
0 1
(cid:26)(cid:20)
= tr (XY T )

e. U = span

X , Y
h

i

f. U = span

,

(cid:21)

(cid:20)

,

1 1
0 0

1 0
1 0

M22 with

(cid:26)(cid:20)
X , Y
h

i

(cid:21)
= tr (XY T )

(cid:20)

p, q
i
1 1
0 0

=

1
0 p(x)q(x)dx

R

(cid:21)(cid:27)

in M22 with

1 0
0 1

,

(cid:21)

(cid:20)

1 1
1 1

,
(cid:21)(cid:27)

(cid:21)

1 0
0 1

1
1

,

(cid:21)

(cid:20)

1
1

−

,

(cid:21)

(cid:20)

1 1
0 0

,

(cid:21)(cid:27)

a. U = span

A =

1
2

(cid:20)

(cid:26)(cid:20)
1
−
3

b. U = span

(cid:26)(cid:20)

2 1
3 2

(cid:21)

A =

(cid:20)

Exercise 10.2.8 In P2, let

p(x), q(x)
i

h

= p(0)q(0) + p(1)q(1) + p(2)q(2)

In each case ﬁnd the polynomial in U closest to f (x).

a. U = span

b. U = span

, f (x) = 1 + x2
1 + x, x2
}
{
1, 1 + x2
{

; f (x) = x
}

Exercise 10.2.9 Using the inner product given by
1
0 p(x)q(x)dx on P2, write v as the sum of a
p, q
h
i
vector in U and a vector in U ⊥.

=

R

a. v = x2, U = span

x + 1, 9x
{

−

b. v = x2 + 1, U = span

1, 2x
{

−

5
}

1
}

Exercise 10.2.10

a. Show that
2 =
u + v
k
k

u, v
{
}
2 +
u
k
k

is orthogonal if and only if
2.
v
k
k

b. If u = v = (1, 1) and w = (
1, 0), show that
−
2 +
2 =
u, v, w
w
v
u + v + w
k
}
{
k
k
k
k
is not orthogonal. Hence the converse to Pythago-
ras’ theorem need not hold for more than two vec-
tors.

2 but
k

2 +
k

u
k

Exercise 10.2.11 Let v and w be vectors in an inner
product space V . Show that:

1 0
1 1

,

(cid:21)

(cid:20)

in

(cid:21)(cid:27)

a. v is orthogonal to w if and only if

v + w
k
k

=

v
k

w
.
k

−

Exercise 10.2.7 Let
case ﬁnd the matrix in U closest to A.

X , Y
h

i

= tr (XY T ) in M22. In each

b. v + w and v
=

w
k

.
k

v
k

k

w are orthogonal if and only if

−

, show that
}

v, w
h

i

= h

v, f1

w, f1
2 +
ih
i
f1
k
k

· · ·

+ h

v, fn

w, fn
ih
2
fn
k
k

i

Exercise 10.2.12 Let U and W be subspaces of an n-
dimensional inner product space V . Suppose
= 0
for all u
W and dim U + dim W = n. Show
that U ⊥ = W .

u, v
i
h

U and w

∈

∈

Exercise 10.2.13 If U and W are subspaces of an inner
product space, show that (U +W )⊥ = U ⊥ ∩
Exercise 10.2.14 If X is any set of vectors in an inner
product space V , deﬁne

W ⊥.

X ⊥ =

v
{

|

v in V ,

v, x
h

i

= 0 for all x in X

}

a. Show that X ⊥ is a subspace of V .

U ⊥ =

b. If U = span

u1, u2, . . . , um
{
u1, . . . , um
}⊥.
{
Y , show that Y ⊥ ⊆
Y ⊥ = (X

X ⊥.

c. If X

⊆
d. Show that X ⊥ ∩

Y )⊥.

∪

Exercise 10.2.15 If dim V = n and w
that dim

v in V ,

= n

= 0 in V , show
1.

v, w
i
h

= 0
}

−

v
{

|

Exercise 10.2.16 If the Gram-Schmidt process is used
of V , show that
on an orthogonal basis
fk = vk holds for each k = 1, 2, . . . , n. That is, show
that the algorithm reproduces the same basis.

v1, . . . , vn
{

}

Exercise 10.2.17 If
f1, f2, . . . , fn
is orthonormal in
{
an inner product space of dimension n, prove that there
1, fn
are exactly two vectors fn such that
is an orthonormal basis.

f1, f2, . . . , fn
{

}

}

−

−

1

Exercise 10.2.18 Let U be a ﬁnite dimensional subspace
of an inner product space V , and let v be a vector in V .

a. Show that v lies in U if and only if v = projU (v).

b. If V = R3, show that (

5, 4,

−
1, 1, 1)
}

−
but that (

3) lies in
1, 0, 2)

−

span
(3,
{
does not.

−

2, 5), (

−

10.2. Orthogonal Sets of Vectors

547

Exercise 10.2.20 Let E =
thonormal basis of V .

f1, f2, . . . , fn
{

}

be an or-

a. Show that

V .

v, w
i
h

= CE(v)

·

CE(w) for all

v, w
i
h

in

n matrix, deﬁne

b. If P = [pi j] is an n
bi = pi1f1 +
B =
and only if P is an orthogonal matrix.

· · ·
b1, b2, . . . , bn
{

+ pinfn for each i. Show that
is an orthonormal basis if

×

}

Exercise 10.2.21 Let
sis of V . If v and w are in V , show that

f1, . . . , fn
{

}

be an orthogonal ba-

Exercise 10.2.22 Let
basis of V , and let v = v1f1 +
w = w1f1 +

· · ·
+ wnfn. Show that

f1, . . . , fn
{

}

+ vnfn and

· · ·

be an orthonormal

v, w
h

i

= v1w1 +

+ vnwn

· · ·

and

2 = v2
k
(Parseval’s formula).

v
k

1 +

+ v2
n

· · ·

Exercise 10.2.23 Let v be a vector in an inner product
space V .

a. Show that

v
k

holds for all ﬁnite di-
mensional subspaces U . [Hint: Pythagoras’ theo-
rem.]

projU v
k

k ≥ k

b. If

f1, f2, . . . , fm
{
Bessel’s inequality:

}

is any orthogonal set in V , prove

h

v, f1
f1
k

2
2 +
i
k

· · ·

+ h

v, fm
fm
k

v
i
2 ≤ k
k

2

2
k

Exercise 10.2.19 Let n
vectors in R3 (as in Chapter 4).

= 0 and w

= 0 be nonparallel

a. Show that

n, n

nal basis of R3.

n

w, w

×

w
n
2 n
·
n
k
k

−

o

is an orthogo-

Exercise 10.2.24 Let B =
be an orthog-
onal basis of an inner product space V . Given v
V ,
let θi be the angle between v and fi for each i (see Exer-
cise 10.1.31). Show that

f1, f2, . . . , fn
{

∈

}

cos2 θ1 + cos2 θ2 +

+ cos2 θn = 1

· · ·

b. Show that span

w
n
2 n
·
n
k
k
through the origin with normal n.
n

w, w

×

−

n

is the plane

o

[The cosθi are called direction cosines for v correspond-
ing to B.]

6
6
6
548

Inner Product Spaces

Exercise 10.2.25

a. Let S denote a set of vectors in a ﬁnite dimen-
sional inner product space V , and suppose that
= 0 for all u in S implies v = 0. Show
u, v
h
that V = span S.
[Hint: Write U = span S and
use Theorem 10.2.6.]

i

b. Let A1, A2, . . . , Ak be n

n matrices. Show that

the following are equivalent.

×

i. If Aib = 0 for all i (where b is a column in

Rn), then b = 0.

Deﬁne componentwise addition and scalar multiplication
on V as follows:

[xi) + [yi) = [xi + yi), and a[xi) = [axi) for a in R.

Given [xi) and [yi) in V , deﬁne

[xi),
h

[yi)
i

=

(Note that this makes sense since only ﬁnitely many xi
and yi are nonzero.) Finally deﬁne

∞
∑
i=0

xiyi.

U =

[xi) in V
{

|

∞
∑
i=0

xi = 0
}

a. Show that V is a vector space and that U is a sub-

ii. The set of all rows of the matrices Ai spans

Rn.

space.

Exercise 10.2.26 Let [xi) = (x1, x2, . . . ) denote a se-
quence of real numbers xi, and let

b. Show that

,

h

i

is an inner product on V .

c. Show that U ⊥ =

0
{

.
}

V =

[xi)
{

|

only ﬁnitely many xi

= 0
}

d. Hence show that U

U ⊥ 6

⊕

= V and U

= U ⊥⊥.

10.3 Orthogonal Diagonalization

There is a natural way to deﬁne a symmetric linear operator T on a ﬁnite dimensional inner product
space V . If T is such an operator, it is shown in this section that V has an orthogonal basis consisting of
eigenvectors of T . This yields another proof of the principal axes theorem in the context of inner product
spaces.

Theorem 10.3.1

Let T : V
areequivalent.

→

V bealinearoperatoronaﬁnitedimensionalspaceV. Thenthefollowingconditions

1. V hasabasisconsistingofeigenvectorsof T.

2. Thereexistsabasis B ofV suchthat MB(T ) isdiagonal.

Proof. We have MB(T ) =
basis of V . By comparing columns:

CB[T (b1)] CB[T (b2)]

(cid:2)

CB[T (bn)]

where B =

b1, b2, . . . , bn

is any

}

{

· · ·

(cid:3)

MB(T ) = 





Theorem 10.3.1 follows.

0
λ1
0 λ2
...
...
0
0

· · ·
· · ·

· · ·

0
0
...
λn








if and only if T (bi) = λibi for each i

6
6
10.3. Orthogonal Diagonalization

549

Deﬁnition 10.4 Diagonalizable Linear Operators

Alinearoperator T onaﬁnitedimensionalspaceV iscalleddiagonalizableifV hasabasis
consistingofeigenvectorsof T.

Example 10.3.1

Let T : P2

→

P2 be given by

T (a + bx + cx2) = (a + 4c)

2bx + (3a + 2c)x2

−

Find the eigenspaces of T and hence ﬁnd a basis of eigenvectors.

Solution. If B0 =

1, x, x2

{

, then

}

MB0(T ) =

1
0
3





−

0 4
2 0
0 2 


so cT (x) = (x + 2)2(x

5), and the eigenvalues of T are λ =
−
1
0
1 


−

is a basis of eigenvectors of MB0(T ), so B =

,

4
0
3 








basis of P2 consisting of eigenvectors of T .








−







,

0
1
0 


2 and λ = 5. One sees that

x, 4

{

−

3x2, 1 + x2

is a

}

If V is an inner product space, the expansion theorem gives a simple formula for the matrix of a linear

operator with respect to an orthogonal basis.

Theorem 10.3.2

Let T : V
orthogonalbasisofV,then

→

V bealinearoperatoronaninnerproductspaceV. If B =

b1, b2, . . . , bn
{

}

isan

MB(T ) =

bi, T (bj)
h
bi
k

2
k

i

h

i

Proof. Write MB(T ) =

ai j

. The jth column of MB(T ) is CB[T (e j)], so

· · ·
On the other hand, the expansion theorem (Theorem 10.2.4) gives

· · ·

(cid:2)

(cid:3)

T (b j) = a1 jb1 +

+ ai jbi +

+ an jbn

bi, v
2 bi +
+ h
i
bi
k
k
for any v in V . The result follows by taking v = T (b j).

b1, v
2 b1 +
v = h
i
b1
k
k

· · ·

bn, v
2 bn
+ h
i
bn
k
k

· · ·

550

Inner Product Spaces

Example 10.3.2

Let T : R3

→

R3 be given by

T (a, b, c) = (a + 2b

c, 2a + 3c,

−

a + 3b + 2c)

−

If the dot product in R3 is used, ﬁnd the matrix of T with respect to the standard basis
where e1 = (1, 0, 0), e2 = (0, 1, 0), e3 = (0, 0, 1).
B =

e1, e2, e3

{

}

Solution. The basis B is orthonormal, so Theorem 10.3.2 gives

MB(T ) =

e1
e2
e3



T (e1) e1
T (e1) e2
T (e1) e3

T (e2) e1
T (e2) e2
T (e2) e3

·
·
·

·
·
·


Of course, this can also be found in the usual way.

T (e3)
T (e3)
T (e3)

·
·
·

1 2
2 0
1 3

−

1
−
3
2 


=









It is not difﬁcult to verify that an n

n matrix A is symmetric if and only if x

all columns x and y in Rn. The analog for operators is as follows:

×

(Ay) = (Ax)

y holds for

·

·

Theorem 10.3.3

LetV beaﬁnitedimensionalinnerproductspace. Thefollowingconditionsareequivalentfora
linearoperator T : V

V.

→

1.

v, T (w)
h

i

=

T (v), w
i

h

forallvandwinV.

2. Thematrixof T issymmetricwithrespecttoeveryorthonormalbasisofV.

3. Thematrixof T issymmetricwithrespecttosomeorthonormalbasisofV.

4. Thereisanorthonormalbasis B =

holdsforall i and j.

f1, f2, . . . , fn
{

}

ofV suchthat

fi, T (fj)
h

i

=

h

T (fi), fj

i

Proof. (1)
ai j =

⇒
fi, T (f j)

h

i

(2). Let B =

f1, . . . , fn

be an orthonormal basis of V , and write MB(T ) =

ai j

. Then

by Theorem 10.3.2. Hence (1) and axiom P2 give

{

}

(cid:2)

(cid:3)

for all i and j. This shows that MB(T ) is symmetric.

ai j =

h

fi, T (f j)

=

i

h

T (fi), f j

=

h

i

f j, T (fi)

= a ji

i

(3). This is clear.
(4). Let B =

f1, . . . , fn
=

fi, T (f j)

{

i

h

and Theorem 10.3.2,

(2)
(3)

⇒
⇒

(4)

⇒

(1). Let v and w be vectors in V and write them as v =

be an orthonormal basis of V such that MB(T ) is symmetric. By (3)
}
f j, T (fi)
h

for all i and j, so (4) follows from axiom P2.
n
∑
i=1

vifi and w =

w jf j. Then

n
∑
j=1

i

v, T (w)

h

=

i

∑
i

*

vifi, ∑
j

w jT f j

= ∑
i

∑
j

viw j

h

+

fi, T (f j)

i

10.3. Orthogonal Diagonalization

551

= ∑
i

∑
j

viw j

h

T (fi), f j

i

=

=

∑
i

viT (fi), ∑
j

w jf j

+

*
T (v), w

h

i

where we used (4) at the third stage. This proves (1).

A linear operator T on an inner product space V is called symmetric if
all v and w in V .

h

v, T (w)

T (v), w

=

i

h

i

holds for

Example 10.3.3

n matrix, let TA : Rn

If A is an n
columns v. If the dot product is used in Rn, then TA is a symmetric operator if and only if A is a
symmetric matrix.

Rn be the matrix operator given by TA(v) = Av for all

→

×

Solution. If E is the standard basis of Rn, then E is orthonormal when the dot product is used. We
have ME(TA) = A (by Example 9.1.4), so the result follows immediately from part (3) of
Theorem 10.3.3.

It is important to note that whether an operator is symmetric depends on which inner product is being

used (see Exercise 10.3.2).

If V is a ﬁnite dimensional inner product space, the eigenvalues of an operator T : V

V are the
same as those of MB(T ) for any orthonormal basis B (see Theorem 9.3.3). If T is symmetric, MB(T ) is a
symmetric matrix and so has real eigenvalues by Theorem 5.5.7. Hence we have the following:

→

Theorem 10.3.4

Asymmetriclinearoperatoronaﬁnitedimensionalinnerproductspacehasrealeigenvalues.

If U is a subspace of an inner product space V , recall that its orthogonal complement is the subspace

U ⊥ of V deﬁned by

Theorem 10.3.5

U ⊥ =

v in V

{

v, u
i

| h

= 0 for all u in U

}

Let T : V
T-invariantsubspaceofV. Then:

→

V beasymmetriclinearoperatoronaninnerproductspaceV,andletU bea

1. Therestrictionof T toU isasymmetriclinearoperatoronU.

2. U ⊥ isalso T-invariant.

Proof.

552

Inner Product Spaces

1. U is itself an inner product space using the same inner product, and condition 1 in Theorem 10.3.3

that T is symmetric is clearly preserved.

2. If v is in U ⊥, our task is to show that T (v) is also in U ⊥; that is,
u is in U , then T (u) also lies in U because U is T -invariant, so

h

T (v), u

i

= 0 for all u in U . But if

using the symmetry of T and the deﬁnition of U ⊥.

T (v), u
i

h

=

h

v, T (u)

i

The principal axes theorem (Theorem 8.2.2) asserts that an n

n matrix A is symmetric if and only if
Rn has an orthogonal basis of eigenvectors of A. The following result not only extends this theorem to an
arbitrary n-dimensional inner product space, but the proof is much more intuitive.

×

Theorem 10.3.6: Principal Axes Theorem

Thefollowingconditionsareequivalentforalinearoperator T onaﬁnitedimensionalinner
productspaceV.

1. T issymmetric.

2. V hasanorthogonalbasisconsistingofeigenvectorsof T.

⇒

Proof. (1)
(2). Assume that T is symmetric and proceed by induction on n = dim V . If n = 1, every
nonzero vector in V is an eigenvector of T , so there is nothing to prove. If n
2, assume inductively
that the theorem holds for spaces of dimension less than n. Let λ1 be a real eigenvalue of T (by Theo-
rem 10.3.4) and choose an eigenvector f1 corresponding to λ1. Then U = Rf1 is T -invariant, so U ⊥ is
also T -invariant by Theorem 10.3.5 (T is symmetric). Because dim U ⊥ = n
1 (Theorem 10.2.6), and
because the restriction of T to U ⊥ is a symmetric operator (Theorem 10.3.5), it follows by induction that
U ⊥ has an orthogonal basis
is an orthogonal
{
basis of V , which proves (2).

of eigenvectors of T . Hence B =

f1, f2, . . . , fn

f2, . . . , fn

≥

−

{

}

}

(2)

(1). If B =
symmetric by Theorem 10.3.3.

f1, . . . , fn

⇒

{

is a basis as in (2), then MB(T ) is symmetric (indeed diagonal), so T is

}

n symmetric matrix, then TA : Rn

The matrix version of the principal axes theorem is an immediate consequence of Theorem 10.3.6. If A
Rn is a symmetric operator, so let B be an orthonormal basis
is an n
of Rn consisting of eigenvectors of TA (and hence of A). Then PT AP is diagonal where P is the orthogonal
matrix whose columns are the vectors in B (see Theorem 9.2.4).

→

×

→

Similarly, let T : V

V be a symmetric linear operator on the n-dimensional inner product space V
and let B0 be any convenient orthonormal basis of V . Then an orthonormal basis of eigenvectors of T can
be computed from MB0(T ). In fact, if PT MB0(T )P is diagonal where P is orthogonal, let B =
f1, . . . , fn
}
be the vectors in V such that CB0(f j) is column j of P for each j. Then B consists of eigenvectors of T by
Theorem 9.3.3, and they are orthonormal because B0 is orthonormal. Indeed

{

holds for all i and j, as the reader can verify. Here is an example.

fi, f j

h

i

= CB0(fi)

CB0(f j)

·

10.3. Orthogonal Diagonalization

553

Example 10.3.4

Let T : P2

→

P2 be given by

T (a + bx + cx2) = (8a

2b + 2c) + (

2a + 5b + 4c)x + (2a + 4b + 5c)x2

Using the inner product
and ﬁnd an orthonormal basis of P2 consisting of eigenvectors.

h

i

−

−
a + bx + cx2, a′ + b′x + c′x2

= aa′ + bb′ + cc′, show that T is symmetric

Solution. If B0 =

1, x, x2

{

}

, then MB0(T ) =

8
2
−
2



is symmetric, so T is symmetric.

This matrix was analyzed in Example 8.2.5, where it was found that an orthonormal basis of

2 1 2
2
eigenvectors is
−
−
orthonormal, the corresponding orthonormal basis of P2 is

. Because B0 is

T , 1
3

T , 1
3

1 2

1
3

T

(cid:3)

(cid:2)

o

(cid:3)

n

(cid:2)

−

2 2
5 4
4 5 

2 2 1

(cid:3)

(cid:2)
2x2), 1

3(2 + x + 2x2), 1
3 (

2 + 2x + x2)

−

(cid:9)

B =

1
3(1 + 2x

−

(cid:8)

Exercises for 10.3

Exercise 10.3.1 In each case, show that T is symmetric
by calculating MB(T ) for some orthonormal basis B.

[Hint: Check that B =
thonormal basis.]

(1, 0), (1,
{

1)
}

−

is an or-

a. T : R3

R3;

→
T (a, b, c) = (a
uct

2b,

−

−

2a+2b+2c, 2b

c); dot prod-

−

b. T : M22

M22;

→

T

a b
c d
inner product:

(cid:20)

(cid:21)

=

a

d

b
c
a + 2c b + 2d

−

−

;

(cid:21)

(cid:20)

x′
y′
z′ w′ (cid:21)(cid:29)

(cid:20)

= xx′ + yy′ + zz′ + ww′

y
x
z w

→

,

(cid:21)
P2;

(cid:28)(cid:20)
c. T : P2

T (a + bx + cx2) = (b + c) + (a + c)x + (a + b)x2;
inner product:
a + bx + cx2, a′ + b′x + c′x2
h

= aa′ + bb′ + cc′

i

Exercise 10.3.2 Let T : R2

R2 be given by

→

T (a, b) = (2a + b, a

b).

−

Exercise 10.3.3 Let T : R2

R2 be given by

→

T (a, b) = (a

Use the dot product in R2.

b, b

a)

−

−

a. Show that T is symmetric.

b. Show that MB(T ) is not symmetric if the orthogo-
is used. Why does
(1, 0), (0, 2)
}
{

nal basis B =
this not contradict Theorem 10.3.3?

Exercise 10.3.4 Let V be an n-dimensional inner prod-
uct space, and let T and S denote symmetric linear oper-
ators on V . Show that:

a. The identity operator is symmetric.

b. rT is symmetric for all r in R.

c. S + T is symmetric.

a. Show that T is symmetric if the dot product is

d. If T is invertible, then T −

1 is symmetric.

used.

b. Show that T is not symmetric if
1 1
1 2

where A =

.

(cid:20)

(cid:21)

x, y
h

i

= xAyT ,

e. If ST = T S, then ST is symmetric.

Exercise 10.3.5 In each case, show that T is symmetric
and ﬁnd an orthonormal basis of eigenvectors of T .

554

Inner Product Spaces

a. T : R3

R3;

→

T (a, b, c) = (2a + 2c, 3b, 2a + 5c); use the dot
product

b. T : R3

R3;

→
T (a, b, c) = (7a
product

b,

−

−

a + 7b, 2c); use the dot

V is symmetric, write

→
. Show that
}

If T : V
T (v) is in W

Exercise 10.3.9
1(W ) =
v
T −
|
{
1(U ⊥) holds for every subspace U of V .
T (U )⊥ = T −
M22 be deﬁned by
Exercise 10.3.10 Let T : M22
→
T (X ) = PX Q, where P and Q are nonzero 2
2 matri-
= tr (XY T ). Show that
X , Y
ces. Use the inner product
h
T is symmetric if and only if either P and Q are both sym-

×

i

c. T : P2

P2;

→

T (a + bx + cx2) = 3b + (3a + 4c)x + 4bx2;
inner product
a + bx + cx2, a′ + b′x + c′x2
h
d. T : P2

i

= aa′ + bb′ + cc′

→

P2;
T (a + bx + cx2) = (c
product as in part (c)

a) + 3bx + (a

−

−

c)x2; inner

Rn
Exercise 10.3.6 If A is any n
be given by TA(x) = Ax. Suppose an inner product on Rn
= xT Py, where P is a positive deﬁnite
is given by
matrix.

n matrix, let TA : Rn

x, y
i
h

→

×

a. Show that TA is symmetric if and only if

PA = AT P.

b. Use part (a) to deduce Example 10.3.3.

Exercise 10.3.7 Let T : M22
→
T (X ) = AX , where A is a ﬁxed 2

M22 be given by

2 matrix.

×

a. Compute MB(T ), where

B =

1 0
0 0
(cid:20)
Note the order!

(cid:26)(cid:20)

(cid:21)

,

0 0
1 0

,

(cid:21)

(cid:20)

0 1
0 0

,

(cid:21)

(cid:20)

0 0
0 1

.
(cid:21)(cid:27)

b. Show that cT (x) = [cA(x)]2.

c. If the inner product on M22 is

= tr (XY T ),
show that T is symmetric if and only if A is a sym-
metric matrix.

X , Y
h

i

Exercise 10.3.8 Let T : R2

R2 be given by

→

T (a, b) = (b

a, a + 2b)

−

Show that T is symmetric if the dot product is used in R2
but that it is not symmetric if the following inner product
is used:

= xAyT , A =

x, y
h

i

1
1

−

(cid:20)

1
−
2

(cid:21)

metric or both are scalar multiples of

If B is as in part (a) of Exercise 10.3.7, then

0 1
1 0

. [Hint:

(cid:21)

(cid:20)

−

aP cP
bP dP

(cid:21)

in block form, where

MB(T ) =

(cid:20)
a b
c d

Q =

(cid:20)
If B0 =

(cid:26)(cid:20)

.

(cid:21)
1 0
0 0

then MB(T ) =

,

(cid:21)
(cid:20)
pQT
rQT

0 1
0 0
qQT
sQT

,

(cid:21)

0 0
1 0

,

(cid:21)

(cid:20)
(cid:20)
, where P =

0 0
0 1

,
(cid:21)(cid:27)
.

(cid:21)

p q
s
r

Use the fact that cP = bPT

(cid:20)

(cid:21)
(c2

(cid:20)
b2)P = 0.]

⇒

−

Exercise 10.3.11 Let T : V
W be any linear transfor-
d1, . . . , dm
mation and let B =
}
{
be bases of V and W , respectively. If W is an inner prod-
uct space and D is orthogonal, show that

→
b1, . . . , bn
{

and D =

}

MDB(T ) =

h

di, T (b j)
di
k

2
k

i

h

i

This is a generalization of Theorem 10.3.2.

Exercise 10.3.12 Let T : V
V be a linear operator on
an inner product space V of ﬁnite dimension. Show that
the following are equivalent.

→

1.

v, T (w)
i
h

=

T (v), w
i

−h

for all v and w in V .

2. MB(T ) is skew-symmetric for every orthonormal

basis B.

3. MB(T ) is skew-symmetric for some orthonormal

basis B.

Such operators T are called skew-symmetric opera-

tors.

Exercise 10.3.13 Let T : V
an n-dimensional inner product space V .

→

V be a linear operator on

a. Show that T is symmetric if and only if it satisﬁes

the following two conditions.

i. cT (x) factors completely over R.
ii. If U is a T -invariant subspace of V , then U ⊥

is also T -invariant.

→

b. Using the standard inner product on R2, show that
R2 with T (a, b) = (a, a + b) satisﬁes

T : R2
condition (i) and that S : R2
S(a, b) = (b,
a) satisﬁes condition (ii), but that
neither is symmetric. (Example 9.3.4 is useful for
S.)

R2 with

→

−

[Hint for part (a): If conditions (i) and (ii) hold,
proceed by induction on n. By condition (i), let
e1 be an eigenvector of T . If U = Re1, then U ⊥
is T -invariant by condition (ii), so show that the
restriction of T to U ⊥ satisﬁes conditions (i) and
(ii). (Theorem 9.3.1 is helpful for part (i)). Then
apply induction to show that V has an orthogonal
basis of eigenvectors (as in Theorem 10.3.6)].

Exercise 10.3.14 Let B =
be an or-
thonormal basis of an inner product space V . Given
T : V

f1, f2, . . . , fn
{

V , deﬁne T ′ : V

V by

}

→
f2 +
v, T (f2)
i
h

+

fn
v, T (fn)
i
h

· · ·

→
T ′(v) =

f1 +
v, T (f1)
h
i
n
∑
i=1h

fi
v, T (fi)
i

=

a. Show that (aT )′ = aT ′.

b. Show that (S + T )′ = S′ + T ′.

c. Show that MB(T ′) is the transpose of MB(T ).

d. Show that (T ′)′ = T , using part (c).
MB(S) = MB(T ) implies that S = T .]

[Hint:

e. Show that (ST )′ = T ′S′, using part (c).

f. Show that T is symmetric if and only if

T = T ′.
Theorem 10.3.3.]

[Hint: Use the expansion theorem and

10.4 Isometries

10.4. Isometries

555

g. Show that T + T ′ and T T ′ are symmetric, using

parts (b) through (e).

h. Show that T ′(v) is independent of the choice of
g1, . . . , gn}
{

orthonormal basis B. [Hint: If D =
is also orthonormal, use the fact that

fi =

n
∑
j=1h

fi, g ji

g j for each i.]

Exercise 10.3.15 Let V be a ﬁnite dimensional inner
product space. Show that the following conditions are
equivalent for a linear operator T : V

V .

→

1. T is symmetric and T 2 = T .

2. MB(T ) =

of V .

(cid:20)

Ir 0
0 0

(cid:21)

for some orthonormal basis B

An operator is called a projection if it satisﬁes
these conditions. [Hint: If T 2 = T and T (v) = λv,
apply T to get λv = λ2v. Hence show that 0, 1 are
the only eigenvalues of T .]

Exercise 10.3.16 Let V denote a ﬁnite dimensional in-
ner product space. Given a subspace U , deﬁne
projU : V

V as in Theorem 10.2.7.

→

a. Show that projU is a projection in the sense of

Exercise 10.3.15.

b. If T is any projection, show that T = projU ,
[Hint: Use T 2 = T to show
where U = im T .
ker T and T (u) = u for all u in
that V = im T
im T . Use the fact that T is symmetric to show that
ker T
( im T )⊥ and hence that these are equal
because they have the same dimension.]

⊕

⊆

We saw in Section 2.6 that rotations about the origin and reﬂections in a line through the origin are linear
operators on R2. Similar geometric arguments (in Section 4.4) establish that, in R3, rotations about a line
through the origin and reﬂections in a plane through the origin are linear. We are going to give an algebraic
proof of these results that is valid in any inner product space. The key observation is that reﬂections and
rotations are distance preserving in the following sense. If V is an inner product space, a transformation
V (not necessarily linear) is said to be distance preserving if the distance between S(v) and S(w)
S : V
is the same as the distance between v and w for all vectors v and w; more formally, if

→

S(v)

S(w)

=

w

v

k

−

k

k

−

k

for all v and w in V

(10.2)

556

Inner Product Spaces

Distance-preserving maps need not be linear. For example, if u is any vector in V , the transformation
V deﬁned by Su(v) = v + u for all v in V is called translation by u, and it is routine to verify that
Su : V
Su is distance preserving for any u. However, Su is linear only if u = 0 (since then Su(0) = 0). Remarkably,
distance-preserving operators that do ﬁx the origin are necessarily linear.

→

Lemma 10.4.1

LetV beaninnerproductspaceofdimension n,andconsideradistance-preservingtransformation
S : V

V. If S(0) = 0,then S islinear.

→

Proof. We have

S(v)

k

−

S(w)

k

2 =

v

−

k
k
S(v), S(w)

Now let
(10.3) and so is a basis because dim V = n. Now compute:

f1, f2, . . . , fn

}

{

h

i
be an orthonormal basis of V . Then

h

i

w

2 for all v and w in V by (10.2), which gives

=

v, w

for all v and w in V

(10.3)

S(f1), S(f2), . . . , S(fn)

is orthonormal by

}

{

S(v + w)

S(v)

−

−

h

S(w), S(fi)

i

S(v + w), S(fi)
v + w, fi

v, fi

i − h

S(v), S(fi)
w, fi

i − h

i − h

i

=
h
=
h
= 0

S(w), S(fi)

i

i − h

S(w) = 0; that
for each i. It follows from the expansion theorem (Theorem 10.2.4) that S(v + w)
is, S(v + w) = S(v) + S(w). A similar argument shows that S(av) = aS(v) holds for all a in R and v in V ,
so S is linear after all.

S(v)

−

−

Deﬁnition 10.5 Isometries

Distance-preservinglinearoperatorsarecalledisometries.

It is routine to verify that the composite of two distance-preserving transformations is again distance
preserving. In particular the composite of a translation and an isometry is distance preserving. Surpris-
ingly, the converse is true.

Theorem 10.4.1

IfV isaﬁnitedimensionalinnerproductspace,theneverydistance-preservingtransformation
S : V

V isthecompositeofatranslationandanisometry.

→

Proof. If S : V
all v in V . Then
k
is distance preserving. Clearly, T (0) = 0, so it is an isometry by Lemma 10.4.1. Since

V is distance preserving, write S(0) = u and deﬁne T : V
T (v)

u for
for all vectors v and w in V as the reader can verify; that is, T

V by T (v) = S(v)

→
k

T (w)

→

−

−

−

=

w

k

v

k

S(v) = u + T (v) = (Su

T )(v)

for all v in V

◦

we have S = Su

◦

T , and the theorem is proved.

In Theorem 10.4.1, S = Su
is true: this factorization is unique in that u and T are uniquely determined by S; and w

T factors as the composite of an isometry T followed by a translation Su. More
V exists such

◦

∈

10.4. Isometries

557

that S = T
10.4.12).

◦

Sw is uniquely the composite of translation by w followed by the same isometry T (Exercise

Theorem 10.4.1 focuses our attention on the isometries, and the next theorem shows that, while they

preserve distance, they are characterized as those operators that preserve other properties.

Theorem 10.4.2

V bealinearoperatoronaﬁnitedimensionalinnerproductspaceV.

Let T : V
Thefollowingconditionsareequivalent:
1. T isanisometry.

→

forallvinV.

v, w
i
h

2.

3.

h
4. If

=

v
T (v)
k
k
k
k
T (v), T (w)

=
f1, f2, . . . , fn
{
then

i

{

}
T (f1), T (f2), . . . , T (fn)

}

forallvandwinV.

(T preservesinnerproducts)

isanorthonormalbasisofV,

isalsoanorthonormalbasis. (T preservesorthonormalbases)

5. T carriessomeorthonormalbasistoanorthonormalbasis.

(T preservesdistance)

(T preservesnorms)

Proof. (1)
(2)
(3)

⇒
⇒

(2). Take w = 0 in (10.2).
⇒
(3). Since T is linear, (2) gives
(4). By (3),

T (v)
−
T (f1), T (f2), . . . , T (fn)
}

k

{
basis because dim V = n.

2 =
T (w)
−
is orthogonal and

T (v

k

k

w)

2 =
k
T (fi)

v
−
k
2 =
k

w
fi

2. Now (3) follows.
k
2 = 1. Hence it is a
k

k

k

(4)
(5)

⇒
⇒

(5). This needs no proof.
(1). By (5), let

f1, . . . , fn

{

}

orthonormal. Given v = v1f1 +
theorem gives

· · ·

be an orthonormal basis of V such that

+ vnfn in V , we have T (v) = v1T (f1) +

T (f1), . . . , T (fn)
is also
{
+ vnT (fn) so Pythagoras’
· · ·

}

Hence

T (v)

=

v
k

k

k

k

T (v)

2 = v2

1 +

+ v2

n =

k
k
for all v, and (1) follows by replacing v by v

· · ·

k

2

v
k

w.

−

Before giving examples, we note some consequences of Theorem 10.4.2.

Corollary 10.4.1

LetV beaﬁnitedimensionalinnerproductspace.

1. EveryisometryofV isanisomorphism.5

2.

a. 1V : V
b. ThecompositeoftwoisometriesofV isanisometry.

V isanisometry.

→

c. TheinverseofanisometryofV isanisometry.

Proof. (1) is by (4) of Theorem 10.4.2 and Theorem 7.3.1. (2a) is clear, and (2b) is left to the reader. If
1
T : V
is an orthonormal basis of V , then (2c) follows because T −
V is an isometry and
carries the orthonormal basis

f1, . . . , fn
{
T (f1), . . . , T (fn)

f1, . . . , fn

back to

→

}

.

{

}

{

}

5V must be ﬁnite dimensional—see Exercise 10.4.13.

558

Inner Product Spaces

The conditions in part (2) of the corollary assert that the set of isometries of a ﬁnite dimensional inner
product space forms an algebraic system called a group. The theory of groups is well developed, and
groups of operators are important in geometry. In fact, geometry itself can be fruitfully viewed as the
study of those properties of a vector space that are preserved by a group of invertible linear operators.

Example 10.4.1

Rotations of R2 about the origin are isometries, as are reﬂections in lines through the origin: They
clearly preserve distance and so are linear by Lemma 10.4.1. Similarly, rotations about lines
through the origin and reﬂections in planes through the origin are isometries of R3.

Example 10.4.2

Let T : Mnn
product is

Mnn be the transposition operator: T (A) = AT . Then T is an isometry if the inner
ai jbi j. In fact, T permutes the basis consisting of all matrices

→
A, B

h

= tr (ABT ) = ∑
i, j

i

with one entry 1 and the other entries 0.

The proof of the next result requires the fact (see Theorem 10.4.2) that, if B is an orthonormal basis,

then

v, w

i

h

= CB(v)

·

Theorem 10.4.3

CB(w) for all vectors v and w.

Let T : V
conditionsareequivalent.

→

V beanoperatorwhereV isaﬁnitedimensionalinnerproductspace. Thefollowing

1. T isanisometry.

2. MB(T ) isanorthogonalmatrixforeveryorthonormalbasis B.

3. MB(T ) isanorthogonalmatrixforsomeorthonormalbasis B.

Proof. (1)
⇒
CB[T (e j)], and we have

(2). Let B =

{

e1, . . . , en

}

be an orthonormal basis. Then the jth column of MB(T ) is

using (1). Hence the columns of MB(T ) are orthonormal in Rn, which proves (2).

CB[T (e j)]

CB[T (ek)] =

T (e j), T (ek)

=

e j, ek

h

i

h

i

·

(2)
(3)

⇒
⇒

(3). This is clear.
(1). Let B =

{

T (e1), . . . , T (en)

so

{

}

e1, . . . , en

be as in (3). Then, as before,

}
T (e j), T (ek)

h

i

= CB[T (e j)]

CB[T (ek)]

·

is orthonormal by (3). Hence Theorem 10.4.2 gives (1).

It is important that B is orthonormal in Theorem 10.4.3. For example, T : V
preserves orthogonal sets but is not an isometry, as is easily checked.

→

V given by T (v) = 2v

If P is an orthogonal square matrix, then P−

1 = PT . Taking determinants yields ( det P)2 = 1, so

det P =

1. Hence:

±

Corollary 10.4.2

If T : V

→

V isanisometrywhereV isaﬁnitedimensionalinnerproductspace,then det T =

1.

±

10.4. Isometries

559

Example 10.4.3

n matrix, the matrix operator TA : Rn

If A is any n
orthogonal using the dot product in Rn. Indeed, if E is the standard basis of Rn, then ME(TA) = A
by Theorem 9.2.4.

Rn is an isometry if and only if A is

→

×

Rotations and reﬂections that ﬁx the origin are isometries in R2 and R3 (Example 10.4.1); we are going
to show that these isometries (and compositions of them in R3) are the only possibilities. In fact, this will
follow from a general structure theorem for isometries. Surprisingly enough, much of the work involves
the two–dimensional case.

Theorem 10.4.4

V beanisometryonthetwo-dimensionalinnerproductspaceV. Thentherearetwo

Let T : V
→
possibilities.
Either

(1) Thereisanorthonormalbasis B ofV suchthat

MB(T ) =

cosθ
sinθ

sinθ
cosθ

−

, 0

θ < 2π

≤

(cid:21)
(2) Thereisanorthonormalbasis B ofV suchthat

(cid:20)

or

MB(T ) =

1
0

(cid:20)

0
1
−

(cid:21)

Furthermore,type (1) occursifandonlyif det T = 1,andtype (2) occursifandonlyif
det T =

1.

−

Proof. The ﬁnal statement follows from the rest because det T = det [MB(T )] for any basis B. Let
B0 =

be any ordered orthonormal basis of V and write

e1, e2

{

}

A = MB0(T ) =

a b
c d

(cid:20)

(cid:21)

; that is,

T (e1) = ae1 + ce2
T (e2) = be1 + de2

Then A is orthogonal by Theorem 10.4.3, so its columns (and rows) are orthonormal. Hence

so (a, c) and (d, b) lie on the unit circle. Thus angles θ and ϕ exist such that

a2 + c2 = 1 = b2 + d2

a = cosθ,
c = sinθ 0
d = cosϕ, b = sinϕ 0

θ < 2π
ϕ < 2π

≤
≤

560

Inner Product Spaces

Then sin(θ +ϕ) = cd + ab = 0 because the columns of A are orthogonal, so θ +ϕ = kπ for some integer
k. This gives d = cos(kπ

1)k cosθ and b = sin(kπ

1)k+1 sinθ. Finally

θ) = (

θ) = (

−

−

−

−

A =

cosθ (
−
sinθ (
−

1)k+1 sinθ
1)k cosθ

(cid:21)

(cid:20)

If k is even we are in type (1) with B = B0, so assume k is odd. Then A =

c
a
−
. Otherwise A has eigenvalues λ1 = 1 and λ2 =

a
c

(cid:20)

. If a =

1 and c = 0,

−

1 with corresponding

(cid:21)
−

we are in type (1) with B =
1 + a
c

eigenvectors x1 =

e2, e2

{

}

and x2 =

c
−
1 + a

(cid:20)

(cid:21)
f1 = (1 + a)e1 + ce2

(cid:20)

as the reader can verify. Write

(cid:21)

and

f2 =

ce2 + (1 + a)e2

−

Then f1 and f2 are orthogonal (verify) and CB0(fi) = CB0(λifi) = xi for each i. Moreover

CB0[T (fi)] = ACB0(fi) = Axi = λixi = λiCB0(fi) = CB0(λifi)

so T (fi) = λifi for each i. Hence MB(T ) =

0
λ1
0 λ2

=

(cid:21)

(cid:20)

1
0

0
1
−

(cid:21)

(cid:20)

and we are in type (2) with

B =

f1,

1
f1
k

k

1
f2
k

k

f2

.

o

n

Corollary 10.4.3

Anoperator T : R2

→

R2 isanisometryifandonlyif T isarotationorareﬂection.

In fact, if E is the standard basis of R2, then the clockwise rotation Rθ about the origin through an angle
θ has matrix

ME(Rθ) =

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

(cid:20)

{

}

MB(S) =

1
0

0
1
−

(see Theorem 2.6.4). On the other hand, if S : R2
R2 is the reﬂection in a line through the origin (called
the ﬁxed line of the reﬂection), let f1 be a unit vector pointing along the ﬁxed line and let f2 be a unit vector
f2, so
perpendicular to the ﬁxed line. Then B =

is an orthonormal basis, S(f1) = f1 and S(f2) =

f1, f2

→

−

(cid:20)
Thus S is of type 2. Note that, in this case, 1 is an eigenvalue of S, and any eigenvector corresponding to
1 is a direction vector for the ﬁxed line.

(cid:21)

Example 10.4.4

In each case, determine whether TA : R2
or ﬁxed line:

R2 is a rotation or a reﬂection, and then ﬁnd the angle

→
1 √3
1

(a) A = 1
2

(cid:20)

√3

−

(b) A = 1
5

3 4
4 3

−

(cid:21)

(cid:20)

(cid:21)

10.4. Isometries

561

Solution. Both matrices are orthogonal, so (because ME(TA) = A, where E is the standard basis)
TA is an isometry in both cases. In the ﬁrst case, det A = 1, so TA is a counterclockwise rotation
through θ, where cosθ = 1

1, so TA is a

π
3 . In (b), det A =

2 and sinθ =
−
reﬂection in this case. We verify that d =

(cid:21)
1. Hence the ﬁxed line Rd has equation y = 2x.

(cid:20)

√3
2 . Thus θ =
1
2

−

is an eigenvector corresponding to the eigenvalue

−

We now give a structure theorem for isometries. The proof requires three preliminary results, each of

interest in its own right.

Lemma 10.4.2

Let T : V
subspaceofV,thenU ⊥ isalso T-invariant.

→

V beanisometryofaﬁnitedimensionalinnerproductspaceV. IfU isa T-invariant

= 0 for all u in U . At
Proof. Let w lie in U ⊥. We are to prove that T (w) is also in U ⊥; that is,
U and so is an isomorphism by the
this point, observe that the restriction of T to U is an isometry U
corollary to Theorem 10.4.2. In particular, each u in U can be written in the form u = T (u1) for some u1
in U , so

T (w), u
i

→

h

h
because w is in U ⊥. This is what we wanted.

h

T (w), u
i

=

T (w), T (u1)

w, u1

=

i

h

i

= 0

To employ Lemma 10.4.2 above to analyze an isometry T : V
= 0 and U

V when dim V = n, it is necessary to
→
= V . We will show, in fact, that such a
show that a T -invariant subspace U exists such that U
subspace U can always be found of dimension 1 or 2. If T has a real eigenvalue λ then Ru is T -invariant
where u is any λ-eigenvector. But, in case (1) of Theorem 10.4.4, the eigenvalues of T are eiθ and e−
iθ
= π. It turns out that every complex
(the reader should check this), and these are nonreal if θ
eigenvalue λ of T has absolute value 1 (Lemma 10.4.3 below); and that U has a T -invariant subspace of
dimension 2 if λ is not real (Lemma 10.4.4).

= 0 and θ

Lemma 10.4.3

V beanisometryoftheﬁnitedimensionalinnerproductspaceV. Ifλ isacomplex

Let T : V
→
eigenvalueof T,then

= 1.

λ
|
|

Proof. Choose an orthonormal basis B of V , and let A = MB(T ). Then A is a real orthogonal matrix so,
using the standard inner product

= xT y in C, we get

x, y

i

h
Ax

k

k

2 = (Ax)T (Ax) = xT AT Ax = xT Ix =

k
2 =

2

x
k
λ
|
|

for all x in Cn. But Ax = λx for some x
required.

= 0, whence

2 =

x

k

k

λx
k

k

2. This gives

2

x

k

k

= 1, as

λ
|
|

6
6
6
6
6
562

Inner Product Spaces

Lemma 10.4.4

Let T : V
eigenvalue,thenV hasatwo-dimensionalT-invariantsubspace.

V beanisometryofthe n-dimensionalinnerproductspaceV. If T hasanonreal

→

Proof. Let B be an orthonormal basis of V , let A = MB(T ), and (using Lemma 10.4.3) let λ = eiα be a
= 0 in Cn. Because A is real, complex conjugation gives
nonreal eigenvalue of A, say Ax = λx where x
x, x
Ax = λx, so λ is also an eigenvalue. Moreover λ
is linearly independent in
Cn (the argument in the proof of Theorem 5.5.4 works). Now deﬁne

= λ (λ is nonreal), so

}

{

z1 = x + x

and

z2 = i(x

x)

−

Then z1 and z2 lie in Rn, and
over C. Moreover

{

z1, z2

is linearly independent over R because

x, x
}

{

is linearly independent

}
x = 1

2(z1

iz2)

and

x = 1

2(z1 + iz2)

−

Now λ +λ = 2 cosα and λ

−

λ = 2i sinα, and a routine computation gives

Az1 = z1 cosα+ z2 sinα
Az2 =

z1 sinα+ z2 cosα

−

Finally, let e1 and e2 in V be such that z1 = CB(e1) and z2 = CB(e2). Then

CB[T (e1)] = ACB(e1) = Az1 = CB(e1 cosα+ e2 sinα)

using Theorem 9.1.2. Because CB is one-to-one, this gives the ﬁrst of the following equations (the other is
similar):

T (e1) = e1 cosα+ e2 sinα
T (e2) =

e1 sinα+ e2 cosα

−

Thus U = span

e1, e2

{

}

is T -invariant and two-dimensional.

We can now prove the structure theorem for isometries.

Theorem 10.4.5

V beanisometryofthe n-dimensionalinnerproductspaceV. Givenanangleθ,write

Let T : V

R(θ) =

→
cosθ
sinθ

sinθ
cosθ

−

oneofthefollowingblockdiagonalforms,classiﬁedforconveniencebywhether n isoddoreven:

. Thenthereexistsanorthonormalbasis B ofV suchthat MB(T ) has
(cid:21)

(cid:20)

n = 2k + 1 





0

1
0 R(θ1)
...
...
0
0

0
0
...
R(θk)

· · ·
· · ·
...

· · ·








or 






0

1
−
0 R(θ1)
...
0

...
0

0
0
...
R(θk)

· · ·
· · ·
...

· · ·








6
6
10.4. Isometries

563

R(θ1)
0
...
0

0
R(θ2)
...
0

0
0
...
R(θk)

· · ·
· · ·
...

· · ·








n = 2k 





or

0
0

1 0
−
1
0
0 R(θ1)
0
...
...
...
0
0
0

· · ·
· · ·
· · ·
...

· · ·










0
0
0
...
R(θk

1)

−










Proof. We show ﬁrst, by induction on n, that an orthonormal basis B of V can be found such that MB(T )
is a block diagonal matrix of the following form:



MB(T ) =

Ir
0
0
...
0

0
Is
−
0
...
0

0
0
R(θ1)
...
0

· · ·
· · ·
· · ·
. . .



0
0
0
...
R(θt)













Is, or the matrices R(θi) may be missing. If n = 1 and V = Rv,
where the identity matrix Ir, the matrix
−
this holds because T (v) = λv and λ =
1 by Lemma 10.4.3. If n = 2, this follows from Theorem 10.4.4. If
3, either T has a real eigenvalue and therefore has a one-dimensional T -invariant subspace U = Ru for
n
any eigenvector u, or T has no real eigenvalue and therefore has a two-dimensional T -invariant subspace
U by Lemma 10.4.4. In either case U ⊥ is T -invariant (Lemma 10.4.2) and dim U ⊥ = n
dim U < n.
Hence, by induction, let B1 and B2 be orthonormal bases of U and U ⊥ such that MB1(T ) and MB2(T ) have
the form given. Then B = B1
B2 is an orthonormal basis of V , and MB(T ) has the desired form with a
suitable ordering of the vectors in B.

· · ·

−

≥

±

∪

Now observe that R(0) =

and R(π) =

1 0
0 1

(cid:20)

(cid:21)

1
−
0

(cid:20)

0
1
−

(cid:21)

. It follows that an even number of 1s or

1s

−

can be written as R(θ1)-blocks. Hence, with a suitable reordering of the basis B, the theorem follows.

As in the dimension 2 situation, these possibilities can be given a geometric interpretation when V = R3
is taken as euclidean space. As before, this entails looking carefully at reﬂections and rotations in R3. If
Q : R3
R3 is any reﬂection in a plane through the origin (called the ﬁxed plane of the reﬂection), take
to be any orthonormal basis of the ﬁxed plane and take f1 to be a unit vector perpendicular to
f2, f3
{
the ﬁxed plane. Then Q(f1) =
is an
orthonormal basis such that

f1, whereas Q(f2) = f2 and Q(f3) = f3. Hence B =

f1, f2, f3

→
}

−

{

}

MB(Q) =



−

1 0 0
0 1 0
0 0 1 




Similarly, suppose that R : R3
R3 is any rotation about a line through the origin (called the axis of the
rotation), and let f1 be a unit vector pointing along the axis, so R(f1) = f1. Now the plane through the
origin perpendicular to the axis is an R-invariant subspace of R2 of dimension 2, and the restriction of R
to this plane is a rotation. Hence, by Theorem 10.4.4, there is an orthonormal basis B1 =
of this

f2, f3

→

is an orthonormal basis of R3 such

{

}

plane such that MB1(R) =
that the matrix of R is

(cid:20)

cosθ
sinθ

sinθ
cosθ

−

(cid:21)

. But then B =

f1, f2, f3

{

}

MB(R) =





0

0
sinθ

1
0 cosθ
−
0 sinθ cosθ 


564

Inner Product Spaces

However, Theorem 10.4.5 shows that there are isometries T in R3 of a third type: those with a matrix of
the form

MB(T ) =

1
−
0
0

0
cosθ
−
sinθ cosθ 

, let Q be the reﬂection in the plane spanned by f2 and f3, and let R be the ro-
If B =
tation corresponding to θ about the line spanned by f1. Then MB(Q) and MB(R) are as above, and
MB(Q)MB(R) = MB(T ) as the reader can verify. This means that MB(QR) = MB(T ) by Theorem 9.2.1,
and this in turn implies that QR = T because MB is one-to-one (see Exercise 9.1.26). A similar argument
shows that RQ = T , and we have Theorem 10.4.6.

0
sinθ

f1, f2, f3





{

}

Theorem 10.4.6
If T : R3

→

R3 isanisometry,therearethreepossibilities.

a. T isarotation,and MB(T ) =



forsomeorthonormalbasis B.

0

0
sinθ

1
0 cosθ
−
0 sinθ cosθ 




−

b. T isareﬂection,and MB(T ) =

1 0 0
0 1 0
0 0 1 

c. T = QR = RQ where Q isareﬂection, R isarotationaboutanaxisperpendiculartotheﬁxed
0
sinθ

forsomeorthonormalbasis B.

forsomeorthonormalbasis B.

planeof Q and MB(T ) =





1
−
0
0





0
cosθ
sinθ cosθ 
−


Hence T isarotationifandonlyif det T = 1.

Proof. It remains only to verify the ﬁnal observation that T is a rotation if and only if det T = 1. But
clearly det T =

1 in parts (b) and (c).

−

A useful way of analyzing a given isometry T : R3

R3 comes from computing the eigenvalues of T .
Because the characteristic polynomial of T has degree 3, it must have a real root. Hence, there must be at
least one real eigenvalue, and the only possible real eigenvalues are
1 by Lemma 10.4.3. Thus Table 10.1
includes all possibilities.

→

±

10.4. Isometries

565

Table 10.1

Eigenvalues of T

Action of T

(1) 1, no other real eigenvalues

Rotation about the line Rf where f is an eigenvector corresponding
to 1. [Case (a) of Theorem 10.4.6.]

(2)

1, no other real eigenvalues Rotation about the line Rf followed by reﬂection in the plane (Rf)⊥
1. [Case (c) of Theo-

−

where f is an eigenvector corresponding to
rem 10.4.6.]

−

(3)

1, 1, 1

−

(4) 1,

1,

−

1
−

(5)

1,

−

1,

−

1
−

(6) 1, 1, 1

Example 10.4.5

Reﬂection in the plane (Rf)⊥ where f is an eigenvector correspond-
ing to

1. [Case (b) of Theorem 10.4.6.]

−

This is as in (1) with a rotation of π.

Here T (x) =

x for all x. This is (2) with a rotation of π.

−

Here T is the identity isometry.

Analyze the isometry T : R3

R3 given by T

→

x
y
z 


=









Solution. If B0 is the standard basis of R3, then MB0(T ) =

y
z
.
x 

0 1 0
0 0 1
1 0 0 


−

−



, so

cT (x) = x3 + 1 = (x + 1)(x2

−

x + 1). This is (2) in Table 10.1. Write:



f1 = 1

√3 

f2 = 1

√6 

f3 = 1

√2 

1
2
1 


1
0
1 


−

1
1
1 
−





Here f1 is a unit eigenvector corresponding to λ1 =
about the line L = Rf1, followed by reﬂection in the plane U through the origin perpendicular to f1
(with equation x
B =

−
is an orthonormal basis of R3 and

is chosen as an orthonormal basis of U , so

1, so T is a rotation (through an angle θ)

y + z = 0). Then,

f1, f2

f1, f2, f3

−



}

{

{

}

MB(T ) = 





0

1
−
0
0 √3
2

1
2 −

0

√3
2
1
2








Hence θ is given by cosθ = 1

2, sinθ = √3

2 , so θ = π
3 .

566

Inner Product Spaces

Let V be an n-dimensional inner product space. A subspace of V of dimension n

1 is called a
hyperplane in V . Thus the hyperplanes in R3 and R2 are, respectively, the planes and lines through the
origin. Let Q : V

V be an isometry with matrix

−

→

MB(Q) =

1
−
0 In

0

1
−

(cid:20)
. Then Q(f1) =

(cid:21)

f1 whereas Q(u) = u for each u in
for some orthonormal basis B =
. Hence U is called the ﬁxed hyperplane of Q, and Q is called reﬂection in U .
f2, . . . , fn
U = span
Note that each hyperplane in V is the ﬁxed hyperplane of a (unique) reﬂection of V . Clearly, reﬂections in
R2 and R3 are reﬂections in this more general sense.

f1, f2, . . . , fn

−

{

}

{

}

Continuing the analogy with R2 and R3, an isometry T : V

orthonormal basis

f1, . . . , fn

{

}

such that

V is called a rotation if there exists an

→

MB(T ) =

Ir
0
0
0 R(θ) 0
Is
0
0









, and where either Ir or Is (or both) may be missing. If

in block form, where R(θ) =

cosθ
sinθ

sinθ
cosθ

−

W with respect to

(cid:21)
(cid:20)
, then W is T -invariant and the
R(θ) occupies columns i and i + 1 of MB(T ), and if W = span
is R(θ). Clearly, if W is viewed as a copy of R2, then
matrix of T : W
{
T is a rotation in W . Moreover, T (u) = u holds for all vectors u in the (n
2)-dimensional subspace
, and U is called the ﬁxed axis of the rotation T . In R3, the axis of
U = span
any rotation is a line (one-dimensional), whereas in R2 the axis is U =
0
}

With these deﬁnitions, the following theorem is an immediate consequence of Theorem 10.4.5 (the

1, fi+1, . . . , fn

f1, . . . , fi

fi, fi+1

fi, fi+1

→

−

{

{

}

}

{

}

−

.

details are left to the reader).

Theorem 10.4.7

Let T : V
isometries T1, . . . , T suchthat

→

V beanisometryofaﬁnitedimensionalinnerproductspaceV. Thenthereexist

1
−
whereeach Ti iseitherarotationorareﬂection,atmostoneisareﬂection,and TiTj = TjTi holds
forall i and j. Furthermore, T isacompositeofrotationsifandonlyif det T = 1.

· · ·

T = TkTk

T2T1

Exercises for 10.4

Throughout these exercises, V denotes a ﬁnite di-
mensional inner product space.

Exercise 10.4.1 Show that the following linear opera-
tors are isometries.

a. T : C

→

C; T (z) = z;

z, w
h

i

= re (zw)

b. T : Rn

→
= (an, an

Rn; T (a1, a2, . . . , an)
1, . . . , a2, a1); dot product

−

c. T : M22

M22;

→
= tr (ABT )

A, B
h
i
d. T : R3

a b
c d

=

(cid:21)

(cid:20)

c d
b a

;

(cid:21)

T

(cid:20)

R3; T (a, b, c) = 1

9 (2a + 2b

c, 2a +

−

→

b, 2b + 2c

2c

−

−

a); dot product

Exercise 10.4.2 In each case, show that T is an isometry
of R2, determine whether it is a rotation or a reﬂection,
and ﬁnd the angle or the ﬁxed line. Use the dot product.

a.

T

c.

T

e.

T

f.

T

(cid:20)

(cid:20)

(cid:20)

(cid:20)

a
b

a
b

a
b

a
b

(cid:21)

(cid:21)

(cid:21)

(cid:21)

a
b

b
a

=

−

(cid:20)

=

−

(cid:20)
= 1
√2

= 1
√2

(cid:20)

(cid:20)

b.

T

d.

T

a
b

a
b

(cid:20)

(cid:20)

=

=

(cid:21)

(cid:21)

(cid:20)

(cid:20)

a
b

b
a

−
−

−
−

(cid:21)

(cid:21)

(cid:21)

(cid:21)

(cid:21)

(cid:21)
a + b
a
b

−

a
b
−
a + b

Exercise 10.4.3 In each case, show that T is an isometry
of R3, determine the type (Theorem 10.4.6), and ﬁnd the
axis of any rotations and the ﬁxed plane of any reﬂections
involved.

10.4. Isometries

567

R3 be an isometry, and let
Exercise 10.4.5 Let T : R3
→
E1 be the subspace of all ﬁxed vectors in R3 (see Exercise
10.4.4). Show that:

a. E1 = R3 if and only if T = 1.

b. dim E1 = 2 if and only if T is a reﬂection (about

the plane E1).

c. dim E1 = 1 if and only if T is a rotation (T

= 1)

(about the line E1).

d. dim E1 = 0 if and only if T is a reﬂection followed

by a (nonidentity) rotation.

1.

Exercise 10.4.6 If T is an isometry, show that aT is an
isometry if and only if a =

±
Exercise 10.4.7 Show that every isometry preserves the
angle between any pair of nonzero vectors (see Exercise
10.1.31). Must an angle-preserving isomorphism be an
isometry? Support your answer.

a.

T

b.

T

c.

T

e.

T

f.

T



















a
b
c 


a
b
c





a
b
c 


a
b
c 


a
b
c

=



−


= 1

2 

a
b
c 

√3c
a
√3a + c
2b

−


b
c
a 

a + √3b
√3a
b
2c

−

2 

=




= 1


= 1

√2 



a + c
√2b
a

−
c

−

a
b
c 


=





a
b
c 


−
−









d.

T













Exercise 10.4.4 Let T : R2
R2 be an isometry. A vec-
→
tor x in R2 is said to be ﬁxed by T if T (x) = x. Let E1
denote the set of all vectors in R2 ﬁxed by T . Show that:

a. E1 is a subspace of R2.

b. E1 = R2 if and only if T = 1 is the identity map.

c. dim E1 = 1 if and only if T is a reﬂection (about

the line E1).

Exercise 10.4.8 If T : V
V is an isometry, show that
→
T 2 = 1V if and only if the only complex eigenvalues of T
are 1 and

1.

−

Exercise 10.4.9 Let T : V
V be a linear operator.
Show that any two of the following conditions implies
the third:

→

1. T is symmetric.

2. T is an involution (T 2 = 1V ).

3. T is an isometry.

[Hint: In all cases, use the deﬁnition

v, T (w)
i
h

=

T (v), w
h

i

of a symmetric operator. For (1) and (3)
use the fact that, if
then T 2(v) = v.]

T 2(v)
h

v, w
i

(2),
= 0 for all w,

⇒

−

Exercise 10.4.10 If B and D are any orthonormal bases
of V , show that there is an isometry T : V
V that carries
B to D.

→

Exercise 10.4.11 Show that the following are equivalent
for a linear transformation S : V
V where V is ﬁnite di-
mensional and S

= 0:

→

d. E1 =

0
}
{

if and only if T is a rotation (T

= 1).

1.

S(v), S(w)
i
h

= 0 whenever

v, w
h

i

= 0;

6
6
6
568

Inner Product Spaces

2. S = aT for some isometry T : V

a

= 0 in R.

→

V and some

b. If S = Su

T , u

V , T an isometry, show that

◦

∈

V exists such that S = T

w

∈

Sw.

◦

3. S is an isomorphism and preserves angles between

nonzero vectors.

[Hint: Given (1), show that
all unit vectors e and f in V .]

S(e)
k
k

=

S(f)
k
k

for

Exercise 10.4.12 Let S : V
transformation where V is ﬁnite dimensional.

→

V be a distance preserving

a. Show that the factorization in the proof of Theo-
T and
V are

rem 10.4.1 is unique. That is, if S = Su
V and T , T ′ : V
S = Su′ ◦
isometries, show that u = u′ and T = T ′.

T ′ where u, u′ ∈

◦
→

P by T ( f ) = x f (x) for
→
P, and deﬁne an inner product on P as follows: If

Exercise 10.4.13 Deﬁne T : P
all f
f = a0 + a1x + a2x2 +
are in P, deﬁne

∈

and g = b0 + b1x + b2x2 +

· · ·
= a0b0 + a1b1 + a2b2 +

· · ·

f , g
i

h

.

· · ·

a. Show that

,

h

i

is an inner product on P.

b. Show that T is an isometry of P.

c. Show that T is one-to-one but not onto.

10.5 An Application to Fourier Approximation6

If U is an orthogonal basis of a vector space V , the expansion theorem (Theorem 10.2.4) presents a vector
V as a linear combination of the vectors in U . Of course this requires that the set U is ﬁnite since
v
otherwise the linear combination is an inﬁnite sum and makes no sense in V .

∈

f1, f2, . . . , fn

However, given an inﬁnite orthogonal set U =

, we can use the expansion theorem
for each n to get a series of “approximations” vn for a given vector v. A natural
for
question is whether these vn are getting closer and closer to v as n increases. This turns out to be a very
fruitful idea.

f1, f2, . . . , fn, . . .

{

}

{

}

In this section we shall investigate an important orthogonal set in the space C[
π, π], using the inner product.

functions on the interval [

−

f , g

=

i

h

π

π

−

Z

f (x)g(x)dx

Of course, calculus will be needed. The orthogonal set in question is

1, sin x, cos x, sin(2x), cos(2x), sin(3x), cos(3x), . . .

{

}

Standard techniques of integration give

π, π] of continuous

−

1

k

2 =

k
2 =

sin kx
k

k

cos kx
k

k

2 =

12dx = 2π

sin2(kx)dx = π for any k = 1, 2, 3, . . .

cos2(kx)dx = π for any k = 1, 2, 3, . . .

π

Z

π
−
π

Z

π
−
π

π

Z

−

We leave the veriﬁcations to the reader, together with the task of showing that these functions are orthog-
onal:

6The name honours the French mathematician J.B.J. Fourier (1768-1830) who used these techniques in 1822 to investigate

sin(kx), sin(mx)

= 0 =

i

h

h

cos(kx), cos(mx)

if k

= m

i

heat conduction in solids.

6
6
and

10.5. An Application to Fourier Approximation

569

(Note that 1 = cos(0x), so the constant function 1 is included.)

sin(kx), cos(mx)

= 0

for all k

h

i

0 and m

0

≥

≥

Now deﬁne the following subspace of C[

π, π]:

−

Fn = span

{

1, sin x, cos x, sin(2x), cos(2x), . . . , sin(nx), cos(nx)

}

The aim is to use the approximation theorem (Theorem 10.2.8); so, given a function f in C[
the Fourier coefﬁcients of f by

π, π], deﬁne

−

f (x)dx

a0 = h

ak = h

bk = h

π

f (x), 1
2 = 1
i
2π
1
k
k
f (x), cos(kx)
cos(kx)

k
f (x), sin(kx)
sin(kx)

π
Z
−
2 = 1
i
π
k
2 = 1
i
π
k

k

π

π
−
π

Z

π

Z

−

f (x) cos(kx)dx

k = 1, 2, . . .

f (x) sin(kx)dx

k = 1, 2, . . .

Then the approximation theorem (Theorem 10.2.8) gives Theorem 10.5.1.

Theorem 10.5.1

Let f beanycontinuousreal-valuedfunctiondeﬁnedontheinterval [
b1, . . . aretheFouriercoefﬁcientsof f ,thengiven n

0,

≥

π, π]. If a0, a1, . . .,and b0,

−

fn(x) = a0 + a1 cos x + b1 sin x + a2 cos(2x) + b2 sin(2x) +

+ an cos(nx) + bn sin(nx)

· · ·

isafunctionin Fn thatisclosestto f inthesensethat

holdsforallfunctions g in Fn.

f

k

−

fn

k ≤ k

f

g

k

−

The function fn is called the nth Fourier approximation to the function f .

570

Inner Product Spaces

Example 10.5.1

Find the ﬁfth Fourier approximation to the function f (x) deﬁned on [

π, π] as follows:

−

y

π

f (x) =

(cid:26)

π+ x if
π

−
x if 0

−

x < 0
π

π
≤
x
≤

≤

x

π

π

−

0
y

4

3

2

1

x

x

-4 -3 -2 -1 0 1 2 3 4
f5(x)
y

4

3

2

1

-4 -3 -2 -1 0 1 2 3 4
f13(x)

Solution. The graph of y = f (x) appears in the top diagram.
The Fourier coefﬁcients are computed as follows. The details
of the integrations (usually by parts) are omitted.

a0 = 1
2π

f (x)dx = π
2

π

π

Z

−
π

Z

π
−
π

π

−

Z

ak = 1
π

bk = 1
π

f (x) cos(kx)dx = 2

πk2 [1

cos(kπ)] =

−

0
4
πk2

(cid:26)

if k is even
if k is odd

f (x) sin(kx)dx = 0

for all k = 1, 2, . . .

Hence the ﬁfth Fourier approximation is

f5(x) = π

2 + 4
π

cos x + 1

32 cos(3x) + 1

52 cos(5x)

n

o

This is plotted in the middle diagram and is already a reasonable
approximation to f (x). By comparison, f13(x) is also plotted
in the bottom diagram.

−

−

x) =

We say that a function f is an even function if f (x) = f (

x) holds for all x; f is called an odd function
−
f (x) holds for all x. Examples of even functions are constant functions, the even powers x2,
if f (
x4, . . . , and cos(kx); these functions are characterized by the fact that the graph of y = f (x) is symmetric
about the y axis. Examples of odd functions are the odd powers x, x3, . . . , and sin(kx) where k > 0, and
the graph of y = f (x) is symmetric about the origin if f is odd. The usefulness of these functions stems
from the fact that

π
π f (x)dx = 0
π
−
π f (x)dx = 2
R
−
These facts often simplify the computations of the Fourier coefﬁcients. For example:
R

if f is odd
π
0 f (x)dx if f is even
R

1. The Fourier sine coefﬁcients bk all vanish if f is even.

2. The Fourier cosine coefﬁcients ak all vanish if f is odd.

This is because f (x) sin(kx) is odd in the ﬁrst case and f (x) cos(kx) is odd in the second case.

The functions 1, cos(kx), and sin(kx) that occur in the Fourier approximation for f (x) are all easy to
generate as an electrical voltage (when x is time). By summing these signals (with the amplitudes given
by the Fourier coefﬁcients), it is possible to produce an electrical signal with (the approximation to) f (x)
as the voltage. Hence these Fourier approximations play a fundamental role in electronics.

10.5. An Application to Fourier Approximation

571

Finally, the Fourier approximations f1, f2, . . . of a function f get better and better as n increases. The

reason is that the subspaces Fn increase:

⊆
So, because fn = projFn f , we get (see the discussion following Example 10.2.6)

⊆ · · · ⊆

⊆ · · ·

⊆

F1

F2

F3

Fn

f

k

−

f1

k ≥ k

f2

f

−

k ≥ · · · ≥ k

fn

f

−

k ≥ · · ·

approach zero; in fact, we have the following fundamental theorem.

These numbers

f

k

−

fn

k

Theorem 10.5.2

Let f beanycontinuousfunctioninC[

π, π]. Then

−

fn(x) approaches f (x) forall x suchthat

π < x < π.7

−

It shows that f has a representation as an inﬁnite series, called the Fourier series of f :

f (x) = a0 + a1 cos x + b1 sin x + a2 cos(2x) + b2 sin(2x) +

· · ·

π < x < π. A full discussion of Theorem 10.5.2 is beyond the scope of this book. This subject
whenever
had great historical impact on the development of mathematics, and has become one of the standard tools
in science and engineering.

−

Thus the Fourier series for the function f in Example 10.5.1 is

f (x) = π

2 + 4
π

cos x + 1

32 cos(3x) + 1

52 cos(5x) + 1

72 cos(7x) +

Since f (0) = π and cos(0) = 1, taking x = 0 leads to the series

n

π2
8 = 1 + 1

32 + 1

52 + 1

72 +

· · ·

Example 10.5.2

· · ·

o

Expand f (x) = x on the interval [

π, π] in a Fourier series, and so obtain a series expansion of π
4 .
Solution. Here f is an odd function so all the Fourier cosine coefﬁcients ak are zero. As to the sine
coefﬁcients:

−

π

π

Z

−

−

bk = 1
π

x sin(kx)dx = 2
k (

1)k+1

−

for k

1

≥

where we omit the details of the integration by parts. Hence the Fourier series for x is
2 sin(2x) + 1
1

1
4 sin(4x) + . . . ]

3 sin(3x)

x = 2[sin x

for

π < x < π. In particular, taking x = π

−

−
2 gives an inﬁnite series for π
4 .
7 + 1
1

3 + 1
1

π
4 = 1

9 − · · ·
Many other such formulas can be proved using Theorem 10.5.2.

5 −

−

7We have to be careful at the end points x = π or x =

π because sin(kπ) = sin(

−

kπ) and cos(kπ) = cos(

−

kπ).

−

572

Inner Product Spaces

Exercises for 10.5

Exercise 10.5.1 In each case, ﬁnd the Fourier approxi-
mation f5 of the given function in C[

π, π].

−

a.

f (x) = π

x

−

b.

f (x) =

=

x
|
|

c.

f (x) = x2

x
x

(cid:26)

−

if 0
if

x
≤
π

π
≤
x < 0
≤

−

d.

f (x) =

(cid:26)

Exercise 10.5.2

0 if
x

−
if 0

x < 0
π

π
≤
x
≤

≤

a. Find f5 for the even function f on [

π, π] satis-

fying f (x) = x for 0

x

≤

≤

π.

b. Find f6 for the even function f on [

π, π] satis-

−

−

fying f (x) = sin x for 0

x

π.

[Hint: If k > 1,
1)x]
cos[(k
= 1
−
R
k
1 −
2
−

h

≤
≤
sin x cos(kx)
cos[(k+1)x]
.]
k+1

i

Exercise 10.5.3

a. Prove that

π
π f (x)dx = 0 if f
π
−
π f (x)dx = 2
−

π
0 f (x)dx if f is even.

R

is odd and that

R

R

b. Prove that
1
2 [ f (x)
f (
that they sum to f (x).

−

−

1
2 [ f (x) + f (
is even and that
x)] is odd for any function f . Note

x)]

−

Exercise 10.5.4 Show that
1, cos x, cos(2x), cos(3x), . . .
{
is an orthogonal set in C[0, π] with respect to the inner
product

=

}

f , g
i

h
Exercise 10.5.5

π
0 f (x)g(x)dx.
R

a. Show that π2
10.5.1(b).

8 = 1 + 1

32 + 1

52 +

using Exercise

· · ·

b. Show that π2
cise 10.5.1(c).

12 = 1

−

1

22 + 1

32 −

1
42 +

· · ·

using Exer-

Chapter 11

Canonical Forms

Given a matrix A, the effect of a sequence of row-operations on A is to produce UA where U is invertible.
Under this “row-equivalence” operation the best that can be achieved is the reduced row-echelon form for
A. If column operations are also allowed, the result is UAV where both U and V are invertible, and the
best outcome under this “equivalence” operation is called the Smith canonical form of A (Theorem 2.5.3).
There are other kinds of operations on a matrix and, in many cases, there is a “canonical” best possible
result.

If A is square, the most important operation of this sort is arguably “similarity” wherein A is carried
1AU where U is invertible. In this case we say that matrices A and B are similar, and write A
B,
to U −
1AU for some invertible matrix U . Under similarity the canonical matrices, called Jordan
when B = U −
canonical matrices, are block triangular with upper triangular “Jordan” blocks on the main diagonal. In
this short chapter we are going to deﬁne these Jordan blocks and prove that every matrix is similar to a
Jordan canonical matrix.

∼

Here is the key to the method. Let T : V

V be an operator on an n-dimensional vector space V , and
suppose that we can ﬁnd an ordered basis B of B so that the matrix MB(T ) is as simple as possible. Then,
if B0 is any ordered basis of V , the matrices MB(T ) and MB0(T ) are similar; that is,

→

MB(T ) = P−

1MB0(T )P for some invertible matrix P

Moreover, P = PB0
B is easily computed from the bases B and D (Theorem 9.2.3). This, combined with
the invariant subspaces and direct sums studied in Section 9.3, enables us to calculate the Jordan canonical
form of any square matrix A. Along the way we derive an explicit construction of an invertible matrix P
such that P−

1AP is block triangular.

←

This technique is important in many ways. For example, if we want to diagonalize an n

n matrix A,
Rn be the operator given by TA(x) = Ax or all x in Rn, and look for a basis B of Rn such that

let TA : Rn
MB(TA) is diagonal. If B0 = E is the standard basis of Rn, then ME(TA) = A, so

→

×

P−

1AP = P−

1ME(TA)P = MB(TA)

and we have diagonalized A. Thus the “algebraic” problem of ﬁnding an invertible matrix P such that
1AP is diagonal is converted into the “geometric” problem of ﬁnding a basis B such that MB(TA) is
P−
diagonal. This change of perspective is one of the most important techniques in linear algebra.

11.1 Block Triangular Form

We have shown (Theorem 8.2.5) that any n
to an upper triangular matrix U . The following theorem shows that U can be chosen in a special way.

n matrix A with every eigenvalue real is orthogonally similar

×

Theorem 11.1.1: Block Triangulation Theorem

Let A bean n

×

n matrixwitheveryeigenvaluerealandlet

cA(x) = (x

λ1)m1(x

λ2)m2

(x

−

· · ·

λk)mk

−

−

573

574

Canonical Forms

whereλ1, λ2, . . . , λk arethedistincteigenvaluesof A. Thenaninvertiblematrix P existssuchthat

P−

1AP =

U1
0
0 U2
0
...
0

0
0
0 U3
...
...
0
0

0
0
0
...
Uk

· · ·
· · ·
· · ·

· · ·



















where,foreach i,Ui isan mi
equaltoλi.

×

mi uppertriangularmatrixwitheveryentryonthemaindiagonal

The proof is given at the end of this section. For now, we focus on a method for ﬁnding the matrix P. The
key concept is as follows.

Deﬁnition 11.1 Generalized Eigenspaces

If A isasinTheorem11.1.1,thegeneralizedeigenspace Gλi(A) isdeﬁnedby

where mi isthemultiplicityofλi.

Gλi(A) = null [(λiI

A)mi]

−

Observe that the eigenspace Eλi(A) = null (λiI
results.

−

A) is a subspace of Gλi(A). We need three technical

Lemma 11.1.1
UsingthenotationofTheorem11.1.1,wehave dim [Gλi(A)] = mi.

Proof. Write Ai = (λiI
Gλi(A) = null (Ai) and null (P−
1AiP = (λiI
Now P−

A)mi for convenience and let P be as in Theorem 11.1.1. The spaces
1x, so we show dim [ null (P−
↔
1AP)mi. If we use the block form in Theorem 11.1.1, this becomes

1AiP) are isomorphic via x

P−

P−

−

1AiP)] = mi.

−

P−

1AiP = 





U1

λiI

λiI

−
0
...
0

0

−
...
0

U2

· · ·
· · ·

(λiI

U1)mi
−
0
...
0

(λiI

−
...
0

λiI

· · ·
0
U2)mi

mi








0
0
...

Uk

−

· · ·
· · ·

0
0
...





= i and zero if j = i (because then Ui is an mi

Uk)mi

(λiI

· · ·

−



= 





U j)mi is invertible if j

The matrix (λiI
matrix with each entry on the main diagonal equal to λi). It follows that mi = dim [ null (P−
required.

mi upper triangular
1AiP)], as

×

−

6
11.1. Block Triangular Form

575

Lemma 11.1.2

If P isasinTheorem11.1.1,denotethecolumnsof P asfollows:

p11, p12, . . . , p1m1; p21, p22, . . . , p2m2;

. . . ; pk1, pk2, . . . , pkmk

Then

pi1, pi2, . . . , pimi}
{

isabasisof Gλi(A).

Proof. It sufﬁces by Lemma 11.1.1 to show that each pi j is in Gλi(A). Write the matrix in Theorem 11.1.1
as P−

1AP = diag (U1, U2, . . . , Uk). Then

Comparing columns gives, successively:

AP = P diag (U1, U2, . . . , Uk)

Ap11 = λ1p11,
Ap12 = up11 +λ1p12,
Ap13 = wp11 + vp12 +λ1p13

...

so (λ1I
so (λ1I
so (λ1I

A)p11 = 0
−
A)2p12 = 0
A)3p13 = 0
...

−

−

where u, v, w are in R. In general, (λ1I
pi j is in Gλi(A) for each i and j.

−

A) jp1 j = 0 for j = 1, 2, . . . , m1, so p1 j is in Gλi(A). Similarly,

Lemma 11.1.3
If Bi isanybasisof Gλi(A),then B = B1

B2

∪

∪ · · · ∪

Bk isabasisof Rn.

Proof. It sufﬁces by Lemma 11.1.1 to show that B is independent. If a linear combination from B vanishes,
let xi be the sum of the terms from Bi. Then x1 +
+ xk = 0. But xi = ∑ j ri jpi j by Lemma 11.1.2, so
∑i, j ri jpi j = 0. Hence each xi = 0, so each coefﬁcient in xi is zero.

· · ·

Lemma 11.1.2 suggests an algorithm for ﬁnding the matrix P in Theorem 11.1.1. Observe that there is

an ascending chain of subspaces leading from Eλi(A) to Gλi(A):

Eλi(A) = null [(λiI

A)]

−

⊆

null [(λiI

A)2]

−

⊆ · · · ⊆

null [(λiI

−

A)mi] = Gλi(A)

We construct a basis for Gλi(A) by climbing up this chain.

Triangulation Algorithm

Suppose A hascharacteristicpolynomial

cA(x) = (x

λ1)m1(x

λ2)m2

(x

−

· · ·

λk)mk

−

−

1. Chooseabasisof null [(λ1I

A)2];enlargethattoabasisof null [(λ1I

A)];enlargeitbyaddingvectors(possiblynone)toabasisof
A)3],andsoon. Continuetoobtainan

−

null [(λ1I
orderedbasis

−

p11, p12, . . . , p1m1}
{

of Gλ1(A).

−

576

Canonical Forms

2. Asin (1) chooseabasis

3. Let P =

p1m1; p21p22 · · ·
basisvectors(inorder)ascolumns.

p11p12 · · ·

(cid:2)

pi1, pi2, . . . , pimi}
{
p2m2;

of Gλi(A) foreach i.

; pk1pk2 · · ·

pkmk

· · ·

(cid:3)

bethematrixwiththese

Then P−

1AP = diag (U1, U2, . . . , Uk) asinTheorem11.1.1.

Proof. Lemma 11.1.3 guarantees that B =
P−

1AP = MB(TA). Now Gλi(A) is TA-invariant for each i because

p11, . . . , pkm1}

{

is a basis of Rn, and Theorem 9.2.4 shows that

A)mix = 0 implies

(λiI

−

(λiI

−

A)mi(Ax) = A(λiI

A)mix = 0

−

By Theorem 9.3.7 (and induction), we have

P−

1AP = MB(TA) = diag (U1, U2, . . . , Uk)

where Ui is the matrix of the restriction of TA to Gλi(A), and it remains to show that Ui has the desired
A)pi j is in
upper triangular form. Given s, let pi j be a basis vector in null [(λiI
null [(λiI

A)s], and therefore is a linear combination of the basis vectors pit coming before pi j. Hence

A)s+1]. Then (λiI

−

−

−

TA(pi j) = Api j = λipi j −

(λiI

−

A)pi j

shows that the column of Ui corresponding to pi j has λi on the main diagonal and zeros below the main
diagonal. This is what we wanted.

Example 11.1.1

, ﬁnd P such that P−

1AP is block triangular.



2 0 0
0 2 0
1 1 2
0 0 0

−

1
1
−
0
2





If A = 





Solution. cA(x) = det [xI
A] = (x
k = 1 of Theorem 11.1.1. Compute:

−

−

2)4, so λ1 = 2 is the only eigenvalue and we are in the case

(2I

−

A) = 





0
0
1
0

0 0
0 0
1 0
0 0

−

1
−
1
0
0



(2I

−

A)2 = 







p11, p12}
of null (2I
A)2]; and ﬁnally get a basis

{

0 0 0
0 0 0
0 0 0
0 0 0

0
0
2
−
0





By gaussian elimination ﬁnd a basis
p11, p12, p13}
{
null [(2I
−

A)3] = R4. One choice is

of null [(2I

−

A); then extend in any way to a basis
−
p11, p12, p13, p14}
of
{



(2I

−

A)3 = 0

1
1
0
0







0
0
1
0







0
1
0
0







p13 = 





p12 = 





0
0
0
1







p14 = 





p11 = 





11.1. Block Triangular Form

577

Hence P =

p11 p12 p13 p14

= 

(cid:2)

(cid:3)





1 0 0 0
1 0 1 0
0 1 0 0
0 0 0 1







gives P−

1AP = 





2 0 0
0 2 1
0 0 2
0 0 0

1
0
2
−
2







Example 11.1.2

2
3
4
−
1

0
5
3
−
0

1
4
3
−
1

1
1
1
−
2

If A = 





Solution. The eigenvalues are λ1 = 1 and λ2 = 2 because



, ﬁnd P such that P−

1AP is block triangular.





x

x

x

5

5

5

0

−
3
0

0

−
3
0

−
3
0

2)

−

x

2
−
3
−
4
1
−
x

1
−
3
−
4
1
−

cA(x) = (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (x

1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1)(x
(cid:12)

−

−

= (x

x

1
−
3
−
4
1
−

1)

−

= (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (x

5

5

0
4
−
x + 3
1
−

4
−
x + 3
1
−

x

x

0

−
3
0

−
3
0

x

5

−
3
0

4
−
x + 2
1
−

−

x + 1
1
−
1
2

x

−
4
−
5
3

−

x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

0
0

2

x

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1)

= (x

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
= (x
(cid:12)

−

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1)2(x
(cid:12)

2)2

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
−
4
−
x + 3
1
−

0
4
−
x + 3
1
−

4
−
x + 3
1
−
x

5

−
3

x

x

1
−
1
−
1
2

−
0
4
−
5
3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
0
(cid:12)
(cid:12)
x + 2
x
2

−

−

−
4
−
x + 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
{

1
1
2
−
1

p11 = 



p12 = 



0
3
4
−
1

−
1
0
1
−
1







p21 = 





and p22 = 

p21, p22}
0
4
−
3
0











(cid:12)
(cid:12)
(cid:12)
By solving equations, we ﬁnd null (I
(cid:12)

A) = span

−

and null (I

p11}

−

A)2 = span

p11, p12}

{

where




Since λ1 = 1 has multiplicity 2 as a root of cA(x), dim Gλ1(A) = 2 by Lemma 11.1.1. Since p11
and p12 both lie in Gλ1(A), we have Gλ1(A) = span
. Turning to λ2 = 2, we ﬁnd that
p21}
null (2I

p11, p12}
{

A)2] = span

and null [(2I

A) = span

where













−

{

{

578

Canonical Forms

Again, dim Gλ2(A) = 2 as λ2 has multiplicity 2, so Gλ2(A) = span

1
1
2
−
1

0
3
4
−
1

1
0
1
−
1

0
4
−
3
0







P = 





gives P−

1AP = 





1
0
0
0

−

3 0 0
1 0 0
0 2 3
0 0 2

.







p21, p22}

{

. Hence

If p(x) is a polynomial and A is an n

n matrix if we interpret A0 = In.
For example, if p(x) = x2
2A + 3I. Theorem 11.1.1 provides another proof
of the Cayley-Hamilton theorem (see also Theorem 8.7.10). As before, let cA(x) denote the characteristic
polynomial of A.

×
2x + 3, then p(A) = A2

n matrix, then p(A) is also an n

×

−

−

Theorem 11.1.2: Cayley-Hamilton Theorem

If A isasquarematrixwitheveryeigenvaluereal,then cA(A) = 0.

Proof. As in Theorem 11.1.1, write cA(x) = (x

λ1)m1

(x

−

· · ·

−

λk)mk = Πk

i=1(x

−

λi)mi, and write

P−

1AP = D = diag (U1, . . . , Uk)

Hence

cA(Ui) = Πk

i=1(Ui

because the factor (Ui
then

−

λiImi)mi = 0. In fact Ui

−

λiImi)mi = 0 for each i

−
λiImi is mi

mi and has zeros on the main diagonal. But

×

P−

1cA(A)P = cA(D) = cA[ diag (U1, . . . , Uk)]

= diag [cA(U1), . . . , cA(Uk)]
= 0

It follows that cA(A) = 0.

Example 11.1.3

1 3
1 2

(cid:21)

If A =

(cid:20)

−
cA(A) = A2

, then cA(x) = det

x

−
1

(cid:20)

1

3
−
x
2
−
3 9
3 6

(cid:21)
+

= x2

−
5 0
0 5

(cid:21)

(cid:20)

3x + 5. Then

=

(cid:21)

(cid:20)

0 0
0 0

.
(cid:21)

2 9
3 1

−
−

(cid:20)

−

(cid:20)

(cid:21)

−

3A + 5I2 =

−

Theorem 11.1.1 will be reﬁned even further in the next section.

11.1. Block Triangular Form

579

Proof of Theorem 11.1.1

The proof of Theorem 11.1.1 requires the following simple fact about bases, the proof of which we leave
to the reader.

Lemma 11.1.4

If

v1, v2, . . . , vn
{

}

isabasisofavectorspaceV,soalsois

v1 + sv2, v2, . . . , vn
{

}

foranyscalar s.

Rn be the matrix
Proof of Theorem 11.1.1. Let A be as in Theorem 11.1.1, and let T = TA : Rn
transformation induced by A. For convenience, call a matrix a λ-m-ut matrix if it is an m
m up-
per triangular matrix and every diagonal entry equals λ. Then we must ﬁnd a basis B of Rn such that
MB(T ) = diag (U1, U2, . . . , Uk) where Ui is a λi-mi-ut matrix for each i. We proceed by induction on n.
If n = 1, take B =

→

×

where v is any eigenvector of T .
If n > 1, let v1 be a λ1-eigenvector of T , and let B0 =

v
}

{

v1, w1, . . . , wn

1
−

{

}

be any basis of Rn containing

v1. Then (see Lemma 5.5.2)

in block form where A1 is (n

1)

−

Hence cA1(x) = (x

λ1)m1

1(x
−

−

−

MB0(T ) =

λ1 X
0 A1

(cid:21)
1). Moreover, A and MB0(T ) are similar, so

(cid:20)

(n

−

×
cA(x) = cMB0 (T )(x) = (x

λ1)cA1(x)

−
λk)mk so (by induction) let

λ2)m2

(x

−

· · ·
1A1Q = diag (Z1, U2, . . . , Uk)

Q−

where Z1 is a λ1-(m1

1)-ut matrix and Ui is a λi-mi-ut matrix for each i > 1.

1 0
0 Q

−
, then P−
(cid:21)

If P =

A so by
Theorem 9.2.4(2) there is a basis B of Rn such that MB1(TA) = A′, that is MB1(T ) = A′. Hence MB1(T )
takes the block form

= A′, say. Hence A′ ∼

MB0(T )

∼

(cid:21)

(cid:20)

(cid:20)

1MB0(T ) =

λ1
0 Q−

X Q
1A1Q

MB1(T ) =

(cid:20)

λ1
0

X Q
diag (Z1, U2, . . . , Uk)

λ1 X1
0 Z1

0

0
U2
...
0

Y
0

· · ·

· · ·

0
0
...
Uk










=

(cid:21)










(11.1)

λ1 X1
0 Z1

(cid:20)

(cid:21)

If we write U1 =

, the basis B1 fulﬁlls our needs except that the row matrix Y may not be zero.

We remedy this defect as follows. Observe that the ﬁrst vector in the basis B1 is a λ1 eigenvector of T ,
which we continue to denote as v1. The idea is to add suitable scalar multiples of v1 to the other vectors in
B1. This results in a new basis by Lemma 11.1.4, and the multiples can be chosen so that the new matrix
be the vectors in B1 corresponding to λ2
of T is the same as (11.1) except that Y = 0. Let

w1, . . . , wm2}

{

580

Canonical Forms

(giving rise to U2 in (11.1)). Write

λ2 u12 u13
u23
0 λ2
0 λ2
0
...
...
...
0
0
0

· · ·
· · ·
· · ·

u1m2
u2m2
u3m2
...
λ2










U2 =










and Y =

y1 y2

(cid:2)

ym2

· · ·

(cid:3)

· · ·
We ﬁrst replace w1 by w′1 = w1 + sv1 where s is to be determined. Then (11.1) gives

T (w′1) = T (w1) + sT (v1)

= (y1v1 +λ2w1) + sλ1v1
= y1v1 +λ2(w′1 −
= λ2w′1 + [(y1
−

s(λ2

sv1) + sλ1v1
λ1)]v1

−

Because λ2
chosen. Then, as before,

= λ1 we can choose s such that T (w′1) = λ2w′1. Similarly, let w′2 = w2 + tv1 where t is to be

T (w′2) = T (w2) + tT (v1)

= (y2v1 + u12w1 +λ2w2) + tλ1v1
= u12w′1 +λ2w′2 + [(y2

u12s)

−

t(λ2

λ1)]v1

−

−

Again, t can be chosen so that T (w′2) = u12w′1 + λ2w′2. Continue in this way to eliminate y1, . . . , ym2.
This procedure also works for λ3, λ4, . . . and so produces a new basis B such that MB(T ) is as in (11.1)
but with Y = 0.

Exercises for 11.1

Exercise 11.1.1 In each case, ﬁnd a matrix P such that
1AP is in block triangular form as in Theorem 11.1.1.
P−

Exercise 11.1.2 Show that the following conditions are
equivalent for a linear operator T on a ﬁnite dimensional
space V .

5 3 1
4 2 1
4 3 0

b.

A =

d.

A =

−
−
−
3
4
4

−









a.

A =

c.

A =









e.

A = 





f.

A = 













2
1
−
1

0
2
1

−

−

1
3
2
2

3
1
2

1
3
1

−

−

−

1
2
1
1

2
1
2

1
6
2

−

−

−

1
3
3
4

3 6 3 2
2 3 2 2
1 3 0 1
1 1 2 0

−
−
−
−







0
1
1
2







−
−
−



1. MB(T ) is upper triangular for some ordered basis

B of E.

−
−
−


1 0
1 3
2 4



2. A basis

of V exists such that, for
each i, T (bi) is a linear combination of b1, . . . , bi.

b1, . . . , bn
{

}



3. There exist T -invariant subspaces

V1

V2

⊆

⊆ · · · ⊆

Vn = V

such that dim Vi = i for each i.

Exercise 11.1.3 If A is an n
1 = r0I + r1A +
that A−
r0, r1, . . . , rn

n invertible matrix, show
1An
1 for some scalars
−
−
1. [Hint: Cayley-Hamilton theorem.]

×
+ rn

· · ·

−

6
Exercise 11.1.4 If T : V
V is ﬁnite dimensional, show that cT (T ) = 0.
[Hint: Exercise 9.1.26.]

→

V is a linear operator where

11.2 The Jordan Canonical Form

11.2. The Jordan Canonical Form

581

Exercise 11.1.5 Deﬁne T : P
Show that:

→

P by T [p(x)] = xp(x).

a. T is linear and f (T )[p(x)] = f (x)p(x) for all poly-

nomials f (x).

b. Conclude that f (T )

= 0 for all nonzero polynomi-

als f (x). [See Exercise 11.1.4.]

×

n matrices A and B are called row-equivalent if A can be carried to B using row operations
Two m
and, equivalently, if B = UA for some invertible matrix U . We know (Theorem 2.6.4) that each m
n
matrix is row-equivalent to a unique matrix in reduced row-echelon form, and we say that these reduced
n matrices using row operations. If we allow column
row-echelon matrices are canonical forms for m

×

×
for invertible U and V , and the canonical forms are the

operations as well, then A

UAV =

→

Ir 0
0 0

(cid:20)

(cid:21)

matrices

where r is the rank (this is the Smith normal form and is discussed in Theorem 2.6.3).

In this section, we discover the canonical forms for square matrices under similarity: A

If A is an n

n matrix with distinct real eigenvalues λ1, λ2, . . . , λk, we saw in Theorem 11.1.1 that A

is similar to a block triangular matrix; more precisely, an invertible matrix P exists such that

P−

1AP.

→

Ir 0
0 0

(cid:20)

(cid:21)

×

U1
0
0 U2
...
...
0
0

0
· · ·
0
· · ·
...
. . .
0 Uk






P−

1AP = 







= diag (U1, U2, . . . , Uk)

(11.2)

where, for each i, Ui is upper triangular with λi repeated on the main diagonal. The Jordan canonical form
is a reﬁnement of this theorem. The proof we gave of (11.2) is matrix theoretic because we wanted to give
an algorithm for actually ﬁnding the matrix P. However, we are going to employ abstract methods here.
Consequently, we reformulate Theorem 11.1.1 as follows:

Theorem 11.2.1

→

V bealinearoperatorwhere dim V = n. Assumethatλ1, λ2, . . . , λk arethedistinct

Let T : V
eigenvaluesof T,andthattheλi areallreal. Thenthereexistsabasis F ofV suchthat
MF (T ) = diag (U1, U2, . . . , Uk) where,foreach i,Ui issquare,uppertriangular,withλi repeated
onthemaindiagonal.

{

Proof. Choose any basis B =
b1, b2, . . . , bn
ues as T , Theorem 11.1.1 shows that an invertible matrix P exists such that P−
where the Ui are as in the statement of the Theorem. If p j denotes column j of P and CB : V
1
coordinate isomorphism, let f j = C−
CB(f j) = p j for each j. This means that PB
PF

of V and write A = MB(T ). Since A has the same eigenval-
1AP = diag (U1, U2, . . . , Uk)
Rn is the
→
is a basis of V and
= P, and hence (by Theorem 9.2.2) that

B (p j) for each j. Then F =
CB(f j)

f1, f2, . . . , fn

F =

B = P−

p j

=

←

}

{

}

1. With this, column j of MF (T ) is
CF (T (f j)) = PF

(cid:3)
(cid:2)
BCB(T (f j)) = P−

(cid:2)

(cid:3)

1MB(T )CB(f j) = P−

1Ap j

←

←

6
582

Canonical Forms

for all j. Hence

MF (T ) =

CF (T (f j))

=

P−

1Ap j

= P−

1A

p j

= P−

1AP = diag (U1, U2, . . . , Uk)

as required.

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

Deﬁnition 11.2 Jordan Blocks

1,deﬁnetheJordanblock Jn(λ) tobethe n
If n
thediagonalabove,and 0selsewhere. Wetake J1(λ) = [λ].

×

≥

n matrixwithλsonthemaindiagonal, 1son

Hence

J1(λ) = [λ] ,

J2(λ) =

λ 1
0 λ

,

(cid:21)

(cid:20)

J3(λ) =

0
λ 1
0 λ 1
0

0 λ 


,





J4(λ) = 

,



. . .

λ 1
0
0 λ 1
0
0

0
0
0 λ 1
0 λ
0









We are going to show that Theorem 11.2.1 holds with each block Ui replaced by Jordan blocks corre-
sponding to eigenvalues. It turns out that the whole thing hinges on the case λ = 0. An operator T is
called nilpotent if T m = 0 for some m
1, and in this case λ = 0 for every eigenvalue λ of T . Moreover,
≥
the converse holds by Theorem 11.1.1. Hence the following lemma is crucial.

Lemma 11.2.1

Let T : V
→
T m = 0 forsome m

V bealinearoperatorwhere dim V = n,andassumethat T isnilpotent;thatis,

1. ThenV hasabasis B suchthat

≥

MB(T ) = diag (J1, J2, . . . , Jk)

whereeach Ji isaJordanblockcorrespondingtoλ = 0.1

A proof is given at the end of this section.

Theorem 11.2.2: Real Jordan Canonical Form

V bealinearoperatorwhere dim V = n,andassumethatλ1, λ2, . . . , λm arethe
Let T : V
distincteigenvaluesof T andthattheλi areallreal. Thenthereexistsabasis E ofV suchthat

→

ME(T ) = diag (U1, U2, . . . , Uk)

inblockform. Moreover,eachU j isitselfblockdiagonal:

whereeach Ji isaJordanblockcorrespondingtosomeλi.

U j = diag (J1, J2, . . . , Jk)

1The converse is true too: If MB(T ) has this form for some basis B of V , then T is nilpotent.

Proof. Let E =
matrix for each i. Let

{

e1, e2, . . . , en

11.2. The Jordan Canonical Form

583

be a basis of V as in Theorem 11.2.1, and assume that Ui is an ni

ni

×

}

,

,

{

E2 =

E1 =

e1, . . . , en1}

en1+1, . . . , en2}
{
{
where nk = n, and deﬁne Vi = span
for each i. Because the matrix ME(T ) = diag (U1, U2, . . . , Um)
is block diagonal, it follows that each Vi is T -invariant and MEi(T ) = Ui for each i. Let Ui have λi repeated
along the main diagonal, and consider the restriction T : Vi
λiIni) is a nilpotent matrix,
→
λiIni) is a nilpotent operator on Vi. But then Lemma 11.2.1 shows that Vi has a basis Bi
and hence (T
−
such that MBi(T
λiIni) = diag (K1, K2, . . . , Kti) where each Ki is a Jordan block corresponding to λ = 0.
Hence

1+1, . . . , enk}

Vi. Then MEi(T

Ek =

. . . ,

enk

Ei

−

−

}

{

−

MBi(T ) = MBi(λiIni) + MBi(T

λiIni)
= λiIni + diag (K1, K2, . . . , Kti) = diag (J1, J2, . . . , Jk)

−

where Ji = λiI fi + Ki is a Jordan block corresponding to λi (where Ki is fi

fi). Finally,

×

∪ · · · ∪
is a basis of V with respect to which T has the desired matrix.

∪

B = B1

B2

Bk

Corollary 11.2.1

If A isan n
P−

×

n matrixwithrealeigenvalues,aninvertiblematrix P existssuchthat

1AP = diag (J1, J2, . . . , Jk) whereeach Ji isaJordanblockcorrespondingtoaneigenvalueλi.

Proof. Apply Theorem 11.2.2 to the matrix transformation TA : Rn
MB(TA) has the desired form. If P is the (invertible) n
P−

1AP = MB(TA) by Theorem 9.2.4.

×

Rn to ﬁnd a basis B of Rn such that
n matrix with the vectors of B as its columns, then

→

Of course if we work over the ﬁeld C of complex numbers rather than R, the characteristic polynomial
of a (complex) matrix A splits completely as a product of linear factors. The proof of Theorem 11.2.2 goes
through to give

Theorem 11.2.3: Jordan Canonical Form2

Let T : V
distincteigenvaluesof T. Thenthereexistsabasis F ofV suchthat

V bealinearoperatorwhere dim V = n,andassumethatλ1, λ2, . . . , λm arethe

→

MF(T ) = diag (U1, U2, . . . , Uk)

inblockform. Moreover,eachU j isitselfblockdiagonal:

whereeach Ji isaJordanblockcorrespondingtosomeλi.

U j = diag (J1, J2, . . . , Jt j)

2This was ﬁrst proved in 1870 by the French mathematician Camille Jordan (1838–1922) in his monumental Traité des

substitutions et des équations algébriques.

584

Canonical Forms

Except for the order of the Jordan blocks Ji, the Jordan canonical form is uniquely determined by the
operator T . That is, for each eigenvalue λ the number and size of the Jordan blocks corresponding to λ
is uniquely determined. Thus, for example, two matrices (or two operators) are similar if and only if they
have the same Jordan canonical form. We omit the proof of uniqueness; it is best presented using modules
in a course on abstract algebra.

Proof of Lemma 1

Lemma 11.2.1

Let T : V
→
T m = 0 forsome m

V bealinearoperatorwhere dim V = n,andassumethat T isnilpotent;thatis,

1. ThenV hasabasis B suchthat

≥

MB(T ) = diag (J1, J2, . . . , Jk)

whereeach Ji = Jni(0) isaJordanblockcorrespondingtoλ = 0.

Proof. The proof proceeds by induction on n. If n = 1, then T is a scalar operator, and so T = 0 and the
1 and we may assume that m is chosen such
= 0, so m
lemma holds. If n
1, we may assume that T
≥
that T m = 0, but T m
1
−
u, T u, T 2u, . . . , T m
Claim.
Proof. Suppose a0u+a1T u+a2T 2u+
T m
10 = a0T m
−
applying T m
proves the Claim.

1u = 0 where each ai is in R. Since T m = 0, applying
−
1 gives 0 = T m
1u = 0 and
−
−
2 gives a1 = 0 in the same way. Continue in this fashion to obtain ai = 0 for each i. This
−

1u
is independent.
1T m
+am
−

1u, whence a0 = 0. Hence a1T u + a2T 2u +
−

= 0. Suppose T m
1u
}

= 0 for some u in V .3

1T m
−

+ am

· · ·

· · ·

≥

{

−

−

Now deﬁne P = span

u, T u, T 2u, . . . , T m

P is nilpotent with matrix MB(T ) = Jm(0) where B =

and T : P
are done, by induction, if V = P
and T : Q
such that P

. Then P is a T -invariant subspace (because T m = 0),
u, T u, T 2u, . . . , T m
. Hence we
= 0,
Q is nilpotent). With this in mind, choose a T -invariant subspace Q of maximal dimension
Q =

Q where Q is T -invariant (then dim Q = n

−
dim P < n because P

1u

1u

→

−

0

{

{

}

}

−

→
∩
Choose x
k

≤

≤

k, 1

{

}

V such that x /
∈
P

m, such that T kx

∈

∈

⊕

⊕

= P
Q. Then T mx = 0
⊕
Q but T k
1x /
−
∈

P

Q and look for a contradiction.
Q while T 0x = x /
P
∈
1x, so that
−

∈
Q. Write v = T k

⊕

P

⊕

⊕
.4 We assume that V
P

Q. Hence there exists

⊕
Q and T v

P

v /
∈

⊕

Q

P

⊕

∈

Let T v = p + q with p in P and q in Q. Then 0 = T m
T -invariant, T m
. Hence
−

1p =
−

Q =

T m

1q

P

−

∈

∩

0
}

{

1(T v) = T m
−

1p + T m
−

1q so, since P and Q are
−

Since p
T m

∈

P we have p = a0u + a1T u + a2T 2u +

1u for ai
+ am
−
∈
1u, whence a0 = 0. Thus p = T (p1) where
−

1T m
−

· · ·

1 gives 0 = T m
−

1p = a0T m
−

R. Since T m = 0, applying

T m

1p = 0
−

1T m
−
V is an operator, we abbreviate S(u) by Su for simplicity.

p1 = a1u + a2T u +

+ am

· · ·

3If S : V
4Observe that there is at least one such subspace: Q =

→

.

0
}

{

2u
−

P

∈

6
6
6
6
6
11.2. The Jordan Canonical Form

585

p = q

Q

∈

−

If we write v1 = v

p1 we have

−

T (v1) = T (v

p1) = T v

−
Q

Since T (Q)
P
⊕
say

⊆

Q, a contradiction). Hence Q

Q, it follows that T (Q + Rv1)

Q + Rv1. Moreover v1 /
∈
Q + Rv1 so, by the maximality of Q, we have (Q + Rv1)

Q (otherwise v = v1 + p1 ∈
0
,
}

=

⊆

⊆

∩

P

{

⊂

Thus av1 = p2 −

0
q1 ∈

= p2 = q1 + av1 where p2 ∈
P
⊕

Q. But since v1 = v

p1 we have

P,

Q,

q1 ∈

and

a

R

∈

−
av = av1 + ap1 ∈

Q) + P = P

(P

⊕

Q

⊕

Since v /
∈
the proof.

P

Q, this implies that a = 0. But then p2 = q1 ∈

⊕

P

∩

Q =

0

}

{

, a contradiction. This completes

Exercises for 11.2

Exercise 11.2.1 By direct computation, show that there
is no invertible complex matrix C such that
1 1 0
1 1 0
0 1 1
0 1 0
0 0 1
0 0 1

C =

C−









1





Exercise 11.2.2 Show that

b 0 0
0 a 1
0 0 a

.











is similar to


a 1 0
0 a 0
0 0 b









Exercise 11.2.3

a. Show that every complex matrix is similar to its

transpose.

b. Show every real matrix is similar to its transpose.
[Hint: Show that Jk(0)Q = Q[Jk(0)]T where Q is
the k
k matrix with 1s down the “counter diago-
nal”, that is from the (1, k)-position to the (k, 1)-
position.]

×

6
6
Appendix A

Complex Numbers

−

The fact that the square of every real number is nonnegative shows that the equation x2 + 1 = 0 has no real
root; in other words, there is no real number u such that u2 =
1. So the set of real numbers is inadequate
for ﬁnding all roots of all polynomials. This kind of problem arises with other number systems as well.
The set of integers contains no solution of the equation 3x + 2 = 0, and the rational numbers had to be
invented to solve such equations. But the set of rational numbers is also incomplete because, for example,
it contains no root of the polynomial x2
2. Hence the real numbers were invented. In the same way, the
set of complex numbers was invented, which contains all real numbers together with a root of the equation
x2 + 1 = 0. However, the process ends here: the complex numbers have the property that every polynomial
with complex coefﬁcients has a (complex) root. This fact is known as the fundamental theorem of algebra.
One pleasant aspect of the complex numbers is that, whereas describing the real numbers in terms of
the rationals is a rather complicated business, the complex numbers are quite easy to describe in terms of
real numbers. Every complex number has the form

−

a + bi

where a and b are real numbers, and i is a root of the polynomial x2 + 1. Here a and b are called the real
part and the imaginary part of the complex number, respectively. The real numbers are now regarded as
special complex numbers of the form a + 0i = a, with zero imaginary part. The complex numbers of the
form 0 + bi = bi with zero real part are called pure imaginary numbers. The complex number i itself is
called the imaginary unit and is distinguished by the fact that

i2 =

1
−

As the terms complex and imaginary suggest, these numbers met with some resistance when they were
ﬁrst used. This has changed; now they are essential in science and engineering as well as mathematics,
and they are used extensively. The names persist, however, and continue to be a bit misleading: These
numbers are no more “complex” than the real numbers, and the number i is no more “imaginary” than
1.
Much as for polynomials, two complex numbers are declared to be equal if and only if they have the

−

same real parts and the same imaginary parts. In symbols,

a + bi = a′ + b′i

if and only if a = a′ and b = b′

The addition and subtraction of complex numbers is accomplished by adding and subtracting real and
imaginary parts:

(a + bi) + (a′ + b′i) = (a + a′) + (b + b′)i
b′)i
(a + bi)

(a′ + b′i) = (a

a′) + (b

−

−

−

This is analogous to these operations for linear polynomials a + bx and a′ + b′x, and the multiplication of
complex numbers is also analogous with one difference: i2 =

1. The deﬁnition is

−

(a + bi)(a′ + b′i) = (aa′

bb′) + (ab′ + ba′)i

−

With these deﬁnitions of equality, addition, and multiplication, the complex numbers satisfy all the basic
arithmetical axioms adhered to by the real numbers (the veriﬁcations are omitted). One consequence of

587

588

Complex Numbers

this is that they can be manipulated in the obvious fashion, except that i2 is replaced by
occurs, and the rule for equality must be observed.

1 wherever it

−

Example A.1

3i and w =

If z = 2
and z2.

−

Solution.

1 + i, write each of the following in the form a + bi: z + w, z

−

w, zw, 1

3z,

−

z + w = (2
w = (2
z

−

3i) + (
3i)
(

−

−
−

1 + i) = (2
1) + (
1 + i) = (2 + 1) + (

−

3 + 1)i = 1
−
1)i = 3
3
−

2i
−
4i
−
−
3i2) + (2 + 3)i = 1 + 5i

−
−
1 + i) = (

zw = (2
−
3 z = 1
1
3(2
z2 = (2

3i)(
−
3i) = 2
−
3i)(2

−

−

2
−

−

i

3 −
3i) = (4 + 9i2) + (

6
−

−

6)i =

5
−

−

12i

Example A.2

Find all complex numbers z such as that z2 = i.

Solution. Write z = a + bi; we must determine a and b. Now z2 = (a2
condition z2 = i becomes

b2) + (2ab)i, so the

−

(a2

b2) + (2ab)i = 0 + i

−

Equating real and imaginary parts, we ﬁnd that a2 = b2 and 2ab = 1. The solution is a = b =
+ 1
so the complex numbers required are z = 1
√2
√2

i and z =

1
√2

i.

1
√2

,

±

1
√2 −

−

As for real numbers, it is possible to divide by every nonzero complex number z. That is, there exists
a complex number w such that wz = 1. As in the real case, this number w is called the inverse of z and
is denoted by z−
= 0. Hence
a2 + b2

z . Moreover, if z = a + bi, the fact that z

= 0, and an explicit formula for the inverse is

= 0 means that a

= 0 or b

1 or 1

a2+b2 −
In actual calculations, the work is facilitated by two useful notions: the conjugate and the absolute value
of a complex number. The next example illustrates the technique.

z = a
1

b
a2+b2 i

6
6
6
6
Example A.3

Write 3+2i

2+5i in the form a + bi.

Solution. Multiply top and bottom by the complex number 2
by negating the imaginary part). The result is

−

5i (obtained from the denominator

589

3+2i

2+5i = (2

(2

5i)(3+2i)

5i)(2+5i) = (6+10)+(4
−
(5i)2

22

15)i

−
−

−

= 16

29 −

11
29 i

Hence the simpliﬁed form is 16

29 −

11
29 i, as required.

The key to this technique is that the product (2

5i)(2 + 5i) = 29 in the denominator turned out to be
−
a real number. The situation in general leads to the following notation: If z = a + bi is a complex number,
the conjugate of z is the complex number, denoted z, given by

Hence z is obtained from z by negating the imaginary part. Thus (2 + 3i) = 2
we multiply z = a + bi by z, we obtain

−

3i and (1

−

i) = 1 + i. If

z = a

−

bi where z = a + bi

zz = a2 + b2 where z = a + bi

The real number a2 + b2 is always nonnegative, so we can state the following deﬁnition: The absolute
, is the positive square root √a2 + b2;

value or modulus of a complex number z = a + bi, denoted by
that is,

z

|

|

z

=

a2 + b2 where z = a + bi

|

|
3)2 = √13 and
p

For example,

2

3i

=

22 + (

1 + i

= √12 + 12 = √2.

|
Note that if a real number a is viewed as the complex number a + 0i, its absolute value (as a complex

−

−

|

|

|

number) is

= √a2, which agrees with its absolute value as a real number.

p

a

|

|

With these notions in hand, we can describe the technique applied in Example A.3 as follows: When
w of complex numbers to the form a + bi, multiply top and bottom by the conjugate

converting a quotient z
w of the denominator.

The following list contains the most important properties of conjugates and absolute values. Through-

out, z and w denote complex numbers.

2 z

z = 1
1
z
|
|
0 for all complex numbers z
z

| ≥
= 0 if and only if z = 0

z

C1.

z

w = z

w

±

±

C2.

C3.

zw = z w
= z
w

z
w

C4.

(z) = z
(cid:0)
(cid:1)

C5.

z is real if and only if z = z

C6.

zz =

2

z

|

|

C7.

C8.

C9.

C10.

C11.

C12.

|

|

|

|

|

w
|

||

z

=

|
zw
|
|
z
= |
|
w
|
|

z
w|
z + w

z

|

+

w
|

|

(triangle inequality)

| ≤ |

All these properties (except property C12) can (and should) be veriﬁed by the reader for arbitrary complex
numbers z = a + bi and w = c + di. They are not independent; for example, property C10 follows from
properties C2 and C6.

590

Complex Numbers

The triangle inequality, as its name suggests, comes from a geometric representation of the complex
numbers analogous to identiﬁcation of the real numbers with the points of a line. The representation is
achieved as follows:

y

(a, b) = a + bi

(0, b) = bi

i

0

1

(a, 0) = a

x

(a,

−

b) = a

bi

−

Figure A.1

Introduce a rectangular coordinate system in the plane (Figure A.1),
and identify the complex number a + bi with the point (a, b). When this
is done, the plane is called the complex plane. Note that the point (a, 0)
on the x axis now represents the real number a = a + 0i, and for this rea-
son, the x axis is called the real axis. Similarly, the y axis is called the
imaginary axis. The identiﬁcation (a, b) = a + bi of the geometric point
(a, b) and the complex number a + bi will be used in what follows without
comment. For example, the origin will be referred to as 0.

This representation of the complex numbers in the complex plane gives
a useful way of describing the absolute value and conjugate of a complex
= √a2 + b2 is just the distance
number z = a + bi. The absolute value
from z to the origin. This makes properties C8 and C9 quite obvious. The
bi of z is just the reﬂection of z in the real axis (x axis),
conjugate z = a

z

|

|

a fact that makes properties C4 and C5 clear.

−

Given two complex numbers z1 = a1 + b1i = (a1, b1) and z2 = a2 + b2i = (a2, b2), the absolute value

of their difference

−
is just the distance between them. This gives the complex distance formula:

q

−

−

|

|

z1

z2

=

(a1

a2)2 + (b1

b2)2

y

z + w

z + w
|

|

0

(z + w)

|

w
|

−

=

z
|

|

w

w
|

|

x

Figure A.2

y

z + w = (a + c, b + d)

z = (a, b)

w = (c, d)

x

0 = (0, 0)

Figure A.3

z1

|

z2

|

−

is the distance between z1 and z2

This useful fact yields a simple veriﬁcation of the triangle inequality,
property C12. Suppose z and w are given complex numbers. Consider the
triangle in Figure A.2 whose vertices are 0, w, and z + w. The three sides
by the complex distance formula, so the
have lengths
inequality

z + w
|

, and

w
|

z

|

|

|

|

,

| ≤ |
expresses the obvious geometric fact that the sum of the lengths of two
sides of a triangle is at least as great as the length of the third side.

|

|

|

z + w

z

+

w
|

The representation of complex numbers as points in the complex plane
has another very useful property: It enables us to give a geometric de-
scription of the sum and product of two complex numbers. To obtain the
description for the sum, let

z = a + bi = (a, b)
w = c + di = (c, d)

denote two complex numbers. We claim that the four points 0, z, w, and
z + w form the vertices of a parallelogram. In fact, in Figure A.3 the lines
from 0 to z and from w to z + w have slopes

b
0 = b
0
−
a
a
−

and

(b+d)
(a+c)

c = b

a

d
−
−

591

(If it happens that a = 0, then both these lines are vertical.)
respectively, so these lines are parallel.
Similarly, the lines from z to z + w and from 0 to w are also parallel, so the ﬁgure with vertices 0, z, w, and
z + w is indeed a parallelogram. Hence, the complex number z + w can be obtained geometrically from
z and w by completing the parallelogram. This is sometimes called the parallelogram law of complex
addition. Readers who have studied mechanics will recall that velocities and accelerations add in the same
way; in fact, these are all special cases of vector addition.

Polar Form

i

1

1

P

y

θ

−

Unit
circle

Radian
measure
of θ

The geometric description of what happens when two complex numbers
are multiplied is at least as elegant as the parallelogram law of addition, but
it requires that the complex numbers be represented in polar form. Before
discussing this, we pause to recall the general deﬁnition of the trigono-
metric functions sine and cosine. An angle θ in the complex plane is in
standard position if it is measured counterclockwise from the positive
real axis as indicated in Figure A.4. Rather than using degrees to measure
angles, it is more natural to use radian measure. This is deﬁned as follows:
The circle with its centre at the origin and radius 1 (called the unit circle)
is drawn in Figure A.4. It has circumference 2π, and the radian measure
of θ is the length of the arc on the unit circle counterclockwise from 1 to
2 , 45◦ = π
the point P on the unit circle determined by θ. Hence 90◦ = π
4 ,
180◦ = π, and a full circle has the angle 360◦ = 2π. Angles measured clockwise from 1 are negative; for
example,

i corresponds to

Figure A.4

−

x

0
0

1

i

π
2 . If θ is plotted in standard position as in Figure A.4,
it determines a unique point P on the unit circle, and P has coordinates (cosθ, sinθ) by elementary
trigonometry. However, any angle θ (acute or not) determines a unique point on the unit circle, so we
deﬁne the cosine and sine of θ (written cosθ and sinθ) to be the x and y coordinates of this point. For
example, the points

≤

≤

θ

2 (or to 3π
π
2 ).
Consider an angle θ in the range 0

−

−

plotted in Figure A.4 are determined by the angles 0, π

1 = (1, 0)

i = (0, 1)

1 = (

1, 0)

−

−
2 , π, 3π

i = (0,

−

1)

−

2 , respectively. Hence

cos 0 = 1

sin 0 = 0

cos π

sin π

2 = 0
2 = 1

cosπ =

1
−
sinπ = 0

cos 3π

sin 3π

2 = 0
2 =

1
−

Now we can describe the polar form of a complex number. Let z = a + bi be a complex number, and

write the absolute value of z as

y

r =

=

a2 + b2

z

|

|

z = (a, b)

r

b

θ

a

0

x

Figure A.5

= 0, the angle θ shown in Figure A.5 is called an argument of z and

p

If z
is denoted

θ = arg z

This angle is not unique (θ + 2πk would do as well for any
k = 0,

2, . . . ). However, there is only one argument θ in the range

±
π, and this is sometimes called the principal argument of z.

±
π < θ

1,

−

≤

6
592

Complex Numbers

and b of z are related to r and θ by

Returning to Figure A.5, we ﬁnd that the real and imaginary parts a

a = r cosθ
b = r sinθ

Hence the complex number z = a + bi has the form

The combination cosθ + i sinθ is so important that a special notation is used:

z = r(cosθ + i sinθ)

r =

, θ = arg (z)

z

|

|

eiθ = cosθ + i sinθ

is called Euler’s formula after the great Swiss mathematician Leonhard Euler (1707–1783). With this
notation, z is written

|
This is a polar form of the complex number z. Of course it is not unique, because the argument can be
changed by adding a multiple of 2π.

|

z = reiθ r =

z

, θ = arg (z)

Example A.4

Write z1 =

2 + 2i and z2 =

−

i in polar form.

−

Solution.

y

z1 =

−

2 + 2i

θ1

0
z2 =

i

−

θ2

x

The two numbers are plotted in the complex plane in Figure A.6.
The absolute values are

r1 =

2 + 2i

|

| −

r2 =

i

=

| −

|

=

(

q
02 + (

1)2 = 1

−

2)2 + 22 = 2√2

−

q
By inspection of Figure A.6, arguments of z1 and z2 are

Figure A.6

θ1 = arg (
θ2 = arg (

2 + 2i) = 3π
4
i) = 3π
2

−

−

The corresponding polar forms are z1 =
could have taken the argument

2 + 2i = 2√2e3πi/4 and z2 =
π
2 for z2 and obtained the polar form z2 = e−

−

−

πi/2.

i = e3πi/2. Of course, we

−

In Euler’s formula eiθ = cosθ + i sinθ, the number e is the familiar constant e = 2.71828 . . . from
calculus. The reason for using e will not be given here; the reason why cosθ + i sinθ is written as an
exponential function of θ is that the law of exponents holds:

where θ and φ are any two angles. In fact, this is an immediate consequence of the addition identities for
sin(θ +φ) and cos(θ +φ):

eiφ = ei(θ+φ)

eiθ

·

eiθeiφ = (cosθ + i sinθ)(cosφ + i sinφ)

593

sinθsinφ) + i(cosθ sinφ + sinθcosφ)

= (cosθcosφ
= cos(θ +φ) + i sin(θ +φ)
= ei(θ+φ)

−

This is analogous to the rule eaeb = ea+b, which holds for real numbers a and b, so it is not unnatural to
use the exponential notation eiθ for the expression cosθ + i sinθ. In fact, a whole theory exists wherein
functions such as ez, sin z, and cos z are studied, where z is a complex variable. Many deep and beautiful
theorems can be proved in this theory, one of which is the so-called fundamental theorem of algebra
mentioned later (Theorem A.4). We shall not pursue this here.

The geometric description of the multiplication of two complex numbers follows from the law of

exponents.

Theorem A.1: Multiplication Rule

If z1 = r1eiθ1 and z2 = r2eiθ2 arecomplexnumbersinpolarform,then

z1z2 = r1r2ei(θ1+θ2)

In other words, to multiply two complex numbers, simply multiply the absolute values and add the ar-
guments. This simpliﬁes calculations considerably, particularly when we observe that it is valid for any
arguments θ1 and θ2.

Example A.5

Multiply (1

−

Solution.

y

0

1 + √3i

π
3

π
4

−

1

i

−

Figure A.7

i)(1 + √3i) in two ways.

We have

1

|

i

|

−

= √2 and

1 + √3i

|

|

= 2 so, from Figure A.7,

i)(1 + √3i)

(1

−

π
12

x

iπ/4

i = √2e−
1
1 + √3i = 2eiπ/3

−

Hence, by the multiplication rule,

(1

−

i)(1 + √3i) = (√2e−
= 2√2ei(
= 2√2eiπ/12

−

iπ/4)(2eiπ/3)
π/4+π/3)

This gives the required product in polar form. Of course, direct multiplication gives
(1
formulas cos( π

i)(1 + √3i) = (√3 + 1) + (√3
12) = √3+1
and sin( π
2√2

−
12 ) = √3
1
−
2√2

1)i. Hence, equating real and imaginary parts gives the

−

.

594

Complex Numbers

Roots of Unity

If a complex number z = reiθ is given in polar form, the powers assume a particularly simple form. In
fact, z2 = (reiθ)(reiθ) = r2e2iθ, z3 = z2
z = (r2e2iθ)(reiθ) = r3e3iθ, and so on. Continuing in this way,
it follows by induction that the following theorem holds for any positive integer n. The name honours
Abraham De Moivre (1667–1754).

·

Theorem A.2: De Moivre’s Theorem
Ifθ isanyangle,then (eiθ)n = einθ holdsforallintegers n.

Proof. The case n > 0 has been discussed, and the reader can verify the result for n = 0. To derive it for
n < 0, ﬁrst observe that

z−
iθ) = 1ei0 = 1 by the multiplication rule. Now assume that n is negative and write it as

r e−

then

= 0

if

1 = 1

iθ

z = reiθ

In fact, (reiθ)( 1
n =

m, m > 0. Then

r e−

−

(reiθ)n = [(reiθ)−

1]m = ( 1

iθ)m = r−

mei(

mθ) = rneinθ
−

r e−

If r = 1, this is De Moivre’s theorem for negative n.

Example A.6

y

1 + √3i

−

Verify that (

1 + √3i)3 = 8.

−

1 + √3i
Solution. We have
(see Figure A.8). Hence De Moivre’s theorem gives

= 2, so

| −

−

|

1 + √3i = 2e2πi/3

2

2π
3

(

1 + √3i)3 = (2e2πi/3)3 = 8e3(2πi/3) = 8e2πi = 8

−

0

x

Figure A.8

De Moivre’s theorem can be used to ﬁnd nth roots of complex numbers where n is positive. The next

example illustrates this technique.

Example A.7

Find the cube roots of unity; that is, ﬁnd all complex numbers z such that z3 = 1.

Solution. First write z = reiθ and 1 = 1ei0 in polar form. We must use the condition z3 = 1 to
determine r and θ. Because z3 = r3e3iθ by De Moivre’s theorem, this requirement becomes

r3e3iθ = 1e0i

These two complex numbers are equal, so their absolute values must be equal and the arguments

6
595

must either be equal or differ by an integral multiple of 2π:

r3 = 1
3θ = 0 + 2kπ,

k some integer

Because r is real and positive, the condition r3 = 1 implies that r = 1. However,

θ = 2kπ
3 ,

k some integer

y

1

2 + √3
2 i

−

2π
3

4π
3

0

x

1

1
2 −

−

√3
2 i

Figure A.9

seems at ﬁrst glance to yield inﬁnitely many different angles for
z. However, choosing k = 0, 1, 2 gives three possible arguments
θ (where 0

θ < 2π), and the corresponding roots are

≤

1e0i = 1

1e2πi/3 =
1e4πi/3 =

−

−

1

2 + √3
2 i
√3
2 i

1
2 −

These are displayed in Figure A.9. All other values of k yield
values of θ that differ from one of these by a multiple of 2π—and

so do not give new roots. Hence we have found all the roots.

The same type of calculation gives all complex nth roots of unity; that is, all complex numbers z such

that zn = 1. As before, write 1 = 1e0i and

in polar form. Then zn = 1 takes the form

z = reiθ

rneniθ = 1e0i

using De Moivre’s theorem. Comparing absolute values and arguments yields

rn = 1
nθ = 0 + 2kπ,

k some integer

Hence r = 1, and the n values

of θ all lie in the range 0
from one of these by a multiple of 2π, so these give the arguments of all the possible roots.

−
θ < 2π. As in Example A.7, every choice of k yields a value of θ that differs

≤

θ = 2kπ
n ,

k = 0, 1, 2, . . . , n

1

Theorem A.3: nth Roots of Unity

If n

≥

1 isaninteger,the nthrootsofunity(thatis,thesolutionsto zn = 1)aregivenby

z = e2πki/n,

k = 0, 1, 2, . . . , n

1

−

596

Complex Numbers

y

0

e2πi/5

1 = e0i
x

e4πi/5

e6πi/5

e8πi/5

Figure A.10

Example A.8

The nth roots of unity can be found geometrically as the points on the unit
circle that cut the circle into n equal sectors, starting at 1. The case n = 5
is shown in Figure A.10, where the ﬁve ﬁfth roots of unity are plotted.

The method just used to ﬁnd the nth roots of unity works equally well
to ﬁnd the nth roots of any complex number in polar form. We give one
example.

Find the fourth roots of √2 + √2i.

Solution. First write √2 + √2i = 2eπi/4 in polar form. If z = reiθ satisﬁes z4 = √2 + √2i, then De
Moivre’s theorem gives

r4ei(4θ) = 2eπi/4

Hence r4 = 2 and 4θ = π

4 + 2kπ, k an integer. We obtain four distinct roots (and hence all) by

r = 4√2, θ = π

16 = 2kπ

16 , k = 0, 1, 2, 3

Thus the four roots are

4√2eπi/16

4√2e9πi/16

4√2e17πi/16

4√2e25πi/16

Of course, reducing these roots to the form a + bi would require the computation of 4√2 and the
sine and cosine of the various angles.

An expression of the form ax2 + bx + c, where the coefﬁcients a

= 0, b, and c are real numbers, is
called a real quadratic. A complex number u is called a root of the quadratic if au2 + bu + c = 0. The
roots are given by the famous quadratic formula:

b
u = −
±

√b2
2a

4ac

−

The quantity d = b2
4ac is called the discriminant of the quadratic ax2 + bx + c, and there is no real
root if and only if d < 0. In this case the quadratic is said to be irreducible. Moreover, the fact that d < 0
means that √d = i
, so the two (complex) roots are conjugates of each other:
|

−

d

|

p

u = 1

2a(

b + i

−

d

)
|

|

and

u = 1

2a (

b
−

−

i

d

)
|

|
p

The converse of this is true too: Given any nonreal complex number u, then u and u are the roots of some
real irreducible quadratic. Indeed, the quadratic

p

has real coefﬁcients (uu =
u and u are not real.

u

|

|

x2

(u + u)x + uu = (x

u)(x

u)

−

−
2 and u + u is twice the real part of u) and so is irreducible because its roots

−

6
597

Example A.9

Find a real irreducible quadratic with u = 3

4i as a root.

−

Solution. We have u + u = 6 and
roots.

u

|

|

2 = 25, so x2

−

6x + 25 is irreducible with u and u = 3 + 4i as

Fundamental Theorem of Algebra

As we mentioned earlier, the complex numbers are the culmination of a long search by mathematicians
to ﬁnd a set of numbers large enough to contain a root of every polynomial. The fact that the complex
numbers have this property was ﬁrst proved by Gauss in 1797 when he was 20 years old. The proof is
omitted.

Theorem A.4: Fundamental Theorem of Algebra

Everypolynomialofpositivedegreewithcomplexcoefﬁcientshasacomplexroot.

If f (x) is a polynomial with complex coefﬁcients, and if u1 is a root, then the factor theorem (Section 6.5)
asserts that

−
where g(x) is a polynomial with complex coefﬁcients and with degree one less than the degree of f (x).
Suppose that u2 is a root of g(x), again by the fundamental theorem. Then g(x) = (x

u2)h(x), so

f (x) = (x

u1)g(x)

−

f (x) = (x

u1)(x

−

−

u2)h(x)

This process continues until the last polynomial to appear is linear. Thus f (x) has been expressed as a
un), where u and un are
product of linear factors. The last of these factors can be written in the form u(x
complex (verify this), so the fundamental theorem takes the following form.

−

Theorem A.5

Everycomplexpolynomial f (x) ofdegree n

1 hastheform

≥
u1)(x

−

f (x) = u(x

u2)

(x

−

· · ·

−

un)

where u, u1, . . . , un arecomplexnumbersand u
f (x) (andneednotallbedistinct),and u isthecoefﬁcientof xn.

= 0. Thenumbers u1, u2, . . . , un aretherootsof

This form of the fundamental theorem, when applied to a polynomial f (x) with real coefﬁcients, can be
used to deduce the following result.

Theorem A.6

Everypolynomial f (x) ofpositivedegreewithrealcoefﬁcientscanbefactoredasaproductof
linearandirreduciblequadraticfactors.

6
598

Complex Numbers

In fact, suppose f (x) has the form

1xn
f (x) = anxn + an
1 +
−
−

· · ·

+ a1x + a0

where the coefﬁcients ai are real. If u is a complex root of f (x), then we claim ﬁrst that u is also a root. In
fact, we have f (u) = 0, so

1un
0 = 0 = f (u) = anun + an
1 +
−
−
= anun + an
1un
1 +
−
−
= anun + an
1un
1 +
−
−
1un
= anun + an
1 +
−
−
= f (u)

· · ·

· · ·

· · ·

· · ·

+ a1u + a0

+ a1u + a0
+ a1u + a0
+ a1u + a0

where ai = ai for each i because the coefﬁcients ai are real. Thus if u is a root of f (x), so is its conjugate
u. Of course some of the roots of f (x) may be real (and so equal their conjugates), but the nonreal roots
come in pairs, u and u. By Theorem A.6, we can thus write f (x) as a product:

f (x) = an(x

r1)

(x

rk)(x

u1)(x

u1)

(x

um)(x

um)

−
where an is the coefﬁcient of xn in f (x); r1, r2, . . . , rk are the real roots; and u1, u1, u2, u2, . . . , um, um
are the nonreal roots. But the product

· · ·

· · ·

−

−

−

−

−

(A.1)

(x

u j)(x

u j) = x2

(u j + u j)x + (u ju j)

−
is a real irreducible quadratic for each j (see the discussion preceding Example A.9). Hence (A.1) shows
that f (x) is a product of linear and irreducible quadratic factors, each with real coefﬁcients. This is the
conclusion in Theorem A.6.

−

−

Exercises for A

Exercise A.1 Solve each of the following for the real
number x.

Exercise A.2 Convert each of the following to the form
a + bi.

(2

(3

3i)

2(2

3i) + 9

−

−

−
2i)(1 + i) +

a.

x

−

4i = (2

i)2

−

b.

(2 + xi)(3

2i)
−
= 12 + 5i

c.

(2 + xi)2 = 4

d.

(2 + xi)(2

xi) = 5

−

a.

b.

c.

e.

g.

i.

i
−
2+3i

−
1+i
3i + 1
2
−
i131
(1 + i)4

−

3√3
i
−
√3+i

+ √3+7i
√3
i
−

3 + 4i
|
|

d.

f.

3
−
1
−
(2

7i
3i

2i
i + 3
−
2
−
i)3
i)2(2 + i)2

−

h.

(1

−

Exercise A.3 In each case, ﬁnd the complex number z.

a.

iz

−

(1 + i)2 = 3

i

−

b.

(i + z)
iz + 1

−

3i(2

−

z) =

c.

z2 =

i

−

e.

z(1 + i) = z + (3 + 2i)

d.

f.

z2 = 3

4i

−
i) = (z + 1)(1 +

z(2
i)

−

Exercise A.4
quadratic equation.

In each case, ﬁnd the roots of the real

599

Exercise A.10 In each case, show that u is a root of the
quadratic equation, and ﬁnd the other root.

a.

b.

c.

d.

x2
−
x2 + ix
x2
(3
−
−
x2 + 3(1

−

3ix + (

3 + i) = 0; u = 1 + i

−

−

(4

2i) = 0; u =

2

−
i) = 0; u = 2

3i

−

2i)x + (5

−

i)x

−

−

5i = 0; u =

2 + i

−

x + 1 = 0

Exercise A.11 Find the roots of each of the following
complex quadratic equations.

a.

c.

x2

−
3x2

−

2x + 3 = 0

4x + 2 = 0

b.

d.

x2

−
2x2

−

5x + 2 = 0

Exercise A.5 Find all numbers x in each case.

a.

c.

x3 = 8

x4 = 16

b.

d.

8

x3 =

−
x4 = 64

Exercise A.6 In each case, ﬁnd a real quadratic with u
as a root, and ﬁnd the other root.

a.

c.

d.

x2 + 2x + (1 + i) = 0
x2
x2

i)x + (3

−
3(1

−
5i = 0

i)x

(2

−

−

−

−

i) = 0

b.

x2

x + (1

−

−

i) = 0

Exercise A.12 In each case, describe the graph of the
equation (where z denotes a complex number).

a.

c.

e.

= 1

z
|
|
z = iz

z =

z
|
|

Exercise A.13

b.

d.

f.

= 2

z
|
−
z =

1
|
z

−
im z = m
·
real number

re z, m a

a.

u = 1 + i

c.

u =

i

−

b.

u = 2

d.

u = 3

3i

4i

−

−

a. Verify

zw
|
|
w = c + di.

=

w

z
||
|

|

directly for z = a + bi and

b. Deduce (a) from properties C2 and C6.

Exercise A.7 Find the roots of x2
any angle.

−

2 cosθx + 1 = 0, θ

Exercise A.14 Prove that
z + w
|
|
for all complex numbers w and z.

=

2 +
z
|
|

w
|

2 + wz + wz
|

Exercise A.8 Find a real polynomial of degree 4 with
2

2i as roots.

i and 3

−

−

Exercise A.9 Let re z and im z denote, respectively, the
real and imaginary parts of z. Show that:

a.

im (iz) = re z

b.

re (iz) =

im z

c.

z + z = 2 re z

d.

z

−

−
z = 2i im z

Exercise A.15 If zw is real and z
for some real number a.

= 0, show that w = az

Exercise A.16 If zw = zv and z
for some u in C with
u
|
|
Exercise A.17 Show that (1 + i)n + (1
all n, using property C5.

= 1.

= 0, show that w = uv

i)n is real for

−

Exercise A.18 Express each of the following in polar
form (use the principal argument).

e.

f.

re (z + w) = re z + re w, and re (tz) = t
real

·

re z if t is

im (z + w) = im z + im w, and im (tz) = t
t is real

·

im z if

a.

c.

e.

3

3i

−
√3 + i

−

−

7i

4i

4 + 4√3i

6 + 6i

b.

d.

f.

−

−

−

6
6
600

Complex Numbers

Exercise A.19 Express each of the following in the form
a + bi.

Exercise A.24 If z = reiθ in polar form, show that:

a.

z = re−

iθ

b.

z−

1 = 1

r e−

iθ if z

= 0

a.

c.

e.

3eπi

2e3πi/4

e5πi/4

b.

d.

f.

e7πi/3

√2e−

πi/4

2√3e−

2πi/6

Exercise A.25 Show that the sum of the nth roots of
unity is zero.
zn = (1
[Hint: 1
−
complex number z.]

z)(1 + z + z2 +

1) for any

+ zn
−

· · ·

−

Exercise A.20 Express each of the following in the form
a + bi.

Exercise A.26

a.

c.

e.

1 + √3i)2

(
−
(1 + i)8

i)6(√3 + i)3

(1

−

b.

d.

f.

4

(1 + √3i)−
i)10

(1

−
(√3

i)9(2

2i)5

−

−

Exercise A.21 Use De Moivre’s theorem to show that:

a. cos 2θ = cos2 θ

−

sin2 θ; sin 2θ = 2 cosθsinθ

b. cos 3θ = cos3 θ

sin 3θ = 3 cos2 θsinθ

3 cosθsin2 θ;
sin3 θ

−

−

Exercise A.22

a. Find the fourth roots of unity.

b. Find the sixth roots of unity.

Exercise A.23 Find all complex numbers z such that:

a.

c.

z4 =
z3 =

1

27i

−

−

b.

d.

z4 = 2(√3i
z6 =

64

−

1)

−

a. Let z1, z2, z3, z4, and z5 be equally spaced around
the unit circle. Show that z1 + z2 + z3 + z4 + z5 = 0.
z5 for any
z)(1 + z + z2 + z3 + z4) = 1
[Hint: (1
−
complex number z.]

−

b. Repeat (a) for any n
around the unit circle.

≥

2 points equally spaced

= 1, show that the sum of the roots of zn = w

c. If

w
|
is zero.

|

Exercise A.27 If zn is real, n

1, show that (z)n is real.
If z2 = z2, show that z is real or pure

≥

Exercise A.28
imaginary.

Exercise A.29 If a and b are rational numbers, let p and
q denote numbers of the form a + b√2. If p = a + b√2,
2b2. Show that each
deﬁne ˜p = a
of the following holds.

b√2 and [p] = a2

−

−

a.

b.

d.

f.

a + b√2 = a1 + b1√2 only if a = a1 and b = b1
]p
±

pq = ˜p ˜q

q = ˜p

±

c.

˜q

[p] = p ˜p

e.

[pq] = [p][q]
f

If f (x) is a polynomial with rational coefﬁcients
and p = a + b√2 is a root of f (x), then ˜p is also a
root of f (x).

6
Appendix B

Proofs

Logic plays a basic role in human affairs. Scientists use logic to draw conclusions from experiments,
judges use it to deduce consequences of the law, and mathematicians use it to prove theorems. Logic
arises in ordinary speech with assertions such as “If John studies hard, he will pass the course,” or “If an
integer n is divisible by 6, then n is divisible by 3.”1 In each case, the aim is to assert that if a certain
statement is true, then another statement must also be true. In fact, if p and q denote statements, most
theorems take the form of an implication: “If p is true, then q is true.” We write this in symbols as

⇒
and read it as “p implies q.” Here p is the hypothesis and q the conclusion of the implication. The
q is valid is called the proof of the implication. In this section we examine the most
veriﬁcation that p
common methods of proof2 and illustrate each technique with some examples.

⇒

p

q

Method of Direct Proof

q, demonstrate directly that q is true whenever p is true.

To prove that p

⇒

Example B.1

If n is an odd integer, show that n2 is odd.

Solution. If n is odd, it has the form n = 2k + 1 for some integer k. Then
n2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1 also is odd because 2k2 + 2k is an integer.

Note that the computation n2 = 4k2 + 4k + 1 in Example B.1 involves some simple properties of arith-
metic that we did not prove. These properties, in turn, can be proved from certain more basic properties
of numbers (called axioms)—more about that later. Actually, a whole body of mathematical information
lies behind nearly every proof of any complexity, although this fact usually is not stated explicitly. Here is
a geometrical example.

1By an integer we mean a “whole number”; that is, a number in the set 0,
2For a more detailed look at proof techniques see D. Solow, How to Read and Do Proofs, 2nd ed. (New York: Wiley, 1990);

3, . . .

±

±

±

2,

1,

or J. F. Lucas. Introduction to Abstract Mathematics, Chapter 2 (Belmont, CA: Wadsworth, 1986).

601

602

Proofs

Example B.2

In a right triangle, show that the sum of the two acute angles is 90 degrees.

Solution.

β

α

β

α

β

α

The right triangle is shown in the diagram. Construct a rectangle
with sides of the same length as the short sides of the original
triangle, and draw a diagonal as shown. The original triangle
appears on the bottom of the rectangle, and the top triangle is
identical to the original (but rotated). Now it is clear that α +β
is a right angle.

Geometry was one of the ﬁrst subjects in which formal proofs were used—Euclid’s Elements was
published about 300 B.C. The Elements is the most successful textbook ever written, and contains many
of the basic geometrical theorems that are taught in school today. In particular, Euclid included a proof of
an earlier theorem (about 500 B.C.) due to Pythagoras. Recall that, in a right triangle, the side opposite
the right angle is called the hypotenuse of the triangle.

Example B.3: Pythagoras’ Theorem

In a right-angled triangle, show that the square of the length
of the hypotenuse equals the sum of the squares of the lengths
of the other two sides.

Solution. Let the sides of the right triangle have lengths a, b, and
c as shown. Consider two squares with sides of length a + b, and
place four copies of the triangle in these squares as in the diagram.
The central rectangle in the second square shown is itself a square
because the angles α and β add to 90 degrees (using Example B.2),
so its area is c2 as shown. Comparing areas shows that both
a2 + b2 and c2 each equal the area of the large square minus
four times the area of the original triangle, and hence are equal.

c

α
b

β

a

a

a2

a

b

b2

b

b

α

a
β

b

α

β

a

c2

β
a

α

b

β

α

a

b

Sometimes it is convenient (or even necessary) to break a proof into parts, and deal with each case

separately. We formulate the general method as follows:

Method of Reduction to Cases

603

To prove that p
⇒
then show that pi

q for each i.

⇒

q, show that p implies at least one of a list p1, p2, . . . , pn of statements (the cases) and

Example B.4

Show that n2

≥

0 for every integer n.

Solution. This statement can be expressed as an implication: If n is an integer, then n2
prove it, consider the following three cases:

≥

0. To

(1) n > 0;

(2) n = 0;

(3) n < 0.

Then n2 > 0 in Cases (1) and (3) because the product of two positive (or two negative) integers is
positive. In Case (2) n2 = 02 = 0, so n2
0 in every case.

≥

Example B.5

If n is an integer, show that n2

−
Solution. We consider two cases:

n is even.

n = n(n

We have n2
−
−
again even. Similarly, n
Hence n2

−

n is even in any case.

−

(1) n is even;

(2) n is odd.

1), so this is even in Case (1) because any multiple of an even number is

1 is even in Case (2) so n(n

1) is again even for the same reason.

−

The statements used in mathematics are required to be either true or false. This leads to a proof
technique which causes consternation in many beginning students. The method is a formal version of a
debating strategy whereby the debater assumes the truth of an opponent’s position and shows that it leads
to an absurd conclusion.

Method of Proof by Contradiction

To prove that p
other words, if p is true, then q must be true; that is, p

⇒

q, show that the assumption that both p is true and q is false leads to a contradiction. In

q.

⇒

Example B.6

If r is a rational number (fraction), show that r2

= 2.

Solution. To argue by contradiction, we assume that r is a rational number and that r2 = 2, and
show that this assumption leads to a contradiction. Let m and n be integers such that r = m
n is in
lowest terms (so, in particular, m and n are not both even). Then r2 = 2 gives m2 = 2n2, so m2 is
even. This means m is even (Example B.1), say m = 2k. But then 2n2 = m2 = 4k2, so n2 = 2k2 is

6
604

Proofs

even, and hence n is even. This shows that n and m are both even, contrary to the choice of these
numbers.

Example B.7: Pigeonhole Principle

If n + 1 pigeons are placed in n holes, then some hole contains at least 2 pigeons.

Solution. Assume the conclusion is false. Then each hole contains at most one pigeon and so,
since there are n holes, there must be at most n pigeons, contrary to assumption.

The next example involves the notion of a prime number, that is an integer that is greater than 1 which
cannot be factored as the product of two smaller positive integers both greater than 1. The ﬁrst few primes
are 2, 3, 5, 7, 11, . . . .

Example B.8
If 2n

−

1 is a prime number, show that n is a prime number.

Solution. We must show that p
statement “n is a prime.” Suppose that p is true but q is false so that n is not a prime, say n = ab
where a
factors:

2 are integers. If we write 2a = x, then 2n = 2ab = (2a)b = xb. Hence 2n

1 is a prime”, and q is the

q where p is the statement “2n

2 and b

⇒

−

−

≥

≥

1

2n

1 = xb

1 = (x

1 + xb
1)(xb
2 +
−
−

+ x2 + x + 1)

−

−

−

· · ·

As x
assumption that 2n

4, this expression is a factorization of 2n
1 is prime.

≥

−

1 into smaller positive integers, contradicting the

−

The next example exhibits one way to show that an implication is not valid.

Example B.9

Show that the implication “n is a prime

2n

1 is a prime” is false.

⇒
Solution. The ﬁrst four primes are 2, 3, 5, and 7, and the corresponding values for 2n
1 are 3, 7,
31, 127 (when n = 2, 3, 5, 7). These are all prime as the reader can verify. This result seems to be
evidence that the implication is true. However, the next prime is 11 and 211
89,
which is clearly not a prime.

1 = 2047 = 23

−

−

−

·

We say that n = 11 is a counterexample to the (proposed) implication in Example B.9. Note that, if you
can ﬁnd even one example for which an implication is not valid, the implication is false. Thus disproving
implications is in a sense easier than proving them.

The implications in Example B.8 and Example B.9 are closely related: They have the form p

q and
p, where p and q are statements. Each is called the converse of the other and, as these examples
p are valid,

q
show, an implication can be valid even though its converse is not valid. If both p
⇒
the statements p and q are called logically equivalent. This is written in symbols as

q and q

⇒

⇒

⇒

p

q

⇔

and is read “p if and only if q”. Many of the most satisfying theorems make the assertion that two
statements, ostensibly quite different, are in fact logically equivalent.

605

Example B.10

If n is an integer, show that “n is odd

n2 is odd.”

⇔

Solution. In Example B.1 we proved the implication “n is odd
converse by contradiction. If n2 is odd, we assume that n is not odd. Then n is even, say n = 2k, so
n2 = 4k2, which is also even, a contradiction.

n2 is odd.” Here we prove the

⇒

Many more examples of proofs can be found in this book and, although they are often more complex,
most are based on one of these methods. In fact, linear algebra is one of the best topics on which the
reader can sharpen his or her skill at constructing proofs. Part of the reason for this is that much of linear
algebra is developed using the axiomatic method. That is, in the course of studying various examples
it is observed that they all have certain properties in common. Then a general, abstract system is studied
in which these basic properties are assumed to hold (and are called axioms). In this system, statements
(called theorems) are deduced from the axioms using the methods presented in this appendix. These
theorems will then be true in all the concrete examples, because the axioms hold in each case. But this
procedure is more than just an efﬁcient method for ﬁnding theorems in the examples. By reducing the
proof to its essentials, we gain a better understanding of why the theorem is true and how it relates to
analogous theorems in other abstract systems.

The axiomatic method is not new. Euclid ﬁrst used it in about 300 B.C. to derive all the propositions of
(euclidean) geometry from a list of 10 axioms. The method lends itself well to linear algebra. The axioms
are simple and easy to understand, and there are only a few of them. For example, the theory of vector
spaces contains a large number of theorems derived from only ten simple axioms.
Exercises for B

Exercise B.1 In each case prove the result and either
prove the converse or give a counterexample.

a. If n is an even integer, then n2 is a multiple of 4.

b. If m is an even integer and n is an odd integer, then

m + n is odd.

c. If n is any integer, n3

n = 3k for some integer k.
[Hint: Use the fact that each integer has one of the
forms 3k, 3k + 1, or 3k + 2, where k is an integer.]

−

Exercise B.3 In each case prove the result by contradic-
tion and either prove the converse or give a counterexam-
ple.

6 = 0.

a. If n > 2 is a prime integer, then n is odd.

c. If x = 2 or x = 3, then x3

6x2 + 11x

−
5x + 6 = 0, then x = 2 or x = 3.

−

d. If x2

−

Exercise B.2
splitting into cases, or give a counterexample.

In each case either prove the result by

a. If n is any integer, then n2 = 4k + 1 for some inte-

ger k.

b. If n is any odd integer, then n2 = 8k + 1 for some

b. If n + m = 25 where n and m are integers, then one

of n and m is greater than 12.

c. If a and b are positive numbers and a

√a

≤

√b.

b, then

≤

d. If m and n are integers and mn is even, then m is

even or n is even.

integer k.

Exercise B.4 Prove each implication by contradiction.

606

Proofs

a. If x and y are positive numbers, then

√x + y

= √x + √y.

b. If x is irrational and y is rational, then x + y is irra-

tional.

c. If 13 people are selected, at least 2 have birthdays

in the same month.

Exercise B.5 Disprove each statement by giving a coun-
terexample.

a. n2 + n + 11 is a prime for all positive integers n.

2n for all integers n

2.

≥

b. n3

≥
c. If n

≥

2 points are arranged on a circle in such a
way that no three of the lines joining them have
a common point, then these lines divide the circle
into 2n
1 regions. [The cases n = 2, 3, and 4 are
−
shown in the diagram.]

n = 2

n = 3

n = 4

Exercise B.6 The number e from calculus has a series
expansion

e = 1 + 1

1! + 1

2! + 1

3! +

· · ·

where n! = n(n
· · ·
Prove that e is irrational by contradiction.
e = m/n, consider

1)

−

2

3

·

·

1 for each integer n

≥
[Hint:

1.
If

k = n!

e

1

1
1! −

1
2! −

1
3! − · · · −

1
n!

−

−

.

Show that k is a positive integer and that

(cid:0)

(cid:1)

k = 1

n+1 +

1
(n+1)(n+2) +

< 1

n .]

· · ·

6
Appendix C

Mathematical Induction

Suppose one is presented with the following sequence of equations:

1 = 1
1 + 3 = 4
1 + 3 + 5 = 9
1 + 3 + 5 + 7 = 16
1 + 3 + 5 + 7 + 9 = 25

It is clear that there is a pattern. The numbers on the right side of the equations are the squares 12, 22, 32,
42, and 52 and, in the equation with n2 on the right side, the left side is the sum of the ﬁrst n odd numbers.
The odd numbers are

1 = 2
3 = 2
5 = 2
7 = 2
9 = 2

1
1
1
1
1

1
2
3
4
5

·
·
·
·
·

−
−
−
−
−

and from this it is clear that the nth odd number is 2n
following is true:

−

1 + 3 +

+ (2n

−

· · ·

1. Hence, at least for n = 1, 2, 3, 4, or 5, the

1) = n2

(Sn)

The question arises whether the statement Sn is true for every n. There is no hope of separately verifying
all these statements because there are inﬁnitely many of them. A more subtle approach is required.

The idea is as follows: Suppose it is veriﬁed that the statement Sn+1 will be true whenever Sn is true.
That is, suppose we prove that, if Sn is true, then it necessarily follows that Sn+1 is also true. Then, if we
can show that S1 is true, it follows that S2 is true, and from this that S3 is true, hence that S4 is true, and
so on and on. This is the principle of induction. To express it more compactly, it is useful to have a short
way to express the assertion “If Sn is true, then Sn+1 is true.” As in Appendix B, we write this assertion as

and read it as “ Sn implies Sn+1.” We can now state the principle of mathematical induction.

Sn

⇒

Sn+1

607

608

Mathematical Induction

The Principle of Mathematical Induction

Suppose Sn isastatementaboutthenaturalnumber n foreach n = 1, 2, 3, . . ..
Supposefurtherthat:

1. S1 istrue.

2. Sn

⇒

Sn+1 forevery n

1.

≥

Then Sn istrueforevery n

1.

≥

This is one of the most useful techniques in all of mathematics. It applies in a wide variety of situations,
as the following examples illustrate.

Example C.1

Show that 1 + 2 +

+ n = 1

2n(n + 1) for n

1.

≥

· · ·

Solution. Let Sn be the statement: 1 + 2 +

1. S1 is true. The statement S1 is 1 = 1

+ n = 1

2 n(n + 1) for n

· · ·
21(1 + 1), which is true.

1. We apply induction.

≥

2. Sn

⇒

Sn+1. We assume that Sn is true for some n

1—that is, that

≥
+ n = 1
2n(n + 1)

We must prove that the statement

1 + 2 +

· · ·

Sn+1 : 1 + 2 +

· · ·

+ (n + 1) = 1

2 (n + 1)(n + 2)

is also true, and we are entitled to use Sn to do so. Now the left side of Sn+1 is the sum of the ﬁrst
n + 1 positive integers. Hence the second-to-last term is n, so we can write

1 + 2 +

· · ·

+ n) + (n + 1)

+ (n + 1) = (1 + 2 +
= 1
= 1

· · ·

2n(n + 1) + (n + 1) using Sn
2(n + 1)(n + 2)

This shows that Sn+1 is true and so completes the induction.

In the veriﬁcation that Sn

Sn+1, we assume that Sn is true and use it to deduce that Sn+1 is true. The

assumption that Sn is true is sometimes called the induction hypothesis.

⇒

Example C.2

If x is any number such that x

Solution. Let Sn be the statement: 1 + x + x2 +

· · ·

= 1, show that 1 + x + x2 +

+ xn = xn+1
1
−
x
1
−

for n

1.

≥

· · ·
+ xn = xn+1
1
1 .
−
x
−

6
1. S1 is true. S1 reads 1 + x = x2
1
1 , which is true because x2
−
x
−

2. Sn

⇒

Sn+1. Assume the truth of Sn : 1 + x + x2 +

· · ·

1 = (x

−
+ xn = xn+1
1
1 .
−
x
−

609

1)(x + 1).

−

We must deduce from this the truth of Sn+1: 1 + x + x2 +
side of Sn+1 and using the induction hypothesis, we ﬁnd

+ xn+1 = xn+2
1
1 . Starting with the left
−
x
−

·

1 + x + x2 +

· · ·

+ xn) + xn+1

1
−

+ xn+1 = (1 + x + x2 +
= xn+1
1 + xn+1
x
−
1+xn+1(x
= xn+1
x
1
−
= xn+2
1
−
x
1
−

−

· · ·

1)

−

This shows that Sn+1 is true and so completes the induction.

Both of these examples involve formulas for a certain sum, and it is often convenient to use summation
notation. For example, ∑n
1), k is to be given the values
1) means that in the expression (2k
k = 1, k = 2, k = 3, . . . , k = n, and then the resulting n numbers are to be added. The same thing applies
to other expressions involving k. For example,

k=1(2k

−

−

n
∑
k=1

k3 = 13 + 23 +

+ n3

· · ·

5
∑
k=1

(3k

−

1) = (3

1) + (3

1

·

−

1) + (3

2

·

−

1) + (3

3

·

−

1) + (3

4

·

−

1)

5

·

−

The next example involves this notation.

Example C.3

Show that ∑n

k=1(3k2

−

k) = n2(n + 1) for each n

1.

≥

Solution. Let Sn be the statement: ∑n

k=1(3k2

k) = n2(n + 1).

−

1. S1 is true. S1 reads (3

12

·

−

1) = 12(1 + 1), which is true.

2. Sn

⇒

Sn+1. Assume that Sn is true. We must prove Sn+1:

n+1
∑
k=1

(3k2

−

This proves that Sn+1 is true.

(n + 1)]

k) =

n
∑
k=1

(3k2

k) + [3(n + 1)2

−

−
= n2(n + 1) + (n + 1)[3(n + 1)
= (n + 1)[n2 + 3n + 2]
= (n + 1)[(n + 1)(n + 2)]
= (n + 1)2(n + 2)

1]

−

(using Sn)

610

Mathematical Induction

We now turn to examples wherein induction is used to prove propositions that do not involve sums.

Example C.4
Show that 7n + 2 is a multiple of 3 for all n

1.

≥

Solution.

1. S1 is true: 71 + 2 = 9 is a multiple of 3.

2. Sn

Sn+1. Assume that 7n + 2 is a multiple of 3 for some n

⇒

integer m. Then

1; say, 7n + 2 = 3m for some

≥

7n+1 + 2 = 7(7n) + 2 = 7(3m

2) + 2 = 21m

12 = 3(7m

−
so 7n+1 + 2 is also a multiple of 3. This proves that Sn+1 is true.

−

4)

−

In all the foregoing examples, we have used the principle of induction starting at 1; that is, we have
1, and then we have concluded that Sn is true for

≥
1. But there is nothing special about 1 here. If m is some ﬁxed integer and we verify that

veriﬁed that S1 is true and that Sn
every n

Sn+1 for each n

⇒

≥
1. Sm is true.

2. Sn

⇒

Sn+1 for every n

m.

≥

then it follows that Sn is true for every n
m. This “extended” induction principle is just as plausible as
the induction principle and can, in fact, be proved by induction. The next example will illustrate it. Recall
that if n is a positive integer, the number n! (which is read “n-factorial”) is the product

≥

of all the numbers from n to 1. Thus 2! = 2, 3! = 6, and so on.

n! = n(n

1)(n

2)

1

3

2

·

·

· · ·

−

−

Example C.5
Show that 2n < n! for all n

4.

≥

Solution. Observe that 2n < n! is actually false if n = 1, 2, 3.

1. S4 is true. 24 = 16 < 24 = 4!.

2. Sn

⇒

Sn+1 if n

≥

4. Assume that Sn is true; that is, 2n < n!. Then

2n
n!

2n+1 = 2
< 2
< (n + 1)n! because 2 < n + 1
= (n + 1)!

because 2n < n!

·
·

Hence Sn+1 is true.

Exercises for C

1.

In Exercises 1–19, prove the given statement by in-
duction for all n
Exercise C.1 1 + 3 + 5 + 7 +
Exercise C.2 12 + 22 +
Exercise C.3 13 + 23 +

1) = n2
6 n(n + 1)(2n + 1)
+ n)2

· · ·
+ n2 = 1
+ n3 = (1 + 2 +

+ (2n

· · ·

≥

−

· · ·
3 +

+ n(n + 1)

· · ·

· · ·

Exercise C.4 1
= 1

3 n(n + 1)(n + 2)

·

2 + 2

·

·

22 + 2

Exercise C.6

Exercise C.5 1
= 1

32 +
·
12 n(n + 1)(n + 2)(3n + 5)
1
1
·

3 +
2
·
Exercise C.7 12 + 32 +
3 + 1

2 + 1

· · ·

· · ·
4 +
3
2
·
·

1
2
1
·
·

Exercise C.8
= n(n+3)

4(n+1)(n+2)

+ n(n + 1)2

· · ·

+ (2n

+ 1

n(n+1) = n
n+1
1)2 = n
−
1
n(n+1)(n+2)

+

· · ·

3 (4n2

Exercise C.9 1 + 2 + 22 +
· · ·
Exercise C.10 3 + 33 + 35 +

1 = 2n

+ 2n
−
+ 32n
−

1
−
1 = 3
8 (9n
1
n

· · ·
+ 1

n2 ≤

2

−

12 + 1
Exercise C.11
Exercise C.12 n < 2n

1

22 +

· · ·

Exercise C.13 For any integer m > 0, m!n! < (m + n)!

Exercise C.14

Exercise C.15

1
√1
1
√1

+ 1
√2
+ 1
√2

+

+ 1

√n ≤

· · ·

2√n

1

−

+

+ 1

√n

· · ·
Exercise C.16 n3 + (n + 1)3 + (n + 2)3 is a multiple of
9.

√n ≥

Exercise C.17 5n + 3 is a multiple of 4.
Exercise C.18 n3
Exercise C.19 32n+1 + 2n+2 is a multiple of 7.

n is a multiple of 3.

−

611

a. Sn

b. Sn

c. Sn

⇒

⇒

⇒

Sn+2 for each n

Sn+8 for each n

Sn+1 for each n

1.

1.

10.

≥

≥

≥

d. Both Sn and Sn+1

Sn+2 for each n

1.

≥

⇒

Exercise C.23 If Sn is a statement for each n
that Sn is true for all n
two conditions hold:

1, argue
1 if it is known that the following

≥

≥

1)

−

1. Sn

Sn

−

⇒

1 for each n

2.

≥

2. Sn is true for inﬁnitely many values of n.

Exercise C.24 Suppose a sequence a1, a2, . . . of num-
bers is given that satisﬁes:

1)

−

1. a1 = 2.

2. an+1 = 2an for each n

1.

≥

Formulate a theorem giving an in terms of n, and
prove your result by induction.

Exercise C.25 Suppose a sequence a1, a2, . . . of num-
bers is given that satisﬁes:

1. a1 = b.

Exercise C.20 Let Bn = 1
1! + 2
·
Find a formula for Bn and prove it.

·

2! + 3

3! +

+ n

n!

·

· · ·

·

Exercise C.21 Let

2. an+1 = can + b for n = 1, 2, 3, . . . .

Formulate a theorem giving an in terms of n, and
prove your result by induction.

An = (1

1
2 )(1

1
3 )(1

−
−
Find a formula for An and prove it.

−

1
4 )

(1

−

· · ·

1
n )

Exercise C.26

Exercise C.22 Suppose Sn is a statement about n for
each n
1. Explain what must be done to prove that Sn
≥
is true for all n

1 if it is known that:

≥

a. Show that n2

b. Show that n3

2n for all n

2n for all n

4.

10.

≥

≥

≤

≤

Appendix D

Polynomials

Expressions like 3
expression of the form

−

5x and 1 + 3x

2x2 are examples of polynomials. In general, a polynomial is an

−
f (x) = a0 + a1x + a2x2 +

+ anxn

· · ·

where the ai are numbers, called the coefﬁcients of the polynomial, and x is a variable called an indeter-
minate. The number a0 is called the constant coefﬁcient of the polynomial. The polynomial with every
coefﬁcient zero is called the zero polynomial, and is denoted simply as 0.

If f (x)

= 0, the coefﬁcient of the highest power of x appearing in f (x) is called the leading coefﬁcient
of f (x), and the highest power itself is called the degree of the polynomial and is denoted deg ( f (x)).
Hence

−
7
6x

1 + 5x + 3x2

3x3 + x4

x5

−

−

has constant coefﬁcient
−
has constant coefﬁcient 7,
has constant coefﬁcient 0,

leading coefﬁcient 3, and degree 2,

1,
leading coefﬁcient 7, and degree 0,
leading coefﬁcient

1, and degree 5.

−

We do not deﬁne the degree of the zero polynomial.

Two polynomials f (x) and g(x) are called equal if every coefﬁcient of f (x) is the same as the corre-

sponding coefﬁcient of g(x). More precisely, if

f (x) = a0 + a1x + a2x2 +

· · ·

and

g(x) = b0 + b1x + b2x2 +

· · ·

are polynomials, then

f (x) = g(x)

if and only if

a0 = b0, a1 = b1, a2 = b2, . . .

In particular, this means that

f (x) = 0 is the zero polynomial if and only if a0 = 0, a1 = 0, a2 = 0, . . .

This is the reason for calling x an indeterminate.

Let f (x) and g(x) denote nonzero polynomials of degrees n and m respectively, say

f (x) = a0 + a1x + a2x2 +

· · ·

+ anxn

and

g(x) = b0 + b1x + b2x2 +

+ bmxm

· · ·

where an

= 0 and bm

= 0. If these expressions are multiplied, the result is

f (x)g(x) = a0b0 + (a0b1 + a1b0)x + (a0b2 + a1b1 + a2b0)x2 +

+ anbmxn+m

· · ·

Since an and bm are nonzero numbers, their product anbm

= 0 and we have

Theorem D.1

If f (x) and g(x) arenonzeropolynomialsofdegrees n and m respectively,theirproduct f (x)g(x) is
alsononzeroand

deg [ f (x)g(x)] = n + m

613

6
6
6
6
614

Polynomials

Example D.1

(2

−

x + 3x2)(3 + x2

5x3) = 6

3x + 11x2

11x3 + 8x4

15x5.

−

−

−

−

If f (x) is any polynomial, the next theorem shows that f (x)
a. In fact we have

−

f (a) is a multiple of the polynomial

x

−

Theorem D.2: Remainder Theorem

If f (x) isapolynomialofdegree n
suchthat

≥

1 and a isanynumber,thenthereexistsapolynomial q(x)

where deg (q(x)) = n

1.

−

f (x) = (x

−

a)q(x) + f (a)

Proof. Write f (x) = a0 + a1x + a2x2 +

· · ·

+ anxn where the ai are numbers, so that

f (a) = a0 + a1a + a2a2 +

+ anan

· · ·

If these expressions are subtracted, the constant terms cancel and we obtain

f (x)

−

f (a) = a1(x

a) + a2(x2

a2) +

−
1, xk

−
ak = (x

+ an(xn

an).

−

· · ·

Hence it sufﬁces to show that, for each k
k

1. This is clear if k = 1. If it holds for some value k, the fact that

−

≥

−

a)p(x) for some polynomial p(x) of degree

shows that it holds for k + 1. Hence the proof is complete by induction.

xk+1

−

ak+1 = (x

−

a)xk + a(xk

ak)

−

trated below for f (x) = x3
−
term at a time as follows: First x2 is chosen because x2(x
subtracted from f (x) to leave a “remainder” of

There is a systematic procedure for ﬁnding the polynomial q(x) in the remainder theorem. It is illus-
1 and a = 2. The polynomial q(x) is generated on the top line one
2) has the same x3-term as f (x), and this is
x because
1. Finally, the third term on top is

2) has the same x2-term, and this is subtracted to leave

−
1. Next, the second term on top is

3x2 + x

x2 + x

−

−

−

−

x(x
1, and the process ends with a “remainder” of

−

x
−

−

−

−
−

3.

−

2

x

−

x3
x3

(cid:17)

−

−

1

1

−

−

x2
x
−
3x2 + x
2x2
x2 + x
x2 + 2x
x
1
−
−
x + 2
−
3

−

1

−
−

−

Hence x3
1 = (x
−
This procedure is called the division algorithm.1

3x2 + x

2)(x2

1) + (

−

−

−

−

−

x

3). The ﬁnal remainder is

A real number a is called a root of the polynomial f (x) if

f (a) = 0

615

3 = f (2) as is easily veriﬁed.

−

Hence for example, 1 is a root of f (x) = 2
−
a, we say that x
If f (x) is a multiple of x
−
immediately that if a is root of f (x), then x
−
factor of f (x), say f (x) = (x

−

−

4x3, but

1 is not a root because f (

x + 3x2
= 0.
a is a factor of f (x). Hence the remainder theorem shows
a is factor of f (x). But the converse is also true: If x
a is a
a)q(a) = 0. This proves the

1) = 10

−

−

−

a)q(x), then f (a) = (a

−

−

Theorem D.3: Factor Theorem

If f (x) isapolynomialand a isanumber,then x
f (x).

−

a isafactorof f (x) ifandonlyif a isarootof

Example D.2

If f (x) = x3
division algorithm gives f (x) = (x + 2)(x2

6x + 4, then f (

2x2

−

−

−

2) = 0, so x

(
4x + 2).

−

−

−

2) = x + 2 is a factor of f (x). In fact, the

Consider the polynomial f (x) = x3

gives f (x) = (x
Hence

−

1)(x2 + x

−

2). But 1 is also a root of x2 + x

3x+2. Then 1 is clearly a root of f (x), and the division algorithm
1)(x + 2).

2; in fact, x2 + x

2 = (x

−

−

−

−

and we say that the root 1 has multiplicity 2.

f (x) = (x

1)2(x + 2)

−

Note that non-zero constant polynomials f (x) = b

constant polynomials with no roots. For example, if g(x) = x2 + 1, then g(a) = a2 + 1
number a, so a is not a root. However the complex number i is a root of g(x); we return to this below.

≥

= 0 have no roots. However, there do exist non-
1 for every real

Now suppose that f (x) is any nonzero polynomial. We claim that it can be factored in the following

form:

−
where a1, a2, . . . , am are the roots of f (x) and g(x) has no root (where the ai may have repetitions, and
may not appear at all if f (x) has no real root).

· · ·

−

−

f (x) = (x

a1)(x

a2)

(x

am)g(x)

By the above calculation f (x) = x3
3x +2 = (x
2, with 1 of multiplicity
two (and g(x) = 1). Counting the root
2 once, we say that f (x) has three roots counting multiplicities.
The next theorem shows that no polynomial can have more roots than its degree even if multiplicities are
counted.

1)2(x +2) has roots 1 and

−
−

−

−

Theorem D.4

If f (x) isanonzeropolynomialofdegree n,then f (x) hasatmost n rootscountingmultiplicities.

1This procedure can be used to divide f (x) by any nonzero polynomial d(x) in place of x

polynomial that is either zero or of degree less than the degree of d(x).

a; the remainder then is a

−

6
6
616

Polynomials

Proof. If n = 0, then f (x) is a constant and has no roots. So the theorem is true if n = 0. (It also holds for
a
n = 1 because, if f (x) = a + bx where b
b .) In general, suppose inductively
that the theorem holds for some value of n
0, and let f (x) have degree n + 1. We must show that f (x)
has at most n + 1 roots counting multiplicities. This is certainly true if f (x) has no root. On the other hand,
if a is a root of f (x), the factor theorem shows that f (x) = (x
a)q(x) for some polynomial q(x), and q(x)
has degree n by Theorem D.1. By induction, q(x) has at most n roots. But if b is any root of f (x), then

= 0, then the only root is

−

−

≥

so either b = a or b is a root of q(x). It follows that f (x) has at most n roots. This completes the induction
and so proves Theorem D.4.

a)q(b) = f (b) = 0

(b

−

i, where i is the complex number such that i2 =

As we have seen, a polynomial may have no root, for example f (x) = x2 + 1. Of course f (x) has
complex roots i and
1. But Theorem D.4 even holds
for complex roots: the number of complex roots (counting multiplicities) cannot exceed the degree of the
polynomial. Moreover, the fundamental theorem of algebra asserts that the only nonzero polynomials with
no complex root are the non-zero constant polynomials. This is discussed more in Appendix A, Theorems
A.4 and A.5.

−

−

6
Selected Exercise Answers

1.1.19 $4.50, $5.20

1;

−

Section 1.2

1.2.1

b. No, no

d. No, yes

f. No, no

1.2.2

b. 

0 1
0 0
0 0
0 0

3 0 0
−
1 0
0
0 1
0
0 0
0

0
0
0
1

0
1
0
1

−






t + 1, x2 = r, x3 =

2s

−
−
6t + 1, x6 = t




b. x1 = 2r

x4 = s, x5 =

5s + 3t

1,

−

−

−
5t

d. x1 =
−
x5 = t

4s

−

4, x2 =

2s + t

−

−

2, x3 = s, x4 = 1,

−

1.2.4

b. x =

1
7 , y =
−
3 (t + 2), y = t

3
7

−

d. x = 1

f. No solution

1.2.5

b. x =

−
d. No solution

15t

−

21, y =

11t

−

−

17, z = t

f. x =

7, y =

9, z = 1

−

−

h. x = 4, y = 3 + 2t, z = t

1.2.6

b. Denote the equations as E1, E2, and E3. Apply

gaussian elimination to column 1 of the augmented
matrix, and observe that E3
Hence E3 = 5E1

E1 =

4(E2

4E2.

E1).

−

−

−

−

Section 1.1

1.1.1

b.

2(2s + 12t + 13) + 5s + 9(
(2s + 12t + 13) + 2s + 4(

−
s
−

s

3t
−
3t
−

3) + 3t =
−
3) = 1
−

1.1.2

b. x = t, y = 1

3 (1

2t) or x = 1

2 (1

−

3s), y = s

−

d. x = 1 + 2s
z = 1

5 (1

−

5t, y = s, z = t or x = s, y = t,

−
s + 2t)

1.1.4 x = 1

4 (3 + 2s), y = s, z = t

1.1.5

a. No solution if b

= 0. If b = 0, any x is a

1.2.3

solution.

b. x = b
a

1.1.7

b.

1 2
0 1

0
1

(cid:21)

(cid:20)

1 0
1
1 1
0
1 0 1

−

d.





1
0
2 


1.1.8

b.

−

y

−

=

2x
1
−
3x + 2y + z = 0
y + z = 3
x2

2x1
1
−
3x1 + 2x2 + x3 = 0
x2 + x3 = 3

=

−

or

−

1.1.9

b. x =

3, y = 2

−
17, y = 13

d. x =

−

1.1.10

b. x = 1

9 , y = 10

9 , z =

7
3

−

d. x1 = 1, x2 = 1

−

1.2.7

b. x1 = 0, x2 =

t, x3 = 0, x4 = t

−

t, x3 = 1 + t, x4 = t

1.1.11

b. No solution

1.2.8

b. If ab

1.1.14

b. F. x + y = 0, x

y = 0 has a unique solution.

−

d. T. Theorem 1.1.1.

1.1.16 x′ = 5, y′ = 1, so x = 23, y =

32

−

1.1.17 a =

1
9 , b =

−

−

5

9 , c = 11

9

ab , y = a+5
5b
2
= 2, unique solution x = −
ab .
−
2
−
−
5; if a =
5, the
−

2

If ab = 2: no solution if a
1 + 2
solutions are x =

−
= 2, unique solution x = 1
−
a
−
= 1; if b = 1, the solutions are

2 , y = ab

2
2 . If a = 2,
−
−

a

−

=
5t, y = t.
b

d. If a

no solution if b
x = 1
2 (1

t), y = t.

−

b. Unique solution x =

y = 3a

b

−

−

6c, z =

−

2a + b + 5c,
2a + b + c, for any a, b, c.

−

1.2.9

617

6
6
6
6
6
618

Selected Exercise Answers

d. If abc
abc =

=

−

1, unique solution x = y = z = 0; if

−
1 the solutions are x = abt, y =

f. If a = 1, solutions x =

−
there is no solution. If a
solution x = a
1
a , y = 0, z = −
−

t, y = t, z =
= 1 and a
1
a .

bt, z = t.

−
1. If a = 0,

−

= 0, unique

1.2.10

b. 1

d. 3

f. 1

1.2.11

b. 2

d. 3

f. 2 if a = 0 or a = 2; 3, otherwise.

1
1
0 


1.2.12

b. False. A =

1
0
0



0
1
0

d. False. A =

1
0
0




0
1
0


y = 0
2x
−
4x + 2y = 0

f. False.

not.

−

1
0
0 


is consistent but

y = 1
2x
−
4x + 2y = 1

is

−

h. True, A has 3 rows, so there are at most 3 leading 1s.

a is nonzero, then

1.2.14









−

→ 

1 a
1 b
1 b
1 a
0 1
0 0

b. Since one of b
b + c
c + a
c + a 
b + c

1
−
0 






→ 

−

a and c
a
1
0
0
1
0
0

b + c
b
c 


−
−

−
−

a a
a
a

b
c
0 b + c + a
1
0

1
−
0





→

1.2.16

b. x2 + y2

2x + 6y

6 = 0

−

−

1.2.18 5

20 in A, 7

20 in B, 8

20 in C.

Section 1.3

1.3.1

b. False. A =

d. False. A =

f. False. A =

h. False. A =

(cid:20)
1 0
0 1

1 0
0 1

1
0
0

0
1
0

(cid:20)

(cid:20)





1
0

0 1
1 1

0
0

(cid:21)

1
1

0
0

1
0

(cid:21)

(cid:21)

0
0
0 


1.3.2

b. a =

d. a = 1, x =
z = t

−

−

3, x = 9t, y =

5t, z = t

−

t, y = t, z = 0; or a =

1, x = t, y = 0,

−

1.3.3

b. Not a linear combination.

d. v = x + 2y

z

−

1.3.4

b. y = 2a1

a2 + 4a3.

−

−

2
1
0
0
0








−

b. r 






1.3.5

d. s 






0
2
1
0
0









+ t 






2
−
0
1
−
1
0

3
−
0
2
−
0
1

















+ t 






+ s 






1
3
0
1
0









1.3.6

b. The system in (a) has nontrivial solutions.

1.3.7

b. By Theorem 1.2.2, there are n

−
parameters and thus inﬁnitely many solutions.

−

r = 6

1 = 5

d. If R is the row-echelon form of A, then R has a row of
zeros and 4 rows in all. Hence R has r = rank A = 1,
2, or 3. Thus there are n
parameters and thus inﬁnitely many solutions.

r = 5, 4, or 3

r = 6

−

−

1.3.9

b. That the graph of ax + by + cz = d contains
three points leads to 3 linear equations homogeneous
in variables a, b, c, and d. Apply Theorem 1.3.1.

1.3.11 There are n
−
are nontrivial solutions if and only if n

r parameters (Theorem 1.2.2), so there

r > 0.

−

Section 1.4

1.4.1

b.

−
−

f7
f4
f1 = 85
f7
f4
f2 = 60
75 + f4 + f6
f3 =
−
f7
f6
f5 = 40
f4, f6, f7 parameters

−
−

−

−

1.4.2

b.

f5 = 15
f4
25

≤

30

≤

1.4.3

b. CD

Section 1.5

1.5.2 I1 =

−

1

5 , I2 = 3

5 , I3 = 4

5

1.5.4 I1 = 2, I2 = 1, I3 = 1

2 , I4 = 3

2 , I5 = 3

2 , I6 = 1

2

6
6
6
Section 1.6

1.6.2 2NH3 + 3CuO

N2 + 3Cu + 3H2O

→

1.6.4 15Pb(N3)2 + 44Cr(MnO4)2
22Cr2O3 + 88MnO2 + 5Pb3O4 + 90NO

→

Supplementary Exercises for Chapter 1

Supplementary Exercise 1.1.

b. No. If the

corresponding planes are parallel and distinct, there is
no solution. Otherwise they either coincide or have a
whole common line of solutions, that is, at least one
parameter.

Supplementary Exercise 1.2.
10 (

b.
6t + 16), x2 = 1

6s

10 (4s

−

−

x1 = 1
x4 = t

t + 1), x3 = s,

−

Supplementary Exercise b..
2t, y =
a = 2, x = 2
−
−
unique solution is x = 8
−
3(a

b. If a = 1, no solution. If
= 1 and a
t, z = t. If a
= 2, the
5a
2
1), y = −
3(a

1), z = a+2

a

3

−
−

−

R1
R2

(cid:20)

→

(cid:21)
R2
R1

Supplementary Exercise 1.4.

R1 + R2
R2

(cid:20)

R1 + R2
R1

−

→

(cid:20)

(cid:21)

→

(cid:20)

(cid:21)

−

→

(cid:21)

(cid:20)

Supplementary Exercise 1.6. a = 1, b = 2, c =

−

Supplementary Exercise 1.8. The (real) solution is x = 2,
t, z = t where t is a parameter. The given complex
y = 3
solution occurs when t = 3
has a unique solution, that solution is real because the
coefﬁcients and constants are all real.

i is complex. If the real system

−

Supplementary Exercise 1.9.

b. 5 of brand 1, 0 of

brand 2, 3 of brand 3

Section 2.1

2.1.1

b. (a b c d) = (

2,

4,

−

−

6, 0) + t(1, 1, 1, 1),

−

t arbitrary

d. a = b = c = d = t, t arbitrary

619

(cid:21)

5
0

−

(cid:21)

h.

4
1

−

(cid:20)

2.1.3

b.

1
6

−
−

15
10

(cid:20)

d. Impossible

f.

5
0

2
1

(cid:20)

(cid:21)
h. Impossible

−

2.1.4

b.

2
1
2 (cid:21)

(cid:20)

−

2.1.5

b. A =

11
3 B

−

2.1.6

b. X = 4A

3B, Y = 4B

5A

−

−

2.1.7

b. Y = (s, t), X = 1

2 (1 + 5s, 2 + 5t); s and t

arbitrary

2.1.8

b. 20A

7B + 2C

−

2.1.9

b. If A =

1
2 (2d, a + b

(cid:20)
−

c

a
c

−

b
d
(cid:21)
d, a
−

, then (p, q, r, s) =

b + c

d,

−

−

a + b + c + d).

R2
R1

(cid:21)

1

−

2.1.11

2.1.13

b. If A + A′ = 0 then

A =

A + 0 =

−
A + A) + A′ = 0 + A′ = A′

−

A + (A + A′) = (

−

−

b. Write A = diag (a1, . . . , an), where a1, . . . , an
are the main diagonal entries. If B = diag (b1, . . . , bn)
then kA = diag (ka1, . . . , kan).

2.1.14

b. s = 1 or t = 0

d. s = 0, and t = 3

2.1.15

b.

2
1

(cid:20)

0
1

−

(cid:21)

d.

(cid:20)

2.1.16

−

7
5

2
9
2 −
b. A = AT , so using Theorem 2.1.2,

(cid:21)

(kA)T = kAT = kA.

2.1.19

b. False. Take B =

A for any A

= 0.

−

d. True. Transposing ﬁxes the main diagonal.

2.1.2

b.

(cid:20)
12, 4,

d. (

−

14
20

−
−

(cid:21)
12)

0
1
−
2

f.





−
1
0
4

−

2
−
4
0 


f. True.

(kA+mB)T = (kA)T +(mB)T = kAT +mBT = kA+mB

2.1.20

W T . Then AT = ST + W T = S

c. Suppose A = S + W , where S = ST and
W, so
AT = 2W . Hence S = 1
AT ) are uniquely determined by A.

W =
−
A + AT = 2S and A
and W = 1

−

−

2 (A + AT )

2 (A

−

6
6
6
3
9
2
0

−
−

−



+ t 



1
4
1
1












2.2.6 We have Ax0 = 0 and Ax1 = 0 and so
A(sx0 + tx1) = s(Ax0) + t(Ax1) = s

0 + t





0 = 0.

·

·

2.2.8

3
−
0
1
−
0
0

b. x = 






2.2.10

b. False.

2
1
0
0
0

+ 






s 













1
2

2
4









+ t 






=

2
1

5
−
0
2
0
1





.













(cid:20)
d. True. The linear combination x1a1 +
a1

Ax where A =

(cid:21) (cid:20)

an

−

(cid:20)

(cid:21)

· · ·

by Theorem 2.2.1.

0
0

.

(cid:21)
+ xnan equals

3
2
4 5

x1
x2
x3



(cid:21)

+ x3

(cid:21)

3

5

(cid:20)

−
2
4

−

=

f. False. If A =

(cid:2)

(cid:20)

1
2

1
2

1
0

−

(cid:21)

(cid:3)
and x =

x1 + 2x2 + 3x3
4x2 + 5x3

−

(cid:20)

(cid:21)

Ax =

1
4

(cid:20)

= s

(cid:21)

(cid:20)

+ t

(cid:21)

(cid:20)

1
2

(cid:21)

, then

2
0
1 






for any s and t.

· · ·

1
2

−

1
1

h. False. If A =

for b =

0
0

(cid:20)

(cid:21)

(cid:20)

2.2.11

b. Here T

+

d. Here T

x
y

(cid:20)

2.2.13

b. Here

1
1

x
y

−
−
but not for b =

1
1

, there is a solution

(cid:21)
1
0

.

(cid:21)

(cid:20)

0 1
1 0

=

=

y
x

(cid:21)

(cid:20)

=

(cid:20)
0 1
1 0

.

(cid:21)

x
y

.

(cid:21)

(cid:21) (cid:20)
x
y

(cid:20)

=

(cid:21)
y
x

−

(cid:21)

(cid:20)

(cid:21)

(cid:20)

−

(cid:21) (cid:20)

620

Selected Exercise Answers

2.1.22

b. If A = [ai j] then

(kp)A = [(kp)ai j] = [k(pai j)] = k [pai j] = k(pA).

d. 

2.2.2 x1 

−



+ x2 

2
0
2
4

−

−
−

−

1
1
7
9



+







+ x3 









Section 2.2

2.2.1

b. x1

−

−

3x2
8x2
x1 + 2x2 + 2x3
x2 + 2x3

3x3 + 3x4 = 5
+ 2x4 = 1
= 2
5x4 = 0

1
1
2
3





5
3
−
8
12

−















1
2
−
0
2

−

x4 







= 









1
0

(cid:20)

1
0

x1

(cid:20)

+ x2

(cid:21)

(cid:20)

2.2.3

b. Ax =







d. Ax =

= x1

x4



−
3
0
8 


−

=



6

5
0 


3
0
8

−

4
2
7

−

1 6
1 5
3 0 




+ x2



3x1


−




+ x3

−



4
2
7 
−
4x2 + x3 + 6x4

2x2 + x3 + 5x4
3x3



1
1
3 






8x1 + 7x2

−

−




b. To solve Ax = b the reduction is
0
3
5

3
1
1
0
1 2

2
1
3

−

−

2.2.4











1
1
0 


−
1
1 0
−
1
0 1
0 0
0
1 + s + 3t
s
t
1
s
t

−

−

3
−
1
0

.



→

4
1
1 

so the general solution is








Hence (1 + s + 3t)a1 + (1
for any choice of s and t. If s = t = 0, we get
a1 + a2 = b; if s = 1 and t = 0, we have 2a1 + a3 = b.

t)a2 + sa3 + ta4 = b

−

−

s




=

(cid:21)
x1
x2
x3
x4







2.2.5

b.

−

2
2
0 






1
3
1 


+ t



−



x
y
z 


=





T





so the matrix is

1 0
0 1
0 0



=

−

x
−
y
z 

1 0 0
−
.
1 0
0
0 1 
0








0
0
1 






,

x
y
z 


an

a2

a1

2.2.16 Write A =
columns. If b = x1a1 + x2a2 +
scalars, then Ax = b by Theorem 2.2.1 where
xn
x1
x =
system Ax = b.

(cid:3)
T . That is, x is a solution to the

+ xnan where the xi are

in terms of its

· · ·
· · ·

· · ·

x2

(cid:2)

(cid:2)

(cid:3)

2.2.18

b. By Theorem 2.2.3, A(tx1) = t(Ax1) = t

that is, tx1 is a solution to Ax = 0.

0 = 0;

·

6
n and x and y are n-vectors, we must show

×

x2

2.2.22 If A is m
that A(x + y) = Ax + Ay. Denote the columns of A by
a1, a2, . . . , an, and write x =
y1
y =
x + y =
(cid:2)
Deﬁnition 2.1 and Theorem 2.1.1 give
A(x + y) = (x1 + y1)a1 + (x2 + y2)a2 +
(x1a1 + x2a2 +
Ax + Ay.

· · ·
+ xnan) + (y1a1 + y2a2 +

x1
T . Then
(cid:2)

y2
· · ·
x1 + y1

yn
x2 + y2
(cid:3)

xn + yn

T , so

· · ·

· · ·

· · ·

· · ·

xn

(cid:2)

(cid:3)

(cid:3)
+ (xn + yn)an =

+ ynan) =

T and

621

ii.

(cid:20)

1
0

0
0

1
0

0
1

,

(cid:21)

(cid:20)

,

(cid:21)

(cid:20)

1
0

1
0

(cid:21)

2.3.12

b. A2k = 



k = 0, 1, 2, . . . ,


A2k+1 = A2kA = 

k = 0, 1, 2, . . .





1
0
0
0

2k
−
1
0
0

0 0
0 0
1 0
0 1

1
0
0
0

−

(2k + 1)
1
0
0







for

2
0
1
−
0

1
−
0
1
1

for







Section 2.3

2.3.1

b.

d.

f.

h.

j.

3

−
23]

[
(cid:2)
−
1
0

(cid:20)

aa′
0
0





(cid:20)

0
1

1
−
0

6
−
6

2
−
10

(cid:21)

15

−

(cid:3)

(cid:21)

0
bb′
0

0
0
cc′





2.3.2

b. BA =

CB =

AC =

(cid:20)
2
−
2
1

4
2

−





(cid:20)

10
4

−

−

4
2

1
1
12
6
6 

, CA =

−

10
1

−

(cid:21)

2.3.13

b.

d. 0k

I
0

0
I

(cid:20)

= I2k

(cid:21)

f.

X m
0

0
X m
(cid:20)
n = 2m + 1

(cid:21)

if n = 2m;

0
X m

X m+1
0

(cid:20)

if

(cid:21)

2.3.14

b. If Y is row i of the identity matrix I, then YA is

row i of IA = A.

, B2 =

(cid:21)

7
1

−

(cid:20)

6
−
6

,

(cid:21)

2.3.16

d. 0

b. AB

BA

−

2
1
−
1

4
1
−
4





−

8
5
2 


2.3.3

b. (a, b, a1, b1) = (3, 0, 1, 2)

2.3.4

b. A2
2
8
5
2

(cid:20)

A

−

−

(cid:21)

(cid:20)

6I =
−
2
2

2
1

−

6 0
0 6

=

(cid:21)

(cid:20)

0 0
0 0

(cid:21)

−

(cid:21)

(cid:20)

2.3.5

b. A(BC) =

1
0

(cid:20)

14
5

−

(cid:20)
(AB)C

17
1

−

=

(cid:21)

(cid:20)

1
−
1

2
3

−

9
5

−

16
1

−

(cid:21) (cid:20)

1
1

−

2
0

−

(cid:21)





=

(cid:21)
1 0
2 1
5 8 


2.3.18

b. (kA)C = k(AC) = k(CA) = C(kA)

2.3.20 We have AT = A and BT = B, so (AB)T = BT AT = BA.
Hence AB is symmetric if and only if AB = BA.

2.3.22

b. A = 0

2.3.24 If BC = I, then AB = 0 gives
0 = 0C = (AB)C = A(BC) = AI = A, contrary to the
assumption that A

= 0.

2.3.26 3 paths v1

v4, 0 paths v2

v3

→

→

2.3.27

b. False. If A =

=

J

= I.

1
0

0
0

(cid:20)

(cid:21)

= J, then AJ = A but

d. True. Since AT = A, we have
(I + AT = IT + AT = I + A.

2.3.6

b. If A =

(cid:21)
entries an AE and EA.

(cid:20)

a
b
c d

and E =

0
1

0
0

(cid:20)

(cid:21)

, compare

f. False. If A =

, then A

= 0 but A2 = 0.

0
0

1
0

(cid:20)

(cid:21)

h. True. We have A(A + B) = (A + B)A; that is,

A2 + AB = A2 + BA. Subtracting A2 gives AB = BA.

2.3.7

b. m

n and n

×

2.3.8

b.

i.

1
0

(cid:20)

×

0
1

m for some m and n

1
0

,

(cid:21)

(cid:20)

0
1

−

,

(cid:21)

(cid:20)

1
0

1
1

−

(cid:21)

j. False. A =

l. False. See (j).

1
2

2
−
4

, B =

(cid:21)

(cid:20)

2
1

4
2

(cid:21)

(cid:20)

6
6
6
622

Selected Exercise Answers

2.3.28

b. If A = [ai j] and B = [bi j] and

∑ j ai j = 1 = ∑ j bi j, then the (i, j)-entry of AB is
ci j = ∑k aikbk j, whence
∑ j ci j = ∑ j ∑k aikbk j = ∑k aik(∑ j bk j) = ∑k aik = 1.
Alternatively: If e = (1, 1, . . . , 1), then the rows of A
sum to 1 if and only if Ae = e. If also Be = e then
(AB)e = A(Be) = Ae = e.

2.3.30

b. If A = [ai j], then
tr (kA) = tr [kai j] = ∑n

i=1 kaii = k ∑n

i=1 aii = k tr (A).

e. Write AT =

a′i j
, where a′i j = a ji. Then
h
i
k=1 aika′k j
∑n

, so
k=1 aika′ki
(cid:17)

i=1 ∑n

= ∑n

i=1

∑n
AAT =
tr (AAT ) = ∑n
(cid:16)

k=1 a2
ik.

e. Observe that PQ = P2 + PAP
PAPQ = P + AP

Q2 = PQ + APQ

P2AP = P, so
PAP = Q.

−
−

(cid:2)

−

(cid:3)

B) = A2

b. (A + B)(A
B)(A + B) = A2 + AB

(A
AB + BA = AB
if and only if
−
2BA = 2AB; that is, BA = AB.

−

−

−

−

AB + BA

B2, and
B2. These are equal

−

−
BA

−
BA; that is,

2.3.32

2.3.34

2.3.35

4
1

3
2

−
−
14
−
4
−
15

0
1

(cid:21) (cid:20)
6
1
5 


−

= 1
5

(cid:21)



−



(cid:20)
1
1
0 


3
2

(cid:21)

−
−

=

(cid:20)
9
4
10

−

2.4.3

b.

x
y

(cid:20)

(cid:21)

= 1
5

d.



x
y
z 



1
5 

−



= 1

5 



23
8
25 


2.4.4

b. B = A−

1AB =

4
7
1

2
2
2

−
−





−

1
4
1 


−

2.4.5

b.

1
10

(cid:20)
1
1

−

2
0
6 1

1
0

0
1

(cid:20)

d. 1
2

f. 1
2

(cid:20)
1
2

−

h.

−
1
1

(cid:20)

3
1

2
1

−

(cid:21)

(cid:21)

(cid:21)

(cid:21)

B) = A2

b. (A + B)(A
−
B)(A + B) = A2

−

(A
if and only if
−
−
2AB = 2BA, that is AB = BA.

−
AB + BA =

−

BA + AB

AB + BA

B2 and
B2. These are equal

−

−

BA + AB, that is

2.4.6

2.4.8

b. A = 1

2 

2
0
2

−

1
−
1
1

3
1
1 


−
−



b. A and B are inverses.

Section 2.4

2.4.2

b. 1
5

2
3

−

1
−
4

(cid:21)

3
1
2 

1
−
2
1 


−
2
5
1 


−

−

1
1
−
1
0

2
−
3
−
2
0

d.

2
3
1





f.

1
10 



h. 1

4 

(cid:20)
1
1
1

−

−
−
4
1
2
2
9 14

−
−
2
0
5 2
3 2



−
−
0
1
−
1
0

j. 





l. 






0
2
−
2
1

−
2
1
0
0
0

−

1
0
0
0
0

−

6
3
1
0
0

−

30
15
5
−
1
0

2.4.9

b. False.

d. True. A−

(cid:20)
1 = 1
3 A3

1 0
0 1

1
0

+

(cid:21)

(cid:20)

0
1

−

(cid:21)

f. False. A = B =

1
0

0
0

(cid:20)

(cid:21)

h. True. If (A2)B = I, then A(AB) = I; use

Theorem 2.4.5.

2.4.10

b. (CT )−
1)−
1 = (A−

1 = (C−
1 = A.

C−

1)T = AT because

2.4.11

b. (i) Inconsistent.

(ii)

(cid:20)

x1
x2

=

(cid:21)

(cid:20)

2
1

−

(cid:21)

2.4.15

b. B4 = I, so B−

1 = B3 =

0 1
1 0

(cid:21)

(cid:20)

−

c2

2

−
c
−
c2
−

2.4.16

3





c
−
1
c

1
0
1 


−

2.4.18

b. If column j of A is zero, Ay = 0 where y is
column j of the identity matrix. Use Theorem 2.4.5.















−

210
105
35
7
−
1

d. If each column of A sums to 0, XA = 0 where X is the
row of 1s. Hence AT X T = 0 so A has no inverse by
Theorem 2.4.5 (X T

= 0).

2.4.38

623

2XX T . Then

b. Write U = In

U T = IT
U 2 = I2
In

−
2X TT X T = U, and
(2XX T )In
4XX T + 4XX T = In.

n −
n −

−

−

In(2XX T ) + 4(XX T )(XX T ) =

2.4.19

b. (ii) (

−

1, 1, 1)A = 0

2.4.20

b. Each power Ak is invertible by Theorem 2.4.4

(because A is invertible). Hence Ak cannot be 0.

2.4.39

b. (I

2P)2 = I
−
and only if P2 = P.

−

4P + 4P2, and this equals I if

2.4.41

b. (A−

1 + B−

1)−

1 = B(A + B)−

1A

2.4.21

b. By (a), if one has an inverse the other is zero

Section 2.5

and so has no inverse.

2.5.1

2.4.22 If A =

, a > 1, then A−

1 =

a 0
0 1

(cid:21)
x-compression because 1
a < 1.

(cid:20)

1
a
0

0
1

(cid:20)

is an

(cid:21)

b. Interchange rows 1 and 3 of I. E −
2) times row 1 of I to row 2.

d. Add (

1 = E.

−
1 =

E −

1
2
0

0 0
1 0
0 1 






1 0
0 1
0 0





0
0
1
5





2.4.24

b. A−

1 = 1

4 (A3 + 2A2

1)

−

f. Multiply row 3 of I by 5. E −

1 =

2.4.25

b. If Bx = 0, then (AB)x = (A)Bx = 0, so x = 0

because AB is invertible. Hence B is invertible by
Theorem 2.4.5. But then A = (AB)B−
Theorem 2.4.4.

1 is invertible by

0
0
1 




2
5
−
13
−

1
−
2
0
0

−

−

1
3
8

14
16
2
1

2.4.26

b.

1
1
−
0
0

d. 





−
8
9
1
1

−
−
−




1 = I + A +

1 0
0 1

(cid:21)

(cid:21)

2.5.2

b.

−

(cid:20)

−

1
1

1
0

0 1
1 0

(cid:21)

d.

f.

(cid:20)

(cid:20)

2.4.28

d. If An = 0, (I

−

A)−

+ An

1.

−

· · ·

2.5.5

b. No, 0 is not invertible.

2.4.30

b. A[B(AB)−

1] = I = [(BA)−

1B]A, so A is

invertible by Exercise 2.4.10.

2.5.6

b.



2.5.3

b. The only possibilities for E are

0
1
0
1
1 0
k
1
(cid:20)
(cid:21)
each case, EA has a row different from C.

k
0
0 1

k
1
0 1

1 0
k
0

, and

(cid:20)

(cid:20)

(cid:21)

(cid:20)

(cid:20)

(cid:21)

,

,

,
(cid:21)

. In

(cid:21)

2.4.32

a. Have AC = CA. Left-multiply by A−
1CA. Then right-multiply by A−

1 to get

1 to get

C = A−
CA−

1 = A−

1C.

2.4.33

b. Given ABAB = AABB. Left multiply by A−
1.

then right multiply by B−

1,

2.4.34 If Bx = 0 where x is n
1, then ABx = 0 so x = 0 as
×
AB is invertible. Hence B is invertible by Theorem 2.4.5, so
A = (AB)B−

1 is invertible.

2.4.35

b. B



−
Theorem 2.4.5.



1
−
3
1 


= 0 so B is not invertible by

1
0
(cid:20)
1 0
0 1
0
1
2 (cid:21) (cid:20)
1 0
0 1

2
−
1

(cid:21) (cid:20)
7
3

(cid:21)
1
1

−

−

1
0

7
3

−

.

(cid:21)

A =

1
0

(cid:20)
A =

(cid:20)

(cid:20)

1 0
5 1

1
0

0
1
2 (cid:21) (cid:20)
. Alternatively,

−

(cid:21)

1
0
5 1

−

(cid:21)

(cid:21) (cid:20)

0
0
0
1
1 1 


−

d.

1
0
0

−

0
0
1















2 0
1 0
0 1 
1 0
0

0
0 1
1 
2 0




1
0
0



−



0 1
1 0
0 0 


A = 





0
1
5
0
1 0
3 1
0 0

1 0

0 1

0 0



1
0
0

0
0
1 
0


0
1 

1
5
7
5 −
0

−

1
5
2
5

0








6
624

Selected Exercise Answers

2.5.7

b. U =

2.5.8

b. A =

1 1
1 0

0 1
1 0

(cid:20)

(cid:20)

1 1
0 1

=

(cid:21)

(cid:20)

1 0
2 1

(cid:21) (cid:20)

(cid:21) (cid:20)

(cid:21) (cid:20)

1
0

0
1

1
0

(cid:21)

0
1

−

(cid:21)

2.6.2

b. As in 1(b), T 





5
1
−
2
4

−

.

4
2
9 


−



=









1R.

So A =



T (e1) T (e2)

1
0

2
1

(cid:20)

d. A =



(cid:21)
0 0
1
1 0
0
2 0 1 
1 0


0 1
0 0



−

0
0
1 


1 0
0 1
0 2
0
4
1 




1 0

0 1
0 0

3
−
0
1 

2.5.10 UA = R by Theorem 2.5.1, so A = U −







2.5.12

d. U =

b. U = A−
2
−
3
2



−
−

1
0
0
0

0
1
0
0



V = 





1, V = I2; rank A = 2
1
0
1 0
,
1 1 
3
1

−
4
1
0
1
1
0



−

; rank A = 2

= U −
(cid:2)

I U −
1

2.5.16 Write U −
1A
=
U A
(cid:3)
→

U A
(cid:2)
(Lemma 2.5.1).
(cid:2)
(cid:2)

(cid:3)

1 = EkEk
U −
= EkEk
−
1A

1
· · ·
−
1U U −
1

(cid:2)
I U −
(cid:3)

(cid:3)




E2E1, Ei elementary. Then
1A
E2E1
(cid:3)

U A

. So

· · ·
by row operations
(cid:3)
(cid:2)

2.5.17

2.5.19

b. (i) A r
∼

A because A = IA. (ii) If A r
∼

A = UB, U invertible, so B = U −
If A r
∼
V invertible. Hence A = U(VC) = (UV )C, so A r
∼

A. (iii)
C, then A = UB and B = VC, U and
C.

B and B r
∼

1A. Thus B r
∼

B, then

b. If B r
A, let B = UA, U invertible. If
∼
b
d
b d

, B = UA =

b
0
0 d

0
0

U =

(cid:20)

−

(cid:21)

and d are not both zero (as U is invertible). Every
such matrix B arises in this way: Use

(cid:21)

(cid:20)

U =

a b
b a

(cid:21)

(cid:20)

−

–it is invertible by Example 2.3.5.

where b

2.6.3

A

(cid:20)

b. T (e1) =
−
T (e1) T (e2)
0
1
(cid:2)
−
1
0

.

−

e2 and T (e2) =

=

e2

−

(cid:3)

(cid:2)

e1. So
e1
=

−
−

d. T (e1) =



and T (e2) =

−





(cid:21)
√2
2
√2
2

(cid:3)

√2
2
√2
2




1
−
1

.
(cid:21)


= √2
2

(cid:20)

1
1

(cid:3)

(cid:2)

2.6.4

b. T (e1) =

e1, T (e2) = e2 and T (e3) = e3.

−

Hence Theorem 2.6.2 gives
T (e1) T (e2) T (e3)
A
1 0
0 1
0 0

−



(cid:2)

0
0
.
1 




=

e1

−

(cid:3)

(cid:2)

e2

e3

=

(cid:3)

2.6.5

b. We have y1 = T (x1) for some x1 in Rn, and

y2 = T (x2) for some x2 in Rn. So
ay1 + by2 = aT (x1) + bT (x2) = T (ax1 + bx2). Hence
ay1 + by2 is also in the image of T .

= 2

(cid:21)(cid:19)

(cid:20)

−

0
1

.

(cid:21)

2.6.7

b. T

2
(cid:18)

(cid:20)

2.6.8

b. A = 1
√2

0
1

(cid:20)

d. A = 1
10

8
6

−
−

(cid:20)

−
6
8

−

1 1
1 1

, rotation through θ =

π
4 .

−

(cid:21)

, reﬂection in the line y =

3x.

−

(cid:21)

2.6.10

b.

cosθ 0
1
sinθ 0

0





sinθ
0

−
cosθ 


2.5.22

b. Multiply column i by 1/k.

2.6.12

b. Reﬂection in the y axis

Section 2.6

2.6.1

b.



= 3

5
6
13 

= 3T





(cid:21)

1
2

−

3
2
1 


−

−
3
2
1 
11

11

(cid:20)





−
=

(cid:21)

2

−





2T





2
0
5 
2

0
5 


=



−
5
6
13 

2

−

(cid:21)

(cid:20)

−
3
5

T

3





(cid:20)

d. Reﬂection in y = x

f. Rotation through π
2

, so

2.6.13

b. T (x) = aR(x) = a(Ax) = (aA)x for all x in R.

Hence T is induced by aA.

2.6.14

b. If x is in Rn, then
1)x] = (
x) = T [(

−

T (

−

1)T (x) =

T (x).

−

−

6
2.6.17

b. If B2 = I then

T 2(x) = T [T (x)] = B(Bx) = B2x = Ix = x = 1R2(x)
for all x in Rn. Hence T 2 = 1R2. If T 2 = 1R2, then
B2x = T 2(x) = 1R2(x) = x = Ix for all x, so B2 = I by
Theorem 2.2.6.

2.6.18

0
1

b. The matrix of Q1
1
0
.

1
0
(cid:21) (cid:20)
(cid:20)
matrix of R π
2

0
1

−

(cid:21)

=

Q0 is
0
1

◦

(cid:20)

1
−
0

(cid:21)

, which is the

d. The matrix of Q0

1
0

0
0
1
1
−
(cid:20)
the matrix of Q

(cid:21) (cid:20)

−

1.

◦

R π
2
1
−
0

is

=

(cid:21)

(cid:20)

0
1

−

1
−
0

(cid:21)

, which is

2.6.20 We have

, so T

(cid:3)

(cid:2)

1

1





· · ·

· · ·

1 1

+ xn =

T (x) = x1 + x2 +

x1
x2
...
xn
is the matrix transformation induced by the matrix
A =
hand, we can use Theorem 2.6.2 to get A, but to do this we
must ﬁrst show directly that T is linear. If we write
y1
y2
...
yn

x1
x2
...
xn

. Then











· · ·





1

1

(cid:2)

(cid:3)

. In particular, T is linear. On the other

x = 










and y = 








x1 + y1
x2 + y2
...
xn + yn

T (x + y) = T 












= (x1 + y1) + (x2 + y2) +
= (x1 + x2 +
· · ·
= T (x) + T (y)

· · ·

+ xn) + (y1 + y2 +

+ (xn + yn)

+ yn)

· · ·

Similarly, T (ax) = aT (x) for any scalar a, so T is linear. By
Theorem 2.6.2, T has matrix
T (e1) T (e2)
A =
before.

T (en)

1 1

· · ·

· · ·

=

1

, as

(cid:3)

(cid:2)

(cid:3)

(cid:2)

2.6.22

b. If T : Rn

→

R is linear, write T (e j) = w j for
e1, e2, . . . , en

is the

}

+ xnen, Theorem 2.6.1 gives

each j = 1, 2, . . . , n where
standard basis of Rn. Since
x = x1e1 + x2e2 +

{

· · ·
T (x) = T (x1e1 + x2e2 +

+ xnen)

· · ·

+ xnT (en)

= x1T (e1) + x2T (e2) +
= x1w1 + x2w2 +
x = Tw(x)
= w

· · ·

· · ·
+ xnwn

·

625



. Since this holds for all x in Rn, it

w1
w2
...
wn






where w = 





shows that T = TW. This also follows from
Theorem 2.6.2, but we have ﬁrst to verify that T is
linear. (This comes to showing that
w
x and y in Rn and all a in R.) Then T has matrix
A =

(x + y) = w

(ax) = a(w

y and w

s + w

·

·

·

·

·

T (e1) T (e2)
wn

· · ·

T (en)

=
· · ·
by Theorem 2.6.2. Hence if
(cid:3)

x) for all

w1 w2
(cid:2)
x1
x2
...
xn

(cid:3)



in R, then T (x) = Ax = w

(cid:2)
x = 




b. Given x in R and a in R, we have






·

x, as required.

2.6.23
(S

◦

T )(ax) = S [T (ax)]
= S [aT (x)]
= a [S [T (x)]]
T (x)]
= a [S

◦

◦

T

Deﬁnition of S
Because T is linear.
Because S is linear.
T
Deﬁnition of S

◦

Section 2.7

2.7.1

b.

2
1
1

−





0
0
3 0
−
9

1 


−

1
1
1
0

0 0
1 0
1 1
2 0

−
−

0
0
0
1

2
1
3
0

0
0 0
2 0 0
−
2 1 0
−
0 1
2







d. 





f. 












1 1

0 1

0 0

0 0






















1

0

0

2

1

0

1

2
3

0

−

3
1
0
0

−

1 0
2 1
0 0
0 0







1
0
0
0

1
0
0
0







1 2 1

−

−

1
2

0

0

0 0

0 0

0 0











2.7.2

b. P =



1

−
0
0

1
0 0
0
1 0
0 
0 1
2
1

1 2
−
0
0
0
1 0
−
0

4 




1
0
0

PA =



1

−
0
0

=





1
0
0
0

0 0
0 1
0 0
1 0

d. P = 





4 

0
0
1
0









2
−
1
0

1
−
2
1 


3
−
2
−
1
0

0
3
−
2
−
1







s and t arbitrary

626

Selected Exercise Answers

−

1
1
2
2

−

2
1
5
4

1
1
2
2

0
1
1
0

−

−

PA = 




−

= 





0
3
1 3
−
10 1
6 5

−

−
0 0
0 0
2 0
0 5

















1 2

0 1
0 0
0 0

2.7.3

b. y =

−

1
0
0 






d. y = 

2
8
1
−
0

→
(cid:20)
R2
R1



x = 







R1 + R2
R2

(cid:21)





R1
R2

(cid:21)

→

(cid:21)

(cid:20)

2.7.5

(cid:20)

R2
R1

(cid:20)

−

x = 




2t
8
−
t
6
−
1
−
t

−

t

→

(cid:21)

(cid:20)

−

1 + 2t
t
−
s
t







t arbitrary






R1 + R2
R1

−

→

(cid:21)

2.8.2

t
t
t 






2.8.4 P =

bt

(1

a)t

(cid:21)
(cid:20)
and a = 1. In that case,

−

are positive, then P =

is nonzero (for some t) unless b = 0

1
1
b

(cid:20)

1

−

(cid:20)

is a solution. If the entries of E

(cid:21)

a

(cid:21)

has positive entries.

2.8.7

b.

0.4 0.8
0.7 0.2

(cid:21)

(cid:20)

2.8.8 If E =

a
c

b
d
(cid:21)
a)(1
−

(cid:20)

E) = (1

det (I

−

det (I

E)

−
= 0, then (I

−

−
so (I
0 if det (I
−
≥
The converse is now clear.

E)−

−

1

, then I

E =

−
bc = 1

(cid:20)
−
1
det (I

d)

−
1 =
E)−

, so

1

b
d

a
−
c
−

−
1
−
(cid:21)
tr E + det E. If
b

d

1

−
c

,

1

a

E)

−

(cid:21)
E) > 0, that is, tr E < 1 + det E.

−

(cid:20)

2.8.9

b. Use p =

in Theorem 2.8.2.

3
2
1 






in Theorem 2.8.2.

2.7.6

b. Let A = LU = L1U1 be LU-factorizations of the

invertible matrix A. Then U and U1 have no row of
zeros and so (being row-echelon) are upper triangular
with 1’s on the main diagonal. Thus, using (a.), the
diagonal matrix D = UU −
diagonal. Thus D = I, U = U1, and L = L1.

1 has 1’s on the main

1

3
2
2 


d. p =




Section 2.9

2.7.7 If A =

then AB =

a
0
X A1
ab

(cid:20)

and B =

(cid:21)

0

b
0
Y B1

(cid:20)
, and A1B1 is lower

(cid:21)

(cid:20)
triangular by induction.

(cid:21)

Xb + A1Y A1B1

in block form,

2.9.1

b. Not regular

2.7.9

b. Let A = LU = L1U1 be two such factorizations.

1

1L1; write this matrix as
1L1. Then D is lower triangular
1L1); and D is also
1
1 ). Hence D is

1 = L−
Then UU −
1
D = UU −
1 = L−
(apply Lemma 2.7.1 to D = L−
upper triangular (consider UU −
1 and L1 are unit
diagonal, and so D = I because L−
triangular. Since A = LU; this completes the proof.

2.9.2

b. 1
3

2
1

, 3
8

(cid:21)

(cid:20)

d. 1

3 



1
20 

f.

1
1
1 


5
7
8 


, 0.312

, 0.306

2.9.4


b. 50% middle, 25% upper, 25% lower

Section 2.8

2.8.1

b.

14t
17t
47t
23t

d. 





t
3t
t 












2.9.6 7

16 , 9
16

2.9.8

a.

7
75

b. He spends most of his time in compartment 3; steady

state 1
16

3
2
5
4
2










.






6
627

2.9.12

a. Direct veriﬁcation.

b. Since 0 < p < 1 and 0 < q < 1 we get 0 < p + q < 2

3.1.8

b. det



whence
1 < 1

1 < p + q
p

−
q < 1, so (1

1 < 1. Finally,
p

−
−

−
−
as m increases.

−

−

q)m converges to zero

Supplementary Exercises for Chapter 2

Supplementary Exercise 2.2.

U −

1 = 1

4 (U 2

−

5U + 11I).

b.

= 3 det

= 3 det

= 3 det













2a + p 2b + q 2c + r
2r + z
2q + y
2p + x
2z + c 
2y + b
2x + a
c + r + z

2r + z
2z + c 


2p + x
2x + a

a + p + x b + q + y

2q + y
2y + b
a + p + x b + q + y
b
−
q
−
3z
c
r 


p
x
3x
a q
y
p

3y
b r
z
q

c + r + z
c
r
r 
z


−
−

−
−

−
−

−
−

−
−

p
x

a
p

q
y

· · ·

Supplementary Exercise 2.4.
z) = y + m(y

b. If xk = xm, then

z). So (k
−
z is not zero (because y and z are distinct), so

z) = 0.

m)(y

−

−

−

m = 0 by Example 2.1.7.

y + k(y
But y
k

−

−

Supplementary Exercise 2.6.
gives IpqAIrs = ∑n
i=1 ∑n
j=1 ai jIpqIi jIrs. The only
nonzero term occurs when i = q and j = r, so
IpqAIrs = aqrIps.

d. Using parts (c) and (b)

3.1.9

b. False. A =

d. False. A =

f. False. A =

h. False. A =

(cid:20)
0
1

1
1

1
1

2
0

1
0

1
0

(cid:20)

(cid:20)

(cid:20)

1 1
2 2

(cid:21)

R =

→

(cid:20)

1 0
0 1

(cid:21)

and B =

1
1

0
1

(cid:20)

(cid:21)

(cid:21)

(cid:21)

(cid:21)

Supplementary Exercise 2.7.

b. If

A = [ai j] = ∑i j ai jIi j, then IpqAIrs = aqrIps by 6(d).
But then aqrIps = AIpqIrs = 0 if q
= r, so aqr = 0 if
q
independent of q. Thus aqq = a11 for all q.

= r. If q = r, then aqqIps = AIpqIrs = AIps is

3.1.10

b. 35

3.1.11

b.

6

−

d.

6

−

Section 3.1

3.1.1

b. 0

d.

1

−

39

f.

−
h. 0

j. 2abc

l. 0

n.

56

−
p. abcd

3.1.14

3.1.15

3.1.16

b.

b.

b.

d. x =

y

±

(x

−

−

2)(x2 + 2x

12)

−

7

−

√6
2

±

x1
x2
3.1.21 Let x = 
...

xn


x + y


, y = 



cn
A =

Expanding det A along column j (the one containing x + y):




where x + y is in column j.


y1
y2
...
yn






and

· · ·

· · ·

c1





(cid:2)

(cid:3)

3.1.5

b.

17

−

d. 106

3.1.6

b. 0

3.1.7

b. 12

T (x + y) = det A =

=

n
∑
i=1
n
∑
i=1

(xi + yi)ci j(A)

xici j(A) +

n
∑
i=1

yici j(A)

= T (x) + T (y)

Similarly for T (ax) = aT (x).

6
6
628

Selected Exercise Answers

3.1.24 If A is n
or n = 2k + 1.

×

n, then det B = (

−

Section 3.2

3.2.1

b.




1
−
2
2

d. 1

3 

1
3
3

−
−

2
1
−
2

3.2.2


b. c

= 0

2
6
4 


= A

−

−

1
1
1

2
2
1 


−

d. any c

f. c

=

1

−

3.2.3

b.

2

−

3.2.4

b. 1

3.2.6

b. 4
9

3.2.7

b. 16

3.2.8

b.

1
11

5
21

(cid:21)

d.

1
79 

−


b.

3.2.9

(cid:20)
12
37
2 


−

4
51

1)k det A where n = 2k

d. T. det A

= 0 means A−

1 exists, so AB = AC implies

that B = C.

f. F. If A =

h. F. If A =





(cid:20)

j. F. If A =

1
−
1
1 + det A = 1.

(cid:20)

1
1
1

1 1
1 1
1 1 


1 1
0 0

(cid:21)

1
1

−

(cid:21)

then adj A = 0.

then adj A =

0
0

1
1

−

(cid:20)

(cid:21)

then det (I + A) =

1 but

−

l. F. If A =

adj A =

(cid:20)

(cid:20)

1 1
0 1
1
1
−
1
0

then det A = 1 but

(cid:21)

(cid:21)

= A

3.2.22

b. 5

3.2.23

b. 1

3.2.24

b. 1

−

−

−

4x + 2x2.

3 x + 1
5

2 x2 + 7

6 x3

0.51x + 2.1x2

−

1.1x3; 1.25, so y = 1.25

3.2.26

b. Use induction on n where A is n

n. It is clear

×
in block form

if n = 1. If n > 1, write A =

where B is (n
1
a−
0

1 =

A−

−

1)

−

×
a−

(n
−
1XB−
1
B−

(cid:20)

(cid:21)

a X
0 B

(cid:20)
1). Then
1

(cid:21)

, and this is upper

triangular because B is upper triangular by induction.

3.2.28

1
21 

−

3
0
3

0
2
1

1
3
1 


−


b. Have ( adj A)A = ( det A)I; so taking inverses,
( adj A)−

det A I. On the other hand,

1( adj A)−

1 = A−

1 adj (A−

1)I = 1

det A I. Comparison
1), and part (b)

1 = 1
1) = det (A−

1
·
1 adj (A−

A−
A−
yields A−
follows.

d. Write det A = d, det B = e. By the adjugate formula

AB adj (AB) = deI, and
AB adj B adj A = A[eI] adj A = (eI)(dI) = deI. Done
as AB is invertible.

3.2.10

b. det A = 1,

1

−

d. det A = 1

3.2.34

f. det A = 0 if n is odd; nothing can be said if n is even

3.2.15 dA where d = det A

= 0

1
0
1

−

c
−
1
c

c2

, c

0 1
c
1
1 
c

6
−
c
−
c2 

c
1
−
−
c + 1
c2

−

8

c2 + 1
c
−
1


c2
c
10

c
−
c2
c

−

1 


−

3.2.19

b. 1

c 

8

c2

d. 1

2 



f.

1
c3+1 

−

−
1

3.2.20


b. T.

det AB = det A det B = det B det A = det BA.

, c

=

1

−

P =

(cid:20)

−

Section 3.3

3.3.1

b. (x

3)(x + 2); 3;

2;

−
1AP =

; P−

−
4
1
1 1

4
1

1
1

.

,

0
2

(cid:21)

−

(cid:20)

(cid:21)

−
3
0

(cid:20)

(cid:20)

;

(cid:21)

(cid:21)
1
1
0 


3
−
0
1 


,





; No such P; Not

d. (x

−

2)3; 2;



diagonalizable.



6
6
6
6
6
6
f. (x + 1)2(x

2);

1,

−

−

2;

−



−

,



1
1
2 


1
2
1 


; No such

P; Not diagonalizable. Note that this matrix and the

matrix in Example 3.3.9 have the same characteristic
polynomial, but that matrix is diagonalizable.



h. (x

−

1)2(x

−

3); 1, 3;

Not diagonalizable.

−

,

1
0
1 






1
0
1 






No such P;

2
1

(cid:21)

3.3.2

b. Vk = 7

3 2k

d. Vk = 3

2 3k

(cid:20)

1
0
1 






3.3.4 Ax = λx if and only if (A
eigenvectors.

−

αI)x = (λ

−

α)x. Same

629

3.3.23

3.3.24

b. If Am = 0 and Ax = λx, x

= 0, then

A2x = A(λx) = λAx = λ2x. In general, Akx = λkx for
1. Hence, λmx = Amx = 0x = 0, so λ = 0
all k
≥
(because x

= 0).

a. If Ax = λx, then Akx = λkx for each k. Hence
1 by

λmx = Amx = x, so λm = 1. As λ is real, λ =
the Hint. So if P−
Theorem 3.3.4. Hence A2 = PD2P = I.

1AP = D is diagonal, then D2 = I by

±

3.3.27

a. We have P−

1AP = λI by the diagonalization

algorithm, so A = P(λI)P−

1 = λPP−

1 = λI.

b. No. λ = 1 is the only eigenvalue.

3.3.31

b. λ1 = 1, stabilizes.
24 (3 + √69) = 1.13, diverges.

d. λ1 = 1

3.3.34 Extinct if α < 1

5 , stable if α = 1

5 , diverges if α > 1
5 .

1 0
0 2

(cid:21)
1 =

3.3.8

b. P−

1AP =

An = P

1
0

(cid:20)

3.3.9

b. A =

(cid:20)

0
2n

(cid:20)

(cid:21)

0 1
0 2

P−

(cid:21)

, so

Section 3.4

9
8
−
6(2n

·
−

2n
1)

12(1
−
2n
9
−

·

2n)
8

(cid:21)

(cid:20)

d. xk = 1
5

4

b. xk = 1
3
2k+2 + (

(cid:2)

2)k

(
−
3)k

(cid:3)

−

−

(cid:2)
b. xk = 1
2

(cid:3)
1)k + 1

(
−

3.4.1

3.4.2

3.4.3

(cid:2)

(cid:3)

b. xk+4 = xk + xk+2 + xk+3; x10 = 169

3.4.5
λ1 = 1

1
λk
3 + √5
1 + (
2√5
−
2 (1 + √5) and λ2 = 1
2 (1

3 + √5)λk
√5).

i

h

2 where

−

3.4.7

1
2√3
and λ2 = 1
(cid:2)
−

2 + √3
√3.

(cid:3)

λk
1 + (

−

2 + √3)λk

2 where λ1 = 1 + √3

k

1
2

. Long term 11 1

3 million tons.

1
λ
λ2 


λ
λ2
a + bλ + cλ2 


=





λ
λ2
λ3 


=

=





3.4.9 34

3 −

4
3

−
(cid:0)

(cid:1)

3.4.11

b. A



λ





3.4.12



1
λ
λ2 

b. xk = 11

10 3k + 11
15 (

2)k

5
6

−

−

1x = 1

λ x.

3.4.13

a.

pk+2 + qk+2 = [apk+1 + bpk + c(k)] + [aqk+1 + bqk] =
a(pk+1 + qk+1) + b(pk + qk) + c(k)

b. and d. PAP−
1(kA)P = kD is diagonal, and d. Q(U −

1 = D is diagonal, then b.

1AU)Q = D

P−
where Q = PU.

1
1

(cid:21)
=

is not diagonalizable by Example 3.3.8.

2
0

1
1

+

1 0
−
2
0

(cid:20)

(cid:21)
has diagonalizing matrix P =

−

(cid:21)

(cid:20)

where

(cid:21)

(cid:20)

1
0

1
3

−

and

(cid:21)

is already diagonal.

3.3.11

3.3.12

But

(cid:20)
1
0

2
0

−

(cid:20)

1
1
−
1 0
0 2

(cid:20)

(cid:20)

1
0
1
1

(cid:21)

(cid:21)

3.3.14 We have λ2 = λ for every eigenvalue λ (as λ = 0, 1)
so D2 = D, and so A2 = A as in Example 3.3.9.

3.3.18

b. crA(x) = det [xI

= rn det

x
r I

A

−

−
= rncA

rA]
x
r

3.3.20

(cid:2)
b. If λ

(cid:3)

(cid:3)
(cid:2)
= 0, Ax = λx if and only if A−

The result follows.

3.3.21

b. (A3

2A
2λx + 3x = (λ3

−

−

3I)x = A3x

2λ

−

−

λ3x

−

2Ax + 3x =

−
3)x.

Section 3.5

6
6
6
630

Selected Exercise Answers

3.5.1

b. c1

1
1

(cid:21)

(cid:20)

e4x + c2

(cid:20)

d. c1

8
−
10
7 
c1 = 0, c2 =






e−

x + c2

−

1
2 , c3 = 3

2

−

5
1

−
1
2
1 


e−

2x; c1 =

2
3 , c2 = 1
3

−

4.1.2

(cid:21)

e2x + c3

e4x;

1
0
1 






4.1.4

d. 3

−
−

2
1
2 


b. 1

3 


b. √2

3.5.3

3.5.5

3.5.6

b. The solution to (a) is m(t) = 10

t/3

4
5
= 5. We solve for t by
(cid:1)

. Hence

(cid:0)

t/3

4
we want t such that 10
5
taking natural logarithms:
(cid:1)

(cid:0)
1
3 ln(
2 )
4
5 )

ln(

t =

= 9.32 hours.

a. If g′ = Ag, put f = g

1b. Then f′ = g′ and
A−
b, so f′ = g′ = Ag = Af + b, as required.

−

Af = Ag

−

b. Assume that f ′1 = a1 f1 + f2 and f ′2 = a2 f1.
Differentiating gives f ′′1 = a1 f ′1 + f2′ = a1 f ′1 + a2 f1,
proving that f1 satisﬁes Equation 3.15.

Section 3.6

3.6.2 Consider the rows Rp, Rp+1, . . . , Rq
adjacent interchanges they can be put in the order
Rp+1, . . . , Rq
1, Rq, Rp. Then in q
interchanges we can obtain the order Rq, Rp+1, . . . , Rq
This uses 2(q

1 adjacent interchanges in all.

1, Rq. In q

1 adjacent

p)

−

−

p

−

−

p

−

−

−

Supplementary Exercises for Chapter 3

Supplementary Exercise 3.2.

b. If A is 1

AT = A. In general,
(Ai j)T
det [Ai j] = det
= det
induction. Write AT =
a′i j
(cid:3)
(cid:2)
(cid:2)
expand det AT along column 1.
h

i

1, then

×

(AT ) ji

by (a) and
where a′i j = a ji, and
(cid:3)

det AT =

=

n
∑
j=1
n
∑
j=1

a′j1(

−

a1 j(

−

1) j+1 det [(AT ) j1]

1)1+ j det [A1 j] = det A

where the last equality is the expansion of det A along
row 1.

Section 4.1

4.1.1

b. √6

d. √5

f. 3√6

4.1.6

b.

−→FE = −→FC + −→CE = 1

2 −→AC + 1

2 −→CB = 1

2 (−→AC + −→CB) = 1

2 −→AB

4.1.7

b. Yes

d. Yes

4.1.8

b. p

(p + q).

d.

−

4.1.9

b.



, √27

−
−

1
1
5 


, 0



0
0
0 

2
2
2 


−

d.

f.









, √12

1, Rp.

−

4.1.10

b. (i) Q(5,

1, 2) (ii) Q(1, 1,

−

4).

−

4.1.11

b. x = u

6v + 5w =

−

−

26
4
19 






4.1.12

4.1.13



−



4.1.14

4.1.17

4.1.18









b.

=

b. If it holds then

a
b
c 


5
−
8
6 

3a + 4b + c
a + c
−
b + c
0
4 4
3 4
1 0 1
1 0
−
0 1
1 1
0
If there is to be a solution then x1 + 3x2 = 4x3 must
hold. This is not satisﬁed.

x1 + 3x2


x2
x3

1 x1
1 x2
1 x3

x1
x2
x3

→ 

=





















.


4 

b. 1

5
5
2 

b. Q(0, 7, 3).

−
−



b. x = 1

40 



−
−

20
13
14 


b. S(

−

1, 3, 2).

4.1.20

4.1.21

b. T.

v

k

d. F.

v
k
k
v = 0.

=

k −

−
v
k

= 0 implies that v

w
k
for all v but v =

4.2.4

w = 0.

−

v only holds if

−

d. s

f. F. If t < 0 they have the opposite direction.

h. F.

= 5

5v
k

k −
j. F. Take w =

v
k

k
v where v

= 0.

for all v, so it fails if v

= 0.

4.2.6

4.2.8

631

t





b.

1
−
1
2 

0
3
1 

b. 29 + 57 = 86

1
2
0 


+ t









b. A = B = C = π

3 or 60◦

−





+ t

3
1
4 

1
1
1 

1
−
0
1 






4.1.22

b.



z = 4 + 5t


−

1
1
1 

2
1
1 


−





+ t

d.

f.









4.2.10

b. 11
18 v

; x = 3 + 2t, y =

1

−

−

t,

d.

1
2 v

−

−

2
1
5 


4.2.11

b.

5
21 

+ 1

21 

2
1
4 


−
−

53
26
20 






+ 1

53 

6
4
1 


√5642, Q( 71

3
−
2
26 

26 , 34
26 , 15
26 )

1
26

; x = 2

t, y =

−

−

1, z = 1 + t

4.2.12



b.

d. 27

53 

−

+ t

; x = y = z = 1 + t

4.1.23

b. P corresponds to t = 2; Q corresponds to t = 5.

4.1.24

b. No intersection

d. P(2,

1, 3); t =

2, s =

3

−

−

−

4.1.29 P(3, 1, 0) or P( 5

3 , −

3 , 4
1
3 )

4.1.31

b. −→CPk =

2n points.

−→CPn+k if 1

−

k

≤

≤

n, where there are

4.1.33 −→DA = 2−→EA and 2−→AF = −→FC, so
2−→EF = 2(−→EF + −→AF) = −→DA+ −→FC = −→CB+ −→FC = −→FC + −→CB = −→FB.
Hence −→EF = 1
2 −→FB. So F is the trisection point of both AC and
EB.

Section 4.2

4.2.1

b. 6

d. 0

f. 0

4.2.2

b. π or 180◦

d. π

3 or 60◦

f. 2π

3 or 120◦

4.2.3

b. 1 or

17

−

4.2.13

b.



0
0
0 




4
15
8 

23x + 32y + 11z = 11

b.

b.



−



4.2.14

d. 2x

−
y + z = 5

−

f. 2x + 3y + 2z = 7

h. 2x

j. x

3z =

−
z = 3

1

−

7y

−

−
y

−

2
1
0 






4.2.15

b.

x
y
z 


x
y
z 








=

+ t

2
1
3 

1
1
1 






=



−

+ t



1
1
1 




=

−
x
1
y
1
z 
2 



b. √6
3 , 2
3 , Q( 7



+ t



−


2
3 , −
3 )

4
1
5 


d.

f.









4.2.16

4.2.17

b. Yes. The equation is 5x

3y

−

−

4z = 0.

4.2.19

b. (

−

2, 7, 0) + t(3,

5, 2)

−

6
6
The six face

4.3.10
×
determined by A, B, and C.

−→AC
k

−→AB

k

is the area of the parallelogram

632

Selected Exercise Answers

4.2.20

b. None
19 , 65
78
19 )

19 , −

d. P( 13

4.2.21

b. 3x + 2z = d, d arbitrary

d. a(x
zero

−

3) + b(y

−

2) + c(z + 4) = 0; a, b, and c not all

f. ax + by + (b

h. ax + by + (a

−

−

a)z = a; a and b not both zero

2b)z = 5a

−

4b; a and b not both zero

4.2.23

b. √10

4.2.24

b. √14

2 , A(3, 1, 2), B( 7
2 ,
3 ), B( 37
3 , 2, 1

6 , 13

6 , 0)

d. √6

6 , A( 19

1
2 , 3)

−

4.2.26

b. Consider the diagonal d =



a
a
a 


diagonals in question are



0
a
a 

. All of these are orthogonal to d. The

a
0
a 


± 

± 

−

−





,

,

result works for the other diagonals by symmetry.



± 

−

a
a
0 


4.2.28 The four diagonals are (a, b, c), (
(a,
are

−
c) or their negatives. The dot products
c2).
b2 + c2), and
(a2

b, c) and (a, b,
−
a2 + b2 + c2),
(
−
±

(a2 + b2

a, b, c),

−
±

−

±

−

4.2.34

b. The sum of the squares of the lengths of the
diagonals equals the sum of the squares of the lengths
of the four sides.

4.2.38

b. The angle θ between u and (u + v + w) is

given by
(u+v+w)
cosθ = u
·
u+v+w
u
w
k
kk
k
k
k
w
v
u
. Similar remarks apply to the other
=
=
k
k
k
k
angles.

2 = 1
√3

k
2+
k

u
k
v
k

√
k

because

u
k

2+

=

k

k

4.2.39

b. Let p0, p1 be the vectors of P0, P1, so
n = p0 ·
p1. Then u
u = p0 −
·
n = (ax0 + by0)
p1 ·
(ax1 + by1) = ax0 + by0 + c.
−
Hence the distance is

n –

as required.

u
n
= |
|
·
n
k
k

n
u
·
2
n
k

k

n

(cid:17)

(cid:13)
(cid:13)
(cid:13)

(cid:16)

(cid:13)
(cid:13)
(cid:13)

4.2.41

v
k

k

b. This follows from (a) because
2 = a2 + b2 + c2.

x1
y1
z1





x
y
z 


=









and

x2
y2
z2





y
z
x 


=









4.2.44

d. Take

in (c).

Section 4.3

4.3.3

b.

√3
3 

±



1
1
.
1 


−
−

4.3.4

b. 0

d. √5

4.3.5

b. 7

4.3.6

b. The distance is

p

k

−

p0k

; use part (a.).

×

(v

4.3.12 Because u and v
them is 0 or π. Hence cos(θ) =
w
u
u
cos(θ) =
=
k
|
angle between v and w is π
2 so
v
v
2 ) =
kk

×
cos( π

v
kk

w)
|

w
k

w
k

kk

×

×

=

v

k

k

k

k

·

w
k

w are parallel, the angle θ between
1, so the volume is
(v

. But the

±

u

k

kk

w)
k

×

. The result follows.

4.3.15

b. If u =

u1
u2
u3



then u

×



(v + w) = det





v1
v2
v3
i u1

j
u2
k u3

, v =



and w =





w1
w2
w3

,




= det

= (u



i u1
j u2
k u3
v) + (u

×

×


+ det

v1
v2
v3
w) where we used Exercise 4.3.21.

















v1 + w1

v2 + w2
v3 + w3
i u1 w1

j
u2 w2
k u3 w3

4.3.16

w)
[(u
b. (v
·
−
w)
v)+(v
(u
×
v) + 0 + v
(u

·
×

v) + (v
×
×
(v
w)
−
×
u) = 0.
(w
·

·
×

(v

−

−
w
·

w) + (w
w)+(v

u)] =
(w

×
w)

−

·

×

u) =

4.3.22 Let p1 and p2 be vectors of points in the planes, so
n = d2. The distance is the length of the
p1 ·
n = d1 and p2 ·
p1)
(p2−
p1 along n; that is |
projection of p2 −
n
k
k

d2|
d1−
n
k
k

= |

n
|
·

.

Section 4.4

4.4.1

b. A =

1
1

1
1

−

(cid:21)

, projection on y =

x.

−

, reﬂection in y = 2x.
(cid:21)
√3
1

, rotation through π
3 .

(cid:21)

−
(cid:20)
3 4
−
3
4

1
√3

−

d. A = 1
5

f. A = 1
2

(cid:20)

(cid:20)

4.4.2

b. The zero transformation.

4.4.3

b.

1
21 

2
20
4

8
−
4
5 


0
1
3 


17
2
8

−
4
−
28
10



−


0
1
3 


d.

1
30 



f.

1
25 



22
4
−
20

9 0
0 0
12 0

h.



1
11 



9
2
6

−

−

−
−

2
9
6

20
10
20 


−



−


1
1
7 

2
5
0 


−

−







12
0
16 


6
−
6
−
7 


b. 1

2 



cosθ 0
1
sinθ 0

0

4.4.4

4.4.6





√3

1 0
−
1 √3 0
0 1
0

1
0
3 










sinθ
0

−
cosθ 


4.4.9

a. Write v =

x
y

.

(cid:21)

(cid:20)

PL(v) =

d
v
·
2
d
k

k

(cid:16)

(cid:17)

d = ax+by
a2+b2

= 1

a2+b2

= 1

a2+b2

(cid:20)

(cid:20)

(cid:20)

Section 4.5

(cid:21)

a
b
a2x + aby
abx + b2y
a2 + ab
ab + b2

(cid:21)

(cid:21) (cid:20)

633

Section 5.1

5.1.1

b. Yes

d. No

f. No.

5.1.2

b. No

d. Yes, x = 3y + 4z.

5.1.3

b. No

{

span

x1, x2, . . . , xk

5.1.10 span
a1x1, a2x2, . . . , akxk
{
by Theorem 5.1.1 because, for each i, aixi is in
span
{
is in span
span
by Theorem 5.1.1.

}
a1x1, a2x2, . . . , akxk

x1, x2, . . . , xk

span

} ⊆

} ⊆

{

{

{

for each i shows that

}
a1x1, a2x2, . . . , akxk

. Similarly, the fact that xi = a−
i

1

(aixi)

, again

}

x1, x2, . . . , xk

}

5.1.12 If y = r1x1 +
Ay = r1(Ax1) +

+ rkxk then
+ rk(Axk) = 0.

· · ·

· · ·

5.1.15

b. x = (x + y)

y = (x + y) + (

y) is in U

because U is a subspace and both x + y and

−

−

y = (

−

−

1)y are in U.

5.1.16

b. True. x = 1x is in U.

d. True. Always span

span
Theorem 5.1.1. Since x is in span
span

x, y, z

span

} ⊆

y, z

{

{
{

x, y, z
x, y

by
}
we have

}

, again by Theorem 5.1.1.

y, z
}

{
2
0

=

(cid:21)

(cid:20)

a + 2b
0

(cid:21)

cannot equal

{

} ⊆

x
y

(cid:21)

1
0

(cid:20)

+ b

(cid:21)

(cid:20)

f. False. a

0
1

.
(cid:21)

(cid:20)

b.
√2 + 2
√2 + 2
7√2 + 2 3√2 + 2
3√2 + 4 3√2 + 4 5√2 + 4 √2 + 4

−

2

2

2

2

−

5.1.20 If U is a subspace, then S2 and S3 certainly hold.
Conversely, assume that S2 and S3 hold for U. Since U is
nonempty, choose x in U. Then 0 = 0x is in U by S3, so S1
also holds. This means that U is a subspace.


5√2 + 2
−
9√2 + 4
2

4.5.1

1
2 



4.5.5

b. P( 9

5 , 18
5 )

Supplementary Exercises for Chapter 4

Supplementary Exercise 4.4. 125 knots in a direction θ
degrees east of north, where cosθ = 0.6 (θ = 53◦ or 0.93
radians).


5.1.22

b. The zero vector 0 is in U + W because
0 = 0 + 0. Let p and q be vectors in U + W , say
p = x1 + y1 and q = x2 + y2 where x1 and x2 are in U,
and y1 and y2 are in W . Then
p + q = (x1 + x2) + (y1 + y2) is in U + W because
x1 + x2 is in U and y1 + y2 is in W . Similarly,
a(p + q) = ap + aq is in U + W for any scalar a
because ap is in U and aq is in W . Hence U + W is
indeed a subspace of Rn.

Supplementary Exercise 4.6. (12, 5). Actual speed 12
knots.

Section 5.2

; dimension 2.

5.2.17

b. Each V −

1xi is in null (AV ) because

634

Selected Exercise Answers

5.2.1

b. Yes. If r



then r + s = 0, r

equations give r = s = t = 0.

−



+ s

1
1
1 
s = 0, and r + s + t = 0. These



1
1
1 


0
0
1 


+ t

=









,

0
0
0 


d. No. Indeed:
1
1
0
0

−



















1
0
1
0

0
0
1
1

0
1
0
1

0
0
0
0

.









= 









−















+ 









5.2.2

b. Yes. If r(x + y) + s(y + z) + t(z + x) = 0, then
is

(r + t)x + (r + s)y + (s + t)z = 0. Since
independent, this implies that r + t = 0, r + s = 0, and
s + t = 0. The only solution is r = s = t = 0.

x, y, z

{

}

d. No. In fact, (x + y)

(y + z) + (z + w)

(w + x) = 0.

−

−

1
−
1
1
1









; dimension 2.




5.2.3

d. 




5.2.4

d. 


2
1
0
1

−









, 









b. 


2
−
0
3
1









, 








1
2
1
−
0

1
1
0
1







b. 


1
0

1
0







, 







, 








−

1
1
0
1








1
1
−
1
0



, 









; dimension 2.

; dimension 3.









0
1


0

1















1
1
0
0







−









, 

, 

1
0
0
1

1
0
1
0

; dimension 3.




b. If r(x + w) + s(y + w) + t(z + w) + u(w) = 0,


f. 


then rx + sy + tz + (r + s + t + u)w = 0, so r = 0,
s = 0, t = 0, and r + s + t + u = 0. The only solution is
r = s = t = u = 0, so the set is independent. Since
dim R4 = 4, the set is a basis by Theorem 5.2.7.









5.2.5

5.2.6

b. Yes

d. Yes

f. No.

5.2.7

b. T. If ry + sz = 0, then 0x + ry + sz = 0 so

r = s = 0 because

x, y, z

}

{

is independent.

d. F. If x

= 0, take k = 2, x1 = x and x2 =

x.

−

f. F. If y =

x and z = 0, then 1x + 1y + 1z = 0.

−

h. T. This is a nontrivial, vanishing linear combination,

so the xi cannot be independent.

5.2.10 If rx2 + sx3 + tx5 = 0 then
0x1 + rx2 + sx3 + 0x4 + tx5 + 0x6 = 0 so r = s = t = 0.

5.2.12 If t1x1 + t2(x1 + x2) +
then (t1 + t2 +
tk)xk
we obtain successively tk = 0, tk

· · ·
+ tk)x1 + (t2 +

· · ·

−

+ tk(x1 + x2 +
+ tk)x2 +

+ xk) = 0,
1 +

· · ·
+ (tk

1 = 0, . . . , t2 = 0, t1 = 0.

−

· · ·
1 + (tk)xk = 0. Hence all these coefﬁcients are zero, so

· · ·

−

5.2.16

b. We show AT is invertible (then A is invertible).
Let AT x = 0 where x = [s t]T . This means as + ct = 0
and bs + dt = 0, so
s(ax + by) + t(cx + dy) = (sa + tc)x + (sb + td)y = 0.
Hence s = t = 0 by hypothesis.

}
1 is invertible. If y is in null (AV ),

1x1, . . . , V −

1xk

is

V −
{

1xi) = Axi = 0. The set

AV (V −
independent as V −
then V y is in null (A) so let V y = t1x1 +
where each tk is in R. Thus
y = t1V −
V −
span
{

1x1 +
· · ·
1x1, . . . , V −

1xk is in
.

+ tkV −
1xk

}

+ tkxk

· · ·

0
} ⊆

5.2.20 We have
U
dim W = 1. Hence dim U = 0 or dim U = 1 by
Theorem 5.2.8, that is U = 0 or U = W , again by
Theorem 5.2.8.

W where dim

0
}

⊆

{

{

= 0 and

Section 5.3

b.
1
1
1 


1
√3 



5.3.1





5.3.3

,

1
√42 

,

1
√14 

−

4
1
5 


−



= 1

a
b
c 

+ 1
9 (2a


b + 2c)

−



b.



2 (a

c)



−

2
3
1 


.





18 (a + 4b +

+ 1

1

4
1 

= 1

c)






a
b
c 


1
6 (a + b

2c)

−

1

1
.
2 






−

d.

3 (a + b + c)



+ 1

2 (a

b)

−

+

−

1
1
0 






−


1
0
1 
2

1
.
2 


−



1
1
1 


5.3.4

b. 





14
1
8
−
5

2
1
−
0
3

2
1
2
1

.








+ 4 





−
−







= 3 









6
635

n, we have

5.4.3

b. No; no

d. No

f. Otherwise, if A is m

m = dim ( row A) = rank A = dim ( col A) = n

×

c1

5.4.4 Let A =
col A = span
Ax

. . .
c1, . . . , cn
(cid:2)
{
x in Rn}.

{

|

cn
=

. Then
x1c1 +
(cid:3)
{

· · ·

}

+ xncn

xi in R

=

}

|

.

5.4.7

b. The basis is





dimension is 2.

Have rank A = 3 and n








6
0
4
1
0



, 











3 = 2.

−

−

so the

5
0
3
−
0
1














5.4.8

5.4.9

5.4.10

b. n

1

−

b. If r1c1 +
· · ·
Then Cx = r1c1 +
Hence each ri = 0.

· · ·

+ rncn = 0, let x = [r1, . . . , rn]T .

+ rncn = 0, so x is in null A = 0.

b. Write r = rank A. Then (a) gives
r.

dim ( null A) = n

r = dim ( col A

≤

−

, in R







−

1
3
10
11

5.3.5

b. t 




b. √29

5.3.6

d. 19

5.3.7

b. F. x =

d. T. Every xi

·

1
0

and y =

0
1

(cid:21)

(cid:20)
(cid:20)
y j = 0 by assumption, every xi

(cid:21)

x j = 0 if

·

= j because the xi are orthogonal, and every
i
yi ·
the vectors are nonzero, this does it.

= j because the yi are orthogonal. As all

y j = 0 if i

f. T. Every pair of distinct vectors in the set
product zero (there are no such pairs).

x
}

{

has dot

5.3.9 Let c1, . . . , cn be the columns of A. Then row i of AT is
cT
i , so the (i, j)-entry of AT A is cT
according as i

= j, i = j. So AT A = I.

i c j = ci

c j = 0, 1

·

5.3.11

b. Take n = 3 in (a), expand, and simplify.

5.3.12

b. We have (x + y)

(x

y) =

·

−

y) = 0 if and only if

2;
—where we used the fact that

y
k

k

− k
x
k
k

k

2

x
k

2.
y
k
2 =

Hence (x + y)
if and only if
0 and

x
k ≥

k

−
=

(x
·
x
k
k
y
k ≥
k

y
k

k
0.

5.3.15 If AT Ax = λx, then
Ax
k

2 = (Ax)

k

·

(Ax) = xT AT Ax = xT (λx) = λ
k

2.

x
k

be a basis

5.4.12 We have rank (A) = dim [ col (A)] and
rank (AT ) = dim [ row (AT )]. Let
{
of col (A); it sufﬁces to show that
{
of row (AT ). But if t1cT
2 +
· · ·
(taking transposes) t1c1 + t2c2 +
· · ·
2 , . . . , cT
Hence
k }
then vT is in col (A); say vT = s1c1 + s2c2 +
R: Hence v = s1cT
1 + s2cT
2 +
spans row (AT ), as required.

c1, c2, . . . , ck
}
2 , . . . , cT
cT
1 , cT
is a basis
k }
+ tkcT
k = 0, t j in R, then
+ tkck = 0 so each t j = 0.
is independent. Given v in row (AT )
+ skck, s j in
2 , . . . , cT
k }

1 + t2cT

· · ·
1 , cT
cT

1 , cT
cT

+ skcT

k , so

· · ·

{

{

5.4.15

b. Let

u1, . . . , ur

be a basis of col (A). Then b

{

is not in col (A), so
independent. Show that
col [A b] = span

{

u1, . . . , ur, b

.

}

{

}

u1, . . . , ur, b
}

is linearly

; 2

(cid:21)(cid:27)

Section 5.5

; 2

2
2
4
6

−

−

1
1
3
0



, 









1
3

,

(cid:21)

(cid:20)

(cid:26)(cid:20)

−








3
2

−

0
0
2
3
−
6
















, 











0
0
1 







5.5.1

traces = 2, ranks = 2, but det A =

b.
det B =

1

−

5,

−

d. ranks = 2, determinants = 7, but tr A = 5, tr B = 4

f.

traces =
−
rank B = 1

5, determinants = 0, but rank A = 2,

5.5.3

B−

b. If B = P−
1 = P−

1A−

1AP, then
1)−

1(P−

1 = P−

1A−

1P.

Section 5.4

5.4.1



−








b.

2
1
1 


,




1
2
1
3

−

d. 
















, 





5.4.2

b.

d.






















1
5
6 


−

0
0
1 






0
0
0
1

;







; 









0
2
2
5
1

−

, 





0
1
1 


−





1
1
0
0
0

,













6
6
6
636

Selected Exercise Answers

5.5.4

b. Yes, P =



P−

1AP =

3

−
0
0



,

1 0 6
−
1 0
0
0 5 
1
0
0

3 0
−
0

d. No, cA(x) = (x + 1)(x



−
2. But dim (E4) = 1 so Theorem 5.5.6 applies.

8 

4)2 so λ = 4 has multiplicity

5.5.8

5.5.9

b. If B = P−

Bk = (P−

1AP)k = P−

1AP and Ak = 0, then
1AkP = P−

10P = 0.

b. The eigenvalues of A are all equal (they are the
1AP = D is diagonal, then

diagonal elements), so if P−
D = λI. Hence A = P−

1(λI)P = λI.

5.5.10

b. A is similar to D = diag (λ1, λ2, . . . , λn) so

(Theorem 5.5.1) tr A = tr D = λ1 + λ2 +

+ λn.

5.5.12

b. TP(A)TP(B) = (P−
1(AB)P = TP(AB).

P−

1AP)(P−

· · ·
1BP) =

4.87x; the estimate of g is 9.74. [The true

5.6.7 s = 99.71
value of g is 9.81]. If a quadratic in s is ﬁt, the result is
s = 101

−
9
2t2 giving g = 9;

3
2t

−

−

(MT M)−

1 = 1

2 



38
42
10

−

42
49
12

−

−

−

10
12
.
3 


5.6.9 y =

= 1

25080 





5.19 + 0.34x1 + 0.51x2 + 0.71x3, (AT A)−
−
517860
8016
5040
22650

22650
400
1090
1975

5040
316
−
1300
1090

8016
208
316
400

−

−

−

−

−



1

−

−





5.6.10

5.6.13

a0)2 = na2

f (x) = a0 here, so the sum of squares is

b.
S = ∑(yi
0 −
1
n ∑ yi]2 + [∑y2
the square gives S = n[a0
−
This is minimal when a0 = 1
n ∑ yi.

2a0 ∑ yi + ∑y2

i −

i . Completing
1
n (∑ yi)2]

−

b. Here f (x) = r0 + r1ex. If f (x1) = 0 = f (x2)
ex2 so

ex1 = 0 = r0 + r1

= x2, then r0 + r1
ex2) = 0. Hence r1 = 0 = r0.

where x1
r1(ex1

·

·

−

5.5.13

b. If A is diagonalizable, so is AT , and they have

the same eigenvalues. Use (a).

Section 5.7

5.5.17

b. cB(x) = [x

k = a2 + b2 + c2

(a + b + c)][x2

k] where

−
−
[ab + ac + bc]. Use Theorem 5.5.7.

−

5.7.2 Let X denote the number of years of education, and let
Y denote the yearly income (in 1000’s). Then x = 15.3,
s2
x = 9.12 and sx = 3.02, while y = 40.3, s2
sy = 10.69. The correlation is r(X, Y ) = 0.599.

y = 114.23 and

−

20
46
95 
10

14
24

−

8

10
18

−
−

, (AT A)−

1

−

18
24
43 


Section 5.6

5.6.1

b.

1
12 

= 1

12 


b. 64

13 −
17
10 x

4
10 −

6
13 x

5.6.2

d.

−

5.6.3

b. y = 0.127
3348
642
426

1
4248 

0.024x + 0.194x2, (MT M)−
−
642
571
187

−
−

426
187
91 


46x + 66x2 + 60

2x), (MT M)−

1 =

·

5.6.4



b.

−

1
92 (
115
0
46

−

−

0
17
18

−
−

46
18
38 


1

−

−
20 [18 + 21x2 + 28 sin( πx
24
2
−
14

2 14
3
1
3 49 


−

5.6.5

1
46 


b.

1
40 



2 )], (MT M)−

5.7.4

1 =

b. Given the sample vector x = 





x1
x2
...
xn



, let








where zi = a + bxi for each i. By (a) we

z1
z2
...
zn






z = 





have z = a + bx, so

[(a + bxi)

(a + bx)]2

(zi

z)2

−

−
x)2

b2(xi

−

1 ∑
i
1 ∑
i
1 ∑
i

s2
z = 1
n
−
= 1
n
−
= 1
n
−
= b2s2
x.

1 =

Now (b) follows because √b2 =

b

.

|

|

Supplementary Exercises for Chapter 5

Supplementary Exercise 5.1.

b. F

6
d. T

f. T

h. F

j. F

l. T

n. F

p. F

r. F

Section 6.1

6.1.1

b. No; S5 fails.

d. No; S4 and S5 fail.

6.1.2

b. No; only A1 fails.

d. No.

f. Yes.

h. Yes.

j. No.

l. No; only S3 fails.

n. No; only S4 and S5 fail.

6.1.13

b. The case n = 1 is clear, and n = 2 is axiom S3.

637

If n > 2, then
(a1 + a2 +
a1v + (a2 +
the induction hypothesis; so it holds for all n.

+ an)v = [a1 + (a2 +
· · ·
+ an)v = a1v + (a2v +

· · ·

· · ·

· · ·

+ an)]v =

+ anv) using

6.1.15

a−

c. If av = aw, then v = 1v = (a−
1(aw) = (a−
1(av) = a−

1a)w = 1w = w.

1a)v =

Section 6.2

6.2.1

b. Yes

d. Yes

f. No; not closed under addition or scalar multiplication,

and 0 is not in the set.

6.2.2

b. Yes.

d. Yes.

f. No; not closed under addition.

6.2.3

b. No; not closed under addition.

d. No; not closed under scalar multiplication.

f. Yes.

6.2.5

b. If entry k of x is xk

y = Ax where the column of A is x−
columns are zero.

= 0, and if y is in Rn, then
1
k y, and the other

6.1.4 The zero vector is (0,
y).
(
−

−

−

x,

2

1); the negative of (x, y) is

−

6.2.6

b.

−
3 (x + 1) + 1

3(x + 1) + 0(x2 + x) + 2(x2 + 2)
1
3 (x2 + 2)

3 (x2 + x)

d. 2

−

6.1.5

b. x = 1

7 (5u

2v), y = 1

7 (4u

3v)

−

−

6.1.6

b. Equating entries gives a + c = 0, b + c = 0,

b + c = 0, a

−

c = 0. The solution is a = b = c = 0.

d. If a sin x + b cosy + c = 0 in F[0, π], then this must
2 , and π,
b + c = 0

hold for every x in [0, π]. Taking x = 0, π
respectively, gives b + c = 0, a + c = 0,
whence, a = b = c = 0.

−

6.1.7

b. 4w

6.2.7

b. No.

d. Yes; v = 3u

w.

−

6.2.8

b. Yes; 1 = cos2 x + sin2 x

d. No. If 1 + x2 = a cos2 x + b sin2 x, then taking x = 0

and x = π gives a = 1 and a = 1 + π2.

6.2.9

b. Because P2 = span
1, x, x2

} ⊆
3 (3x); 1 = (1 + x)

{

show that
x = 1

1, x, x2

, it sufﬁces to

{
}
span
x and x2 = 1

1 + 2x2, 3x, 1 + x
2 [(1 + 2x2)

{

−

. But
1].

}
−

6.1.10 If z + v = v for all v, then z + v = 0 + v, so z = 0 by
cancellation.

6.2.11

b. u = (u + w)

and w = w

w, v =

(u

−

−

−

v) + (u + w)

w,

−

6.1.12

b. (

a)v + av = (

a + a)v = 0v = 0 by

6.2.14 No.

(av) + av = 0 (by the

(av) in axiom A5), this means that
(av) by cancellation. Alternatively, use

−

−

−

−

Theorem 6.1.3. Because also
deﬁnition of
a)v =
(
−
Theorem 6.1.3(4) to give
(
−

1)a]v = (

a)v = [(

−

−

−

1)(av) =

(av).

−

6.2.17

b. Yes.

6.2.18 v1 = 1
a1
V
span

v2
−
u, v2, . . . , vn

u

a2
a1

⊆

{

− · · · −
}

an
a1

vn, so

6
6.3.8

b.

1
0

0
0

0
0

1
0

,

(cid:21)

(cid:20)

(cid:21)(cid:27)

(cid:26)(cid:20)

1)u

U.

∈

6.3.10

b. dim V = 7

638

Selected Exercise Answers

6.2.21

b. v = (u + v)

u is in U.

−

6.2.22 Given the condition and u
The converse holds by the subspace test.

∈

U, 0 = u + (

−

Section 6.3

6.3.1

b. If ax2 + b(x + 1) + c(1

x2) = 0, then
x
c = 0, b + c = 0, so a = b = c = 0.

a + c = 0, b

−

−

d. If a

1 1
1 0
1
1

(cid:20)
1
0

−

(cid:21)
=

+ b

0 1
1 1
0
0

(cid:20)
0
0

+ c

(cid:21)

(cid:20)

1
1

0
1

+

(cid:21)

d

(cid:20)

, then a + c + d = 0,
(cid:21)
a + b + d = 0, a + b + c = 0, and b + c + d = 0, so
a = b = c = d = 0.

(cid:20)

(cid:21)

6.3.2

b.
3(x2

−
1
−
0
0
0

d. 2

(cid:20)
0
0

(cid:20)

f.

5
x2+x

−

x + 3)

2(2x2 + x + 5) + (x2 + 5x + 1) = 0

−

0
1

−

+

(cid:21)

(cid:20)

−

1
1

1
1

−

+

(cid:21)

(cid:20)

1
1

1
1

=

(cid:21)

(cid:21)
6 + 1
x2

5x+6 −

−

x2

6

−

9 = 0

6.3.3

b. Dependent: 1

sin2 x

−

−

cos2 x = 0

6.3.4

b. x

=

1
3

−

6.3.5

b. If
1, 1, 1) + s(1,

−

−

1, 1) + t(1, 1,

s + t = 0, and r

−
r + s + t = 0, r

1) = (0, 0, 0),
t = 0,

r(
−
then
−
and this implies that r = s = t = 0. This proves
independence. To prove that they span R3, observe
that (0, 0, 1) = 1
2 [(
−
1, 1), (1, 1,
1, 1, 1), (1,
lies in span
proof is similar for (0, 1, 0) and (1, 0, 0).

1, 1)] so (0, 0, 1)
. The
1)
}

1, 1, 1) + (1,

(
−

−

−

−

−

−

{

s

d. If r(1 + x) + s(x + x2) + t(x2 + x3) + ux3 = 0, then

r = 0, r + s = 0, s + t = 0, and t + u = 0, so
r = s = t = u = 0. This proves independence. To show
that they span P3, observe that x2 = (x2 + x3)
x = (x + x2)
x2, and 1 = (1 + x)
1, x, x2, x3

−
−
1 + x, x + x2, x2 + x3, x3

span

x, so

x3,

.

−
} ⊆

{

}

{

1, x + x2

; dimension = 2

{
; dimension = 2

}

6.3.6

b.

d.

1, x2

{

6.3.7

b.

}

6.3.11

x2
b.
dim V = 4

{

−

x, x(x2

−

x), x2(x2

x), x3(x2

;

x)
}

−

−

6.3.12

b. No. Any linear combination f of such

polynomials has f (0) = 0.

d. No.

1 0
0 1

1
1
(cid:26)(cid:20)
(cid:20)
consists of invertible matrices.

1
0

1
1

(cid:21)

(cid:20)

(cid:21)

,

,

0
1

,

(cid:21)

(cid:20)

0 1
1 1

;
(cid:21)(cid:27)

f. Yes. 0u + 0v + 0w = 0 for every set

u, v, w

.

}

{

h. Yes. su + t(u + v) = 0 gives (s + t)u + tv = 0, whence

s + t = 0 = t.

j. Yes. If ru + sv = 0, then ru + sv + 0w = 0, so

r = 0 = s.

l. Yes. u + v + w

= 0 because

{
n. Yes. If I is independent, then

u, v, w

}

is independent.

n by the

I

|

| ≤

fundamental theorem because any basis spans V .

6.3.15 If a linear combination of the subset vanishes, it is a
linear combination of the vectors in the larger set (coefﬁcients
outside the subset are zero) so it is trivial.

6.3.19 Because

{

is equivalent to

Theorem 2.4.5.

(cid:20)

u, v
}
a
c
b d

is linearly independent, su′ + tv′ = 0

s
t

=

(cid:21)

(cid:20)

0
0

(cid:21)

(cid:21) (cid:20)

. Now apply

6.3.23

b. Independent.

d. Dependent. For example,

(u + v)

−

(v + w) + (w + z)

(z + u) = 0.

−

6.3.26 If z is not real and az + bz2 = 0, then
= 0, then z =
= 0). Hence if b
a + bz = 0(z
b = 0, and so a = 0. Conversely, if z is real, say z = a, then
(
−

a)z + 1z2 = 0, contrary to the independence of

z, z2

ab−

−

{

}

.

1 is real. So

6.3.29

b. If Ux = 0, x

= 0 in Rn, then Rx = 0 where

= 0 is row 1 of U. If B

R
R, then Bx
Bx = ∑ riAiUx = 0. So

∈
= 0. But if B = ∑ riAiU, then
AiU

cannot span Mmn.

Mmn has each row equal to

{

}

1
1
1 0

−

,

(cid:21)

(cid:20)

−

,

(cid:20)
(cid:21)
1
0
1 0

1
0

0
1

(cid:21)(cid:27)

; dimension = 2

6.3.33

; dimension = 2

(cid:21)(cid:27)

b. If U

W = 0 and ru + sw = 0, then ru =
sw
W , so ru = 0 = sw. Hence r = 0 = s because

−

∩

∩
= w. Conversely, if v

= 0 lies in U
1)v = 0, contrary to hypothesis.

W , then

∩

is in U
u
= 0
1v + (

−

(cid:26)(cid:20)
0
1

1
1

d.

(cid:26)(cid:20)

6
6
6
6
6
6
6
6
6
6
6.3.36

b. dim On = n

2 if n is even and dim On = n+1

2

is odd.

Section 6.4

6.4.1

b.

d.

x2

{

−

(0, 1, 1), (1, 0, 0), (0, 1, 0)
}

{
x + 1, 1, x

}

639

if n

6.4.25

b.
Ru + Rw =

ru + sw

{

|

r, s in R

= span

u, w
}

{

}

Section 6.5

6.5.2

b. 3 + 4(x

d. 1 + (x

1)3

−

1) + 3(x

1)2 + (x

1)3

−

−

−

6.4.2

b. Any three except

x2 + 3, x + 2, x2

{

2x

1

}

−

−

6.5.6

b. The polynomials are (x

1)(x

(x
−
a2 = 1.

−

3), (x

2)(x

−

−

1)(x

2),
3). Use a0 = 3, a1 = 2, and

−

−

6.4.3

b. Add (0, 1, 0, 0) and (0, 0, 1, 0).

d. Add 1 and x3.

6.4.4

b. If z = a + bi, then a

= 0 and b
rz + sz = 0, then (r + s)a = 0 and (r
−
means that r + s = 0 = r
s, so r = s = 0. Thus
is independent; it is a basis because dim C = 2.

s)b = 0. This
z, z

= 0. If

−

{

6.4.5

b. The polynomials in S have distinct degrees.

6.5.7

b.
3
2 (x

−

f (x) =
2)(x

−

7(x

3)

−

−

1)(x

−

3) + 13

2 (x

1)(x

2).

−

−

6.5.10

}

b. If r(x

a)2 + s(x

a)(x

b) + t(x

b)2 = 0,

−

−
then evaluation at x = a(x = b) gives t = 0(r = 0).
Thus s(x
a)(x
Theorem 6.4.4.

b) = 0, so s = 0. Use

−

−

−

−

6.4.6

b.

4, 4x, 4x2, 4x3

is one such basis of P3.

{

}
However, there is no basis of P3 consisting of
polynomials that have the property that their
coefﬁcients sum to zero. For if such a basis exists,
then every polynomial in P3 would have this property
(because sums and scalar multiples of such
polynomials have the same property).

6.4.7

b. Not a basis.

d. Not a basis.

6.4.8

b. Yes; no.

6.4.10 det A = 0 if and only if A is not invertible; if and only
if the rows of A are dependent (Theorem 5.2.3); if and only if
some row is a linear combination of the others (Lemma 6.4.2).

6.4.11

b. No.

(0, 1), (1, 0)

} ⊆ {

(0, 1), (1, 0), (1, 1)
}

.

{

d. Yes. See Exercise 6.3.15.

6.4.15 If v

U then W = U; if v /
∈

∈

U then

v1, v2, . . . , vk, v

is a basis of W by the independent lemma.

{

}

6.5.11

b. Suppose

p0(x), p1(x), . . . , pn

2(x)
}

−

is a

{

−

−

a)(x

−
−

is a basis of Un. It is a spanning set

b)p1(x), . . . , (x

2. We show that
b)p0(x), (x
2(x)
}

basis of Pn
−
a)(x
(x
{
−
b)pn
a)(x
−
by part (a), so assume that a linear combination
vanishes with coefﬁcients r0, r1, . . . , rn
2. Then
(x
2(x)] = 0, so
a)(x
· · ·
−
2(x) = 0 by the Hint. This
+ rn
r0 p0(x) +
implies that r0 =

b)[r0p0(x) +

−
· · ·

2 = 0.

−
= rn

+ rn

2 pn

2 pn

−

−

−

−

−
· · ·

−

Section 6.6

6.6.1

x

−

b. e1
3x
3

d.

e2x
e2

e−
−
e−
−
f. 2e2x(1 + x)

h.

x)

eax
ea(2
−
−
e2a
1
−
2x sin x

j. eπ
−

6.6.4

b. ce−

x + 2, c a constant

6.6.5

b. ce−

3x + de2x

x3
3

−

6.4.18

b. Two distinct planes through the origin (U and

6.6.6

b.

t =

W ) meet in a line through the origin (U

W ).

∩

1
3 ln(
2 )
4
5 )

ln(

= 9.32 hours

6.4.23

b. The set

(1, 0, 0, 0, . . . ), (0, 1, 0, 0, 0, . . . ),
contains independent subsets

{

(0, 0, 1, 0, 0, . . . ), . . .
of arbitrary size.

}

6.6.8 k = ( π

15 )2 = 0.044

Supplementary Exercises for Chapter 6

6
6
640

Selected Exercise Answers

Supplementary Exercise 6.2.

b. If YA = 0, Y a row, we

7.1.8

b. T (v) = (

1)v for all v in V , so T is the scalar

show that Y = 0; thus AT (and hence A) is invertible.
Given a column c in Rn write c = ∑
each ri is in R. Then Y c = ∑
i

i
riYAvi, so

ri(Avi) where

Y = Y In = Y
Y e1 Y e2
as required.
(cid:2)

e1

(cid:2)

· · ·

e2
Y en

· · ·

=

en

=
0 0
(cid:3)

· · ·

0

= 0,

7.1.15

(cid:3)

(cid:2)

(cid:3)

Supplementary Exercise 6.4. We have null A
because Ax = 0 implies (AT A)x = 0. Conversely, if
(AT A)x = 0, then
Ax = 0.

Ax
k

2 = (Ax)T (Ax) = xT AT Ax = 0. Thus

⊆

k

null (AT A)

operator

1.

−

−

7.1.12 If T (1) = v, then T (r) = T (r
r in R.

·

1) = rT (1) = rv for all

v

b. 0 is in U =

V
|
T (0) = 0 is in P. If v and w are in U, then T (v) and
T (w) are in P. Hence T (v + w) = T (v) + T (w) is in P
and T (rv) = rT (v) is in P, so v + w and rv are in U.

because

T (v)

∈

∈

P

{

}

= 0). If s

7.1.18 Suppose rv + sT (v) = 0. If s = 0, then r = 0 (because
= 0, then T (v) = av where a =
v
v = T 2(v) = T (av) = a2v, so a2 = 1, again because v
1. Conversely, if T (v) =
Hence a =
±
certainly not independent.

v, T (v)
}

1r. Thus

v, then

= 0.

s−

±

−

is

{

Section 7.1

7.1.1

b. T (v) = vA where A =



−
d. T (A + B) = P(A + B)Q = PAQ + PBQ =



1
0
0

0
1
0

7.1.21

0
0
1 


b. Given such a T , write T (x) = a. If

p = p(x) = ∑n
∑ ai [T (x)]i = ∑ aiai = p(a) = Ea(p). Hence T = Ea.

i=0 aixi, then T (p) = ∑ aiT (xi) =

T (A) + T (B); T (rA) = P(rA)Q = rPAQ = rT (A)

Section 7.2

f. T [(p + q)(x)] = (p + q)(0) = p(0) + q(0) =

T [p(x)] + T [q(x)];
T [(rp)(x)] = (rp)(0) = r(p(0)) = rT [p(x)]

h. T (X +Y ) = (X +Y )
and T (rX) = (rX)

Z = X
·
Z = r(X

·

Z = T (X) + T (Y ),

Z +Y
·
Z) = rT (X)

·
·

j. If v = (v1, . . . , vn) and w = (w1, . . . , wn), then

T (v + w) = (v1 + w1)e1 +
+ vnen) + (w1e1 +

· · ·

· · ·
T (av) = (av1)e +
aT (v)

· · ·

+ (vn + wn)en = (v1e1 +

+ wnen) = T (v) + T (w)

· · ·
+ (avn)en = a(ve +

+ vnen) =

· · ·

7.2.2

7.1.2

b.

rank (A + B)

example, A =

(cid:20)

d. T (0) = 0 + u = u
Theorem 7.1.1.

= rank A + rank B in general. For
1
0

and B =

0
1

1 0
0 1

.

(cid:21)

−
= 0, so T is not linear by

(cid:20)

(cid:21)

7.1.3

b. T (3v1 + 2v2) = 0

1
7

=

−

3
4

(cid:21)

(cid:20)
x + 3x2) = 46

d. T

(cid:20)
f. T (2

−

−

7.1.4

b. T (x, y) = 1
1, 2) = (

3 (x
1, 2,

T (

−

d. T

−
a
c

(cid:20)

b
d

(cid:21)

= 3a

−

3c + 2b

(cid:21)

y, 3y, x
1)

−
−

y);

−

7.2.1










d.

d.

f.

h.

j.

7.2.3

1
1
0
1

−

b.
3
−
7
1
0












, 








1
−
2
1 


b.

x2

−
;

{
(0, 0, 1)
}
0
1

1
0

{

,

1
0
1 






; 2, 2

0
1
1 







−










;





1
0
1
1

,




0
1
1
2





x

; 


;
{



, 

−
−











(1, 0), (0, 1)
}







; 2, 1





}
(1, 1, 0, 0), (0, 0, 1, 1)
}

{

0 1
0 0

0
1

0
0

,

(cid:21)

;
(cid:21)(cid:27)
(cid:20)
1), (0, 1, 0, . . . , 0,
1

;

−

1

}

{

1),

{

}

(cid:21)

−
(cid:26)(cid:20)
(cid:20)
(1, 0, 0, . . . , 0,
{
−
. . . , (0, 0, 0, . . . , 1,

0 1
0 0
1 1
0 0

0
0
0
1

(cid:21)

,

,

(cid:20)

(cid:21)

(cid:20)

(cid:26)(cid:20)

(cid:26)(cid:20)

−
0
1
0
1

1)
}

;
(cid:21)(cid:27)

(cid:21)(cid:27)

b. T (v) = 0 = (0, 0) if and only if P(v) = 0 and
ker Q.

Q(v) = 0; that is, if and only if v is in ker P

∩

7.2.4

b. ker T = span

B =
{
im T = span

(1, 0, 0), (0, 1, 0), (

−
(1, 2, 0, 3), (1,

(
−

{

;
4, 1, 3)
}
,
4, 1, 3)
}
3, 0)
1,
}
−

−

{

7.1.5

b. T (v) = 1

3 (7v

−

9w), T (w) = 1

3 (v + 3w)

7.2.6

b. Yes. dim ( im T ) = 5

im T = W as dim W = 3.

−

dim ( ker T ) = 3, so

6
6
6
6
6
d. No. T = 0 : R2
f. No. T : R2

→
ker T = im T

R2

→
R2, T (x, y) = (y, 0). Then

h. Yes. dim V = dim ( ker T ) + dim ( im T )

dim W + dim W = 2 dim W

≤

j. No. Consider T : R2

→

R2 with T (x, y) = (y, 0).

l. No. Same example as (j).

n. No. Deﬁne T : R2

R2 by T (x, y) = (x, 0). If

→

v1 = (1, 0) and v2 = (0, 1), then R2 = span
but R2
T (v1), T (v2)
}

= span

{

.

v1, v2

{

}

7.2.7

b. Given w in W , let w = T (v), v in V , and write

Section 7.3

641

7.3.1

b. T is onto because T (1,

1, 0) = (1, 0, 0),

−
1) = (0, 1, 0), and T (0, 0, 1) = (0, 0, 1).

T (0, 1,
−
Use Theorem 7.3.3.

d. T is one-to-one because 0 = T (X) = UXV implies that
X = 0 (U and V are invertible). Use Theorem 7.3.3.

f. T is one-to-one because 0 = T (v) = kv implies that
= v

v = 0 (because k
for all v. [Here Theorem 7.3.3 does not apply if dim V
is not ﬁnite.]

= 0). T is onto because T

1
k v

(cid:1)

(cid:0)

h. T is one-to-one because T (A) = 0 implies AT = 0,

whence A = 0. Use Theorem 7.3.3.

v = r1v1 +
· · ·
w = T (v) = r1T (v1) +

+ rnvn. Then

+ rnT (vn).

· · ·

7.3.4

b. ST (x, y, z) = (x + y, 0, y + z),

T S(x, y, z) = (x, 0, z)

7.2.8

b.

im T =

∑i rivi

{

|

ri in R

}

= span

vi

.

}

{

7.2.10 T is linear and onto. Hence 1 = dim R =
dim ( im T ) = dim (Mnn)

dim ( ker T ) = n2

−

dim ( ker T ).

−

7.2.12 The condition means ker (TA)
dim [ ker (TA)]
dim [ im (TA)]

⊆
dim [ ker (TB)]. Then Theorem 7.2.4 gives
dim [ im (TB)]; that is, rank A

ker (TB), so

rank B.

≤
≥

≥

7.2.15

b. B =

x

1, . . . , xn

1

is independent

{

−
(distinct degrees) and contained in ker T . Hence B is a
basis of ker T by (a).

−

}

Mnn by T (A) = A

7.2.20 Deﬁne T : Mnn
Mnn. Then ker T = U and im T = V by Example 7.2.3, so
the dimension theorem gives
n2 = dim Mnn = dim (U) + dim (V ).

→

−

AT for all A in

→

Rn by T (A) = Ay for all A in Mnn.
7.2.22 Deﬁne T : Mnn
Then T is linear with ker T = U, so it is enough to show that
T is onto (then dim U = n2
n). We have
y2
y1
T (0) = 0. Let y =
= 0
· · ·
1
k y, and let c j = 0 if j
let ck = y−
(cid:2)
A =
, then
T (A) = Ay = y1c1 +
+ ykck +
(cid:3)
that T is onto, as required.

−
= 0 in Rn. If yk

+ yncn = y. This shows

yn
= k. If
(cid:3)

dim ( im T ) = n2

· · ·

· · ·

· · ·

c2

c1

cn

−

(cid:2)

T

7.2.29

b. By Lemma 6.4.2, let

u1, . . . , um, . . . , un

be

{

is a basis of U. By

}

}

{

→

u1, . . . , um

a basis of V where
Theorem 7.1.3 there is a linear transformation
V such that S(ui) = ui for 1
S : V
S(ui) = 0 if i > m. Because each ui is in im S,
im S. But if S(v) is in im S, write
U
v = r1u1 +
S(v) = r1S(u1) +
in U. So im S

+ rnun. Then
+ rmS(um) = r1u1 +

+ rmum +

· · ·
U.

· · ·

· · ·

· · ·

⊆

≤

≤

i

m, and

+ rmum is

⊆

d. ST

T S

(cid:20)

(cid:20)

a
c
a
c

b
d
b
d

c
0
0
d

=

=

(cid:21)

(cid:21)

(cid:20)

(cid:20)

0
d
a
0

,

(cid:21)

(cid:21)

7.3.5

b. T 2(x, y) = T (x + y, 0) = (x + y, 0) = T (x, y).

d. T 2

Hence T 2 = T .
a
c

b
d
a + c b + d
a + c b + d

1
2

(cid:20)

(cid:21)

= 1

2 T

(cid:20)

(cid:20)

(cid:21)

7.3.6

b. No inverse; (1,

d. T −

1

a
c

b
d

= 1
5

(cid:21)

(cid:20)
1(a, b, c) = 1
2

(cid:20)
2a + (b

f. T −

7.3.7

b.

T 2(x, y) = T (ky

(cid:2)

−

d. T 2(X) = A2X = IX = X

a + c b + d
a + c b + d

=

(cid:21)

1) is in ker T .

1, 1,

−
3a
−
a + c

−
2c 3b

2d
−
b + d

c)x

−

−

(2a

−

(cid:21)
b

−

c)x2

(cid:3)

x, y) = (ky

(ky

−

−

x), y) = (x, y)

7.3.8

b. T 3(x, y, z, w) = (x, y, z,

w) so

−
T 3(x, y, z, w)

= (x, y, z, w).

T 6(x, y, z, w) = T 3
1 = T 5. So
Hence T −
(cid:2)
1(x, y, z, w) = (y
T −
−

x, z,

x,

−

−

(cid:3)
w).

7.3.9

b. T −

1(A) = U −

1A.

7.3.10

b. Given u in U, write u = S(w), w in W

(because S is onto). Then write w = T (v), v in V (T is
onto). Hence u = ST (v), so ST is onto.

7.3.12

b. For all v in V , (RT )(v) = R [T (v)] is in im (R).

6
6
6
6
6
642

Selected Exercise Answers

7.3.13

b. Given w in W , write w = ST (v), v in V (ST is
onto). Then w = S [T (v)], T (v) in U, so S is onto. But
then im S = W , so dim U =
dim ( ker S) + dim ( im S)

dim ( im S) = dim W .

≥

7.3.16
Theorem 7.2.5. So T : span
isomorphism by Theorem 7.3.1.

T (e1), T (e2), . . . , T (er)
}
e1, . . . , er

{

{

is a basis of im T by

im T is an

} →

{

}

}

be a basis of V

{
er+1, . . . , en

e1, . . . , er, er+1, . . . , en
a basis of ker T . If

7.3.29 Let B =
with
T (e1), . . . , T (er), wr+1, . . . , wn
{
S[T (ei)] = ei for 1
≤
Then S is an isomorphism by Theorem 7.3.1, and
T ST (ei) = T (ei) clearly holds for 1
then T (ei) = 0 = T ST (ei), so T = T ST by Theorem 7.1.2.

is a basis of V , deﬁne S by
n.

r, and S(w j) = e j for r + 1

r. But if i

r + 1,

≤

≤

≤

≤

≤

≥

}

i

i

j

7.3.19

b. T (x, y) = (x, y + 1)

Section 7.5

7.3.24

b.

T S[x0, x1, . . . ) = T [0, x0, x1, . . . ) = [x0, x1, . . . ), so
T S = 1V . Hence T S is both onto and one-to-one, so T
is onto and S is one-to-one by Exercise 7.3.13. But
[1, 0, 0, . . . ) is in ker T while [1, 0, 0, . . . ) is not in
im S.

7.3.26

b. If T (p) = 0, then p(x) =

p(x) = a0 + a1x + a2x2 +
a0 + a1x + a2x2 +

a1x

2a2x2

· · ·
− · · · −

xp′(x). We write

−

+ anxn, and this becomes

· · ·
+ anxn =
nanxn. Equating coefﬁcients

−

−
yields a0 = 0, 2a1 = 0, 3a2 = 0, . . . , (n + 1)an = 0,
whence p(x) = 0. This means that ker T = 0, so T is
one-to-one. But then T is an isomorphism by
Theorem 7.3.3.

7.3.27

b. If ST = 1V for some S, then T is onto by

}

{

e1, . . . , er, . . . , en
}
is a basis of
is a basis

Exercise 7.3.13. If T is onto, let
{
er+1, . . . , en
be a basis of V such that
{
T (e1), . . . , T (er)
ker T . Since T is onto,
}
of im T = W by Theorem 7.2.5. Thus S : W
isomorphism where by S
{
i = 1, 2, . . . , r. Hence T S[T (ei)] = T (ei) for each i,
that is T S[T (ei)] = 1W [T (ei)]. This means that
T S = 1W because they agree on the basis
T (e1), . . . , T (er)
}

T (ei)] = ei for

of W .

→

{

V is an

7.3.28

⊆
⊆

e1, . . . , er, er+1, . . . , en

b. If T = SR, then every vector T (v) in im T has
im S. Since
im T .

the form T (v) = S[R(v)], whence im T
1 implies im S
R is invertible, S = T R−
Conversely, assume that im S = im T . Then
dim ( ker S) = dim ( ker T ) by the dimension theorem.
Let
f1, . . . , fr, fr+1, . . . , fn
{
er+1, . . . , en
{
{
and ker T , respectively. By Theorem 7.2.5,
T (f1), . . . , T (fr)
S(e1), . . . , S(er)
are both
and
{
}
}
bases of im S = im T . So let g1, . . . , gr in V be such
that S(ei) = T (gi) for each i = 1, 2, . . . , r. Show that

be bases of V such that

are bases of ker S

fr+1, . . . , fn

and

and

}

}

}

{

}

{

B =

g1, . . . , gr, fr+1, . . . , fn

is a basis of V .

}

{
Then deﬁne R : V
i = 1, 2, . . . , r, and R(f j) = e j for j = r + 1, . . . , n.
Then R is an isomorphism by Theorem 7.3.1. Finally
SR = T since they have the same effect on the basis B.

V by R(gi) = ei for

→

[1), [2n), [(
b.
{
−
xn = 1
20 (15 + 2n+3 + (

3)n)
;
}
3)n+1)

−

7.5.1

7.5.2

b.

[1), [n), [(

{
[1), [n), [n2)
}

2)n)
−
}
; xn = 2(n

; xn = 1

9 (5

6n + (

−

−

2)n+2)

1)2

1

−

−

d.

{

7.5.3

b.

[an), [bn)
}

{

7.5.4

b.

[1, 0, 0, 0, 0, . . . ), [0, 1, 0, 0, 0, . . . ),

[0, 0, 1, 1, 1, . . . ), [0, 0, 1, 2, 3, . . . )

7.5.7 By Remark 2,

[in + (
[i(in

−
(
−

−

i)n) = [2, 0,
i)n)) = [0,

2, 0, 2, 0,

−

2, 0, . . . )

−

2, 0, 2, 0,

−

−

2, 0, 2, . . . )

are solutions. They are linearly independent and so are a
basis.

Section 8.1

8.1.1

d.

{

8.1.2

d. x = 1

f. x =
1
12 (5a
3d,
−
7b + c

b.

(2, 1), 3
5 (

1, 2)
}
(0, 1, 1), (1, 0, 0), (0,

−

{

−

2, 2)
}

b. x = 1

182 (271,
−
4 (1, 7, 11, 17) + 1

4 (7,

7,

−

−

7, 7)

221, 1030) + 1

182 (93, 403, 62)

5b + c

3d,
−
3a + 3b + 3c + 3d) + 1

5a + 5b

−

−

3d,

−

−

a + b + c

−

c + 3d, a

−
12 (7a + 5b
3b
3d, 3a

−

−
−

b + 11c +
−
c + 3d, 5a +
3c + 9d)

8.1.3

a.

c.

1
70 (

−

1
10 (

−
63, 21,

9, 3,

−

21, 33) = 3

10 (

147, 231) = 3

10 (

−

3, 1,

−
3, 1,

7, 11)

−
7, 11)

−

−

8.1.4

d.

1,

1, 2)
}

;

−

−

b.

(1,
{
projU x = (1, 0,

1, 0), 1
2 (
1)

−

−
1, 0, 1), (1, 1, 0, 0), 1
3 (

(1,
{
projU x = (2, 0, 0, 1)

−

1, 1, 0, 2)
}

;

−

643

c√2 a
k
0
a√2
c

a
k
c

−




x1 + 2x2) and y2 = 1
√5

(2x1 + x2);

8.1.5

b. U ⊥ = span

(1, 3, 1, 0), (

−

1, 0, 0, 1)
}

{

8.2.6 P = 1

√2k 

8.1.8 Write p = projU x. Then p is in U by deﬁnition. If x is
U, then x
−
Theorem 8.1.3, so x

p is also in U ⊥ by

p is in U. But x

−
p is in U

. Thus x = p.

U ⊥ =

−

∩

0
}

{

8.2.10

q =

−


b. y1 = 1
(
√5
−
1 + 2y2
3y2
2.

−

f1, f2, . . . , fm

8.1.10 Let
is in U the expansion theorem gives
x = (x

f1)f1 + (x

f2)f2 +

+ (x

}

{

·

· · ·

·

·

fm)fm = projU x.

be an orthonormal basis of U. If x

y1, y2, . . . , ym}
n matrix with rows yT

8.1.14 Let
{
the n
Ax = 0 if and only if yi ·
only if x is in U ⊥⊥ = U.

×

be a basis of U ⊥, and let A be
1 , yT
m, 0, . . . , 0. Then

2 , . . . , yT

x = 0 for each i = 1, 2, . . . , m; if and

8.1.17

d. E T = AT [(AAT )−1]T (AT )T =
1A = AT [AAT ]−
1A = E
1A = AT (AAT )−
1AAT (AAT )−

AT [(AAT )T ]−
E 2 = AT (AAT )−

1A = E

Section 8.2

8.2.1

b. 1
5

d.

1
√a2+b2

(cid:20)

(cid:20)

2
√6
1
√3 −

0

f. 




3
4

4
3

−

b
a
b a

−
1
√6 −
1
√3
1
√2

(cid:21)

(cid:21)
1
√6
1
√3
1
√2

h. 1

7 



6
2
3
2
6 3

−

−

3
6
2 







8.2.2 We have PT = P−
1; this matrix is lower triangular (left
side) and also upper triangular (right side–see Lemma 2.7.1),
and so is diagonal. But then P = PT = P−
1, so P2 = I. This
implies that the diagonal entries of P are all

1.

±

8.2.11

⇒

a. By Theorem 8.2.1 let

c.
1AP = D = diag (λ1, . . . , λn) where the λi are the
1 for each i,

P−
eigenvalues of A. By c. we have λi =
whence D2 = I. But then
A2 = (PDP−
1)2 = PD2P−
this is AAT = I, proving a.

±
1 = I. Since A is symmetric

8.2.13

b. If B = PT AP = P−
B2 = PT APPT AP = PT A2P.

1, then

8.2.15 If x and y are respectively columns i and j of In, then
xT AT y = xT Ay shows that the (i, j)-entries of AT and A are
equal.

= 1

(cid:21)
=

1

−

×

−

8.2.18

b. det

cosθ
sinθ

sinθ
cosθ

−

(cid:20)
cosθ
sinθ

(cid:20)

sinθ
cosθ

−

(cid:21)

and det

[Remark: These are the only 2

2 examples.]

d. Use the fact that P−

P) =

PT (I
−
use the hypothesis that det P

−

−

(I

1 = PT to show that
P)T . Now take determinants and
1)n.

= (

8.2.21 We have AAT = D, where D is diagonal with main
1 = AT D−
1,
2. Hence A−
diagonal entries
k
k
1 has diagonal entries
and the result follows because D−
2, . . . , 1/
1/

2, . . . ,

R1

Rn

2.

k

k

Rn

R1

k

k

k

k

8.2.23

b. Because I

A and I + A commute,

PPT = (I
(I

−
A)(I + A)−

−
A)(I + A)−
1(I

A)−

1[(I + A)−

1]T (I
1(I + A) = I.

−

A)T =

−

−

8.2.5

b.

d.

1
√2 



f.

1
3√2 

1
1

1
√2

(cid:20)
0 1
√2 0
0 1

2√2
√2
2√2

1
1

−

(cid:21)

1
0
1 

1
4
−
1

−
3
0
3



1
1
−
1
−
1

h. 1

2 





−

−
1 √2
1 √2
1
−
1


0
0
0 √2
0 √2









or 1

3 



2
1
2

−

2
2
1

Section 8.3

8.3.1

b. U = √2
2

2
0

1
−
1

(cid:21)

(cid:20)

60√5 12√5

15√5
6√30 10√30
5√15

0

d. U = 1

30 

0
0







8.3.2

b. If λk > 0, k odd, then λ > 0.

= 0, then xT Ax > 0 and xT Bx > 0. Hence
8.3.4 If x
xT (A + B)x = xT Ax + xT Bx > 0 and xT (rA)x = r(xT Ax) > 0,
as r > 0.

1
2
2 


−

6
6
644

Selected Exercise Answers

= 0 in Rn. Then xT (U T AU)x = (Ux)T A(Ux) > 0
c2
= 0. But if U =

8.3.6 Let x
provided Ux
x = (x1, x2, . . . , xn), then Ux = x1c1 + x2c2 +
because x

= 0 and the ci are independent.

and
+ xncn
(cid:3)

= 0

. . .

· · ·

c1

cn

(cid:2)

8.3.10 Let PT AP = D = diag (λ1, . . . , λn) where PT = P.
Since A is positive deﬁnite, each eigenvalue λi > 0. If
B = diag (√λ1, . . . , √λn) then B2 = D, so
A = PB2PT = (PBPT )2. Take C = PBPT . Since C has
eigenvalues √λi > 0, it is positive deﬁnite.

8.3.12

b. If A is positive deﬁnite, use Theorem 8.3.1 to

1U) so
1U,

1 . Conversely, let

write A = U TU where U is upper triangular with
1U)T D2(D−
positive diagonal D. Then A = (D−
A = L1D1U1 is such a factorization if U1 = D−
D1 = D2, and L1 = U T
AT = A = LDU be such a factorization. Then
U T DT LT = AT = A = LDU, so L = U T by (a). Hence
A = LDLT = V T V where V = LD0 and D0 is diagonal
with D2
0 = D (the matrix D0 exists because D has
positive diagonal entries). Hence A is symmetric, and
it is positive deﬁnite by Example 8.3.1.

Section 8.4

8.4.1

b. Q = 1
√5

2
1

1
−
2

, R = 1
√5

(cid:21)

(cid:20)

5 3
0 1

(cid:21)

(cid:20)

d. Q = 1

√3 




R = 1

√3 



−

1
1
0
1
3 0
0 3
0 0

−

1 0
0 1
1 1
1 1
1
−
1
2 


,






3 1
1 0

, Q1 = 1
√10

3
1

1
3

−

,

(cid:21)

(cid:20)

(cid:20)

(cid:21)
10
0
33
1

−

(cid:21)
,

3
1
−
1
−
3
−
33

(cid:21)
1
1 33

(cid:20)

−
109
0

A1 =

(cid:20)
R1 = 1
√10

A2 = 1
10

(cid:20)
Q2 = 1

√1090

R2 = 1

√1090

A3 = 1
109

=

(cid:20)
360
1
(cid:20)
3.302775
0.009174

,
(cid:21)
3
−
10
−
1
33
(cid:21)
0.009174
0.302775

(cid:21)

−

(cid:20)

(cid:21)

1
k AkQk = QT

−
8.5.4 Use induction on k. If k = 1, A1 = A. In general
Ak+1 = Q−
AT
k+1 = Ak+1. The eigenvalues of A are all real (Theorem
5.5.5), so the Ak converge to an upper triangular matrix T .
But T must also be symmetric (it is the limit of symmetric
matrices), so it is diagonal.

k AkQk, so the fact that AT

k = Ak implies

Section 8.6

8.6.4

b.

tσ1, . . . , tσr.

8.6.7 If A = UΣV T then Σ is invertible, so A−
a SVD.

1 = V Σ−

1U T is

8.6.8

b. First AT A = In so ΣA = In.

A = 1
√2

(cid:20)

= 1
√2

1
1

1
1

1 0
0 1

1
√2

(cid:21)
1 1
−
1
1

(cid:21) (cid:20)
1
√2

(cid:21)

(cid:20)

1
1
1 1

−

(cid:21)

(cid:20)

(cid:21)

1
1

−

1
−
1

(cid:21)

(cid:20)
1 0
0 1

−

=

(cid:20)

A = F

= 1
5

3
4

h

4
3

−

i h

20
0

0
10

0
0

0
0

1
1
1
1

1
1
−
1
1

−

1
1
1
1

−

1
2 



i

1
1
1
1

−
−
−





8.6.13

b. If x

Rn then

xT (G + H)x = xT Gx + xT Hx

∈

0 + 0 = 0.

≥

8.4.2 If A has a QR-factorization, use (a). For the converse
use Theorem 8.4.1.

8.6.9

b.

Section 8.5

8.5.1

b. Eigenvalues 4,

1; eigenvectors

2
1

−

,

(cid:21)

(cid:20)

−
409
203

1
3

(cid:21)

(cid:20)

(cid:20)

−

; r3 = 3.94

; x4 =
−
(cid:21)
2 (3 + √13), λ2 = 1
d. Eigenvalues λ1 = 1
λ1
1

λ2
1
r3 = 3.3027750 (The true value is λ1 = 3.3027756, to
seven decimal places.)

2 (3
−
142
43

eigenvectors

; x4 =

(cid:20)

(cid:21)

(cid:20)

(cid:21)

(cid:20)

(cid:21)

;

,

8.5.2

b. Eigenvalues λ1 = 1
√13) =

λ2 = 1

2 (3

0.302776

−

−

2 (3 + √13) = 3.302776,

√13);

8.6.17

b.

1
4
1
4

0
0

1
4
1
4 (cid:21)

−

(cid:20)

−

Section 8.7

8.7.1

b. √6

d. √13

8.7.2

b. Not orthogonal

6
6
6
6
d. Orthogonal

8.7.21

b. Let U =

8.7.3

b. Not a subspace. For example,

i(0, 0, 1) = (0, 0, i) is not in U.

d. This is a subspace.

8.7.4

b. Basis

(i, 0, 2), (1, 0,

{
(1, 0,

d. Basis

{

2i), (0, 1, 1

−

−

; dimension 2

1)
}
; dimension 2

−
i)
}

8.7.5

b. Normal only

d. Hermitian (and normal), not unitary

f. None

645

be real and invertible, and

a
c

b
d

(cid:20)
1AU =

(cid:21)
λ µ
v
0

. Then

(cid:20)

(cid:21)
, and ﬁrst column entries are

assume that U −

AU = U

λ µ
v
0

(cid:20)

(cid:21)

a = cλ. Hence λ is real (c and a are

c = aλ and
both real and are not both 0), and (1 + λ2)a = 0. Thus
a = 0, c = aλ = 0, a contradiction.

−

Section 8.8

8.8.1

b. 1−

1 = 1, 9−

1 = 9, 3−
d. 21 = 2, 22 = 4, 23 = 8, 24 = 16 = 6, 25 = 12 = 2,
26 = 22 . . . so a = 2k if and only if a = 2, 4, 6, 8.

1 = 7, 7−

1 = 3.

8.8.2

b. If 2a = 0 in Z10, then 2a = 10k for some integer

h. Unitary (and normal); hermitian if and only if z is real

k. Thus a = 5k.

8.7.8

b. U = 1
√14

U HAU =

d. U = 1
√3

(cid:20)

f. U = 1

√3 


U HAU =

(cid:20)
1 0
−
6
0

2
−
3 + i

(cid:21)
1

(cid:20)
1 + i
1

−
√3
0
0
1
0
0





(cid:21)

1

1

i

−
0
1 + i
1
0
0
3 


−
0
0
0

0
1

−

,
i 


3

i

−
2

,

(cid:21)

, U H AU =

1
0

0
4

(cid:20)

(cid:21)

8.8.3

8.8.6

8.8.7

b. 11−

1 = 7 in Z19.

b. det A = 15
exists. Since 5−

A−

1 = 3

(cid:20)

3
3

−

24 = 1 + 4 = 5
1 = 3 in Z7, we have
6
−
5

= 3

3
3

1
5

=

= 0 in Z7, so A−

1

2
2

3
1

.

(cid:20)

(cid:21)
(cid:21)
3 = 1 in Z7 so the reduction of the

(cid:20)

(cid:21)

b. We have 5
augmented matrix is:

·

3
4

1 4
3 1

3
1

(cid:20)

→

(cid:21)

→

→

→

1 5
4 3

1 5
0 4

1 5
0 1

1 0
0 1

(cid:20)

(cid:20)

(cid:20)

(cid:20)

6
1

6
5

6
3

5
3

1
1

1
4

1
1

3
1

(cid:21)

(cid:21)

(cid:21)

(cid:21)

.

Hence x = 3 + 2t, y = 1 + 4t, z = t; t in Z7.

8.8.9

b. (1 + t)−

1 = 2 + t.

8.7.10

8.7.11

8.7.14

b.

2 =

λZ
k

k

λZ, λZ
h

i

Z, Z
= λλ
h

i

=

2

2

λ
|
|

k

Z

k

b. If the (k, k)-entry of A is akk, then the
(k, k)-entry of A is akk so the (k, k)-entry of
(A)T = AH is akk. This equals a, so akk is real.

b. Show that (B2)H = BH BH = (
i)(

(iB)H = iBH = (

B) = iB.

−

−
d. If Z = A + B, as given, ﬁrst show that ZH = A
−
ZH).

hence that A = 1

2 (Z + ZH ) and B = 1

2 (Z

−

B) = B2;

B)(

−

8.8.10

b. The minimum weight of C is 5, so it detects 4

errors and corrects 2 errors.

−

B, and

8.8.11

b.

{

00000, 01110, 10011, 11101

.

}

8.8.12

b. The code is

8.7.16

b. If U is unitary, (U −

so U −

1 is unitary.

1)−

1 = (U H )−

1 = (U −

1)H ,

0000000000, 1001111000, 0101100110,

{
0011010111, 1100011110, 1010101111,
0110110001, 1111001001
distance 5 and so corrects 2 errors.

}

. This has minimum

8.7.18

b. H =

iH =

i
1

(cid:20)

−
(cid:20)
1
−
0

(cid:21)

i
1
i 0

(cid:21)
is not.

is hermitian but

8.8.13

b.

00000, 10110, 01101, 11011

is a

{

(5, 2)-code of minimal weight 3, so it corrects single
errors.

}

6
646

Selected Exercise Answers

8.8.14

b. G =

1 u

where u is any nonzero vector

8.9.9

in the code. H =

(cid:2)

u
(cid:3)
In
−

1

.

(cid:21)

(cid:20)

b. By Theorem 8.3.3 let A = U TU where U is
upper triangular with positive diagonal entries. Then
q = xT (U TU)x = (Ux)TUx =

2.

Ux
k
k

Section 8.9

8.9.1

b. A =

1 0
0 2

(cid:20)

d. A =

1
3
2





3
1
1

−

−

8.9.2

(cid:21)
2
1
3 

1
1

d. P = 1

;

;

(cid:21)



−

3 

;
(cid:21)

(cid:20)
1 −

y = 1
√2
q = 3y2

1
b. P = 1
1
√2
(cid:20)
x1 + x2
x2
x1
−
y2
2; 1, 2
1
2
2
−
2
2
1
−
2 
2
1
−
x3
2x1 + 2x2

−
2x1
x2 + 2x3
x1 + 2x2 + 2x3
−
1 + 9y2
q = 9y2
3; 2, 3

2
1
2
−
2
1
2
;
2 2 
1
2x1 + 2x2 + x3

x1 + 2x2
2x3
2x1 + x2 + 2x3

y = 1

y = 1

2 −

9y2

3 

3 

3 

−

−

−

−



1 + 9y2
q = 9y2


;




;



f. P = 1



2; 2, 2
√2 √3
1
√2
2
0
√2 √3
1
√2x1 + √2x2 + √2x3
+ √3x3
√3x1
x3

2x2

−





;

−

−

−

x1 +
y2
3; 2, 3

;




h. P = 1

√6 



y = 1

√6 

q = 2y2

1 + y2


2 −

Section 9.1

9.1.1

b.





d. 1

2 

a
2b
c

c
−
b 
−

a
b
−
a + b

a + 3b + 2c 


−


b. Let v = a + bx + cx2. Then

9.1.2

CD[T (v)] = MDB(T )CB(v) =

1
2
1 0

−
(cid:20)
Hence

3
2

−

a
b
c 


(cid:21)





=

(cid:20)

2a + b + 3c

a

−

−

2c

(cid:21)

T (v) = (2a + b + 3c)(1, 1) + (

−
= (2a + b + 3c, a + b + c).

a

−

2c)(0, 1)

9.1.3

b. 

0 0
1 0
0 0
0 1







1 0
0 0
0 1
0 0




1 1
1 2
0 1 


d.

1
0
0





9.1.4

b. 





1 2
5 3
4 0
1 1

;







CD[T (a, b)] = 

1
5
4
1

2
3
0
1

b

−

a

b



(cid:20)

(cid:21)

= 








; CD[T (a + bx + cx2)] =

b
2a
−
3a + 2b
4b
a







a
b
c 


= 1
2

(cid:20)

a + b
c
a + b + c

−

(cid:21)

(cid:21)








1
−
1

(cid:21)

1
−
1

8.9.3

b. x1 = 1
√5

hyperbola

(2x

−

y), y1 = 1
√5

(x + 2y); 4x2

1 −

y2
1 = 2;

d. x1 = 1
√5
ellipse

(x + 2y), y1 = 1
√5

(2x

−

y); 6x2

1 + y2

1 = 1;

1 1
1 1

1 1
1 1

d. 1
2

(cid:20)

(cid:20)

1
2

8.9.4

b. Basis

{
(1, 0,

(i, 0, i), (1, 0,

2i), (0, 1, 1

−

, dimension 2

1)
}
, dimension 2

−
i)
}

−

d. Basis

{

8.9.7

b. 3y2

y1 = 1
√2
y3 = 1
√6

y2
3 −

1 + 5y2
2 −
(x2 + x3), y2 = 1
√3
(2x1

x2 + x3)

3√2y1 + 11
3
(x1 + x2

−

√6y3 = 7

√3y2 + 2
3
x3),

−

1
0
0
0
1
0
0
0

0 0
1 1
1 1
0 0
0 0
1 1
1 1
0 0

0
0
0
1
0
0
0
1

f. 













; CD

T

(cid:18)

(cid:20)

a
c

b
d

=

(cid:21)(cid:19)

















a
b
c
d

a
b + c
b + c
d









= 









9.1.5

b. MED(S)MDB(T ) =

1 1
0 1
1 0
1 1

0
1
1
0

−







= MEB(ST )

1
0

1 0
0 1

(cid:20)

0
1

−

(cid:21)

1
2

2
1
1 1

(cid:20)

−
d. MED(S)MDB(T ) =

(cid:21)

1
1
−
0

1 0
−
1
0
0 
1


= MEB(ST )

1 0
−
1
0

1
−
1

(cid:21)
1
−
0





(cid:21)

1
0

2
0

(cid:20)

(cid:20)

9.1.7

b.



=





=

647

CD[T (xn)]

=

· · ·

(cid:3)

(cid:2)


CD[T (1)] CD[T (x)]
an
a2
1
0
0
an
a2
1
1
1
an
a2
1
2
2
...
...
...
an
a2
1
n
n

a0
a1
a2
...
an

· · ·
· · ·
· · ·


.













This matrix has nonzero determinant by
Theorem 3.2.7 (since the ai are distinct), so T is an
isomorphism.

· · ·

9.1.20

d.

[(S + T )R](v) = (S + T )(R(v)) = S[(R(v))] +
T [(R(v))] = SR(v) + T R(v) = [SR + TR](v) holds for
all v in V . Hence (S + T )R = SR + TR.

a, a + c

b, a + b

c);

−

−

9.1.21

b. If w lies in im (S + T ), then w = (S + T )(v)

for some v in V . But then w = S(v) + T (v), so w lies
in im S + im T .

T −

1(a, b, c) = 1
2 (b + c
0
1
1

MDB(T ) =



MBD(T −


1) = 1

2 

;

−
1 1
0 1
1 0 
1

−
1
1

1
1
−
1


1(a, b, c) = (a

d. T −

b) + (b

MDB(T ) =




1) =

MBD(T −

1
0
0





1
0
0

−
1 1
1 1
0 1 
1

1
0

−

;

−

0
1
1 


1
1
1 
−

c)x + cx2;

−

9.1.8



b. MDB(T −
1 0
1 1
1 0
0 1
1 0
0 0
0 1
0 0

1) = [MBD(T )]−

1

−



= 

1 =
1
−
1
0
0

1
0
0
0

0
0
1 0
−
0
1
1
0

.






9.1.22

9.1.24

b. If X

X1, let T lie in X 0

1 . Then T (v) = 0 for
all v in X1, whence T (v) = 0 for all v in X. Thus T is
in X 0 and we have shown that X 0

X 0.

⊆

1 ⊆

b. R is linear means Sv+w = Sv + Sw and
Sav = aSv. These are proved as follows: Sv+w(r) =
r(v + w) = rv + rw = Sv(r) + Sw(r) = (Sv + Sw)(r),
and Sav(r) = r(av) = a(rv) = (aSv)(r) for all r in R.
To show R is one-to-one, let R(v) = 0. This means
Sv = 0 so 0 = Sv(r) = rv for all r. Hence v = 0 (take
r = 1). Finally, to show R is onto, let T lie in L(R, V ).
We must ﬁnd v such that R(v) = T , that is Sv = T . In
fact, v = T (1) works since then
T (r) = T (r
so T = Sv.

1) = rT (1) = rv = Sv(r) holds for all r,

·





1(a, b, c, d)] =







Hence CB[T −
1)CD(a, b, c, d) =
MBD(T −
a
0 0
1
1
b
1 0
1
0
c
1 0
0
0
d
0 1
0
0


b b


−

−









1(a, b, c, d) =




T −





a


−
c

(cid:20)

a
b

b
c

−
−
c
d

.

(cid:21)

= 



c


−
d

, so







9.1.27

9.1.25

b. Given T : R

V , let T (1) = a1b1 +

→

ai in R. For all r in R, we have
(a1S1 +
· · ·
(a1rb1 +
· · ·
that a1S1 +

+ anSn = T .

+ anSn(r) =
+ anSn)(r) = a1S1(r) +
+ anrbn) = rT (1) = T (r). This shows

· · ·

+ anbn,

· · ·

· · ·

b. Write v = v1b1 +
to get Ei(v) = v1Ei(b1) +
deﬁnition of the Ei.

· · ·
· · ·

+ vnbn, v j in R. Apply Ei
+ vnEi(bn) = vi by the

9.1.12 Have CD[T (e j)] = column j of In. Hence
CD[T (e1)] CD[T (e2)]
MDB(T ) =

CD[T (en)]

· · ·

= In.

Section 9.2

(cid:2)
b. If D is the standard basis of Rn+1 and

(cid:3)

9.1.16

1, x, x2, . . . , xn

B =

{

}

, then MDB(T ) =

9.2.1

b. 1

2 



3
−
2
0

2 1
−
0
2
2 
0


B =

←

0
0
1





0 1
1 0
0 0 


9.3.8

648

Selected Exercise Answers

1
0
,
1 


−

−

1
1
0
1
1
,
2 1
1 2 
1

0
, PE
1 


−

−
−

1

1
1

−

0
1
−
1

9.2.4

b. PB

D =

←



1
1
1

PD

B = 1

←

3 

PE

D =

←

1

1
1





b. A = PD
(1, 2,

B =

{
1 = PB

A−

←

9.2.5

9.2.7

b. P =


1 1
0 1
1 0





−

−

0
2
1 


. Hence

B, where
←
1), (2, 3, 0), (1, 0, 2)
}
−
4
−
3
2

6
4
−
3



D =

3
−
2
1 


−

9.2.8

b. B =

3
7

2
5

,

(cid:21)

(cid:20)

(cid:26)(cid:20)

9.2.9

b. cT (x) = x2
d. cT (x) = x3 + x2
f. cT (x) = x4

−

6x

−
8x

−
3

−

(cid:21)(cid:27)

1

→

9.2.12 Deﬁne TA : Rn
Rn by TA(x) = Ax for all x in Rn. If
null A = null B, then ker (TA) = null A = null B = ker (TB)
so, by Exercise 7.3.28, TA = STB for some isomorphism
S : Rn
Rn. If B0 is the standard basis of Rn, we have
A = MB0(TA) = MB0(STB) = MB0(S)MB0(TB) = UB where
U = MB0(S) is invertible by Theorem 9.2.1. Conversely, if
A = UB with U invertible, then Ax = 0 if and only Bx = 0, so
null A = null B.

→

9.2.16

{

, then

b. Showing S(w + v) = S(w) + S(v) means
b1, b2

MB(Tw+v) = MB(Tw) + MB(Tv). If B =
column j of MB(Tw+v) is
CB[(w + v)b j] = CB(wb j + vb j) = CB(wb j) + CB(vb j)
because CB is linear. This is column j of
MB(Tw) + MB(Tv). Similarly MB(Taw) = aMB(Tw); so
S(aw) = aS(w). Finally TwTv = Twv so
S(wv) = MB(TwTv) = MB(Tw)MB(Tv) = S(w)S(v) by
Theorem 9.2.1.

}

Section 9.3

9.3.2

b. T (U)

U, so T [T (U)]

T (U).

⊆

⊆

9.3.3

b. If v is in S(U), write v = S(u), u in U. Then
T (v) = T [S(u)] = (T S)(u) = (ST )(u) = S[T (u)] and
this lies in S(U) because T (u) lies in U (U is
T -invariant).

= 0 in U. Choose a basis B =

9.3.6 Suppose U is T -invariant for every T . If U
u
containing u. Given any v in V , there is (by Theorem 7.1.3) a
linear transformation T : V
T (u2) =
is T -invariant. This shows that V = U.

= T (un) = 0. Then v = T (u) lies in U because U

= 0, choose
of V

V such that T (u) = v,

u, u2, . . . , un

· · ·

→

}

{

3x2 = 3(1
2x2), so both are in U. Hence

2x2) + 3(x + x2)

−

1

}

{

−

−

then

MB(T ) =

b.
2x2) = 3 + 3x
T (1
−
and T (x + x2) =
(1
−
−
U is T -invariant by Example 9.3.3. If
2x2, x + x2, x2
B =
1 1
0 1
0 3 
3 1

−
x
3
−
0 x
0
3 1
−
x
3
−

1
1
3 

3)(x2


cT (x) = det

−
−
−
= (x

3) det

3
3
0

, so

x


(x

−

=

−

−





(cid:20)

(cid:21)

x

3x + 3)

−

9.3.9

b. Suppose Ru is TA-invariant where u

= 0. Then

TA(u) = ru for some r in R, so (rI
cosθ)2 + sin2θ
det (rI
0 < θ < π. Hence u = 0, a contradiction.

A) = (r

−

−

−
= 0 because

A)u = 0. But

9.3.10

b. U = span

W = span
four vectors form a basis of R4. Use Example 9.3.9.

(1, 0, 1, 0), (0, 1, 0,

and
(1, 1, 0, 0), (0, 0, 1, 1)
}
, and these

1)
}

−

{

{

d. U = span

W = span

1
0

(cid:26)(cid:20)

1
0

(cid:21)
1
0
1 0

,

(cid:20)
,

0 0
1 1

(cid:21)(cid:27)

0 1
0 1

and

and these

(cid:21)
vectors are a basis of M22. Use Example 9.3.9.

(cid:21)(cid:27)

(cid:26)(cid:20)

−

(cid:20)

9.3.14 The fact that U and W are subspaces is easily veriﬁed
V , then A = AE = 0;
using the subspace test. If A lies in U
V = 0. To show that M22 = U + V , choose any A
that is, U
in M22. Then A = AE + (A
AE), and AE lies in U [because
−
(AE)E = AE 2 = AE], and A
−
AE 2 = 0].
(A

AE lies in W [because

AE)E = AE

∩

∩

−

−

9.3.17

b. By (a) it remains to show U + W = V ; we

show that dim (U + W ) = n and invoke
Theorem 6.4.2. But U + W = U
U

W = 0, so dim (U + W ) = dim U + dim W = n.

W because

⊕

∩

9.3.18

b. First, ker (TA) is TA-invariant. Let U = Rp be

TA-invariant. Then TA(p) is in U, say TA(p) = λp.
Hence Ap = λp so λ is an eigenvalue of A. This
means that λ = 0 by (a), so p is in ker (TA). Thus
U
= 2 because TA
= 0,
so dim [ ker (TA)] = 1 = dim (U). Hence U = ker (TA).

ker (TA). But dim [ ker (TA)]

⊆

6
6
6
6
6
6
9.3.20 Let B1 be a basis of U and extend it to a basis B of V .

Then MB(T ) =

cT (x) = det [xI
cT 1(x)q(x).

(cid:20)
−

MB1(T ) Y
Z

0

MB(T )] = det [xI

(cid:21)

, so

MB1(T )] det [xI

Z] =

−

−

P4:

r f , g
h

i

649

g(i)h(i)

f (i)h(i) + ∑
i
.
i
g(i)

g, h
h
i
(r f )(i)

+

= ∑
i
f , h

=
h
= ∑
i
= ∑
i
= r∑
i
f , g

= r

h

r f (i)

f (i)

·

g(i)

g(i)

·

·

P5: If f

= 0, then

f , f

h

i

i

= ∑
i

f (i)2 > 0 because some

f (i)

= 0.

10.1.12

b.
= 5v2

= 3v2

v, v
i
h
v, v
i
h

d.

6v1v2 + 2v2
1 −
1 + 8v1v2 + 6v2

2 = 1
2 = 1

3v2)2 + v2
5 [(5v1
2]
3 [(3v1 + 4v2)2 + 2v2
2]

−

10.1.13

b.

1
2

−

2
1

−

(cid:21)

(cid:20)
1 0
0 2
2 0



d.

−

2
−
0
5 

10.1.14 By the condition,
x + y, x + y
i
x, y. Let ei denote column i of I. If A = [ai j], then
ai j = eT

= 0 for all i and j.

x, y
i
h

= 1
2 h

ei, e j

i Ae j =



{

}

= 0 for all

10.1.16

b.

15

−

=

v, u
v + w, u
=
h
i
h
v, rw
=
i
h
0 + 0, v
=
h

10.1.20 1. Using P2:
u, v + w
h
i
2. Using P2 and P4:
3. Using P3:
0, v
i
h
4. Assume that
v, v
i
h
v = 0. Conversely, if v = 0, then
theorem.

0, v
i
h
= 0. The rest is P2.

= 0. If v

u, v
w, u
=
+
+
h
i
i
h
i
w, v
rw, v
= r
= r
i
h
h
0, v
, so
+
=
i
h
i

i
0, v
i
h

u, w
h
v, w
h

.
i
.
i

= 0 this contradicts P5, so

v, v
i
h

= 0 by Part 3 of this

10.1.22

d.

k

10.1.26

b. 15
k
2 =

2

−

17

u, v
h

u
k
u + v, u + v
i
h

4

k
u
k

i −
=

k

2

v
k
2 + 2

u + v
k

u, v
h

i

+

2

v
k

k

b.

(1, 1, 0), (0, 2, 1)
}

{

10.1.28
v, vi
h
v = w by Exercise 10.1.27.

w, vi

v
h

=

−

i

w, vi

i − h

i

= 0 for each i, so

10.1.29

b. If u = (cosθ, sinθ) in R2 (with the dot
= 1. Use (a) with v = (x, y).

product) then

u
k

k

Section 10.2

{

{

9.3.22

B =

b. T 2[p(x)] = p[
1, x2; x, x3

x)] = p(x), so T 2 = 1;

(
−

−

d. T 2(a, b, c) = T (
so T 2 = 1; B =

a + 2b + c, b + c,
−
(1, 1, 0); (1, 0, 0), (0,

−

c) = (a, b, c),
1, 2)
}

−

}

{

9.3.23

b. Use the Hint and Exercise 9.3.2.

9.3.25

b. T 2(a, b, c) = T (a + 2b, 0, 4b + c) =
(a + 2b, 0, 4b + c) = T (a, b, c), so T 2 = T ;
B =

(1, 0, 0), (0, 0, 1); (2,

1, 4)
}

−

9.3.29

b. Tf , z[Tf , z(v)] = Tf , z[ f (v)z] = f [ f (v)z]z =

}

f [z]z

f (v)
= f (v) f (z)z. This equals
{
Tf , z(v) = f (v)z for all v if and only if
f (v) f (z) = f (v) for all v. Since f
and only if f (z) = 1.

= 0, this holds if

9.3.30

b. If A =

p1 p2

pn

where Upi = λpi

for each i, then UA = λA. Conversely, UA = λA
means that Up = λp for every column p of A.

(cid:2)

(cid:3)

· · ·

Section 10.1

10.1.1

b. P5 fails.

d. P5 fails.

f. P5 fails.

10.1.2 Axioms P1–P5 hold in U because they hold in V .

10.1.3

d.

1
√17

b.

1
√π f

3
1

−

(cid:21)

(cid:20)

10.1.4

b. √3

d. √3π

10.1.8 P1 and P2 are clear since f (i) and g(i) are real
numbers.

P3:

h

f + g, h

i

( f + g)(i)

h(i)

·

= ∑
i
= ∑
i
= ∑
i

( f (i) + g(i))

h(i)

·

[ f (i)h(i) + g(i)h(i)]

6
6
6
6
650

Selected Exercise Answers

1
1
1 


+ (7c

7a)

−

−

1
0
1 






1

6
1 

+





a

d

−
2

(cid:21)

(cid:21)

(cid:1)

c

(cid:0)

b

−
2

+

(cid:0)

1
0

(cid:20)

0
1
−
0 1
1 0

+

(cid:21)

(cid:21)

(cid:20)

−

(cid:1)
5, 1), (3, 0,

(1, 1, 1), (1,

−

2)
}

−

10.2.1

b.

(6a + 2b + 6c)



1
14 



+ (a

−

2b + c)



−

a+d
2

b+c
2

(cid:20)

(cid:1)


1 0
0 1
1
0
0
1

d.

(cid:0)

(cid:0)

10.2.2

10.2.3

1 1
0 1

(cid:26)(cid:20)

(cid:20)

{

(cid:1)
b.

b.

,

(cid:21)

(cid:20)

1
3

2
−
1

,

(cid:21)

(cid:20)

−

1
2

2
1

−

1
0

,

(cid:21)

(cid:20)

0
1

−

(cid:21)(cid:27)

10.2.4

b.

1, x

{

−

1, x2

2x + 2

3 }

−

10.2.6
1

span

{

b. U ⊥ =
1 0 0

−

,

0

0 1

0

,

0

0 0

1

dim U ⊥ = 3, dim U = 1

(cid:2)
d. U ⊥ = span
dim U = 1

(cid:3)

(cid:2)
3x, 1

2

{

−

2x2

}

−

(cid:3)

(cid:2)
, dim U ⊥ = 2,

f. U ⊥ = span

dim U = 3

(cid:26)(cid:20)

−

1
1

1
0

−

, dim U ⊥ = 1,

(cid:21)(cid:27)

10.2.7

b.

U = span

(cid:26)(cid:20)

1
0

projU A =

(cid:20)

,

0
1
(cid:21)
3 0
2 1

(cid:20)

(cid:21)

1
1

1
1

−

,

(cid:21)

(cid:20)

−

0 1
1 0

;
(cid:21)(cid:27)

10.2.8

b. U = span

1, 5

{

3x2

}

−

; projU x = 3

13 (1 + 2x2)

10.2.9

1

1, 2x

b. B =
{
−
1
0 (2x
because
−
projU (x2 + 1) = x + 5
R
x2 + 1 = (x + 5
6 ) + (x2

}

6 , so

x + 1

6 ).

−

1)dx = 0. Using it, we get

10.2.11

b. This follows from
=

v

2

v + w, v
h

−

w
i

k

k

− k

2.

w
k

10.2.14

b. U ⊥ ⊆ {
U. Conversely, if
u = r1u1 +
v, u
= r1
i
h

· · ·
v, u1
h

u1, . . . , um
v, ui
h

i

}⊥ because each ui is in

= 0 for each i, and

+ rmum is any vector in U, then

+

i

· · ·

+ rm

v, um
h

i

= 0.

10.2.19

b. The plane is U =

n

w, w

span
because both spaces have dimension 2 (using (a)).
o

−

×

n

w
n
2 n
·
n
k

k

x

n = 0

x
|
U. This is equality

so

}

·

{
⊆

10.2.20

CE (bi)

b. CE (bi) is column i of P. Since
bi, b j
h

CE (b j) =

i

·

by (a), the result follows.

10.2.23

f1, f2, . . . , fm

, then

}

{

projU v =

2 fi by Theorem 10.2.7. Hence
i

h

b. If U = span
m
∑
i=1
2 =

v1, fi
fi
k
k
m
∑
i=1

k

k

h

v1, fi
fi

k

projU v

k
Now use (a).

2 fi by Pythagoras’ theorem.
i

Section 10.3

10.3.1

B =

b.
1
0

(cid:26)(cid:20)

0
0

,

(cid:21)

(cid:20)
−

MB(T ) = 

,

(cid:21)

0
0 1
1
0 0
(cid:20)
0
1
1 0
1 0 1
0
−
2 0
0
1
0 2
1
0

0
0

,

(cid:21)

(cid:20)

0 0
0 1

;
(cid:21)(cid:27)



,

}
(cid:3)

10.3.4








v, rT (w)
= r
=
i
h
rT (v), w
(rT )(v), w
=
i
i
h
i
h
1(v) = v1 and T −
d. Given v and w, write T −

b.
T (v), w
h

v, (rT )w
h
=

r

i

v, T (w)
i
h

=

1(w) = w1.

=

v1, T (w1)
i
h

i

=

T (v1), w1
h

i

=

1(v), w
T −
Then
h
1(w)
v, T −
.
i
h

10.3.5

b. If B0 =

, then



−

−

MB0(T ) =

(1, 0, 0), (0, 1, 0), (0, 0, 1)
{
}
7
1
0

1 0
7 0
0 2 
0
1

0
1
,
1 
0 
Hence an orthonormal basis of eigenvectors of T is



of eigenvectors

1
√2 

1
√2 




−











,

has an orthonormal basis

.





1
1
0 

.

d. If B0 =

n

o
1 0
−
3
0
0
1
has an orthonormal basis of eigenvectors

, then MB0(T ) =

1, x, x2



{

}



1
0
1 


−

,

1
√2 

0
1
0 


(1 + x2),

1
√2

,

1
0
1 

(1

1
√2





Hence an orthonormal basis of eigenvectors of T is



x2)
x,






−

.

1
0
1 


1
√2 

.

n

10.3.7

b. MB(T ) =

−

A 0
0 A
A

o

, so

(cid:21)
0

(cid:20)
xI2
−
0

10.2.18

b. projU (

5, 4,
−
1, 0, 2) = 1
38 (

projU (

−

−
−

5, 4,

3) = (
17, 24, 73)

−

3);

−

cT (x) = det

(cid:20)

= [cA(x)]2.

xI2

A

−

(cid:21)

is an orthogonal basis of U

1
√2

(1, 1, 0),


1
√2

(1,

−

1, 0), (0, 0, 1)

⇒

f1, . . . , fn

10.3.12 (1)
(2). If B =
basis of V , then MB(T ) = [ai j] where ai j =
Theorem 10.3.2. If (1) holds, then
T (f j), fi
f j, T (fi)
a ji =
=
=
i
h
i
Hence [MV (T )]T =
MV (T ), proving (2).

−h

{

}

fi, T (f j)
i

is an orthonormal
fi, T (f j)
by
i
h

11.1.1

P =



1
1
1

b. cA(x) = (x + 1)3;
0
0
1
0
;
3 1 
0
1

1
0
0
0


1AP =

−

−

−



=

ai j.

−

P−

10.3.14

c. The coefﬁcients in the deﬁnition of

−h
−

n
∑
i=1h

651

1
0
1 


−

−

P =


d. cA(x) = (x
−
1 0
4 1
4 2
1
0
0


1AP =

P−







f. cA(x) = (x + 1)2(x
1
0
0
1

P = 





P−

1AP = 

;

1
1
0

1)2(x + 2);
1
−
1
1 
0

0
2 
−

1)2;
−
1
1
−
0
1


1 0

1 1
0 1
0 0

1 5
0 2
1 2
0 1
1
0
0
0

−

−



;





0
0
2
−
1







11.1.4 If B is any ordered basis of V , write A = MB(T ). Then
cT (x) = cA(x) = a0 + a1x +
Since MB is linear and MB(T k) = MB(T )k, we have
MB[cT (T )] = MV [a0 + a1T +
a0I + a1A +
theorem. Hence cT (T ) = 0 because MB is one-to-one.

+ anT n] =
+ anAn = cA(A) = 0 by the Cayley-Hamilton

+ anxn for scalars ai in R.

· · ·

· · ·

· · ·

Section 11.2

11.2.2




=

a
0
0







1 0
a 0
0 b 
0 1
0

1
0 0
0 
1 0




0 1
0 0
1 0
a
0
0





0
1
0 
1 0

a 1
0 a 


Appendix A

A.1

b. x = 3

d. x =

1

±

T ′(f j) =

fi are the entries in the jth
f j, T (fi)
i

column CB[T ′(f j)] of MB(T ′). Hence
MB(T ′) = [
MB(T ) by Theorem 10.3.2.

f j, T (f j)
], and this is the transpose of
i
h

Section 10.4

10.4.2

b. Rotation through π

d. Reﬂection in the line y =
f. Rotation through π
4

x

−

10.4.3

b. cT (x) = (x
−
1 √3 √3

e =

1)(x2 + 3
T

2 x + 1). If

, then T is a rotation about Re.

d. cT (x) = (x + 1)(x + 1)2. Rotation (of π) about the x

(cid:3)

(cid:2)
axis.

f. cT (x) = (x + 1)(x2

π
4 )
−
about the y axis followed by a reﬂection in the x
plane.

√2x + 1). Rotation (of

−

−

z

10.4.6 If
v

v
k
k
= 0, then
a
|

k

(aT )(v)
=
k
= 1 so a =
±

|

=
|
1.

a

T (v)
k

|k

=

a

|

|k

v
k

for some

10.4.12

b. Assume that S = Su

T , u

V , T an isometry

◦

∈

of V . Since T is onto (by Theorem 10.4.2), let
u = T (w) where w
(T
◦
(ST (w) ◦

V . Then for any v
V , we have
Sw) = T (w + v) = T (w) + T (w) = ST (w)(T (v)) =
T .

T )(v), and it follows that T

Sw = ST (w) ◦

∈

∈

◦

Section 10.5

10.5.1

b. π

2 −

d. π

sin x

4 +
2
π

−

−
cos x + cos 3x
(cid:2)
h

10.5.2

b.

2
π −

8
π

h

32 + cos 5x
52
i
4 + sin 5x
sin 4x
5

4
π

cos x + cos3x
h
2 + sin 3x
sin 2x
3 −
32 + cos 5x
52

i
1 + cos4x
42

−

cos 2x
22

−

1 + cos 6x
62

1

−

i

10.5.4
= 1
2

cos kx cos lx dx
l)x]

sin[(k+l)x]

R
k+l −

sin[(k
k

−
l
−

h

= 0 provided that k

= l.

π

0
i

Section 11.1

(cid:3)

A.2

b. 10 + i
26 + 23
26 i
11i

d. 11

f. 2

−

h. 8

A.3

6i

−
b. 11

d.

(2

±
f. 1 + i

−

5 + 3
5 i
i)

6
6
b. 1

2 ±

√3
2 i

d. 2, 1
2

B.1

b. If m = 2p and n = 2q + 1 where p and q are

integers, then m + n = 2(p + q) + 1 is odd. The
converse is false: m = 1 and n = 2 is a
counterexample.

652

Selected Exercise Answers

A.4

A.5

2, 1

√3i

b.

−
2√2,

±
2√i

d.

±

±

−

A.6

b. x2

4x + 13; 2 + 3i

d. x2

−

6x + 25; 3 + 4i

A.8 x4

−

10x3 + 42x2

82x + 65

−

b. (

2)2 + 2i

−
2 + i)2 + 3(1

(4

2i) = 0; 2

i

−
i)(

−
1 + 2i)

−
5i = 0;

−

−

−

A.10

d. (

−

A.11

b.

i, 1 + i

−
i, 1

2i

−

d. 2

−

A.12

b. Circle, centre at 1, radius 2

d. Imaginary axis

f. Line y = mx

A.18

b. 4e−

πi/2

d. 8e2πi/3

f. 6√2e3πi/4

A.19

b. 1

2 + √3
2 i

d. 1

−
f. √3

i

3i

−

A.20

d.

f.

−

−

1

32 + √3
32 i

b.

32i

−

216(1 + i)

A.23

b.

d.

2i,

±

±

√2
2 (√3 + i),

√2
2 (

±
(√3 + i),

±
(√3

−
i)

±

−

d. x2

−

5x + 6 = (x

2)(x

3) so, if this is zero, then

x = 2 or x = 3. The converse is true: each of 2 and 3
satisﬁes x2

5x + 6 = 0.

−

−

−

1 + 2i

−

B.2

b. This implication is true. If n = 2t + 1 where t is
an integer, then n2 = 4t2 + 4t + 1 = 4t(t + 1) + 1. Now
t is either even or odd, say t = 2m or t = 2m + 1. If
t = 2m, then n2 = 8m(2m + 1) + 1; if t = 2m + 1, then
n2 = 8(2m + 1)(m + 1) + 1. Either way, n2 has the
form n2 = 8k + 1 for some integer k.

B.3

b. Assume that the statement “one of m and n is

≤

12, so n + m

greater than 12” is false. Then both n
m
that n + m = 25. This proves the implication. The
converse is false: n = 13 and m = 13 is a
counterexample.

24, contradicting the hypothesis

12 and

≤

≤

d. Assume that the statement “m is even or n is even” is
false. Then both m and n are odd, so mn is odd,
contradicting the hypothesis. The converse is true: If
m or n is even, then mn is even.

B.4

b. If x is irrational and y is rational, assume that

x + y is rational. Then x = (x + y)
of two rationals, and so is rational, contrary to the
hypothesis.

y is the difference

−

B.5

b. n = 10 is a counterexample because 103 = 1000
2n is false if

while 210 = 1024, so the statement n3
n = 10. Note that n3

≥
2n does hold for 2

9.

n

≤

≤

≥

Appendix C

C.6

n
n+1 +

(n+1)(n+2) = n(n+2)+1

(n+1)(n+2) = (n+1)2

(n+1)(n+2) = n+1
n+2

1

1 + √3i)

C.14
2√n

−

1 + 1

√n+1 = 2√n2+n+1

√n+1 −

1 < 2(n+1)

√n+1 −

1 = 2√n + 1

1

−

A.26

b. The argument in (a) applies using β = 2π
zn
z = 0.

+ zn

−

1 + z +

n . Then

· · ·

1 = 1
−
1
−

Appendix B

C.18 If n3
(n + 1)3

−

n = 3k, then

−
(n + 1) = 3k + 3n2 + 3n = 3(k + n2 + n)

C.20 Bn = (n + 1)!

1

−

C.22

b. Verify each of S1, S2, . . . , S8.

(i, j)-entry, 35
3-dimensional space, 58
A-invariance, 174
B-matrix, 505
T -invariant, 515
n matrix
m

×
canonical forms, 581
deﬁned, 35
difference, 38
elementary row operation,

95

main diagonal, 43
matrix
495

transformation,

negative, 38
subspaces, 262
transpose, 41
zero matrix, 38

n-parity-check code, 472
n-tuples, 261, 288, 328
n-vectors, 47
n-words, 469
nth roots of unity, 595
r-ball, 470
x-axis, 207
x-compression, 60
x-expansion, 60
x-shear, 61
y-axis, 207
y-compression, 60
y-expansion, 60
z-axis, 207
Disquisitiones Arithmeticae

(Gauss), 11

How to Read and Do Proofs

(Solow), 601

Introduction to Abstract Al-
gebra (Nicholson), 467
to Abstract
(Lucas),

Introduction

Mathematics
601

Introduction to the Theory
Error-Correcting

of
Codes (Pless), 476
Mécanique Analytique (La-

grange), 243

Calcolo

Geometrico

(Peano), 327
Elements (Euclid), 602

Index

Interpolation and Approxi-
mation (Davis), 545
Introduction to Matrix Com-
(Stewart),

putations
437

angles

angle between two vec-

tors, 226, 538

ordered basis, 495, 497
orthogonal basis,

285,

409, 541

radian measure, 60, 109,

orthonormal basis, 542,

591

550

Raum-Zeit-Materie (“Space-

standard position,

109,

standard basis, 104, 270,

Time-Matter”)(Weyl),
327

The Algebraic Eigenvalue
Problem (Wilkinson),
437

“Linear Programming and
Extensions” (Wu and
Coppins), 492
“if and only if”, 36
“mixed” cancellation, 84
3-dimensional space, 207

absolute value

complex number,

588,

589

notation, 109
real number, 208
symmetric matrices, 305
triangle inequality, 535
abstract vector space, 327
action

same action, 59, 331, 374
transformations, 59, 495,

497
addition

closed under, 328
closed under addition, 47,

261

complex number, 587
matrix addition, 37
pointwise addition, 330
transformations

preserving addition, 103
vector addition, 328, 591

adjacency matrix, 74
adjugate, 81, 158
adjugate formula, 160
adult survival rate, 169
aerodynamics, 491
algebraic method, 4, 9
algebraic multiplicity, 302
algebraic sum, 29
altitude, 259
analytic geometry, 47

591

unit circle, 109, 591

approximation theorem, 544,

569
Archimedes, 11
area

linear transformations of,

252

parallelogram

equal to zero, 244

argument, 591
arrows, 207
associated

homogeneous

system, 53
associative law, 37, 70
attractor, 185
augmented matrix, 3, 4, 13,

14

auxiliary theorem, 95
axiomatic method, 605
axioms, 601
axis, 207, 563

back substitution, 14, 117
balanced reaction, 31
ball, 470
Banach, Stephan, 327
bases, 274
basic eigenvectors, 173
basic solutions, 24, 447
basis

choice of basis, 495, 500
dual basis, 504
enlarging subset to, 276
geometric

problem of

ﬁnding, 508, 509, 573

independent set, 409
isomorphisms, 389
linear operators

and choice of basis, 510
matrix of T corresponding
to the ordered bases B
and D, 497
of subspace, 274

653

274, 276, 347, 454

vector spaces, 347
Bessel’s inequality, 547
best approximation, 308
best approximation theorem,

309

bilinear form, 489
binary codes, 469
Binet formula, 194
binomial coefﬁcients, 364
binomial theorem, 364
block matrix, 152
block multiplication, 73
block triangular form, 573
block triangular matrix, 516
block triangulation theorem,

574

blocks, 73
boundary condition,

197,

367

cancellation, 331, 332
cancellation laws, 84
canonical forms

×

n matrix, 581

m
block triangular form, 573
form,
Jordan canonical

581

Cartesian coordinates, 207
cartesian geometry, 207
category, 393
Cauchy inequality, 282, 323
Cauchy, Augustin Louis,

156, 305

Cauchy-Schwarz inequality,

241

Cayley, Arthur, 35, 143
Cayley-Hamilton

theorem,

578
centred, 320
centroid, 223
change matrix, 505
channel, 469
characteristic polynomial

654

INDEX

block triangular matrix,

linear combination, 264,

517

complex matrix, 458
diagonalizable

matrix,

298

eigenvalues, 171
root of, 171, 366
similarity invariant, 510,

511

square matrix, 171, 462

chemical reaction, 31
choice of basis, 495, 500
Cholesky algorithm, 429
Cholesky factorization, 429
Cholesky, Andre-Louis, 429
circuit rule, 29
classical adjoint, 159
closed economy, 128
closed under addition, 47,

261, 328

closed under scalar multipli-
cation, 47, 261, 328

code

(n, k)-code, 472
n-code, 469
binary codes, 469
decoding, 476
deﬁned, 469
error-correcting
465, 469

codes,

Hamming (7,4)-code, 477
linear codes, 472
matrix generators, 473
minimum distance, 470
nearest neighbour decod-

ing, 470

orthogonal codes, 476
parity-check code, 472
parity-check

matrices,

475

perfect, 471
syndrome decoding, 476
use of, 465

269, 339

of the polynomial, 317,

330, 613
of vectors, 339
sample correlation coefﬁ-

cient, 322

cofactor, 144
cofactor expansion, 145, 202
cofactor expansion theorem,

146, 202
cofactor matrix, 158
column matrix, 35, 47
column space, 288, 439
column vectors, 170
columns

(i, j)-entry, 35
as notations for ordered n-

tuples, 273
convention, 36
elementary column opera-

tions, 147

equal, 21
leading column, 119
shape of matrix, 35
Smith normal form, 98
transpose, 41

commutative law, 37
commute, 69, 71
companion matrix, 155
compatibility rule, 67
compatible

blocks, 73
for multiplication, 67

complement, 520
completely

diagonalized,

486

complex conjugation, 305
complex distance formula,

590

complex eigenvalues, 304,

438
complex matrix

code words, 469, 470, 473,

Cayley-Hamilton

theo-

478

coding theory, 469
coefﬁcient matrix, 4, 164
coefﬁcients

binomial coefﬁcients, 364
constant coefﬁcient, 613
Fourier coefﬁcients, 286,

540, 569, 570
in linear equation, 1
leading coefﬁcient, 164,

330, 613

rem, 462

characteristic polynomial,

458

conjugate, 454
conjugate transpose, 456
deﬁned, 454
eigenvalues, 458
eigenvector, 458
hermitian matrix, 457
normal, 461
Schur’s theorem, 460, 461

skew-hermitian, 464
spectral theorem, 461
standard inner product,

455
unitarily
460
unitary
460

diagonalizable,

diagonalization,

unitary matrix, 459
upper
460

triangular matrix,

complex number

absolute value, 399, 588,

589

addition, 587
advantage

of working

with, 458
conjugate, 589
equal, 587
extension of concepts to,

454
form, 587
fundamental

theorem of

algebra, 587
imaginary axis, 590
imaginary part, 587
imaginary unit, 587
in complex plane, 590
inverse, 588
modulus, 589
multiplication, 587
parallelogram law, 591
polar form, 591
product, 590
pure imaginary numbers,

587

real axis, 590
real part, 587
regular
590

representation,

root of the quadratic, 599
roots of unity, 594
scalars, 467
subtraction, 587
sum, 591
triangle inequality, 589

complex plane, 590
complex subspace, 463
composite, 64, 107, 392
composition, 64, 392, 499
computer graphics, 255
conclusion, 601
congruence, 485
congruent matrices, 485
conic graph, 21, 483

conjugate, 454, 589
conjugate matrix, 305
conjugate transpose, 456
consistent system, 1, 16
constant, 351, 613
constant matrix, 4
constant sequences, 401
constant term, 1
constrained
489

optimization,

continuous functions, 530
contraction, 63
contradiction, proof by, 603
convergence, 436
converges, 138
converse, 604
coordinate

isomorphism,

391

coordinate

transformation,

496

coordinate vectors, 213, 234,

253, 266, 496
coordinates, 207, 495
correlation, 320
correlation coefﬁcient

computation

with dot product, 322
Pearson correlation coefﬁ-

cient, 322

sample correlation coefﬁ-

cient, 322

correlation formula, 324
coset, 476
cosine, 110, 225, 547, 591
counterexample, 9, 604
covariance, 493
covariance matrix, 493
Cramer’s Rule, 156
Cramer, Gabriel, 162
cross product

and dot product, 234, 243
coordinate vectors, 234
coordinate-free
tion, 245

descrip-

deﬁned, 234, 242
determinant

form, 234,

242

Lagrange Identity, 243
properties of, 243
right-hand rule, 245
shortest distance between
nonparallel lines, 236

cryptography, 465

data scaling, 324
Davis, Philip J., 545

general differential sys-

tems, 197

division algorithm, 465, 615
dominant eigenvalue, 182,

linear dynamical systems,

435

De Moivre’s Theorem, 594
De Moivre, Abraham, 594
decoding, 476
deﬁned, 67
deﬁning transformation, 59
degree of the polynomial,

330, 613
demand matrix, 130
dependent, 271, 343, 356
dependent lemma, 356
derivative, 399
Descartes, René, 207
determinants
3, 144
n, 145

×
×

3
n
adjugate, 81, 158
and eigenvalues, 143
and inverses, 81, 143
block matrix, 152
coefﬁcient matrix, 164
cofactor expansion, 144,

145, 202

Cramer’s Rule, 156
cross product, 234, 242
deﬁned, 81, 143, 150, 202,

510

180

matrix, 171
multivariate analysis, 493
diagonaliza-
orthogonal

diagonalization,

tion, 418, 548
quadratic form, 479
test, 300
unitary
460
diagonalization
179
diagonalization
481

algorithm,

theorem,

diagonalizing matrix, 176
difference

×

m
n matrices, 38
of two vectors, 211, 332
differentiable function, 196,
338, 365, 398, 399
differential equation of order

n, 365, 398

differential equations, 196,

365, 398

inductive method of deter-

differential system, 197

mination, 144

initial development of,

254

notation, 143
polynomial

interpolation,

163

product of matrices (prod-
uct theorem), 156, 166

similarity invariant, 510
square matrices, 143, 157
theory of determinants, 35
triangular matrix, 152
Vandermonde
nant, 165

determi-

Vandermonde matrix, 151

deviation, 320
diagonal matrices, 45, 79,
170, 176, 298, 508
diagonalizable linear opera-

tor, 549

diagonalizable matrix, 176,

298, 508

diagonalization

deﬁned, 196
exponential function, 196
general differential sys-

tems, 197

general solution, 198
simplest differential sys-

tem, 196
differentiation, 373
digits, 469
dilation, 63
dimension, 274, 347, 398
dimension theorem,

371,

382, 391
direct proof, 601
direct sum, 358, 520, 528
directed graphs, 74
direction, 209
direction cosines, 241, 547
direction vector, 216
discriminant, 483, 596
distance, 283, 533
distance function, 393
distance preserving,

248,

completely diagonalized,

555

486

distance preserving isome-

described, 176, 298
eigenvalues, 143, 171, 298
example, 169

tries, 555

distribution, 492
distributive laws, 41, 71

dominant eigenvector, 435
dot product

and cross product, 234,

243

and matrix multiplication,

67

as inner product, 529
basic properties, 224
correlation coefﬁcients
computation of, 322

deﬁned, 224
dot product rule, 55, 67
in set of all ordered n-
tuples (Rn), 280, 455
inner product space, 532
length, 533
of two ordered n-tuples,

54

of two vectors, 224
variances

computation of, 323
doubly stochastic matrix,

141
dual, 504
dual basis, 504

economic models

input-output, 127
economic system, 127
edges, 74
eigenspace, 263, 301, 518,

574
eigenvalues

and determinants, 143
and diagonalizable matri-

ces, 177

and eigenspace, 263, 301
and Google PageRank,

187

complex eigenvalues, 175,

304, 438

complex matrix, 458
computation of, 435
deﬁned, 171, 298
dominant eigenvalue, 182,

435

iterative methods, 435
linear operator, 518
multiple eigenvalues, 300
multiplicity, 178, 302
power method, 435
real eigenvalues, 305

INDEX

655

root of the characteristic

polynomial, 171

solving for, 171
spectrum of the matrix,

420

symmetric linear operator
on ﬁnite dimensional
inner product
space,
551
eigenvector

basic eigenvectors, 173
complex matrix, 458
deﬁned, 171, 298
dominant eigenvector, 435
fractions, 172
linear combination, 436
linear operator, 518
nonzero linear combina-

tion, 172

nonzero multiple, 172
nonzero vectors, 263, 301
orthogonal basis, 418
orthogonal
421, 458

eigenvectors,

orthonormal basis, 553
principal axes, 423
electrical networks, 29
elementary matrix
and inverses, 95
deﬁned, 94
LU-factorization, 118
operating
to, 94

corresponding

permutation matrix, 122
self-inverse, 95
Smith normal form, 98
uniqueness

of

reduced

row-echelon form, 99

elementary operations, 5
elementary row operations

corresponding, 94
inverses, 7, 95
matrices, 5
reversed, 7
scalar product, 21
sum, 21

elements of the set, 261
ellipse, 483
entries of the matrix, 35
equal

columns, 21
complex number, 587
fractions, 210
functions, 331
linear

transformations,

656

INDEX

374

Fourier

coefﬁcients,

286,

matrices, 36
polynomials, 330, 613
sequences, 401
sets, 261
transformation, 59, 374

equal modulo, 466
equilibrium, 129
equilibrium condition, 129
equilibrium price structures,

129

equivalence relation, 296
equivalent

matrices, 102
statements, 86
systems of
tions, 4

linear equa-

error, 312, 476
error-correcting codes, 465
Euclid, 602, 605
euclidean n-space, 529
euclidean algorithm, 468
euclidean geometry, 280
euclidean inner product, 529
Euler’s formula, 592
Euler, Leonhard, 592
evaluation, 180, 307, 338,

372, 377, 406
even function, 570
even parity, 472
even polynomial, 352
exact formula, 182
expansion theorem,

540, 569, 570
Fourier expansion, 286
Fourier series, 571
Fourier, J.B.J., 540
fractions

eigenvectors, 172
equal fractions, 210
ﬁeld, 467
probabilities, 133

free variables, 13
function

composition, 392
continuous functions, 530
deﬁned, 330
derivative, 399
differentiable

function,
196, 338, 365, 398, 399

equal, 331
even function, 570
exponential function, 196,

399

objective function, 489,

491

odd function, 570
of a complex variable, 282
pointwise addition, 331
real-valued, 330
scalar multiplication, 331
identities, 90,

fundamental
394

fundamental subspaces, 445
fundamental theorem, 274,

285,

540, 549
expectation, 492
exponential
399

function, 196,

factor, 302, 615
factor theorem, 361, 615
feasible region, 489
Fibonacci sequence, 193
ﬁeld, 328, 467
ﬁeld of integers modulo, 467
spaces,
ﬁnite dimensional

353
ﬁnite ﬁelds, 467
ﬁnite sets, 467
ﬁxed axis, 566
ﬁxed hyperplane, 566
ﬁxed line, 560
ﬁxed plane, 563
ﬁxed vectors, 567
formal proofs, 602
forward substitution, 117
Fourier approximation, 569

346

fundamental theorem of al-
gebra, 175, 304, 587

Galois ﬁeld, 469
Galton, Francis, 322
Gauss, Carl Friedrich, 11,

175

gaussian algorithm, 11, 301,

467

gaussian elimination

deﬁned, 14
example, 14
LU-factorization, 117
normal equations, 309
scalar multiple, 39
systems of

linear equa-

tions and, 9

general differential systems,

197

general solution, 2, 14, 198
general theory of relativity,

327

generalized eigenspace, 574
310,
generalized inverse,

319
generator, 474
geometric vectors
deﬁned, 210
described, 210
difference, 212
intrinsic descriptions, 210
midpoint, 214
parallelogram law, 210
Pythagoras’ theorem, 219
scalar multiple law, 213,

215

scalar multiplication, 212
scalar product, 212
sum, 211
tip-to-tail rule, 211
unit vector, 213
vector subtraction, 212

geometry, 35
Google PageRank, 187
Gram matrix, 440
Gram, Jörgen Pederson, 411
Gram-Schmidt orthogonal-
ization algorithm, 286,
411, 420, 421, 432,
512, 541

graphs

attractor, 185
conic, 21
directed graphs, 74
ellipse, 483
hyperbola, 483
linear dynamical system,

185

saddle point, 186
trajectory, 185

Grassmann, Hermann, 327
group theory, 35
groups, 558

Hamming (7,4)-code, 477
Hamming bound, 471
Hamming distance, 469
Hamming weight, 469
Hamming, Richard, 469
heat conduction in solids,

568

Hermite, Charles, 457
hermitian matrix, 457
higher-dimensional geome-

try, 35

Hilbert spaces, 411
Hilbert, David, 411
hit, 380

homogeneous
257

coordinates,

homogeneous equations

associated homogeneous

system, 53

basic solutions, 24
deﬁned, 20
general solution, 22
linear combinations, 21
nontrivial
171

solution,

20,

trivial solution, 20
homogeneous system, 24
Hooke’s law, 369
Householder matrices, 438
hyperbola, 483
hyperplanes, 3, 566
hypotenuse, 602
hypothesis, 601

idempotents, 79, 526
identity matrix, 52, 56, 122
identity operator, 372
identity transformation, 60,

115

image

of linear transformations,

59, 377, 378

of the parallelogram, 252

image space, 262, 439
imaginary axis, 590
imaginary parts, 399, 587
imaginary unit, 587
implication, 601
implies, 86
inconsistent system, 1
independence, 269, 343
independence test, 269
independent, 269, 343, 352
independent lemma, 352
indeterminate, 330, 613
index, 486
induction

cofactor expansion theo-

rem, 203
determinant

determination of, 144

mathematical
104, 608

induction,

on path of length r, 75
induction hypothesis, 608
inﬁnite dimensional, 353
initial condition, 197
initial state vector, 136
inner product

and norms, 533

transformations,

line, 312

leading variables, 13
least squares approximating

least squares approximating
polynomial, 315

least squares approximation,

INDEX

657

neous system, 24
spanning sets, 264, 339
trivial, 269, 343
unique, 269
vanishes, 269
vectors, 339

coordinate isomorphism,

linear transformation, 89,

391

deﬁned, 529
euclidean inner product,

393
matrix
89

529

Moore-Penrose

inverse,

positive deﬁnite n

trix, 531

n ma-

×

properties of, 531
inner product space
deﬁned, 529
distance, 533
dot product
use of, 532

Fourier
568

approximation,

isometries, 555
norms, 533
orthogonal
tion, 548

diagonaliza-

orthogonal sets of vectors,

538

unit vector, 533

input-output economic mod-

els, 127

input-output matrix, 129
integers, 465, 601
integers modulo, 466
integration, 373
interpolating
164

polynomial,

intersection, 268, 357
interval, 330
intrinsic descriptions, 209
invariance theorem, 274
invariant subspaces, 514
invariants, 174
inverse theorem, 87
inverses

adjugate, 158
and elementary matrices,

95

and linear systems, 82
and zero matrices, 80
cancellation laws, 84
complex number, 588
Cramer’s Rule, 156, 162
deﬁned, 79
determinants, 143, 156
elementary row opera-

tions, 7, 95
ﬁnite ﬁelds, 467
generalized inverse, 310,

319

inverse theorem, 87
inversion algorithm, 83

319

nonzero matrix, 80
properties of inverses, 84
square matrices

application to, 83, 156

inversion algorithm, 83
invertibility condition, 156
invertible matrix

“mixed” cancellation, 84
deﬁned, 79
determinants, 143
left cancelled, 84
LU-factorization, 124
orthogonal matrices, 433
product of elementary ma-

trix, 96

right cancelled, 84

involutions, 523
irreducible, 596
isometries, 249, 555, 556
isomorphic, 388
isomorphism, 388, 500, 557

Jacobi identity, 247
Jordan blocks, 582
Jordan canonical form, 581
Jordan canonical matrices,

573

Jordan, Camille, 583
junction rule, 27, 29
juvenile survival rate, 169

kernel, 378
kernel lemma, 400
Kirchhoff’s Laws, 29

Lagrange identity, 243
Lagrange interpolation ex-
pansion, 363, 540
Lagrange polynomials, 363,

540

Lagrange, Joseph Louis, 243
Lancaster, P., 129
Laplace, Pierre Simon de,

146

law of cosines, 225, 538
law of exponents, 592
law of sines, 247
leading 1, 10
leading coefﬁcient, 330, 613
leading column, 119

311

linear discrete dynamical

least squares best approxi-

system, 170

mation, 317

linear

dynamical

system,

left cancelled invertible ma-

trix, 84

Legendre polynomials, 542
Legendre, A.M., 542
Leibnitz, 143
lemma, 95
length

geometric vector, 210
linear recurrence, 402
linear recurrence relation,

191

norm, where dot product

is used, 456

norm, where dot product

used, 533

path of length, 75
recurrence, 402
vector, 207, 280, 456
Leontief, Wassily, 127
line

ﬁxed line, 560
in space, 216
least squares approximat-

ing line, 312

parametric equations of a

line, 217

perpendicular lines, 224
point-slope formula, 219
shortest distance between
nonparallel lines, 236

straight, pair of, 483
through the origin, 262
vector equation of a line,

216

linear codes, 472
linear combinations

and

transforma-

linear
tions, 103
deﬁned, 21, 103
eigenvectors, 436
homogeneous equations,

21, 62

of columns of coefﬁcient

matrix, 49

170, 180
linear equation

conic graph, 21
constant term, 1
Cramer’s Rule, 161
deﬁned, 1
vs. linear inequalities, 17

linear independence

dependent, 271, 343, 356
geometric
description,
271

independent, 269, 343,

352

orthogonal sets, 285, 539
properties, 345
set of vectors, 269, 343
vector spaces, 343
linear inequalities, 17
linear operator

B-matrix, 505
change matrix, 505
choice of basis, 510
deﬁned, 248, 371, 504
diagonalizable, 549
distance preserving, 248
distance

preserving

isometries, 555
eigenvalues, 518
eigenvector, 518
idempotents, 526
involutions, 523
isometries, 249, 556
on ﬁnite dimensional in-
ner product space, 548
projection, 249, 415, 555
properties
of matrices,
508

reducible, 523
reﬂections, 249
restriction, 516
rotations, 251
standard matrix, 505
symmetric, 551
transformations of areas

of orthogonal basis, 286
of solutions to homoge-

and volumes, 252

linear programming, 492

658

INDEX

linear

recurrence relation,

103

diagonalizable

matrix,

191, 402
linear recurrences

diagonalization, 192
length, 191, 402
linear
401

transformations,

polynomials

associated
with the linear recur-
rence, 404

shift operator, 405
vector spaces, 401

linear system of differential

equations, 197

linear transformations

n matrix, 495

m
action of a transformation,

×

495

as category, 393
as matrix transformation,

495

association with matrix,

495

composite, 64, 107, 392
composition, 392, 499
coordinate
tion, 496

transforma-

deﬁned, 59, 103, 371, 375
described, 35, 103
differentiation, 373
dimension theorem, 382
distance preserving, 248
equal, 59, 374
evaluation, 372, 406
examples, 371
fundamental

identities,

394
hit, 380
identity operator, 372
image, 59, 378
in computer graphics, 255
integration, 373
inverses, 89, 393
isomorphism, 388
kernel, 378
linear recurrences, 401
matrix of T corresponding
to the ordered bases B
and D, 497

matrix of a linear transfor-

mation, 371, 495

matrix transformation in-

duced, 59, 248

matrix transformations

nullity, 379
nullspace, 378
of areas, 252
of volume, 252
one-to-one

transforma-

tions, 380

onto transformations, 380
projections, 113, 249
properties, 373
range, 378
rank, 379
reﬂections, 112, 249
rotations, 110, 251
scalar multiple law, 109
scalar operator, 372
zero transformation, 60,

372

linearly dependent, 271, 343,

356

linearly independent, 269,

343, 352

logically equivalent, 604
lower reduced, 118
lower triangular matrix, 117,

152

LU-algorithm, 120, 124
LU-factorization, 119

magnitude, 209
main diagonal, 43, 117
Markov chains, 133
Markov, Andrei Andreye-

vich, 133
mathematical
104, 608

induction,

mathematical statistics, 320
matrices, 35
matrix, 35

(i, j)-entry, 35
adjacency matrix, 74
augmented matrix, 3, 4,

13, 14

block matrix, 73
change matrix, 505
coefﬁcient matrix, 4, 161
column matrix, 35
companion matrix, 155
congruent matrices, 485
conjugate matrix, 305
constant matrix, 4
covariance matrix, 493
deﬁned, 3, 35
demand matrix, 130
diagonal matrices, 45, 79,

another perspective on,

170, 176, 508

176

diagonalizing matrix, 176
doubly stochastic matrix,

141

elementary matrix, 94
elementary row opera-

tions, 5

entries of the matrix, 35,

36

equal matrices, 36
equivalent matrices, 102
Gram, 440
hermitian matrix, 457
Householder

matrices,

438

identity matrix, 52, 56
input-output matrix, 129
invertible matrix, 79
linear transformation

association with, 495

lower

triangular matrix,

117, 152
migration, 181
Moore-Penrose

450

inverse,

nilpotent matrix, 188
orthogonal matrix, 158,

418

orthonormal matrix, 418
over ﬁnite ﬁeld, 475
parity-check

matrices,

475

partitioned into blocks, 73
permutation matrix, 122,

426, 513

polar decomposition, 448
positive, 448
positive semi-deﬁnite, 448
projection matrix, 63, 426
pseudoinverse, 450
rank, 15, 289
reduced row-echelon ma-

trix, 10, 99

regular stochastic matrix,

138

row matrix, 35
row-echelon matrix, 10,

288

row-equivalent, 102
shapes, 35
similar matrices, 296
singular matrix, 441
spectrum, 420
standard generator, 474

standard matrix, 505
stochastic matrices, 129,

135

submatrix, 296
subtracting, 38
systemic generator, 474
transition matrix, 134
transpose, 41
triangular matrices, 117,

152

unitary matrix, 459
upper

triangular matrix,

117, 152, 460

Vandermonde matrix, 151
zero matrix, 38
zeros, creating in matrix,

147

matrix addition, 37
matrix algebra

dot product, 54
elementary matrix, 94
input-output
models
application to, 127

economic

inverses, 79
LU-factorization, 120
Markov chains

application to, 133
matrices as entities, 39
matrix addition, 37
matrix multiplication, 64
matrix subtraction, 38
matrix-vector multiplica-

tion, 49, 64

numerical division, 79
scalar multiplication, 39
size of matrices, 35
transformations, 57
transpose of a matrix, 41
usefulness of, 35

matrix form

deﬁned, 50
reduced

row-echelon

form, 10, 11, 99

row-echelon form, 10, 11
upper Hessenberg form,

438

matrix generators, 473
matrix inversion algorithm,

83, 97

matrix multiplication

and composition of trans-

formations, 64
associative law, 70
block, 73

commute, 69, 71
compatibility rule, 67
deﬁnition, 65
directed graphs, 74
distributive laws, 71
dot product rule, 67
left-multiplication, 86
matrix of composite of
two linear transforma-
tions, 107

matrix products, 65
non-commutative, 86
order of the factors, 71
results of, 67
right-multiplication, 86
matrix of T corresponding to
the ordered bases B and
D, 497

matrix of a linear transfor-

mation, 495
matrix recurrence, 170
matrix theory, 35
matrix transformation in-
duced, 59, 103, 248

matrix transformations, 89
matrix-vector products, 49
mean

”average” of the sample

values, 320
calculation, 492
sample mean, 320

median

tetrahedron, 223
triangle, 223
messages, 473
metric, 393
midpoint, 214
migration matrix, 181
minimum distance, 470
modular arithmetic, 465
modulo, 465
modulus, 465, 589
Moore-Penrose inverse, 319,

450
morphisms, 393
multiplication

block multiplication, 73
compatible, 67, 73
matrix multiplication, 64
matrix-vector multiplica-

tion, 48

matrix-vector products, 49
scalar multiplication, 39,

328, 330

multiplicity, 178, 302, 405,

615

orthogonal matrix, 158, 418
orthogonal projection, 414,

multivariate analysis, 493

543

orthogonal set of vectors,

nearest neighbour decoding,

459, 538

470
negative

correlation, 321
of m
vector, 47, 328

×

n matrix, 38

negative x, 47
negative x-shear, 61
network ﬂow, 27
Newton, Sir Isaac, 11
Nicholson, W. Keith, 467
nilpotent, 188, 582
noise, 465
nonleading variable, 20
nonlinear recurrences, 194
nontrivial solution, 20, 171
nonzero scalar multiple of a
basic solution, 24
nonzero vectors, 215, 281
norm, 456, 533
normal, 231, 461
normal equations, 309
normalizing the orthogonal

set, 284, 539

null space, 262
nullity, 379
nullspace, 378

objective function, 489, 491
objects, 393
odd function, 570
odd polynomial, 352
Ohm’s Law, 29
one-to-one transformations,

380

onto transformations, 380
open model of the economy,

130

open sector, 130
ordered n-tuple, 47, 273
ordered basis, 495, 497
origin, 207
orthocentre, 259
orthogonal basis, 409, 541
orthogonal codes, 476
orthogonal

complement,

412, 476, 542

orthogonal diagonalization,

418, 548

orthogonal hermitian matrix,

458

orthogonal sets, 283, 409
orthogonal

vectors,

227,

283, 459, 538

orthogonality

complex matrices, 454
constrained optimization,

489

dot product, 280
eigenvalues, computation

of, 435

expansion theorem, 540
ﬁnite ﬁelds, 467
Fourier expansion, 286
orthogo-
Gram-Schmidt
nalization
algorithm,
411, 420, 421, 432,
512, 541

normalizing the orthogo-

nal set, 284

complement,

orthogonal codes, 476
orthogonal
412, 476
orthogonal
tion, 418
orthogonal
414

diagonaliza-

projection,

orthogonal sets, 283, 409
orthogonally similar, 425
positive deﬁnite matrix,

427

principal axes

theorem,

419

projection theorem, 309
Pythagoras’ theorem, 284
QR-algorithm, 437
QR-factorization, 431
quadratic forms, 423, 479
real spectral theorem, 420
statistical principal com-
ponent analysis, 492

triangulation

theorem,

424

orthogonally diagonalizable,

419

orthogonally similar, 425
orthonormal basis, 542, 550
orthonormal matrix, 418
orthonormal set, 459, 538
orthonormal vector, 283

multiplication rule, 593

orthogonal lemma, 409, 541

PageRank, 187

INDEX

659

paired samples, 322
parabola, 483
parallel, 215
parallelepiped, 244, 252
parallelogram

area equal to zero, 244
deﬁned, 109, 210
determined by geometric

vectors, 210

image, 252
law, 109, 210, 591
rhombus, 228
parameters, 2, 13
parametric equations of a

line, 217
parametric form, 2
parity digits, 474
parity-check code, 472
parity-check matrices, 475
Parseval’s formula, 547
particle physics, 491
partitioned into blocks, 73
path of length, 75
Peano, Guiseppe, 327
Pearson correlation coefﬁ-

cient, 322

perfect code, 471
period, 369
permutation matrix,

426, 513

122,

perpendicular lines, 224
physical dynamics, 462
pigeonhole principle, 604
Pisano, Leonardo, 193
planes, 231, 262
Pless, V., 476
PLU-factorization, 123
point-slope formula, 219
pointwise addition, 330, 331
polar decomposition, 448
polar form, 591
polynomials

as matrix entries and de-

terminants, 150

associated with the linear

recurrence, 404

coefﬁcients, 317, 330, 613
companion matrix, 155
complex roots, 175, 458,

616

constant, 613
deﬁned, 329, 613
degree of the polynomial,

330, 613

distinct degrees, 344

660

INDEX

division algorithm, 615
equal, 330, 613
evaluation, 180, 338
even, 352
factor theorem, 615
form, 613
indeterminate, 613
interpolating the polyno-

mial, 163

Lagrange

polynomials,

363, 540

leading coefﬁcient, 330,

613

least squares approximat-
ing polynomial, 315

Legendre
542

polynomials,

nonconstant

polynomial
with complex coefﬁ-
cients, 304

odd, 352
remainder theorem, 614
root, 171, 435, 587
root of characteristic poly-

nomial, 171, 366
Taylor’s theorem, 361
vector spaces, 329, 360
with no root, 615, 616
zero polynomial, 613

position vector, 211
positive x-shear, 61
positive correlation, 321
positive deﬁnite, 427, 489
positive deﬁnite matrix, 427,

531

positive matrix, 448
positive semi-deﬁnite ma-

trix, 448

positive semideﬁnite, 493
power method, 435
power sequences, 401
practical problems, 1
preimage, 377
prime, 467, 604
principal argument, 591
principal axes, 420, 481
principal axes theorem, 419,
458, 461, 490, 552
principal components, 493
principal submatrices, 428
probabilities, 133
probability law, 492
probability theory, 493
product

determinant of product of

matrices, 156
dot product, 224
matrix products, 65
matrix-vector products, 49
scalar product, 224
standard inner product,

455

theorem, 156, 166

product rule, 400
projection

linear operator, 555
linear operators, 249
orthogonal
414, 543

projection,

projection matrix, 63, 417,

426

projection on U with kernel

W , 543

projection theorem,

309,

414, 543

projections, 113, 228, 412
proof

by contradiction, 603
deﬁned, 601
direct proof, 601
formal proofs, 602
reduction to cases, 603
proper subspace, 261, 277
pseudoinverse, 450
pure

imaginary numbers,

587

Pythagoras, 219, 602
Pythagoras’ theorem, 219,
226, 284, 539, 602

QR-algorithm, 437
QR-factorization, 431
quadratic equation, 489
quadratic form, 423, 479,

532

quadratic formula, 596
quotient, 465

radian measure, 109, 591
random variable, 492
range, 378
rank

Rayleigh quotients, 436
real axis, 590
real Jordan canonical form,

582

real numbers, 1, 47, 328,
330, 454, 458, 467

real parts, 399, 587
real quadratic, 596
real spectral theorem, 420
recurrence, 191
recursive algorithm, 11
recursive sequence, 191
reduced row-echelon form,

10, 11, 99

reduced row-echelon matrix,

10, 11
reducible, 523
reduction to cases, 603
reﬂections

about a line through the

origin, 158

ﬁxed hyperplane, 566
ﬁxed line, 560
ﬁxed plane, 563
isometries, 559
linear operators, 249
linear
112

transformations,

regular representation, 514
regular

stochastic matrix,

138
remainder, 465
remainder theorem, 361, 614
repellor, 185
reproduction rate, 169
restriction, 516
reversed, 7
rhombus, 228
right

cancelled

invertible

matrix, 84

right-hand coordinate sys-

tems, 245
right-hand rule, 245
root

and orthogonal matrices,
158
axis, 563
describing rotations, 110
ﬁxed axis, 566
isometries, 559
linear operators, 251
linear
110

transformations,

round-off error, 172
row matrix, 35
row space, 288
row-echelon form, 10, 11
row-echelon matrix, 10, 11
row-equivalent matrices, 102
rows

(i, j)-entry, 35
as notations for ordered n-

tuples, 273
convention, 36
elementary row opera-

tions, 5
leading 1, 10
shape of matrix, 35
Smith normal form, 98
zero rows, 10

saddle point, 186
same action, 59, 331, 374
sample

analysis of, 320
comparison of two sam-

ples, 321
deﬁned, 320
paired samples, 322

sample correlation coefﬁ-

cient, 322
sample mean, 320
sample standard deviation,

321

sample variance, 321
sample vector, 320
satisfy the relation, 402
scalar, 39, 328, 467
scalar equation of a plane,

of characteristic polyno-

231

mial, 171, 366

of polynomials, 338, 435,

scalar matrix, 142
scalar multiple law, 109,

linear

transformation,

587

379, 500

matrix, 15, 289, 379
quadratic form, 486
similarity invariant, 510
symmetric matrix, 486
theorem, 289

of the quadratic, 596

roots of unity, 594
rotation, 566
rotations

about a line through the

origin, 511
about the origin

213, 215

scalar multiples, 21, 39, 109
scalar multiplication

axioms, 328
basic properties, 332
closed under, 47, 328
closed under scalar multi-

plication, 261

complex number, 590

rational numbers, 328

described, 39
distributive laws, 41
geometric vectors, 212
geometrical

description,

109

of functions, 331
transformations

preserving scalar multi-
plication, 103

vectors, 328
scalar operator, 372
scalar product
deﬁned, 224
elementary row opera-

tions, 21

geometric vectors, 212

scatter diagram, 321
Schmidt, Erhardt, 411
Schur’s theorem, 460
Schur, Issai, 460
second-order

differential

equation, 365, 398

Seneta, E., 129
sequence

Fibonacci, 193

sequences

constant sequences, 401
equal, 401
Fibonacci, 193
linear recurrences, 190
notation, 401
of column vectors, 170
ordered sequence of real

numbers, 47

power sequences, 401
recursive sequence, 191
satisfy the relation, 402

set, 261
set notation, 262
set of all ordered n-tuples

(Rn)

n-tuples, 288
as inner product space,

529

closed under addition and
scalar multiplication,
47

complex eigenvalues, 304
dimension, 274
dot product, 280, 455
expansion theorem, 285
linear independence, 269
linear operators, 248
notation, 47
orthogonal sets, 283

projection on, 415
rank of a matrix, 289
rules of matrix arithmetic,

328

similar matrices, 296
spanning sets, 264
special types of matrices,

328

standard basis, 104
subspaces, 261, 329
symmetric matrix, 305

Shannon, Claude, 469
shift operator, 405
shifting, 438
sign, 144
similar matrices, 296
similarity invariant, 510
simple harmonic motions,

369

simplex algorithm, 17, 492
sine, 110, 591
single vector equation, 48
singular matrix, 441
singular value decomposi-

tion, 439, 444
singular values, 441
size m
n matrix, 35
skew-hermitian, 464
skew-symmetric, 46, 520,

×

554

Smith normal form, 98
solution

algebraic method, 4, 9
basic solutions, 24, 447
best

approximation to,

308

consistent system, 1, 16
general solution, 2, 14
geometric description, 3
in parametric form, 2
inconsistent system, 1
nontrivial
171

solution,

20,

solution to a system, 1, 14
to linear equation, 1
trivial solution, 20
solution to a system, 1
span, 264, 339
spanning sets, 264, 339
spectral theorem, 461
spectrum, 420
sphere, 491
spring constant, 369
square matrix (n

n matrix)
characteristic polynomial,

×

171, 462

cofactor matrix, 158
deﬁned, 35
determinants, 81, 143, 157
diagonal matrices, 79, 170
diagonalizable
matrix,
176, 508

diagonalizing matrix, 176
elementary matrix, 94
hermitian matrix, 457
idempotent, 79
identity matrix, 52, 56
invariants, 174
lower
152

triangular matrix,

matrix of an operator, 508
nilpotent matrix, 188
orthogonal matrix, 158
positive deﬁnite matrix,

427, 531

regular representation of
complex numbers, 514

scalar matrix, 142
similarity invariant, 510
skew-symmetric, 46
square, 78
trace, 297
triangular matrix, 152
unitary matrix, 459
upper
152

triangular matrix,

staircase form, 10
standard basis, 104, 266,
270, 274, 276, 347, 454

standard deviation, 493
standard generator, 474
standard inner product, 455
standard matrix, 505
standard position, 109, 591
state vectors, 136
statistical principal compo-
nent analysis, 492
steady-state vector, 139
stochastic matrices, 129, 135
structure theorem, 561
submatrix, 296
subset, 261
subspace test, 337
subspaces

×

n matrix, 262

m
basis, 274
closed under addition, 261
closed under scalar multi-

plication, 261
column space, 439

INDEX

661

complex subspace, 463
deﬁned, 261, 336
dimension, 274
eigenspace, 301
fundamental, 445
fundamental theorem, 274
image, 377, 378, 439
intersection, 268, 357
invariance theorem, 274
invariant subspaces, 515
kernel, 378
planes and lines through

the origin, 262

projection, 414
proper subspace, 261
spanning sets, 264
subspace test, 337
sum, 268, 357
vector spaces, 336
zero subspace, 261, 337

subtraction

complex number, 587
matrix, 38
vector, 212

sum

algebraic sum, 29
complex number, 591
direct sum, 358, 520
elementary row opera-

tions, 21

geometric vectors, 211
geometrical

description,

109

matrices of the same size,

37

matrix addition, 37
of product of matrix en-

tries, 150

of scalar multiples, 21
of two vectors, 328
of vectors in two sub-

spaces, 520

subspaces, 268, 358
subspaces of
space, 358

a vector

variances of set of random

variables, 493
summation notation, 202
Sylvester’s Law of Inertia,

486

symmetric

bilinear

form,

489

symmetric form, 223, 479
symmetric linear operator,

551

662

INDEX

symmetric matrix

third-order differential equa-

Vandermonde determinant,

absolute value, 305
congruence, 485
deﬁned, 43
index, 486
orthogonal
419

eigenvectors,

positive deﬁnite, 427
rank and index, 486
real eigenvalues, 305

syndrome, 476
syndrome decoding, 476
system of linear equations
algebraic method, 4
associated homogeneous

system, 53

augmented matrix, 3
chemical reactions
application to, 31
coefﬁcient matrix, 4
consistent system, 1, 16
constant matrix, 4
deﬁned, 1
electrical networks
application to, 29

elementary operations, 5
equivalent systems, 4
gaussian elimination, 9,

14

general solution, 2
homogeneous equations,

20

inconsistent system, 1
inﬁnitely many solutions,

3

inverses and, 82
matrix multiplication, 69
network ﬂow application,

27

no solution, 3
nontrivial solution, 20
normal equations, 309
positive integers, 31
rank of a matrix, 16
solutions, 1
trivial solution, 20
unique solution, 3
with m

×
trix, 49

n coefﬁcient ma-

systematic generator, 474

tail, 210
Taylor’s theorem, 361, 507
tetrahedron, 223
theorems, 605
theory of Hilbert spaces, 411

tion, 365, 398
time, functions of, 170
tip, 210
tip-to-tail rule, 211
total variance, 493
trace, 78, 297, 372, 510
trajectory, 185
transformations

action, 59, 495
composite, 64
deﬁning, 59
described, 59
equal, 59
identity
60

transformation,

matrix transformation, 59
zero transformation, 60
transition matrix, 134, 135
transition probabilities, 133,

135

translation, 61, 376, 556
transpose of a matrix, 41
transposition, 41, 372
triangle

altitude, 259
centroid, 223
hypotenuse, 602
inequality, 242, 282, 589
median, 223
orthocentre, 259

triangle inequality, 242, 535,

589

triangular matrices, 117, 152
triangulation algorithm, 576
triangulation theorem, 424
trigonometric functions, 110
linear combinations,
trivial

269, 343
trivial solution, 20

uncorrelated, 493
unit ball, 491, 533
unit circle, 109, 533, 591
unit cube, 253
unit square, 253
unit triangular, 127
unit vector, 213, 280, 456,

533

unitarily diagonalizable, 460
unitary diagonalization, 460
unitary matrix, 459
upper Hessenberg form, 438
upper triangular matrix, 117,

165

Vandermonde matrix, 151
variance, 320, 493
variance formula, 324
vector addition, 328, 591
vector equation of a line, 216
vector equation of a plane,

232

vector geometry

angle between two vec-

tors, 226

computer graphics, 255
cross product, 234
deﬁned, 207
direction vector, 216
perpendicular
line

to

plane, 224

linear operators, 248
lines in space, 216
planes, 231
projections, 229
symmetric form, 223
vector equation of a line,

216

vector product, 234
vector quantities, 209
vector spaces

3-dimensional space, 207
abstract, 327
as category, 393
axioms, 328, 331, 333
basic properties, 327
basis, 347
cancellation, 331
continuous functions, 530
deﬁned, 328
differential equations, 365
dimension, 347
direct sum, 520
examples, 327
ﬁnite dimensional spaces,

352

inﬁnite dimensional, 353
introduction of concept,

327

isomorphic, 388
linear independence, 343
linear recurrences, 401
linear
371

transformations,

polynomials, 329, 360
scalar multiplication

basic properties of, 332

subspaces, 336, 358
theory of vector spaces,

331

zero vector space, 334

vectors

addition, 328
arrow representation, 58
column vectors, 170
complex matrices, 454
coordinate vectors, 213,

234, 253, 266, 496
deﬁned, 47, 328, 454
difference of, 332
direction of, 209
direction vector, 216
ﬁxed vectors, 567
initial state vector, 136
intrinsic descriptions, 209
length, 210, 280, 456, 533
matrix recurrence, 170
matrix-vector multiplica-

tion, 48

matrix-vector products, 49
negative, 47
nonzero, 215
orthogonal vectors, 227,

283, 459, 538

orthonormal vector, 283,

459

position vector, 211
sample vector, 320
scalar multiplication, 328
single vector equation, 48
state vector, 134
steady-state vector, 139
subtracted, 332
sum of two vectors, 328
unit vector, 213, 280, 456
zero n-vector, 47
zero vector, 261, 328

velocity, 209
vertices, 74
vibrations, 491
volume

linear transformations of,

252

of parallelepiped,

245,

253

Weyl, Hermann, 327
whole number, 601
Wilf, Herbert S., 187
words, 469
wronskian, 370

152, 460

spanning sets, 339

zero n-vector, 47

zero matrix

described, 38
no inverse, 80

scalar multiplication, 40

zero polynomial, 330
zero rows, 10

zero subspace, 261, 337
zero transformation, 60, 116,

zero vector, 261, 328
zero vector space, 334

372

INDEX

663

www.lyryx.com

